[{"title": "Ludicrous Speed Linear Mixed Models for Genome-Wide Association Studies", "url": "https://www.biorxiv.org/content/early/2018/01/03/154682", "abstract": "We have developed Ludicrous Speed Linear Mixed Models, a version of FaST-LMM optimized for the cloud. The approach can perform a genome-wide association analysis on a dataset of one million SNPs across one million individuals at a cost of about 868 CPU days with an elapsed time on the order of two weeks.", "tag": "Bioinformatics"}, {"title": "The GCTx format and cmap{Py, R, M} packages: resources for the optimized storage and integrated traversal of dense matrices of data and annotations", "url": "https://www.biorxiv.org/content/early/2018/01/03/227041", "abstract": "Motivation: Computational analysis of datasets generated by treating cells with pharmacological and genetic perturbagens has proven useful for the discovery of functional relationships. Facilitated by technological improvements, perturbational datasets have grown in recent years to include millions of experiments. While initial studies, such as our work on Connectivity Map, used gene expression readouts, recent studies from the NIH LINCS consortium have expanded to a more diverse set of molecular readouts, including proteomic and cell morphological signatures. Sharing these diverse data creates many opportunities for research and discovery, but the unprecedented size of data generated and the complex metadata associated with experiments have also created fundamental technical challenges regarding data storage and cross-assay integration. Results: We present the GCTx file format and a suite of open-source packages for the efficient storage, serialization, and analysis of dense two-dimensional matrices. The utility of this format is not just theoretical; we have extensively used the format in the Connectivity Map to assemble and share massive data sets comprising 1.7 million experiments. We anticipate that the generalizability of the GCTx format, paired with code libraries that we provide, will stimulate wider adoption and lower barriers for integrated cross-assay analysis and algorithm development. Availability: Software packages (available in Matlab, Python, and R) are freely available at https://github.com/cmap", "tag": "Bioinformatics"}, {"title": "DeepSimulator: a deep simulator for Nanopore sequencing", "url": "https://www.biorxiv.org/content/early/2018/01/03/238683", "abstract": "Motivation: Oxford Nanopore sequencing is a rapidly developed sequencing technology in recent years. To keep pace with the explosion of the downstream data analytical tools, a versatile Nanopore sequencing simulator is needed to complement the experimental data as well as to benchmark those newly developed tools. However, all the currently available simulators are based on simple statistics of the produced reads, which have difficulty in capturing the complex nature of the Nanopore sequencing procedure, the main task of which is the generation of raw electrical current signals. Results: Here we propose a deep learning based simulator, DeepSimulator, to mimic the entire pipeline of Nanopore sequencing. Starting from a given reference genome or assembled contigs, we simulate the electrical current signals by a context-dependent deep learning model, followed by a base-calling procedure to yield simulated reads. This workflow mimics the sequencing procedure more naturally. The thorough experiments performed across four species show that the signals generated by our context-dependent model are more similar to the experimentally obtained signals than the ones generated by the official context-independent pore model. In terms of the simulated reads, we provide a parameter interface to users so that they can obtain the reads with different accuracies ranging from 83% to 97%. The reads generated by the default parameter have almost the same properties as the real data. Two case studies demonstrate the application of DeepSimulator to benefit the development of tools in de novo assembly and in low coverage SNP detection. Availability: The software can be accessed freely at: https://github.com/lykaust15/DeepSimulator.", "tag": "Bioinformatics"}, {"title": "A Simulation Analysis and Screening of Deleterious Non-Synonymous Single Nucleotide Polymorphisms (SNPs) in Human CDKN1A Gene", "url": "https://www.biorxiv.org/content/early/2018/01/03/240820", "abstract": "CDKN1A also known as p21CIP1 /p21WAF1, a cyclin dependent kinase 1, interacts with proliferating cell nuclear antigen (PCNA) resulting in cell cycle inhibition in human. Polymorphism in the CDKN1A gene is related to the onset of several cancers and Alzheimer\u2032s disease. Non-synonymous single nucleotide polymorphisms (nsSNPs), which reside in the coding region of a gene, might cause loss of function in the corresponding protein. In silico analysis used in this study exerts many different algorithms such as SIFT, Polyphen-2, Predict SNP, I-Mutant 3.0 and Mupro etc. to screen out the most deleterious nsSNPs and their effect on the corresponding protein. Following the screening of 118 nsSNPs, finally, 12 missense SNPs (R19C (C\u2192T), G23D (A\u2192G), V25G (G\u2192T), V25L (C\u2192G), Q29P (A\u2192C\u2192G), F51L (C\u2192T), E56K (A\u2192G), T57I (C\u2192T), G61R (C\u2192G), G61D (A\u2192G), Y151C (A\u2192G) and R156W (C\u2192G\u2192T) were predicted to have deleterious effect by all the algorithms. Of them, R19C, G23D, F51L, Y151C and R156W occurred at the highly conserved site. G23D, F51L variants also occurred at the CDI domain. Homology structures of the protein predicted decrease of energy in mutant models. GV-GD scores predicted only two variants as neutral (V25L, F51L).", "tag": "Bioinformatics"}, {"title": "DAtest: a framework for choosing differential abundance or expression method", "url": "https://www.biorxiv.org/content/early/2018/01/02/241802", "abstract": "DAtest is an R package for directly comparing different statistical methods for differential abundance and expression analysis on a dataset of interest; be it data from RNA-seq, proteomics, metabolomics or a microbial marker-gene survey. A myriad of statistical methods exists for conducting these analyses, and with this tool we give the analyst an empirical foundation for choosing a method suitable for a specific dataset. The package supports categorical and quantitative variables, paired/block experimental designs, and the inclusion of covariates. It is freely available at GitHub: https://github.com/Russel88/DAtest along with detailed instructions.", "tag": "Bioinformatics"}, {"title": "Benchmark of lncRNA Quantification for RNA-Seq of Cancer Samples", "url": "https://www.biorxiv.org/content/early/2018/01/02/241869", "abstract": "Long non-coding RNAs (lncRNAs) emerge as important regulators of various biological processes. Many lncRNAs with tumor-suppressor or oncogenic functions in cancer have been discovered. While many studies have exploited public resources such as RNA-Seq data in The Cancer Genome Atlas (TCGA) to study lncRNAs in cancer, it is crucial to choose the optimal method for accurate expression quantification of lncRNAs. In this benchmarking study, we compared the performance of pseudoalignment methods Kallisto and Salmon, and alignment-based methods HTSeq, featureCounts, and RSEM, in lncRNA quantification, by applying them to a simulated RNA-Seq dataset and a pan-cancer RNA-Seq dataset from TCGA. We observed that full transcriptome annotation, including both protein coding and noncoding RNAs, greatly improves the specificity of lncRNA expression quantification. Pseudoalignment-based methods detect more lncRNAs than alignment-based methods and correlate highly with simulated ground truth. On the contrary, alignment-based methods tend to underestimate lncRNA expression or even fail to capture lncRNA signal in the ground truth. These underestimated genes include cancer-relevant lncRNAs such as TERC and ZEB2-AS1. Overall, 10-16% of lncRNAs can be detected in the samples, with antisense and lincRNAs the two most abundant categories. A higher proportion of antisense RNAs are detected than lincRNAs. Moreover, among the expressed lncRNAs, more antisense RNAs are discordant from ground truth than lincRNAs when measured by alignment-based methods, indicating that antisense RNAs are more susceptible to mis-quantification. In addition, the lncRNAs with fewer transcripts, less than three exons, and lower sequence uniqueness tend to be more discordant. In summary, pseudoalignment methods Kallisto or Salmon in combination with the full transcriptome annotation is our recommended strategy for RNA-Seq analysis for lncRNAs.", "tag": "Bioinformatics"}, {"title": "Finding de novo methylated DNA motifs", "url": "https://www.biorxiv.org/content/early/2018/01/02/043810", "abstract": "There is increasing evidence that posttranslational modifications (PTMs) such as methylation and hydroxymethylation on cytosine would greatly impact the binding of transcription factors (TFs). We modified a motif finding method Epigram to discover methylated motifs and other motifs containing other PTMs. When applied to TF ChIP-seq and DNA methylome data in H1 and GM12878, our method successfully identified novel methylated motifs that can be recognized by the TFs or their co-factors. We also observed spacing constraint between the canonical motif of the TF of interest and the newly discovered methylated motifs, which suggests operative recognition of these cis-elements by collaborative proteins.", "tag": "Bioinformatics"}, {"title": "NanoPack: visualizing and processing long read sequencing data", "url": "https://www.biorxiv.org/content/early/2018/01/02/237180", "abstract": "Summary: Here we describe NanoPack, a set of tools developed for visualization and processing of long read sequencing data from Oxford Nanopore Technologies and Pacific Biosciences. Availability and Implementation: The NanoPack tools are written in Python3 and released under the GNU GPL3.0 Licence. The source code can be found at https://github.com/wdecoster/nanopack, together with links to separate scripts and their documentation. The scripts are compatible with Linux, Mac OS and the MS Windows 10 subsystem for linux and are available as a graphical user interface, web service at http://nanoplot.bioinf.be and command line tools.", "tag": "Bioinformatics"}, {"title": "DECODE-ing sparsity patterns in single-cell RNA-seq", "url": "https://www.biorxiv.org/content/early/2018/01/01/241646", "abstract": "An inherent challenge of interpreting single-cell transcriptomic profiles is the high frequency of zero values. This phenomenon has been attributed to both biological and technical sources, although the extent of the contribution of each remains unclear. Here, we show that the underlying gene presence/absence sparsity patterns are themselves highly information-rich. We develop an algorithm, called DECODE, to assess the extent of joint presence/absence of genes across different cells. We show that this network captures biologically-meaningful pathways, cell-type specific modules, and connectivity patterns characteristic of complex networks. We develop a model that uses this network to discriminate biological vs. technical zeros, by exploiting each gene's local neighborhood. For non-biological zeros, we build a predictive model to impute the missing value using their most informative neighbors. In three diverse datasets, we show that our framework accurately infers gene-gene functional dependencies, pinpoints technical zeros, and biologically-meaningful missing values.", "tag": "Bioinformatics"}, {"title": "A deep convolutional neural network approach for astrocyte detection", "url": "https://www.biorxiv.org/content/early/2017/12/31/241505", "abstract": "Astrocytes are involved in brain pathologies such as trauma or stroke, neurodegenerative disorders like Alzheimer's and Parkinson's disease, chronic pain, and many others. Determining cell density and timing of morphological and biochemical changes is important for a proper understanding of the role of astrocytes in physiological and pathological conditions. One of the most important of such analyses is astrocytes count within a complex tissue environment in microscopy images. The most widely used approaches for the quantification of microscopy images data are either manual stereological cell counting or semi-automatic segmentation techniques. Detecting astrocytes automatically is a highly challenging computational task, for which we currently lack efficient image analysis tools. In this study, we developed a fast and fully automated software that assesses the number of astrocytes using Deep Convolutional Neural Networks (DCNN). The method highly outperforms state-of-the-art image analysis and machine learning methods and provides detection accuracy and precision comparable to that of human experts. Additionally, the runtime of cell detection is significantly less than other three analyzed computational methods, and it is faster than human observers by orders of magnitude. We applied DCNN-based method to examine the number of astrocytes in different brain regions of rats with opioid-induced hyperalgesia/tolerance (OIH/OIT) as morphine tolerance is believed to activate glial cells in the brain. We observed strong positive correlation between manual cell detection and DCNN-based analysis method for counting astrocytes in the brains of experimental animals.", "tag": "Bioinformatics"}, {"title": "Comparison of computational methods for imputing single-cell RNA-sequencing data", "url": "https://www.biorxiv.org/content/early/2017/12/31/241190", "abstract": "Single-cell RNA-sequencing (scRNA-seq) is a recent breakthrough technology, which paves the way for measuring RNA levels at single cell resolution to study precise biological functions. One of the main challenges when analyzing scRNA-seq data is the presence of zeros or dropout events, which may mislead downstream analyses. To compensate the dropout effect, several methods have been developed to impute gene expression since the first Bayesian-based method being proposed in 2016. However, these methods have shown very diverse characteristics in terms of model hypothesis and imputation performance. Thus, large-scale comparison and evaluation of these methods is urgently needed now. To this end, we compared eight imputation methods, evaluated their power in recovering original real data, and performed broad analyses to explore their effects on clustering cell types, detecting differentially expressed genes, and reconstructing lineage trajectories in the context of both simulated and real data. Simulated datasets and case studies highlight that there are no one method performs the best in all the situations. Some defects of these methods such as scalability, robustness and unavailability in some situations need to be addressed in future studies.", "tag": "Bioinformatics"}, {"title": "AFCMEasyModel: An Easy Interface for Modeling Competing Endogenous RNA Networks using ODEs", "url": "https://www.biorxiv.org/content/early/2017/12/31/241026", "abstract": "Competing endogenous RNA networks have been considered to be important regulators of genetic data expression. Circular RNAs and microRNAs interact to form a circular sponge that have been shown to regulate messenger RNAs and hence regulating gene expression. The kinetics by which these non-coding RNAs interact together affecting gene expression are crucial to understand the mechanism of their regulatory function. Herein, we developed AFCMEasyModel as a user-friendly shiny app that enables users to modify regulation parameters of a competing endogenous RNA network based on interaction between circular RNAs and microRNAs in the simulation environment to form a sponge complex. The App provides the source-code for more customized models and allow users to download simulation plots for supplementation of their publications.", "tag": "Bioinformatics"}, {"title": "DeepGS: Predicting phenotypes from genotypes using Deep Learning", "url": "https://www.biorxiv.org/content/early/2017/12/31/241414", "abstract": "Motivation: Genomic selection (GS) is a new breeding strategy by which the phenotypes of quantitative traits are usually predicted based on genome-wide markers of genotypes using conventional statistical models. However, the GS prediction models typically make strong assumptions and perform linear regression analysis, limiting their accuracies since they do not capture the complex, non-linear relationships within genotypes, and between genotypes and phenotypes. Results: We present a deep learning method, named DeepGS, to predict phenotypes from geno-types. Using a deep convolutional neural network, DeepGS uses hidden variables that jointly repre-sent features in genotypic markers when making predictions; it also employs convolution, sampling and dropout strategies to reduce the complexity of high-dimensional marker data. We used a large GS dataset to train DeepGS and compare its performance with other methods. In terms of mean normalized discounted cumulative gain value, DeepGS achieves an increase of 27.70%~246.34% over a conventional neural network in selecting top-ranked 1% individuals with high phenotypic values for the eight tested traits. Additionally, compared with the widely used method RR-BLUP, DeepGS still yields a relative improvement ranging from 1.44% to 65.24%. Through extensive simulation experiments, we also demonstrated the effectiveness and robustness of DeepGS for the absent of outlier individuals and subsets of genotypic markers. Finally, we illustrated the complementarity of DeepGS and RR-BLUP with an ensemble learning approach for further improving prediction performance. Availability: DeepGS is provided as an open source R package available at https://github.com/cma2015/DeepGS.", "tag": "Bioinformatics"}, {"title": "MAGpy: a reproducible pipeline for the downstream analysis of metagenome-assembled genomes (MAGs)", "url": "https://www.biorxiv.org/content/early/2017/12/30/233544", "abstract": "Recent advances in bioinformatics have enabled the rapid assembly of genomes from metagenomes (MAGs), and there is a need for reproducible pipelines that can annotate and characterise thousands of genomes simultaneously. Here we present MAGpy, a Snakemake pipeline that takes FASTA input and compares MAGs to several public databases, checks quality, assigns a taxonomy and draws a phylogenetic tree.", "tag": "Bioinformatics"}, {"title": "Multi-modal meta-analysis of 1494 hepatocellular carcinoma samples reveals vast impacts of consensus driver genes on phenotypes", "url": "https://www.biorxiv.org/content/early/2017/12/30/166090", "abstract": "To characterize the phenotypic associations of driver genes in hepatocellular carcinoma (HCC), we identify 11 consensus driver genes across six HCC cohorts and 1,494 samples in total. The consensus driver genes include TP53, CTNNB1, ALB, AXIN1, RB1, ARID1A, RPS6KA3, ACVR2A, NFE2L2, CDKN2A and HNF1A. Integrative analysis of driver mutations, copy number variations and transcriptomic data reveals that these are associated with majority (63%) of the mRNA transcriptome, but only a small fraction (9%) of miRNAs. Genes associated with TP53, CTNNB1, ARID1A and HNF1A mutations contribute to four most densely connected clusters of biological pathways. Phenotypically, these driver genes are significantly associated with patients' overall survival. Some driver genes are significantly linked to HCC gender and age disparities. CTNNB1, ALB and TP53 have higher relative risks (RR=1.2, 1.1 and 1.1) in males. Oppositely, AXIN1, which encodes axin-1, a component of the beta-catenin (encoded by CTNNB1) destruction complex has higher RR (1.6) in females. RB1 mutations are more frequent in younger patients (RR=1.3). This study consolidates a group of consensus driver genes in HCC, which collectively show vast impacts on the phenotypes. These driver genes may warrant as valuable therapeutic targets of HCC.", "tag": "Bioinformatics"}, {"title": "TCIApathfinder: an R client for The Cancer Imaging Archive REST API", "url": "https://www.biorxiv.org/content/early/2017/12/30/240986", "abstract": "Summary: The Cancer Imaging Archive (TCIA) hosts publicly available de-identified medical images of cancer from over 25 body sites and over 30,000 patients. Over 400 published studies have utilized freely available TCIA images. Image series and metadata are available for download through a web interface or a REST API. We present TCIApathfinder, an R client for the TCIA REST API. TCIApathfinder wraps API access in user-friendly R functions that can be called within an R session or easily incorporated into scripts. Functions are provided to explore the contents of the large database and to download image files. Availability and implementation: TCIApathfinder is available under the MIT license as a package on CRAN (https://cran.r-project.org/web/packages/TCIApathfinder/index.html) and at https://github.com/pamelarussell/TCIApathfinder.", "tag": "Bioinformatics"}, {"title": "PEA: an integrated R toolkit for plant epitranscriptome analysis", "url": "https://www.biorxiv.org/content/early/2017/12/30/240887", "abstract": "Motivation: The epitranscriptome, also known as chemical modifications of RNA (CMRs), is a newly discovered layer of gene regulation, the biological importance of which emerged through analysis of only a small fraction of CMRs detected by high-throughput sequencing technologies. Understanding of the epitranscriptome is hampered by the absence of computational tools for the systematic analysis of epitranscriptome sequencing data. In addition, no tools have yet been designed for accurate prediction of CMRs in plants, or to extend epitranscriptome analysis from a fraction of the transcriptome to its entirety. Results: Here, we introduce PEA, an integrated R toolkit to facilitate the analysis of plant epitran-scriptome data. The PEA toolkit contains a comprehensive collection of functions required for read mapping, CMR calling, motif scanning and discovery, and gene functional enrichment analysis. PEA also takes advantage of machine learning technologies for transcriptome-scale CMR prediction, with high prediction accuracy, using the Positive Samples Only Learning algorithm, which addresses the two-class classification problem by using only positive samples (CMRs), in the absence of negative samples (non-CMRs). Hence PEA is a versatile epitranscriptome analysis pipeline covering CMR calling, prediction, and annotation, and we describe its application to predict N6-methyladenosine (m6A) modifications in Arabidopsis thaliana. Experimental results demonstrate that the toolkit achieved 71.6% sensitivity and 73.7% specificity, which is superior to existing m6A predictors. PEA is potentially broadly applicable to the in-depth study of epitranscriptomics. Availability: PEA is implemented using R and available at https://github.com/cma2015/PEA.", "tag": "Bioinformatics"}, {"title": "Discovering lncRNA Mediated Sponge Interactions in Breast Cancer Molecular Subtypes", "url": "https://www.biorxiv.org/content/early/2017/12/29/209015", "abstract": "Motivation: Long non-coding RNAs (lncRNAs) can indirectly regulate mRNAs expression levels by sequestering microRNAs (miRNAs), and act as competing endogenous RNAs (ceRNAs) or as sponges. Previous studies identified lncRNA-mediated sponge interactions in various cancers including the breast cancer. However, breast cancer subtypes are quite distinct in terms of their molecular profiles; therefore, ceRNAs are expected to be subtype-specific as well. Results: To find lncRNA-mediated ceRNA interactions in breast cancer subtypes, we develop an integrative approach. We conduct partial correlation analysis and kernel independence tests on patient gene expression profiles and further refine the candidate interactions with miRNA target information. We find that although there are sponges common to multiple subtypes, there are also distinct subtype-specific interactions. Functional enrichment of mRNAs that participate in these interactions highlights distinct biological processes for different subtypes. Interestingly, some of the ceRNAs also reside in close proximity in the genome; for example, those involving HOX genes, HOTAIR, miR-196a-1 and miR-196a-2. We also discover subtype-specific sponge interactions with high prognostic potential. For instance, when grouping is based on the expression patterns of specific sponge interactions, patients differ significantly in their survival distributions. If on the other hand, patients are grouped based on the individual RNA expression profiles of the sponge participants, they do not exhibit a significant difference in survival. These results can help shed light on subtype-specific mechanisms of breast cancer, and the methodology developed herein can help uncover sponges in other diseases.", "tag": "Bioinformatics"}, {"title": "NGSphy: phylogenomic simulation of next-generation sequencing data", "url": "https://www.biorxiv.org/content/early/2017/12/29/197715", "abstract": "Motivation: Advances in sequencing technologies have made it feasible to obtain massive datasets for phylogenomic inference, often consisting of large numbers of loci from multiple species and individuals. The phylogenomic analysis of next-generation sequencing (NGS) data implies a complex computational pipeline where multiple technical and methodological decisions are necessary that can influence the final tree obtained, like those related to coverage, assembly, mapping, variant calling and/or phasing. Results: To assess the influence of these variables we introduce NGSphy, an open-source tool for the simulation of Illumina reads/read counts obtained from haploid/diploid individual genomes with thousands of independent gene families evolving under a common species tree. In order to resemble real NGS experiments, NGSphy includes multiple options to model sequencing coverage (depth) heterogeneity across species, individuals and loci, including off-target or uncaptured loci. For comprehensive simulations covering multiple evolutionary scenarios, parameter values for the different replicates can be sampled from user-defined statistical distributions. Availability: Source code, full documentation and tutorials including a quick start guide are available at http://github.com/merlyescalona/ngsphy.", "tag": "Bioinformatics"}, {"title": "Deep learning reveals many more inter-protein residue-residue contacts than direct coupling analysis", "url": "https://www.biorxiv.org/content/early/2017/12/29/240754", "abstract": "Intra-protein residue-level contact prediction has drawn a lot of attentions in recent years and made very good progress, but much fewer methods are dedicated to inter-protein contact prediction, which are important for understanding how proteins interact at structure and residue level. Direct coupling analysis (DCA) is popular for intra-protein contact prediction, but extending it to inter-protein contact prediction is challenging since it requires too many interlogs (i.e., interacting homologs) to be effective, which cannot be easily fulfilled especially for a putative interacting protein pair in eukaryotes. We show that deep learning, even trained by only intra-protein contact maps, works much better than DCA for inter-protein contact prediction. We also show that a phylogeny-based method can generate a better multiple sequence alignment for eukaryotes than existing genome-based methods and thus, lead to better inter-protein contact prediction. Our method shall be useful for protein docking, protein interaction prediction and protein interaction network construction.", "tag": "Bioinformatics"}, {"title": "Automated evaluation of quaternary structures from protein crystals", "url": "https://www.biorxiv.org/content/early/2017/12/28/224717", "abstract": "A correct assessment of the quaternary structure of proteins is a fundamental prerequisite to understanding their function, physico-chemical properties and mode of interaction with other proteins. Currently about 90% of structures in the Protein Data Bank are crystal structures, in which the correct quaternary structure is embedded in the crystal lattice among a number of crystal contacts. Computational methods are required to 1) classify all protein-protein contacts in crystal lattices as biologically relevant or crystal contacts and 2) provide an assessment of how the biologically relevant interfaces combine into a biological assembly. In our previous work we addressed the first problem with our EPPIC (Evolutionary Protein Protein Interface Classifier) method. Here, we present our solution to the second problem with a new method that combines the interface classification results with symmetry and topology considerations. The new algorithm enumerates all possible valid assemblies within the crystal using a graph representation of the lattice and predicts the most probable biological unit based on the pairwise interface scoring. Our method achieves 85% precision on a new dataset of 1,481 biological assemblies with consensus of PDB annotations. Although almost the same precision is achieved by PISA, currently the most popular quaternary structure assignment method, we show that, due to the fundamentally different approach to the problem, the two methods are complementary and could be combined to improve biological assembly assignments. The software for the automatic assessment of protein assemblies (EPPIC version 3) has been made available through a web server at http://www.eppic-web.org.", "tag": "Bioinformatics"}, {"title": "Whole Genomes Define Concordance of Matched Primary, Xenograft, and Organoid Models of Pancreas Cancer", "url": "https://www.biorxiv.org/content/early/2017/12/28/209692", "abstract": "Pancreatic ductal adenocarcinoma (PDAC) has the worst prognosis among solid malignancies and improved therapeutic strategies are needed to improve outcomes. Patient-derived xenografts (PDX) and patient-derived organoids (PDO) serve as promising tools to identify new drugs with therapeutic potential in PDAC. For these preclinical disease models to be effective, they should both recapitulate the molecular heterogeneity of PDAC and validate patient-specific therapeutic sensitivities. To date however, deep characterization of PDAC PDX and PDO models and comparison with matched human tumour remains largely unaddressed at the whole genome level. We conducted a comprehensive assessment of the genetic landscape of 16 whole-genome pairs of tumours and matched PDX, from primary PDAC and liver metastasis, including a unique cohort of 5 'trios' of matched primary tumour, PDX, and PDO. We developed a new pipeline to score concordance between PDAC models and their paired human tumours for genomic events, including mutations, structural variations, and copy number variations. Comparison of genomic events in the tumours and matched disease models displayed single-gene concordance across major PDAC driver genes, and genome-wide similarities of copy number changes. Genome-wide and chromosome-centric analysis of structural variation (SV) events revealed high variability across tumours and disease models, but also highlighted previously unrecognized concordance across chromosomes that demonstrate clustered SV events. Our approach and results demonstrate that PDX and PDO recapitulate PDAC tumourigenesis with respect to simple somatic mutations and copy number changes, and capture major SV events that are found in both resected and metastatic tumours.", "tag": "Bioinformatics"}, {"title": "Predictions of Protein-Protein Interactions in Schistosoma mansoni", "url": "https://www.biorxiv.org/content/early/2017/12/28/233072", "abstract": "Background: Schistosoma mansoni invasion of the human host involves a variety of cross-species protein-protein interactions. The pathogen expresses a diverse arsenal of proteins that facilitate the breach of physical and biochemical barriers present in skin, evasion of the immune system, and digestion of human hemoglobin, allowing schistosomes to reside in the host for years. However, only a small number of specific interactions between S. mansoni and human proteins have been identified. We present and apply a protocol that generates testable predictions of S. mansoni-human protein interactions. Methods: In this study, we first predict S. mansoni-human protein interactions based on similarity to known protein complexes. Putative interactions were then scored and assessed using several contextual filters, including the use of annotation automatically derived from literature using a simple natural language processing methodology. Our method predicted 7 out of the 10 previously known cross-species interactions. Conclusions: Several predictions that warrant experimental follow-up were presented and discussed, including interactions involving potential vaccine candidate antigens, protease inhibition, and immune evasion. The application framework provides an integrated methodology for investigation of host-pathogen interactions and an extensive source of orthogonal data for experimental analysis. We have made the predictions available online for community perusal.", "tag": "Bioinformatics"}, {"title": "MSAC: Compression of multiple sequence alignment files", "url": "https://www.biorxiv.org/content/early/2017/12/28/240341", "abstract": "Motivation: Bioinformatics databases grow rapidly and achieve values hardly to imagine a decade ago. Among numerous bioinformatics processes generating hundreds of GB is multiple sequence alignments of protein families. Its largest database, i.e., Pfam, consumes 40-230GB, depending of the variant. Storage and transfer of such massive data has become a challenge. Results: We propose a novel compression algorithm, MSAC (Multiple Sequence Alignment Compressor), designed especially for aligned data. It is based on a generalisation of the positional Burrows-Wheeler transform for non-binary alphabets. MSAC handles FASTA, as well as Stockholm files. It offers up to six times better compression ratio than other commonly used compressors, i.e., gzip. Performed experiments resulted in an analysis of the influence of a protein family size on the compression ratio. Availability: MSAC is available for free at https://github.com/refresh-bio/msac and http://sun.aei.polsl.pl/REFRESH/msac.", "tag": "Bioinformatics"}, {"title": "Whisper: Read sorting allows robust mapping of sequencing data", "url": "https://www.biorxiv.org/content/early/2017/12/28/240358", "abstract": "Motivation: Mapping reads to a reference genome is often the first step in a sequencing data analysis pipeline. Mistakes made at this computationally challenging stage cannot be recovered easily. Results: We present Whisper, an accurate and high-performant mapping tool, based on the idea of sorting reads and then mapping them against suffix arrays for the reference genome and its reverse complement. Employing task and data parallelism as well as storing temporary data on disk result in superior time efficiency at reasonable memory requirements. Whisper excels at large NGS read collections, in particular Illumina reads with typical WGS coverage. The experiments with real data indicate that our solution works in about 15% of the time needed by the well-known Bowtie2 and BWA-MEM tools at a comparable accuracy (validated in variant calling pipeline). Availability: Whisper is available for free from https://github.com/refresh-bio/Whisper or http://sun.aei.polsl.pl/REFRESH/Whisper/.", "tag": "Bioinformatics"}, {"title": "bcGST - an interactive bias-correction method to identify over-represented gene-sets in boutique arrays", "url": "https://www.biorxiv.org/content/early/2017/12/28/240234", "abstract": "Gene annotation and pathway databases such as Gene Ontology and Kyoto Encyclopedia of Genes and Genomes are important tools in Gene Set Test (GST) that describe gene biological functions and associated pathways. GST aims to establish an association relationship between a gene set of interest and an annotation. Importantly, GST tests for over-representation of genes in an annotation term. One implicit assumption of GST is that the gene expression platform captures the complete or a very large proportion of the genome. However, this assumption is neither satisfied for the increasingly popular boutique array nor the custom designed gene expression profiling platform. Specifically, conventional GST is no longer appropriate due to the gene set selection bias induced during the construction of these platforms. We propose bcGST, a bias-corrected Gene Set Test by introducing bias correction terms in the contingency table needed for calculating the Fisher's Exact Test (FET). The adjustment method works by estimating the proportion of genes captured on the array with respect to the genome in order to assist filtration of annotation terms that would otherwise be falsely included or excluded. We illustrate the practicality of bcGST and its stability through multiple differential gene expression analyses in melanoma and TCGA cancer studies. The bcGST method is made available as a Shiny web application.", "tag": "Bioinformatics"}, {"title": "ClassificaIO: machine learning for classification graphical user interface", "url": "https://www.biorxiv.org/content/early/2017/12/28/240184", "abstract": "Summary: ClassificaIO is an open-source Python graphical user interface (GUI) for machine learning classification for the scikit-learn module. ClassificaIO aims to provide an easy-to-use interactive way to train, validate, and test data on a range of classification algorithms. The GUI enables fast comparisons within and across classifiers, and facilitates uploading and exporting of trained models, and both validated, and tested data results. Availability: ClassificaIO is implemented as a Python application and is available for download and installation through the Python Package Index (PyPI) (http://pypi.python.org/pypi/ClassificaIO) and it can be deployed using the \"import\" function once installed. The application is distributed under an MIT license and source code is available for download (for Mac OS X, Unix and Mi-crosoft Windows) through PyPI and GitHub (http://github.com/gmiaslab/ClassificaIO), and at https://doi.org/10.5281/zenodo.1133266.", "tag": "Bioinformatics"}, {"title": "False discovery rate estimation and heterobifunctional cross-linkers", "url": "https://www.biorxiv.org/content/early/2017/12/28/239715", "abstract": "False discovery rate (FDR) estimation is a cornerstone of proteomics that has recently been adapted to cross-linking/mass spectrometry. Here we demonstrate that heterobifunctional cross-linkers, while theoretically different from homobifunctional cross-linkers, need not be considered separately in practice. We develop and then evaluate the impact of applying a correct FDR formula for use of heterobifunctional cross-linkers and conclude that there are minimal practical advantages. Hence a single formula can be applied to data generated from the many different non-cleavable cross-linkers.", "tag": "Bioinformatics"}, {"title": "De novo profile generation based on sequence context specificity with the long short-term memory network", "url": "https://www.biorxiv.org/content/early/2017/12/28/240515", "abstract": "Amino acid sequence profiles are widely used for bioinformatics studies, such as sequence similarity searches, multiple alignments, and evolutionary analyses. Currently, many biological sequences are becoming available, and the rapidly increasing amount of sequence data emphasizes the importance of scalable generators of amino acid sequence profiles. We employed a long short-term memory (LSTM) network and developed a novel profile generator to construct profiles without any assumptions, except for input sequence context. Our method could generate better profiles than existing de novo profile generators, including CSBuild and RPS-BLAST on the basis of profile-sequence similarity search performance with linear calculation costs against input sequence size. In addition, we analyzed the effects of the memory power of LSTM and found that LSTM had high potential power to detect long-range interactions between amino acids, as in the case of beta strand formation, which has been a difficult problem in protein bioinformatics using sequence information.", "tag": "Bioinformatics"}, {"title": "Interaction of quercetin with transcriptional regulator LasR of Pseudomonas aeruginosa: Mechanistic insights of the inhibition of virulence through quorum sensing", "url": "https://www.biorxiv.org/content/early/2017/12/27/239996", "abstract": "Pseudomonas aeruginosa is one of the most dangerous superbugs in the list of bacteria for which new antibiotics are urgently needed, which was published by World Health Organization. P. aeruginosa is an antibiotic-resistant opportunistic human pathogen. It affects patients with AIDS, cystic fibrosis, cancer, burn victims and people with prosthetics and implants. P. aeruginosa also forms biofilms. Biofilms increase resistance to antibiotics and host immune responses. Because of biofilms, current therapies are not effective. It is important to find new antibacterial treatment strategies against P. aeruginosa. Biofilm formation is regulated through a system called quorum sensing. Thus disrupting this system is considered a promising strategy to combat bacterial pathogenicity. It is known that quercetin inhibits Pseudomonas aeruginosa biofilm formation, but the mechanism of action is unknown. In the present study, we tried to analyse the mode of interactions of LasR with quercetin. We used a combination of molecular docking, molecular dynamics (MD) simulations and machine learning techniques for the study of the interaction of the LasR protein of P. aeruginosa with quercetin. We assessed the conformational changes of the interaction and analysed the molecular details of the binding of quercetin with LasR. We show that quercetin has two binding modes. One binding mode is the interaction with ligand binding domain, this interaction is not competitive and it has also been shown experimentally. The second binding mode is the interaction with the bridge, it involves conservative amino acid interactions from LBD, SLR, and DBD and it is also not competitive. Experimental studies show hydroxyl group of ring A is necessary for inhibitory activity, in our model the hydroxyl group interacts with Leu177 during the second binding mode. This could explain the molecular mechanism of how quercetin inhibits LasR protein. This study may offer insights on how quercetin inhibits quorum sensing circuitry by interacting with transcriptional regulator LasR. The capability of having two binding modes may explain why quercetin is effective at inhibiting biofilm formation and virulence gene expression.", "tag": "Bioinformatics"}, {"title": "SeqsLab: an integrated platform for cohort-based annotation and interpretation of genetic variants on Spark", "url": "https://www.biorxiv.org/content/early/2017/12/27/239962", "abstract": "Summary: SeqsLab is a platform that helps researchers to easily annotate and interpret genetic variants derived from a large quantity of personal genomes. It provides an integrated interface to annotate the variants based on curated databases as well as in silico estimation on the effects of the variants. SeqsLab adopts the scalable cluster computing framework, Spark, and incorporates several customized algorithms to speed up the process of variant annotation and interpretation. The key features of SeqsLab include efficient annotation on large structural variations, diverse combinations of variant filters, easy incorporation with a vast amount of public databases, and scalable architecture of analyzing hundreds of human whole genomes simultaneously. Availability and Implementation: SeqsLab is implemented with JAVA. The generated annotation will then be stored in Elasticsearch for real-time query and exploratory analysis. SeqsLab can be accessed by web browsers and is freely available at https://portal.seqslab.net/.", "tag": "Bioinformatics"}, {"title": "Reproducible Bioinformatics Project: A community for reproducible bioinformatics analysis pipelines", "url": "https://www.biorxiv.org/content/early/2017/12/26/239947", "abstract": "Background: Reproducibility of a research is a key element in the modern science and it is mandatory for any industrial application. It represents the ability of replicating an experiment independently by the location and the operator. Therefore, a study can be considered reproducible only if all used data are available and the exploited computational analysis workflow is clearly described. However, today for reproducing a complex bioinformatics analysis, the raw data and a list of tools used in the workflow could be not enough to guarantee the reproducibility of the results obtained. Indeed, different releases of the same tools and/or of the system libraries (exploited by such tools) might lead to sneaky reproducibility issues. Results: To address this challenge, we established the Reproducible Bioinformatics Project (RBP), which is a non-profit and open-source project, whose aim is to provide a schema and an infrastructure, based on docker images and R package, to provide reproducible results in Bioinformatics. One or more Docker images are then defined for a workflow (typically one for each task), while the workflow implementation is handled via R-functions embedded in a package available at github repository. Thus, a bioinformatician participating to the project has firstly to integrate her/his workflow modules into Docker image(s) exploiting an Ubuntu docker image developed ad hoc by RPB to make easier this task. Secondly, the workflow implementation must be realized in R according to an R-skeleton function made available by RPB to guarantee homogeneity and reusability among different RPB functions. Moreover she/he has to provide the R vignette explaining the package functionality together with an example dataset which can be used to improve the user confidence in the workflow utilization. Conclusions: Reproducible Bioinformatics Project provides a general schema and an infrastructure to distribute robust and reproducible workflows. Thus, it guarantees to final users the ability to repeat consistently any analysis independently by the used UNIX-like architecture.", "tag": "Bioinformatics"}, {"title": "Accelerating SNP genotyping from whole genome sequencing data for bedside diagnostics", "url": "https://www.biorxiv.org/content/early/2017/12/26/239871", "abstract": "Motivation: Genotyping a set of variants from a database is an important step for identifying known genetic traits and disease related variants within an individual. The growing size of variant databases as well as the high depth of sequencing data pose an efficiency challenge. In clinical applications, where time is crucial, alignment-based methods are often not fast enough. To fill the gap, (Shajii et al. 2016) propose LAVA, an alignment-free genotyping method which is able to more quickly genotype SNPs; however, there remains large room for improvements in running time. Results: We present the VarGeno method for SNP genotyping from Illumina whole genome sequencing data. Our method performs 2-8 times faster than LAVA with similar memory usage. VarGeno uses Bloom filters to achieve a 2x speedup without changing the accuracy, while a 8x speedup is achieved by using a quality value filtering method (VarGeno-QV) at the cost of only slight decrease (0.04%) in accuracy. Availability: VarGeno is freely available at: https://github.com/medvedevgroup/vargeno.", "tag": "Bioinformatics"}, {"title": "Efficient graph-color compression with neighborhood-informed Bloom filters", "url": "https://www.biorxiv.org/content/early/2017/12/26/239806", "abstract": "Technological advancements in high throughput DNA sequencing have led to an exponential growth of sequencing data being produced and stored as a byproduct of biomedical research. Despite its public availability, a majority of this data remains inaccessible to the research com- munity through a lack efficient data representation and indexing solutions. One of the available techniques to represent read data on a more abstract level is its transformation into an assem- bly graph. Although the sequence information is now accessible, any contextual annotation and metadata is lost. We present a new approach for a compressed representation of a graph coloring based on a set of Bloom filters. By dropping the requirement of a fully lossless compression and using the topological information of the underlying graph to decide on false positives, we can reduce the memory requirements for a given set of colors per edge by three orders of magnitude. As insertion and query on a Bloom filter are constant time operations, the complexity to compress and decompress an edge color is linear in the number of color bits. Representing individual colors as independent filters, our approach is fully dynamic and can be easily parallelized. These properties allow for an easy upscaling to the problem sizes common in the biomedical domain. A prototype implementation of our method is available in Java.", "tag": "Bioinformatics"}, {"title": "TARGETED SEARCHES FOR NOVEL PEPTIDES IN BIG MASS SPECTROMETRY DATA SETS", "url": "https://www.biorxiv.org/content/early/2017/12/25/239863", "abstract": "We present Post-Acquisition Targeted Searches (PATS), an easy-to-use tool that allows the identification of novel peptide/protein sequences from existing big mass spectrometry data sets. PATS filters out the unrelated peptidome before the time-consuming database search to significantly speed up the identification. Using interactome data sets, PATS visualizes protein interaction network and helps to assign putative functions to the target protein based on the guilt-by-association concept.", "tag": "Bioinformatics"}, {"title": "AMBER: Assessment of Metagenome BinnERs", "url": "https://www.biorxiv.org/content/early/2017/12/25/239582", "abstract": "Reconstructing the genomes of microbial community members is key to the interpretation of shotgun metagenome samples. Genome binning programs deconvolute reads or assembled contigs of such samples into individual bins, but assessing their quality is difficult due to the lack of evaluation software and standardized metrics. We present AMBER, an evaluation package for the comparative assessment of genome reconstructions from metagenome benchmark data sets. It calculates the performance metrics and comparative visualizations used in the first benchmarking challenge of the Initiative for the Critical Assessment of Metagenome Interpretation (CAMI). As an application, we show the outputs of AMBER for ten different binnings on two CAMI benchmark data sets. AMBER is implemented in Python and available under the Apache 2.0 license on GitHub (https://github.com/CAMI-challenge/AMBER).", "tag": "Bioinformatics"}, {"title": "Modeling Spatio-temporal Dynamics of Chromatin Marks", "url": "https://www.biorxiv.org/content/early/2017/12/24/239442", "abstract": "To model spatial changes of chromatin mark peaks over time we developed ChromTime, a computational method that identifies regions for which peaks either expand or contract significantly or hold steady between time points. We applied our method to chromatin marks in a range of biological systems. Predicted expanding and contracting peaks likely mark regulatory regions associated with transcription factor binding dynamics and gene expression changes. Spatial dynamics of peaks are informative about gene expression changes beyond localized signal changes. In proximity of gene starts, peaks preferentially expand in the same direction as transcription and contract in the opposite direction.", "tag": "Bioinformatics"}, {"title": "FarmCPUpp: Efficient Large-Scale GWAS", "url": "https://www.biorxiv.org/content/early/2017/12/24/238832", "abstract": "Genome-wide association studies (GWAS) are computationally demanding analyses that use large sample sizes and dense marker sets to discover associations between quantitative trait variation and genetic variants. FarmCPU is a powerful new method for performing GWAS. However, its performance is hampered by details of its implementation and its reliance on the R programming language. In this paper we present an efficient implementation of FarmCPU, called FarmCPUpp, that retains the R user interface but improves memory management and speed through the use of C++ code and parallel computing.", "tag": "Bioinformatics"}, {"title": "DeepMHC: Deep Convolutional Neural Networks for High-performance peptide-MHC Binding Affinity Prediction", "url": "https://www.biorxiv.org/content/early/2017/12/24/239236", "abstract": "Convolutional neural networks (CNN) have been shown to outperform conventional methods in DNA-protien binding specificity prediction. However, whether we can transfer this success to protien-peptide binding affinity prediction depends on appropriate design of the CNN architecture that calls for thorough understanding how to match the architecture to the problem. Here we propose DeepMHC, a deep convolutional neural network (CNN) based protein-peptide binding prediction algorithm for achieving better performance in MHC-I peptide binding affinity prediction than conventional algorithms. Our model takes only raw binding peptide sequences as input without needing any human-designed features and other physichochemical or evolutionary information of the amino acids. Our CNN models are shown to be able to learn non-linear relationships among the amino acid positions of the peptides to achieve highly competitive performance on most of the IEDB benchmark datasets with a single model architecture and without using any consensus or composite ensemble classifier models. By systematically exploring the best CNN architecture, we identified critical design considerations in CNN architecture development for peptide-MHC binding prediction.", "tag": "Bioinformatics"}, {"title": "Chromatin interaction data visualization in the WashU Epigenome Browser", "url": "https://www.biorxiv.org/content/early/2017/12/24/239368", "abstract": "Motivation: Long-range chromatin interactions are critical for gene regulations and genome maintenance. HiC and Cool are the two most common data formats used by the community, including the 4D Nucleome Consortium (4DN), to represent chromatin interaction data from a variety of chromatin conformation capture experiments, and specialized tools were developed for their analysis, visualization, and conversion. However, there does not exist a tool that can support visualization of both data formats simultaneously. Results: The WashU Epigenome Browser has integrated both HiC and Cool data formats into its visualization platform. Investigators can seamlessly explore chromatin interaction data regardless of their underlying data format. For developers it is straightforward to benchmark the differences in rendering speed and computational resource usage between the two data formats. Availability: http://epigenomegateway.wustl.edu/browser/.", "tag": "Bioinformatics"}, {"title": "An accurate and rapid continuous wavelet dynamic time warping algorithm for unbalanced global mapping in nanopore sequencing", "url": "https://www.biorxiv.org/content/early/2017/12/23/238857.1", "abstract": "Long-reads, point-of-care, and PCR-free are the promises brought by nanopore sequencing. Among various steps in nanopore data analysis, the global mapping between the raw electrical current signal sequence and the expected signal sequence from the pore model serves as the key building block to base calling, reads mapping, variant identification, and methylation detection. However, the ultra-long reads of nanopore sequencing and an order of magnitude difference in the sampling speeds of the two sequences make the classical dynamic time warping (DTW) and its variants infeasible to solve the problem. Here, we propose a novel multi-level DTW algorithm, cwDTW, based on continuous wavelet transforms with different scales of the two signal sequences. Our algorithm starts from low-resolution wavelet transforms of the two sequences, such that the transformed sequences are short and have similar sampling rates. Then the peaks and nadirs of the transformed sequences are extracted to form feature sequences with similar lengths, which can be easily mapped by the original DTW. Our algorithm then recursively projects the warping path from a lower-resolution level to a higher-resolution one by building a context-dependent boundary and enabling a constrained search for the warping path in the latter. Comprehensive experiments on two real nanopore datasets on human and on Pandoraea pnomenusa, as well as two benchmark datasets from previous studies, demonstrate the efficiency and effectiveness of the proposed algorithm. In particular, cwDTW can almost always generate warping paths that are very close to the original DTW, which are remarkably more accurate than the state-of-the-art methods including FastDTW and PrunedDTW. Meanwhile, on the real nanopore datasets, cwDTW is about 440 times faster than FastDTW and 3000 times faster than the original DTW. Our program is available at https://github.com/realbigws/cwDTW.", "tag": "Bioinformatics"}, {"title": "STRetch: detecting and discovering pathogenic short tandem repeats expansions", "url": "https://www.biorxiv.org/content/early/2017/12/23/159228", "abstract": "Short tandem repeat (STR) expansions have been identified as the causal DNA mutation in dozens of Mendelian human diseases. Traditionally, pathogenic STR expansions could only be detected by single locus techniques, such as PCR and electrophoresis. The ability to genotype STRs directly from next-generation sequencing data has the potential to reduce both the time and cost to reaching diagnosis and to discovering new causal STR loci. Most existing tools detect STR variation within the read length, and so are unable to detect the majority of pathogenic expansions. Those tools that can detect large expansions are limited to a set of known disease loci. Here we address this by presenting STRetch, a new genome-wide method to detect pathogenic STR expansions at known and novel loci. We demonstrate the use of STRetch for detecting pathogenic STR expansions in short-read whole genome sequencing data by applying it to the analysis of 97 whole genomes to reveal variation at STR loci. We further demonstrate the application of STRetch to solve cases of patients with undiagnosed disease, where STR expansions are a likely cause. STRetch assesses expansions at all STR loci in the genome and has the potential to detect novel disease-causing STR loci. STRetch is open source software, available from github.com/Oshlack/STRetch.", "tag": "Bioinformatics"}, {"title": "Alignment-Free Approaches Predict Novel Nuclear Mitochondrial Segments (NUMTs) in the Human Genome", "url": "https://www.biorxiv.org/content/early/2017/12/23/239053", "abstract": "Regions of the nuclear genome may harbor sequences of mitochondrial origin, indicating an ancestral transfer of mitochondrial DNA into the nuclear genome. These Nuclear Mitochondrial Segments (NUMTs) are traditionally detected by sequence similarity search, as implemented in the Basic Local Alignment Search Tool (BLAST). Confidently detecting such NUMTs is important for the comprehensive annotation of the human genome. Here we explore the possibility of detecting NUMTs in the human genome by alignment-free approaches, such as k-mers (k-tuples, k-grams, oligos of length k) distributions. We find that when k=6 or larger, the k-mer approach and BLAST search produce almost identical results, e.g., detect the same set of NUMTs longer than 3kb. However, when k=5 or k=4, certain signals are only detected by the alignment-free approach, and these may indicate yet unrecognized, and potentially more ancestral NUMTs. We introduce a \"Manhattan plot\" style representation of NUMTs predictions across the genome, which are calculated based on the reciprocal of the Jensen-Shannon divergence between the nuclear and mitochondrial k-mer frequencies. An inspection of Genome Browser annotations shows that most of the k-mer specific NUMT predictions contain Long Terminal Repeat (LTR), whereas BLAST based NUMT predictions do not.", "tag": "Bioinformatics"}, {"title": "Learning and Mapping Lyme Disease Patient Trajectories from Electronic Medical Data for Stratification of Disease Risk and Therapeutic Response", "url": "https://www.biorxiv.org/content/early/2017/12/23/239020", "abstract": "Background: Lyme disease (LD) is an epidemic, tick-borne illness with approximately 329,000 incidences diagnosed each year in United States. Long-term use of antibiotics is associated with serious complications, including post-treatment Lyme disease syndrome (PTLDS). The landscape of comorbidities and health trajectories associated with LD and associated treatments is not fully understood. Consequently, there is an urgent need to improve clinical management of LD based on a more precise understanding of disease and patient stratification. Methods: We used a precision medicine machine-learning approach based on high-dimensional electronic medical records (EMRs) to characterize the heterogeneous comorbidities in a LD population and develop systematic predictive models for identifying medications that influence the risk of subsequent comorbidities. Findings: We identified 3, 16, and 17 comorbidities at broad disease categories associated with LD within 2, 5, and 10 years of diagnosis, respectively. At higher resolution of ICD-9 levels, we pinpointed specific co-morbid diseases on a timescale that matched the symptoms associated with PTLDS. We identified 7, 30, and 35 medications that influenced the risks of the reported comorbidities within 2, 5, and 10 years, respectively. These medications included six previously associated with the identified comorbidities and 29 new findings. For instance, the first-line antibiotic doxycycline exhibited a consistently protective effect for typical symptoms of LD, including 'backache Not Otherwise Specified (NOS)' and 'chronic rhinitis', but consistently increased the risk of 'cataract NOS', 'tear film insufficiency NOS', and 'nocturia'. Interpretation: Our approach and findings suggest new hypotheses for precision medicine treatments regimens and drug repurposing opportunities tailored to the phenotypic profiles of LD patients. Funding: The Steven & Alexandra Cohen Foundation", "tag": "Bioinformatics"}, {"title": "Hybrid correction of highly noisy Oxford Nanopore long reads using a variable-order de Bruijn graph", "url": "https://www.biorxiv.org/content/early/2017/12/22/238808", "abstract": "The recent rise of long read sequencing technologies such as Pacific Biosciences and Oxford Nanopore allows to solve assembly problems for larger and more complex genomes than what allowed short reads technologies. However, these long reads are very noisy, reaching an error rate of around 10 to 15% for Pacific Biosciences, and up to 30% for Oxford Nanopore. The error correction problem has been tackled by either self-correcting the long reads, or using complementary short reads in a hybrid approach, but most methods only focus on Pacific Biosciences data, and do not apply to Oxford Nanopore reads. Moreover, even though recent chemistries from Oxford Nanopore promise to lower the error rate below 15%, it is still higher in practice, and correcting such noisy long reads remains an issue. We present HG-CoLoR, a hybrid error correction method that focuses on a seed-and-extend approach based on the alignment of the short reads to the long reads, followed by the traversal of a variable-order de Bruijn graph, built from the short reads. Our experiments show that HG-CoLoR manages to efficiently correct Oxford Nanopore long reads that display an error rate as high as 44%. When compared to other state-of-the-art long read error correction methods able to deal with Oxford Nanopore data, our experiments also show that HG-CoLoR provides the best trade-off between runtime and quality of the results, and is the only method able to efficiently scale to eukaryotic genomes.", "tag": "Bioinformatics"}, {"title": "Comprehensive analysis of mobile genetic elements in the gut microbiome reveals phylum-level niche-adaptive gene pools", "url": "https://www.biorxiv.org/content/early/2017/12/22/214213", "abstract": "Mobile genetic elements (MGEs) drive extensive horizontal transfer in the gut microbiome. This transfer could benefit human health by conferring new metabolic capabilities to commensal microbes, or it could threaten human health by spreading antibiotic resistance genes to pathogens. Despite their biological importance and medical relevance, MGEs from the gut microbiome have not been systematically characterized. Here, we present a comprehensive analysis of chromosomal MGEs in the gut microbiome using a method called Split Read Insertion Detection (SRID) that enables the identification of the exact mobilizable unit of MGEs. Leveraging the SRID method, we curated a database of 5600 putative MGEs encompassing seven MGE classes called ImmeDB (Intestinal microbiome mobile element database) (https://immedb.mit.edu/). We observed that many MGEs carry genes that confer an adaptive advantage to the gut environment including gene families involved in antibiotic resistance, bile salt detoxification, mucus degradation, capsular polysaccharide biosynthesis, polysaccharide utilization, and sporulation. We find that antibiotic resistance genes are more likely to be spread by conjugation via integrative conjugative elements or integrative mobilizable elements than transduction via prophages. Additionally, we observed that horizontal transfer of MGEs is extensive within phyla but rare across phyla. Taken together, our findings support a phylum level niche-adaptive gene pools in the gut microbiome. ImmeDB will be a valuable resource for future fundamental and translational studies on the gut microbiome and MGE communities.", "tag": "Bioinformatics"}, {"title": "Necklace: combining reference and assembled transcriptomes for more comprehensive RNA-Seq analysis", "url": "https://www.biorxiv.org/content/early/2017/12/22/200287", "abstract": "Background: RNA-Seq analyses can benefit from performing a genome-guided and de novo assembly, in particular for species where the reference genome or the annotation is incomplete. However, tools for integrating assembled transcriptome with reference annotation are lacking. Findings: Necklace is a software pipeline that runs genome-guided and de novo assembly and combines the resulting transcriptomes with reference genome annotations. Necklace constructs a compact but comprehensive superTranscriptome out of the assembled and reference data. Reads are subsequently aligned and counted in preparation for differential expression testing. Conclusions: Necklace allows a comprehensive transcriptome to be built from a combination of assembled and annotated transcripts which results in a more comprehensive transcriptome for the majority of organisms. In addition RNA-seq data is mapped back to this newly created superTranscript reference to enable differential expression testing with standard methods. Necklace is available from https://github.com/Oshlack/necklace/wiki under GPL 3.0.", "tag": "Bioinformatics"}, {"title": "OmicsNet: Integration of Multi-Omics Data using Path Analysis in Multilayer Networks", "url": "https://www.biorxiv.org/content/early/2017/12/22/238766", "abstract": "Integrative analysis of heterogeneous omics data is essential to obtain a comprehensive overview of otherwise fragmented information and to better understand dysregulated biological pathways leading to a specific condition. One of the major challenges in systems biology is to develop computational methods for proper integration of multi-omics datasets. We propose OmicsNet that uses a multilayer network for the integration and analysis of multi-omics data of heterogeneous types. Each layer of the multilayer network represents a certain data type: input layers correspond to genotype features and nodes in the output layer correspond to phenotypes, while intermediate layers may represent genesets or biological concepts to facilitate functional interpretation of the data. OmicsNet then calculates the highest coefficient paths in multilayer network from each genomic feature to the phenotype by computing an integrated score along the paths. These paths may indicate the most plausible signalling cascade caused by perturbed genotype features leading to a particular phenotype response. With example applications, we illustrate the potential power of OmicsNet in the functional analysis, biomarker discovery and drug response prediction in personalized medicine using multi-omics data.", "tag": "Bioinformatics"}, {"title": "Transcriptional evaluation of the developmental accuracy, reproducibility and robustness of kidney organoids derived from human pluripotent stem cells", "url": "https://www.biorxiv.org/content/early/2017/12/22/238428", "abstract": "We have previously reported a protocol for the directed differentiation of human induced pluripotent stem cells to kidney organoids comprised of nephrons, proximal and distal epithelium, vasculature and surrounding interstitial elements. The utility of this protocol for applications such as disease modelling will rely implicitly on the developmental accuracy of the model, technical robustness of the protocol and transferability between iPSC lines. Here we report extensive transcriptional analyses of the sources of variation across the timecourse of differentiation from pluripotency to complete kidney organoid, focussing on repeated differentiations to day 18 organoid. Individual organoids generated within the same differentiation experiment show Spearmans correlation coefficients of >0.99. The greatest source of variation was seen between experimental batch, with the enrichment for genes that also varied temporally between day 10 and day 25 organoids implicating nephron maturation as contributing to transcriptional variance between individual differentiation experiments. A morphological analysis revealed a transition from renal vesicle to capillary loop stage nephrons across the same time period. Distinct iPSC clones were also shown to display congruent transcriptional programs with inter-experimental and inter-clonal variation most strongly associated with nephron patterning. Even epithelial cells isolated from organoids showed transcriptional alignment with total organoids of the same day of differentiation. This data provides a framework for managing experimental variation, thereby increasing the utility of this approach for personalised medicine and functional genomics.", "tag": "Bioinformatics"}, {"title": "A hypothesis-driven approach to assessing significance of differences in RNA expression levels among specific groups of genes", "url": "https://www.biorxiv.org/content/early/2017/12/22/136143", "abstract": "Genome-wide molecular gene expression studies generally compare expression values for each gene across multiple conditions followed by cluster and gene set enrichment analysis to determine whether differentially expressed genes are enriched in specific biochemical pathways, cellular components, biological processes, and/or molecular functions, etc. This approach to analyzing differences in gene expression enables discovery of gene function, but is not useful to determine whether pre-defined groups of genes share or diverge in their expression patterns in response to treatments nor to assess the correctness of pre-defined gene set groupings. Here we present a simple method that changes the dimension of comparison by treating genes as variable traits to directly assess significance of differences in expression levels among pre-defined gene groups. Because expression distributions are typically skewed (thus unfit for direct assessment using Gaussian statistical methods) our method involves transforming expression data to approximate a normal distribution followed by dividing the genes into groups, then applying Gaussian parametric methods to assess significance of observed differences. This method enables the assessment of differences in gene expression distributions within and across samples, enabling hypothesis-based comparison among groups of genes. We demonstrate this method by assessing the significance of specific gene groups' differential response to heat stress conditions in maize.", "tag": "Bioinformatics"}, {"title": "Identification and analysis of mobile genetic elements in Gibbon genome", "url": "https://www.biorxiv.org/content/early/2017/12/22/237685", "abstract": "Recent sequencing of genome of northern white-cheeked gibbon (Nomascus leucogenys) has provided important insight into fast evolution of gibbons and signatures relevant to gibbon biology. It was revealed that mobile genetic elements (MGE) seems to play major role in gibbon evolution. Here we report that most of the gibbon genome is occupied by the MGEs such as ALUs, MIRs, LINE1, LINE 2, LINE 3, ERVL, ERV-class1, ERV-class II and other DNA elements which include hAT Charlie and TcMar tigger. We provide detailed description and genome wide distribution of all the MGEs present in gibbon genome. Previously, it was reported that gibbon-specific retrotransposon (LAVA) tend to insert into chromosome segregation genes and alter transcription by providing a premature termination site, suggesting a possible molecular mechanism for the genome plasticity of the gibbon lineage. We show that insertion sites of LAVA elements present atypical signals/patterns which are different from typical signals present at insertion sites of Alu elements. This suggests possibility of distinct insertion mechanism used by LAVA elements for their insertions. We also find similarity in signals of LAVA elements insertion sites with atypical signals present at Alus /L1s insertion sites disrupting the genes leading to diseases such as cancer and Duchenne muscular dystrophy. This suggest role of LAVA in premature transcription termination.", "tag": "Bioinformatics"}, {"title": "Visualizing genome synteny with xmatchview", "url": "https://www.biorxiv.org/content/early/2017/12/22/238220", "abstract": "In genomics research, the visual representation of DNA sequences is of prime importance. When displayed with additional information, or tracks, showing the position of annotated genes, alignments of sequence of interest, etc., these displays facilitate our understanding of genome and gene structure, and become powerful tools to assess the relationship between various sequence data. They can be used for troubleshooting sequence assemblies, in-depth sequence analysis, and eventually find their way in publications and oral presentations as they often translate complex and abundant data succinctly, with esthetically pleas- ing images. Here, I introduce xmatchview and xmatchview-conifer, two python applications for comparing genomes visually and assessing their synteny. Availability: https://github.com/warrenlr/xmatchview", "tag": "Bioinformatics"}, {"title": "Content-Aware Image Restoration: Pushing the Limits of Fluorescence Microscopy", "url": "https://www.biorxiv.org/content/early/2017/12/21/236463", "abstract": "Fluorescence microscopy is a key driver of discoveries in the life-sciences, with observable phenomena being limited by the optics of the microscope, the chemistry of the fluorophores, and the maximum photon exposure tolerated by the sample. These limits necessitate trade-offs between imaging speed, spatial resolution, light exposure, and imaging depth. In this work we show how deep learning enables biological observations beyond the physical limitations of microscopes. On seven concrete examples we illustrate how microscopy images can be restored even if 60-fold fewer photons are used during acquisition, how isotropic resolution can be achieved even with a 10-fold under-sampling along the axial direction, and how diffraction-limited structures can be resolved at 20-times higher frame-rates compared to state-of-the-art methods. All developed image restoration methods are freely available as open source software.", "tag": "Bioinformatics"}, {"title": "Leveraging multiple transcriptome assembly methods for improved gene structure annotation", "url": "https://www.biorxiv.org/content/early/2017/12/21/216994", "abstract": "The performance of RNA-Seq aligners and assemblers varies greatly across different organisms and experiments, and often the optimal approach is not known beforehand. Here we show that the accuracy of transcript reconstruction can be boosted by combining multiple approaches, and we present a novel algorithm to integrate multiple RNA-Seq assemblies into a coherent transcript annotation. Our algorithm can remove redundancies and select the best transcript models according to user-specified metrics, while solving common artefacts such as erroneous transcript chimerisms. We have implemented this method in an open-source Python3 and Cython program, Mikado, available at https://github.com/lucventurini/Mikado.", "tag": "Bioinformatics"}, {"title": "Bystro: rapid online variant annotation and natural-language filtering at whole-genome scale", "url": "https://www.biorxiv.org/content/early/2017/12/21/146514", "abstract": "Accurately selecting relevant alleles in large sequencing experiments remains technically challenging. Bystro (https://bystro.io/) is the first online, cloud-based application that makes variant annotation and filtering accessible to all researchers for terabyte-sized whole-genome experiments containing thousands of samples. Its key innovation is a general-purpose, natural-language search engine that enables users to identify and export alleles and samples of interest in milliseconds. The search engine dramatically simplifies complex filtering tasks that previously required programming experience or specialty command-line programs. Critically, Bystro's annotation and filtering capabilities are orders of magnitude faster than previous solutions, saving weeks of processing time for large experiments.", "tag": "Bioinformatics"}, {"title": "Dr.Paso: Drug response prediction and analysis system for oncology research", "url": "https://www.biorxiv.org/content/early/2017/12/21/237727", "abstract": "The prediction of anticancer drug response is crucial for achieving a more effective and precise treatment of patients. Models based on the analysis of large cell line collections have shown potential for investigating drug efficacy in a clinically-meaningful, cost-effective manner. Using data from thousands of cancer cell lines and drug response experiments, we propose a drug sensitivity prediction system based on a 47-gene expression profile, which was derived from an unbiased transcriptomic network analysis approach. The profile reflects the molecular activity of a diverse range of cancer-relevant processes and pathways. We validated our model using independent datasets and comparisons with published models. A high concordance between predicted and observed drug sensitivities was obtained, including additional validated predictions for four glioblastoma cell lines and four drugs. Our approach can accurately predict anti-cancer drug sensitivity and will enable further pre-clinical research. In the longer-term, it may benefit patient-oriented investigations and interventions.", "tag": "Bioinformatics"}, {"title": "Decomposing the Apoptosis Pathway Into Biologically Interpretable Principal Components", "url": "https://www.biorxiv.org/content/early/2017/12/21/237883", "abstract": "Principal component analysis (PCA) is one of the most common techniques in the analysis of biological data sets, but applying PCA raises two challenges. First, one must determine the number of significant principal components (PCs). Second, because each PC is a linear combination of genes, it rarely has a biological interpretation. Existing methods to determine the number of PCs are either subjective or computationally extensive. We review several methods and describe a new R package, PCDimension, that implements additional methods, the most important being an algorithm that extends and automates a graphical Bayesian method. Using simulations, we compared the methods. Our newly automated procedure performs best when considering both accuracy and speed. We applied the method to a proteomics data set from acute myeloid leukemia patients. Proteins in the apoptosis pathway could be explained using six PCs. By clustering the proteins in PC space, we were able to replace the PCs by six \"biological components\", three of which could be immediately interpreted from the current literature. We expect this approach combining PCA with clustering to be widely applicable.", "tag": "Bioinformatics"}, {"title": "Non-biological synthetic spike-in controls and the AMPtk software pipeline improve fungal high throughput amplicon sequencing data", "url": "https://www.biorxiv.org/content/early/2017/12/21/213470", "abstract": "High throughput amplicon sequencing (HTAS) of conserved DNA regions is a powerful technique to characterize biological communities from environmental samples. Recently, spike-in mock communities have been used to measure accuracy of sequencing platforms and data analysis pipelines. The fungal internal transcribed spacer (ITS) region is difficult to sequence due to its variability (length and sequence divergence) across the fungal kingdom. To assess the ability of sequencing platforms and data processing pipelines using fungal ITS amplicons, we created two ITS spike-in control mock communities composed of single copy plasmid DNA: a biological mock community (BioMock), consisting of cloned ITS sequences, and a synthetic mock community (SynMock), consisting of non-biological ITS-like sequences. Using these spike-in controls we show that pre-clustering steps for variable length amplicons are critically important and a major source of bias is attributed to initial PCR reactions. These data suggest HTAS read abundances are not representative of starting values. We developed AMPtk (amplicon toolkit), a versatile software solution equipped to deal with variable length amplicons featuring a method to quality filter HTAS data based on spike-in controls. While we describe herein a non-biological (synthetic) mock community for ITS sequences, the concept can be widely applied to any HTAS dataset.", "tag": "Bioinformatics"}, {"title": "Robust expression variability testing reveals heterogeneous T cell responses", "url": "https://www.biorxiv.org/content/early/2017/12/21/237214", "abstract": "Cell-to-cell transcriptional variability in otherwise homogeneous cell populations plays a crucial role in tissue function and development. Single-cell RNA sequencing can characterise this variability in a transcriptome-wide manner. However, technical variation and the confounding between variability and mean expression estimates hinders meaningful comparison of expression variability between cell populations. To address this problem, we introduce a novel analysis approach that extends the BASiCS statistical framework to derive a residual measure of variability that is not confounded by mean expression. Moreover, we introduce a new and robust procedure for quantifying technical noise in experiments where technical spike-in molecules are not available. We illustrate how our method provides biological insight into the dynamics of cell-to-cell expression variability, highlighting a synchronisation of the translational machinery in immune cells upon activation. Additionally, our approach identifies new patterns of variability across CD4+ T cell differentiation.", "tag": "Bioinformatics"}, {"title": "INFERENCE OF CELL TYPE COMPOSITION FROM HUMAN BRAIN TRANSCRIPTOMIC DATASETS ILLUMINATES THE EFFECTS OF AGE, MANNER OF DEATH, DISSECTION, AND PSYCHIATRIC DIAGNOSIS", "url": "https://www.biorxiv.org/content/early/2017/12/20/089391", "abstract": "Psychiatric illness is unlikely to arise from pathology occurring uniformly across all cell types in affected brain regions. Despite this, transcriptomic analyses of the human brain have typically been conducted using macro-dissected tissue due to the difficulty of performing single-cell type analyses with donated post-mortem brains. To address this issue statistically, we compiled a database of several thousand transcripts that were specifically-enriched in one of 10 primary cortical cell types, as identified in previous publications. Using this database, we predicted the relative cell type composition for 833 human cortical samples using microarray or RNA-Seq data from the Pritzker Consortium (GSE92538) or publicly-available databases (GSE53987, GSE21935, GSE21138, CommonMind Consortium). These predictions were generated by averaging normalized expression levels across transcripts specific to each cell type using our R-package BrainInABlender (validated and publicly-released: https://github.com/hagenaue/BrainInABlender). Using this method, we found that the principal components of variation in the datasets were largely explained by the neuron to glia ratio of the samples. This variability was not simply due to dissection - the relative balance of brain cell types was influenced by a variety of demographic, pre- and post-mortem variables. Prolonged hypoxia around the time of death predicted increased astrocytic and endothelial content in the tissue, illustrating vascular upregulation. Aging was associated with decreased neuronal content. Red blood cell content was reduced in individuals who died following systemic blood loss. Subjects with Major Depressive Disorder had decreased astrocytic content, mirroring previous morphometric observations. Subjects with Schizophrenia had reduced red blood cell content, resembling the hypofrontality detected in fMRI experiments. Finally, in datasets containing samples with especially variable cell content, we found that controlling for predicted sample cell content while evaluating differential expression improved the detection of previously-identified psychiatric effects. We conclude that accounting for cell type can greatly improve the interpretability of microarray data.", "tag": "Bioinformatics"}, {"title": "Siccuracy: An R-package for executing genotype imputation strategy simulations with AlphaImpute", "url": "https://www.biorxiv.org/content/early/2017/12/20/236760", "abstract": "Background: The reported R-package provides an easy way for executing and evaluating genotype imputation studies, by providing functions for preparing input files for AlphaImpute and efficiently calculating imputation accuracies. Using the correlation between true and imputed genotypes is used here as it is directly related to the accuracy of genomic prediction using imputed genotypes. This R-package calculates both correlation and counts correct and incorrect imputed genotypes. Results: Implementing the correlation using a Fortran resulted in faster calculations and using less memory than using base R functions. Reporting the performance of an imputation should not be done only by the average correlation between true and imputed genotype. It is demonstrated that the highest average correlation is not necessarily the best correlation and that the range of obtained correlations provides a more nuanced grasp of the performance of the imputation. Conclusions: An R-package is available that provides a fast, standardized, and tested implementation for computing the correlations.", "tag": "Bioinformatics"}, {"title": "bioSyntax: Syntax Highlighting For Computational Biology", "url": "https://www.biorxiv.org/content/early/2017/12/20/235820", "abstract": "Computational biology requires the reading and comprehension of biological data files. Plain-text formats such as SAM, VCF, GTF, PDB and FASTA, often contain critical information that is obfuscated by the complexity of the data structures. bioSyntax (http://bioSyntax.org) is a freely available suite of syntax highlighting packages for vim, gedit, Sublime, and less, which aids computational scientists to parse and work with their data more efficiently.", "tag": "Bioinformatics"}, {"title": "Real-value and confidence prediction of protein backbone dihedral angles through a hybrid method of clustering and deep learning", "url": "https://www.biorxiv.org/content/early/2017/12/20/236851", "abstract": "Background: Protein dihedral angles provide a detailed description of protein local conformation. Predicted dihedral angles can be used to narrow down the conformational space of the whole polypeptide chain significantly, thus aiding protein tertiary structure prediction. However, direct angle prediction from sequence alone is challenging. Results: In this article, we present a novel method (named RaptorX-Angle) to predict real-valued angles by combining clustering and deep learning. Tested on a subset of PDB25 and the targets in the latest two Critical Assessment of protein Structure Prediction (CASP), our method outperforms the existing state-of-art method SPIDER2 in terms of Pearson Correlation Coefficient (PCC) and Mean Absolute Error (MAE). Our result also shows approximately linear relationship between the real prediction errors and our estimated bounds. That is, the real prediction error can be well approximated by our estimated bounds. Conclusions: Our study provides an alternative and more accurate prediction of dihedral angles, which may facilitate protein structure prediction and functional study.", "tag": "Bioinformatics"}, {"title": "Metannot: A succinct data structure for compression of colors in dynamic de Bruijn graphs", "url": "https://www.biorxiv.org/content/early/2017/12/20/236711", "abstract": "Much of the DNA and RNA sequencing data available is in the form of high-throughput sequencing (HTS) reads and is currently unindexed by established sequence search databases. Recent succinct data structures for indexing both reference sequences and HTS data, along with associated metadata, have been based on either hashing or graph models, but many of these structures are static in nature, and thus, not well-suited as backends for dynamic databases. We propose a parallel construction method for and novel application of the wavelet trie as a dynamic data structure for compressing and indexing graph metadata. By developing an algorithm for merging wavelet tries, we are able to construct large tries in parallel by merging smaller tries constructed concurrently from batches of data. When compared against general compression algorithms and those developed specifically for graph colors (VARI and Rainbowfish), our method achieves compression ratios superior to gzip and VARI, converging to compression ratios of 6.5% to 2% on data sets constructed from over 600 virus genomes. While marginally worse than compression by bzip2 or Rainbowfish, this structure allows for both fast extension and query. We also found that additionally encoding graph topology metadata improved compression ratios, particularly on data sets consisting of several mutually-exclusive reference genomes. It was also observed that the compression ratio of wavelet tries grew sublinearly with the density of the annotation matrices. This work is a significant step towards implementing a dynamic data structure for indexing large annotated sequence data sets that supports fast query and update operations. At the time of writing, no established standard tool has filled this niche.", "tag": "Bioinformatics"}, {"title": "Clustering of Circular Consensus Sequences: Accurate Error Correction and Assembly of Single Molecule Real-Time Reads from Multiplexed Amplicon Libraries", "url": "https://www.biorxiv.org/content/early/2017/12/20/236893", "abstract": "Background: Targeted resequencing with high-throughput sequencing (HTS) platforms can be used to efficiently interrogate the genomes of large numbers of individuals. A critical challenge for research and applications using HTS data, especially from long-read platforms, is errors arising from technological limits and bioinformatic algorithms. Results: A single molecule real-time (SMRT) sequencing-error correction and assembly pipeline, C3S-LAA, was developed for libraries of pooled amplicons. By uniquely leveraging the structure of SMRT sequence data (comprised of multiple low quality subreads from which higher quality circular consensus sequences are formed) to cluster raw reads, C3S-LAA produced accurate consensus sequences and assemblies of overlapping amplicons from single sample and multiplexed libraries. In contrast, despite read depths in excess of 100X per amplicon, the standard long amplicon analysis module from Pacific Biosciences generated unexpected numbers of amplicon sequences with substantial inaccuracies in the consensus sequences. A bootstrap analysis showed that the C3S-LAA pipeline per se was effective at removing bioinformatic sources of error, but in rare cases a read depth of nearly 400X was not sufficient to overcome minor but systematic errors inherent to amplification or sequencing. Conclusions: C3S-LAA uses a novel processing algorithm for SMRT amplicon-sequence data that produces accurate consensus sequences and local sequence assemblies. The community standard long amplicon analysis module from Pacific Biosciences is prone to substantial errors that raise concerns about findings based on this pipeline. The method developed here removed this confounding bioinformatics source of error, allowing for the identification of limited instances of errors due to DNA amplification or sequencing.", "tag": "Bioinformatics"}, {"title": "Conservation of separase N-terminal domain", "url": "https://www.biorxiv.org/content/early/2017/12/19/236216", "abstract": "We report a reanalysis of the sequence conservation of the cell cycle regulatory protease, separase. The sequence and structural conservation of the protease domain has long been recognized. Here we reexamine the protein sequence conservation at the N-terminus using PSI-BLAST analysis and report our discovery of a cysteine rich motif (CxCXXC) conserved in nematodes and vertebrates. This motif is found in a solvent exposed linker region connecting two TPR-like helical motifs. Mutation of this motif in Caenorhabditis elegans separase leads to a temperature sensitive hypomorphic protein, and several N-terminal residues identified as intragenic suppressors are not conserved. Conservation of this motif in multiple organisms raises the possibility that the motif plays similar roles across species.", "tag": "Bioinformatics"}, {"title": "Exploring Single-Cell Data with Multitasking Deep Neural Networks", "url": "https://www.biorxiv.org/content/early/2017/12/19/237065", "abstract": "Handling the vast amounts of single-cell RNA-sequencing and CyTOF data, which are now being generated in patient cohorts, presents a computational challenge due to the noise, complexity, sparsity and batch effects present. Here, we propose a unified deep neural network-based approach to automatically process and extract structure from these massive datasets. Our unsupervised architecture, called SAUCIE (Sparse Autoencoder for Unsupervised Clustering, Imputation, and Embedding), simultaneously performs several key tasks for single-cell data analysis including 1) clustering, 2) batch correction, 3) visualization, and 4) denoising/imputation. SAUCIE is trained to recreate its own input after reducing its dimensionality in a 2-D embedding layer which can be used to visualize the data. Additionally, it uses two novel regularizations: (1) an information dimension regularization to penalize entropy as computed on normalized activation values of the layer, and thereby encourage binary-like encodings that are amenable to clustering and (2) a Maximal Mean Discrepancy penalty to correct batch effects. Thus SAUCIE has a single architecture that denoises, batch-corrects, visualizes and clusters data using a unified representation. We show results on artificial data where ground truth is known, as well as mass cytometry data from dengue patients, and single-cell RNA-sequencing data from embryonic mouse brain.", "tag": "Bioinformatics"}, {"title": "Topology independent structural matching discovers novel templates for protein interfaces", "url": "https://www.biorxiv.org/content/early/2017/12/19/235812", "abstract": "Motivation: Protein-protein interactions (PPI) are essential for the function of the cellular machinery. The rapid growth of protein-protein complexes with known 3D structures offers a unique opportunity to study PPI to gain crucial insights into protein function and the causes of many diseases. In particular, it would be extremely useful to compare interaction surfaces of monomers, as this would enable the pinpointing of potential interaction surfaces based solely on the monomer structure, without the need to predict the complete complex structure. While there are many structural alignment algorithms for individual proteins, very few have been developed for protein interfaces, and none that can align only the interface residues to other interfaces or surfaces of interacting monomer subunits in a topology independent (non-sequential) manner. Results: We present InterComp, a method for topology and sequence-order independent structural comparisons. The method is general and can be applied to various structural comparison applications. By representing residues as independent points in space rather than as a sequence of residues, InterComp can can be applied to a wide range of problems including: interface-surface comparisons, interface-interface comparisons and even comparisons of small molecule ligands. We demonstrate a use-case by applying InterComp to find similar protein interfaces on the surface of proteins. We show that InterComp pinpoints the correct interface for almost half of the targets (283 of 586) when considering the top 10 hits, and for 24% of the top 1, even when no templates can be found with the already available sequence-order dependent methods like TM-align.", "tag": "Bioinformatics"}, {"title": "ProteomeGenerator: A framework for comprehensive proteomics based on de novo transcriptome assembly and high-accuracy peptide mass spectral matching", "url": "https://www.biorxiv.org/content/early/2017/12/19/236844", "abstract": "Modern mass spectrometry now permits genome-scale and quantitative measurements of biological proteomes. However, analyses of specific specimens are currently hindered by the incomplete representation of biological variability of protein sequences in canonical reference proteomes, and the technical demands for their construction. Here, we report ProteomeGenerator, a framework for de novo and reference-assisted proteogenomic database construction and analysis based on sample-specific transcriptome sequencing and high resolution and high-accuracy mass spectrometry proteomics. This enables assembly of proteomes encoded by actively transcribed genes, including sample-specific protein isoforms resulting from non-canonical mRNA transcription, splicing, or editing. To improve the accuracy of protein isoform identification in non-canonical proteomes, ProteomeGenerator relies on statistical target-decoy database matching augmented with spectral-match calibrated sample-specific controls. We applied this method for the proteogenomic discovery of splicing factor SRSF2-mutant leukemia cells, demonstrating high-confidence identification of non-canonical protein isoforms arising from alternative transcriptional start sites, intron retention, and cryptic exon splicing, as well as improved accuracy of genome-scale proteome discovery. Additionally, we report proteogenomic performance metrics for the current state-of-the-art implementations of SEQUEST HT, Proteome Discoverer, MaxQuant, Byonic, and PEAKS mass spectral analysis algorithms. Finally, ProteomeGenerator is implemented as a Snakemake workflow, enabling open, scalable, and facile discovery of sample-specific, non-canonical and neomorphic biological proteomes (https://github.com/jtpoirier/proteomegenerator).", "tag": "Bioinformatics"}, {"title": "Learning causal biological networks with generalized Mendelian randomization", "url": "https://www.biorxiv.org/content/early/2017/12/19/171348", "abstract": "Although large amounts of genomic data are available, it remains a challenge to reliably infer causal relationships among molecular phenotypes (such as gene expression), especially when many phenotypes are involved. We present MRPC, which learns a causal biological network efficiently and robustly from integrating genotype and molecular phenotype data. MRPC is the first machine learning algorithm that incorporates generalized Mendelian randomization (gMR) in classical algorithms for learning directed acyclic graphs in computer science. The causal network contains directed edges, which indicate causal directions. We demonstrate through simulation that MRPC outperforms existing large-scale network inference methods and MR-based methods. We apply MRPC to distinguish direct and indirect targets among multiple genes associated with expression quantitative trait loci (eQTLs). We also construct a causal network for frequently altered cancer genes.", "tag": "Bioinformatics"}, {"title": "A protein standard that emulates homology for the characterization of protein inference algorithms", "url": "https://www.biorxiv.org/content/early/2017/12/19/236471", "abstract": "A natural way to benchmark the performance of an analytical experimental setup is to use samples of known content, and see to what degree one can correctly infer the content of such a sample from the data. For shotgun proteomics, one of the inherent problems of interpreting data is that the measured analytes are peptides and not the actual proteins themselves. As some proteins share proteolytic peptides, there might be more than one possible causative set of proteins resulting in a given set of peptides and there is a need for mechanisms that infer proteins from lists of detected peptides. A weakness of commercially available samples of known content is that they consist of proteins that are deliberately selected for producing tryptic peptides that are unique to a single protein. Unfortunately, such samples do not expose any complications in protein inference. For a realistic benchmark of protein inference procedures, there is, therefore, a need for samples of known content where the present proteins share peptides with known absent proteins. Here, we present such a standard, that is based on E. coli expressed human protein fragments. To illustrate the usage of this standard, we benchmark a set of different protein inference procedures on the data. We observe that inference procedures excluding shared peptides provide more accurate estimates of errors compared to methods that include information from shared peptides, while still giving a reasonable performance in terms of the number of identified proteins. We also demonstrate that using a sample of known protein content without proteins with shared tryptic peptides can give a false sense of accuracy for many protein inference methods.", "tag": "Bioinformatics"}, {"title": "A general method to predict the effect of single amino acid substitutions on enzyme catalytic activity", "url": "https://www.biorxiv.org/content/early/2017/12/19/236265", "abstract": "Over the past thirty years, site-directed mutagenesis has become established as one of the most powerful techniques to probe enzyme reaction mechanisms. Substitutions of active site residues are most likely to yield significant perturbations in kinetic parameters, but there are many examples of profound changes in these values elicited by remote mutations. Ortholog comparisons of extant sequences show that many mutations do not have profound influence on enzyme function. As the number of potential single natural amino acid substitutions that can be introduced in a protein of N amino acids in length by directed mutation is very large (19 * N), it would be useful to have a method to predict which amino acid substitutions are more likely to introduce significant changes in kinetic parameters in order to design meaningful probes into enzyme function. What is especially desirable is the identification of critical residues that do not contact the substrate directly, and may be remote from the active site. We collected literature data reflecting the effects of 2,804 mutations on kinetic properties for 12 enzymes. These data along with characteristic predictors were used in a machine-learning scheme to train a classifier to predict the effect of mutation. Use of this algorithm allows one to predict with a 2.5-fold increase in precision, if a given mutation, made anywhere in the enzyme, will cause a decrease in kcat/Km value of \u2265 95%. The improved precision allows the experimentalist to reduce the number of mutations necessary to probe the enzyme reaction mechanism.", "tag": "Bioinformatics"}, {"title": "SemEHR: A General-purpose Semantic Search System to Surface Semantic Data from Clinical Notes for Tailored Care, Trial Recruitment and Clinical Research", "url": "https://www.biorxiv.org/content/early/2017/12/18/235622", "abstract": "Objective: Unlocking the data contained within both structured and unstructured components of Electronic Health Records (EHRs) has the potential to provide a step change in data available forsecondary research use, generation of actionable medical insights, hospital management and trial recruitment. To achieve this, we implemented SemEHR - a semantic search and analytics, open source tool for EHRs. Methods: SemEHR implements a generic information extraction (IE) and retrieval infrastructure by identifying contextualised mentions of a wide range of biomedical concepts within EHRs. Natural Language Processing (NLP) annotations are further assembled at patient level and extended with EHR-specific knowledge to generate a timeline for each patient. The semantic data is serviced via ontology-based search and analytics interfaces. Results: SemEHR has been deployed to a number of UK hospitals including the Clinical Record Interactive Search (CRIS), an anonymised replica of the EHR of the UK South London and Maudsley (SLaM) NHS Foundation Trust, one of Europes largest providers of mental health services. In two CRIS-based studies, SemEHR achieved 93% (Hepatitis C case) and 99% (HIV case) F-Measure results in identifying true positive patients. At Kings College Hospital in London, as part of the CogStack programme (github.com/cogstack), SemEHR is being used to recruit patients into the UK Dept of Health 100k Genome Project (genomicsengland.co.uk). The validation study suggests that the tool can validate previously recruited cases and is very fast in searching phenotypes - time for recruitment criteria checking reduced from days to minutes. Validated on an open intensive care EHR data - MIMICIII, the vital signs extracted by SemEHR can achieve around 97% accuracy. Conclusion: Results from the multiple case studies demonstrate SemEHR`s efficiency - weeks or months of work can be done within hours or minutes in some cases. SemEHR provides a more comprehensive view of a patient, bringing in more and unexpected insight compared to study-oriented bespoke information extraction systems.", "tag": "Bioinformatics"}, {"title": "Multi-Parametric and Multi-Regional Histogram Analysis of MRI: Revealing Imaging Phenotypes of Glioblastoma Correlated with Patient Survival", "url": "https://www.biorxiv.org/content/early/2017/12/18/235861", "abstract": "Introduction: Glioblastoma is characterized by its remarkable heterogeneity and dismal prognosis. Histogram analysis of quantitative magnetic resonance imaging (MRI) is an important in vivo method to study intratumoral heterogeneity. With large amounts of histogram features generated, integrating these modalities effectively for clinical decision remains a challenge. Methods: A total of 80 patients with supratentorial primary glioblastoma were recruited. All patients received surgery and standard regimen of temozolomide chemoradiotherapy. Diagnosis was confirmed by pathology. Anatomical T2-weighted, T1-weighted post-contrast and FLAIR images, as well as dynamic susceptibility contrast (DSC), diffusion tensor imaging (DTI) and chemical shift imaging were acquired preoperatively using a 3T MRI scanner. DTI-p, DTI-q, relative cerebral blood volume (rCBV), mean transit time (MTT) and relative cerebral blood flow (rCBF) maps were generated. Contrast-enhancing (CE) and non-enhancing (NE) regions of interest were manually delineated. Voxel intensity histograms were constructed from the CE and NE regions independently. Patient clustering was performed by the Multi-View Biological Data Analysis (MVDA) approach. Kaplan-Meier and Cox proportional hazards regression analyses were performed to evaluate the relevance of the patient clustering to survival. The histogram features selected from MVDA approach were evaluated using receiver operator characteristics (ROC) curve analysis. The metabolic signatures of the patient clusters were analyzed by multivoxel MR spectroscopy (MRS). Results: The MVDA approach yielded two final patient clusters, consisting of 53 and 27 patients respectively. The two patient subgroups showed significance for overall survival (p = 0.007, HR = 0.32) and progression-free survival (p < 0.001, HR = 0.33) in multivariate Cox regression analysis. Among the features selected by MVDA, higher mean value of DTI-q in the non-enhancing region contributed to a worse OS (HR = 1.40, p = 0.020) and worse PFS (HR = 1.36, p = 0.031). Multivoxel MRS showed N-acetylaspartate/creatine (NAA/Cr) ratio between the two clusters, both in the CE region (p < 0.001) and NE region (p = 0.013). Glutamate/Cr (Glu/Cr) ratio and glutamate + glutamine/Cr (Glx/Cr) of the cluster 1 was significantly lower than cluster 2 (p = 0.037, and 0.027 respectively) In the NE region. Discussion: This study demonstrated that integrating multi-parametric and multi-regional MRI histogram features may help to stratify patients. The histogram features selected from the proposed approach may be used as potential imaging markers in personalized treatment strategy and response determination.", "tag": "Bioinformatics"}, {"title": "Personal genomics: new concepts for future community data banks", "url": "https://www.biorxiv.org/content/early/2017/12/18/230516.1", "abstract": "A general framework for future genomic data libraries, where individual details are protected while remaining open to investigators, is grounded on two new concepts: i) the genomic data is split into parts whose information content is low enough not to put at stake confidentiality and integrity. Data does not need to be encrypted.ii) management of the bank is organized along a blockchain scheme that would permit free access and permanent control on the traffic of information between the bank and the investigators.", "tag": "Bioinformatics"}, {"title": "Real-time search of all bacterial and viral genomic data", "url": "https://www.biorxiv.org/content/early/2017/12/18/234955", "abstract": "Genome sequencing of pathogens is now ubiquitous in microbiology, and the sequence archives are effectively no longer searchable for arbitrary sequences. Furthermore, the exponential increase of these archives is likely to be further spurred by automated diagnostics. To unlock their use for scientific research and real-time surveillance we have combined knowledge about bacterial genetic variation with ideas used in web-search, to build a DNA search engine for microbial data that can grow incrementally. We indexed the complete global corpus of bacterial and viral whole genome sequence data (447,833 genomes), using four orders of magnitude less storage than previous methods. The method allows future scaling to millions of genomes. This renders the global archive accessible to sequence search, which we demonstrate with three applications: ultra-fast search for resistance genes MCR1-3, analysis of host-range for 2827 plasmids, and quantification of the rise of antibiotic resistance prevalence in the sequence archives.", "tag": "Bioinformatics"}, {"title": "Influence of fecal collection conditions and 16S rRNA gene sequencing protocols at two centers on human gut microbiota analysis", "url": "https://www.biorxiv.org/content/early/2017/12/18/175877", "abstract": "Background: To optimise fecal sampling and analysis yielding reproducible microbiome data, and gain further insight into sources of its variation, we compared different collection conditions and 16S rRNA gene sequencing protocols in two centers. Fecal samples were collected on three sequential days from six healthy adults and placed in commercial collection tubes (OMNIgeneGut OMR-200) at room temperature or in sterile 5 ml screw-top tubes in a home fridge or home freezer for 6-24 h, before transfer at 4C to the laboratory and storage at -80C within 24 hours. Replicate samples were shipped on dry ice to centers in Australia and the USA for DNA extraction and sequencing of the V4 region of the 16S rRNA gene, using different PCR protocols. Sequences were analysed with the QIIME pipeline and Greengenes database at the Australian center and with an in-house pipeline and SILVA database at the USA center. Results: Variation in gut microbiome composition and diversity was dominated by differences between individuals. Minor differences in the abundance of taxa were found between collection-processing methods and day of collection. Larger differences were evident between the two centers, including in the relative abundances of genus Akkermansia, in phylum Verrucomicrobiales, and Bifidobacteria in Actinobacteria. Conclusions: Collection with storage and transport at 4C within 24 h is adequate for 16S rRNA analysis of the gut microbiome. However, variation between sequencing centers suggests that cohort samples should be sequenced by the same method in one center. Differences in handling, shipping and methods of PCR gene amplification and sequence analysis in different centers introduce variation in ways that are not fully understood. These findings are particularly relevant as microbiome studies shift towards larger population-based and multi-center studies.", "tag": "Bioinformatics"}, {"title": "Hi-TOM: a platform for high-throughput tracking of mutations induced by CRISPR/Cas systems", "url": "https://www.biorxiv.org/content/early/2017/12/18/235903", "abstract": "The CRISPR/Cas system has been extensively applied to make precise genetic modifications in various organisms. Despite its importance and widespread use, large-scale mutation screening remains time-consuming, labour-intensive and costly. Here, we describe a cheap, practicable and high-throughput screening strategy that allows parallel screening of 96 * N (N denotes the number of targets) genome- modified sites. The strategy simplified the construction of next-generation sequencing (NGS) library by fixing the second-round PCR primers. We also developed Hi-TOM (available at http://www.hi-tom.net/hi-tom/), an online tool to track the mutations with precise percentage. Hi-TOM does not require cumbersome parameter configuration or additional data analysis; thus, it can be exploited by researchers who are unfamiliar with NGS or bioinformatics. Analysis of the samples from rice and human cells reveals that the Hi-TOM tool has high reliability and sensitivity. The simplicity, convenience and comprehensive output make Hi-TOM particularly suitable for high-throughput identification of all types of mutations induced by CRISPR/Cas systems.", "tag": "Bioinformatics"}, {"title": "Testing hypotheses on population dynamics: A test for the inhomogeneous Poisson point process model", "url": "https://www.biorxiv.org/content/early/2017/12/15/234948", "abstract": "In this paper, a test for hypotheses on population dynamics is presented alongside an implementation of said test for R. The test is based on the assumption that the sample, consisting of points on a time axis, is a realization of a Poisson point process (PPP). There are no restrictions on the shapes of the rate functions that are regulating the PPP, type 2 errors can be calculated and the test is optimal in the sense that it is a uniform most powerful (UMP) test. So for every significance level \u03b1, the presented test has a lower type 2 error than every other test having the same significance level \u03b1. The test is applicable to all models based on PPPs, including models in spatial dimensions. It can be generalized and expanded in different ways, such as testing larger hypotheses, incorporating prior knowledge, and constructing confidence regions that can be used to obtain upper or lower bounds on rate functions.", "tag": "Bioinformatics"}, {"title": "SUPPA2 provides fast, accurate, and uncertainty-aware differential splicing analysis across multiple conditions", "url": "https://www.biorxiv.org/content/early/2017/12/15/086876", "abstract": "Despite the many approaches to study differential splicing from RNA-seq, many challenges remain unsolved, including computing capacity and sequencing depth requirements. Here we present SUPPA2, a new method for differential splicing analysis that addresses these challenges and enables streamlined analysis across multiple conditions taking into account biological variability. Using experimental and simulated data SUPPA2 achieves higher accuracy compared to other methods; especially at low sequencing depth and short read length, with important implications for cost-effective use of RNA-seq for splicing; and was able to identify novel Transformer2-regulated exons. We further analyzed two differentiation series to support the applicability of SUPPA2 beyond binary comparisons. This identified clusters of alternative splicing events enriched in microexons induced during differentiation of bipolar neurons, and a cluster enriched in intron retention events that are present at late stages during erythroblast differentiation. Our data suggest that SUPPA2 is a valuable tool for the robust investigation of the biological complexity of alternative splicing.", "tag": "Bioinformatics"}, {"title": "Integrative inference of subclonal tumour evolution from single-cell and bulk sequencing data", "url": "https://www.biorxiv.org/content/early/2017/12/15/234914", "abstract": "Understanding the evolutionary history and subclonal composition of a tumour represents one of the key challenges in overcoming treatment failure due to resistant cell populations. Most of the current data on tumour genetics stems from short read bulk sequencing data. While this type of data is characterised by low sequencing noise and cost, it consists of aggregate measurements across a large number of cells. It is therefore of limited use for the accurate detection of the distinct cellular populations present in a tumour and the unambiguous inference of their evolutionary relationships. Single-cell DNA sequencing instead provides data of the highest resolution for studying intra-tumour heterogeneity and evolution, but is characterised by higher sequencing costs and elevated noise rates. In this work, we develop the first computational approach that infers trees of tumour evolution from combined single-cell and bulk sequencing data. Using a comprehensive set of simulated data, we show that our approach systematically outperforms existing methods with respect to tree reconstruction accuracy and subclone identification. High fidelity reconstructions are obtained even with a modest number of single cells. We also show that combining single-cell and bulk sequencing data provides more realistic mutation histories for real tumours.", "tag": "Bioinformatics"}, {"title": "Comparative Qualitative Phosphoproteomics Analysis Identifies Shared Phosphorylation Motifs and Associated Biological Processes in Flowering Plants", "url": "https://www.biorxiv.org/content/early/2017/12/15/233668", "abstract": "Phosphorylation is regarded as one of the most prevalent post-translational modifications and plays a key role in regulating cellular processes. In this work we carried out a comparative bioinformatics analysis of phosphoproteomics data, to profile two model species representing the largest subclasses in flowering plants the dicot Arabidopsis thaliana and the monocot Oryza sativa, to understand the extent to which phosphorylation signaling and function is conserved across evolutionary divergent plants. Using pre-existing mass spectrometry phosphoproteomics datasets and bioinformatic tools and resources, we identified 6,537 phosphopeptides from 3,189 phosphoproteins in Arabidopsis and 2,307 phosphopeptides from 1,613 phosphoproteins in rice. The relative abundance ratio of serine, threonine, and tyrosine phosphorylation sites in rice and Arabidopsis were highly similar: 88.3: 11.4: 0.4 and 86.7: 12.8: 0.5, respectively. Tyrosine phosphorylation shows features different from serine and threonine phosphorylation and was found to be more frequent in doubly-phosphorylated peptides in Arabidopsis. We identified phosphorylation sequence motifs in the two species to explore the similarities, finding nineteen pS motifs and two pT motifs that are shared in rice and Arabidopsis; among them are five novel motifs that have not previously been described in both species. The majority of shared motif-containing proteins were mapped to the same biological processes with similar patterns of fold enrichment, indicating high functional conservation. We also identified shared patterns of crosstalk between phosphoserines with motifs pSXpS, pSXXpS and pSXXXpS, where X is any amino acid, in both species indicating this is an evolutionary conserved signaling mechanism in flowering plants. However, our results are suggestive that there is greater co-occurrence of crosstalk between phosphorylation sites in Arabidopsis, and we were able to identify several pairs of motifs that are statistically significantly enriched to co-occur in Arabidopsis proteins, but not in rice.", "tag": "Bioinformatics"}, {"title": "A methodology for unsupervised clustering using iterative pruning to capture fine-scale structure", "url": "https://www.biorxiv.org/content/early/2017/12/15/234989", "abstract": "SNP-based information is used in several existing clustering methods to detect shared genetic ancestry or to identify population substructure. Here, we present a methodology for unsupervised clustering using iterative pruning to capture fine-scale structure called IPCAPS. Our method supports ordinal data which can be applied directly to SNP data to identify fine-scale population structure. We compare our method to existing tools for detecting fine-scale structure via simulations. The simulated data do not take into account haplotype information, therefore all markers are independent. Although haplotypes may be more informative than SNPs, especially in fine-scale detection analyses, the haplotype inference process often remains too computationally intensive. Therefore, our strategy has been to restrict attention to SNPs and to investigate the scale of the structure we are able to detect with them. We show that the experimental results in simulated data can be highly accurate and an improvement to existing tools. We are convinced that our method has a potential to detect fine-scale structure.", "tag": "Bioinformatics"}, {"title": "Differential Proportionality - A Normalization-Free Approach To Differential Gene Expression", "url": "https://www.biorxiv.org/content/early/2017/12/15/134536", "abstract": "Gene expression data, such as those generated by next generation sequencing technologies (RNA-seq), are of an inherently relative nature: the total number of sequenced reads has no biological meaning. This issue is most often addressed with various normalization techniques which all face the same problem: once information about the total mRNA content of the origin cells is lost, it cannot be recovered by mere technical means. Additional knowledge, in the form of an unchanged reference, is necessary; however, this reference can usually only be estimated. Here we propose a novel method where sample normalization is unnecessary, but important insights can be obtained nevertheless. Instead of trying to recover absolute abundances, our method is entirely based on ratios, so normalization factors cancel by default. Although the differential expression of individual genes cannot be recovered this way, the ratios themselves can be differentially expressed (even when their constituents are not). Yet, most current analyses are blind to these cases, while our approach reveals them directly. Specifically, we show how the differential expression of gene ratios can be formalized by decomposing log-ratio variance (LRV) and deriving intuitive statistics from it. Although small LRVs have been used to detect proportional genes in gene expression data before, we focus here on the change in proportionality factors between groups of samples (e.g. tissue-specific proportionality). For this, we propose a statistic that is equivalent to the squared t-statistic of one-way ANOVA, but for gene ratios. In doing so, we show how precision weights can be incorporated to account for the peculiarities of count data, and, moreover, how a moderated statistic can be derived in the same way as the one following from a hierarchical model for individual genes. We also discuss approaches to deal with zero counts, deriving an expression of our statistic that is able to incorporate them. In providing a detailed analysis of the connections between the differential expression of genes and the differential proportionality of pairs, we facilitate a clear interpretation of new concepts. The proposed framework is applied to a data set from GTEx consisting of 98 samples from the cerebellum and cortex, with selected examples shown. A computationally efficient implementation of the approach in R has been released as an addendum to the propr package.", "tag": "Bioinformatics"}, {"title": "Network perturbation analysis of gene transcriptional profiles reveals protein targets and mechanism of action of drugs and influenza A viral infection", "url": "https://www.biorxiv.org/content/early/2017/12/15/175364", "abstract": "Genome-wide transcriptional profiling provides a global view of cellular state and how this state changes under different treatments (e.g. drugs) or conditions (e.g. healthy and diseased). Here, we present ProTINA (Protein Target Inference by Network Analysis), a network perturbation analysis method for inferring protein targets of compounds from gene transcriptional profiles. ProTINA uses a dynamic model of the cell-type specific protein-gene transcriptional regulation to infer network perturbations from steady state and time-series differential gene expression profiles. A candidate protein target is scored based on the gene network's dysregulation, including enhancement and attenuation of transcriptional regulatory activity of the protein on its downstream genes, caused by drug treatments. For benchmark datasets from three drug treatment studies, ProTINA was able to provide highly accurate protein target predictions and to reveal the mechanism of action of compounds with high sensitivity and specificity. Further, an application of ProTINA to gene expression profiles of influenza A viral infection led to new insights of the early events in the infection.", "tag": "Bioinformatics"}, {"title": "Reliable detection of translational regulation with Ribo-seq", "url": "https://www.biorxiv.org/content/early/2017/12/15/234344", "abstract": "Ribosome profiling (Ribo-Seq) reveals genome-wide translation rates via the quantification of ribosome protected fragments (RPFs) of mRNAs. Several methods have recently been developed to detect differentially translated genes (DTGs) using Ribo-seq: Xtail, Ribodiff and Riborex. At their core, all of these approaches either utilize existing differential expression programs or use similar statistical assumptions to model the data. However, none of them allow for complex experimental design or the use of alternative statistical setups and crucially, they do not allow for correction of any batch effects. We tailored the open design of a well established tool, DEseq2 to identify DTGs directly which can then also be extended to accommodate covariates and other experimental setups, making it a more suitable tool for identifying DTGs. We performed a comprehensive benchmarking analysis on simulated and primary human fibroblast dataset and show that this approach outperforms all the other methods in presence of a batch effect. With increasing batch effect, the sensitivity of DESeq drops by 22.7%, whereas all other methods drop by greater than 80%, making them substantially less reliable. Since almost all high-throughput sequencing datasets contain batch effects, particularly heterogeneous samples such as human tissues or primary cells, DESeq2-based analysis of differential translation will deliver the most reliable results.", "tag": "Bioinformatics"}, {"title": "JASPAR RESTful API: accessing JASPAR data from any programming language", "url": "https://www.biorxiv.org/content/early/2017/12/14/160184", "abstract": "JASPAR is a widely used open-access database of curated, non-redundant transcription factor binding profiles. Currently, data from JASPAR can be retrieved as flat files or by using programming language-specific interfaces. Here, we present a programming language-independent application programming interface (API) to access JASPAR data using the Representational State Transfer (REST) architecture. The REST API enables programmatic access to JASPAR by most programming languages and returns data in seven widely used formats. Further, it provides an endpoint to infer the TF binding profile(s) likely bound by a given DNA binding domain protein sequence. Additionally, it provides an interactive browsable interface for bioinformatics tool developers. The REST API is implemented in Python using the Django REST Framework. It is accessible at http://jaspar.genereg.net/api/ and the source code is freely available at https://bitbucket.org/CBGR/jaspar under GPL v3 license.", "tag": "Bioinformatics"}, {"title": "Host_microbe_mapper allows to analyse and interpret the expression of dual RNA-seq measurements and reveals potential microbial contaminations in the data", "url": "https://www.biorxiv.org/content/early/2017/12/14/234278", "abstract": "Next-generation sequencing technologies provide a wealth of sequencing data. To handle these data amounts, various tools were developed over the last years for mapping, normalisation and functional analyses. To support researchers in the interpretation of expression measurements originating from dual RNA-seq studies and from host-microbe systems in particular, the computational pipeline host_microbe_mapper can be applied to quantify and interpret dual RNA-seq datasets in host-microbe experiments. The pipeline with all the required scripts is stored at the Github repository (https://github.com/nthomasCUBE/host_microbe_mapper).", "tag": "Bioinformatics"}, {"title": "iterativeWGCNA: iterative refinement to improve module detection from WGCNA co-expression networks", "url": "https://www.biorxiv.org/content/early/2017/12/14/234062", "abstract": "Weighted-gene correlation network analysis (WGCNA) is frequently used to identify highly co-expressed clusters of genes (modules) within whole-transcriptome datasets. However, transcriptome-scale networks tend to be highly connected, making it challenging for the hierarchical clustering underlying the WGCNA-based classification to discriminate coherently expressed gene sets without significant information loss from either a priori filtering of the expression dataset or a posteriori pruning of the cluster dendrogram. Here we present iterativeWGCNA, a Python-wrapped extension for the WGCNA R software package that improves the robustness of detected modules and minimizes information loss. The method works by pruning poorly fitting genes from estimated modules and then rerunning WGCNA to refine gene clusters. After refining, pruned genes are assembled into a new expression dataset to isolate overlapping modules and the process repeated. In doing so, iterativeWGCNA provides an unsupervised, non-biased filtering to generate a robust, comprehensive network-based classification of whole-transcriptome expression datasets.", "tag": "Bioinformatics"}, {"title": "CLAN: the CrossLinked reads ANalysis tool", "url": "https://www.biorxiv.org/content/early/2017/12/14/233841", "abstract": "The crosslinked RNA sequencing technology ligates interacting RNA strands followed by next-generation sequencing. Mapping of the resulting duplex reads allows for functional inference of the corresponding intramolecular/intermolecular RNA-RNA interactions. However, duplex read mapping remains computationally challenging, and the existing best-performing software fails to map a significant portion of the duplex reads. To address this challenge, we develop a novel algorithm for duplex read mapping, called CrossLinked reads ANalysis tool (CLAN). CLAN demonstrates drastically improved sensitivity and high alignment accuracy when applied to real crosslinked RNA sequencing data. CLAN is implemented in GNU C++, and is freely available from https://sourceforge.net/projects/clan-mapping.", "tag": "Bioinformatics"}, {"title": "AntAngioCOOL: An R Package for Computational Detection of Anti-Angiogenic Peptides", "url": "https://www.biorxiv.org/content/early/2017/12/14/233601", "abstract": "Angiogenesis inhibition research is a cutting edge in angiogenesis-dependent disease therapy, and especially in cancer therapy. Recently, studies on anti-angiogenic peptides have provided promising results in the cancer treatment field. In the current study we propose an effective machine learning based R package (AntAngioCOOL) to predict anti-angiogenic peptides. We have examined more than 200 different classifiers to build an efficient predictor. Also, more than 17000 features have been extracted to encode the peptides. However, finally, more than 2000 informative features have been selected to train the classifiers. According to the obtained results AntAngioCOOL can effectively predict anti-angiogenic peptides: this tool achieved sensitivity of 88%, specificity of 77% and accuracy of 75% on independent test set. AntAngioCOOL can be accessed at https://cran.r-project.org/.", "tag": "Bioinformatics"}, {"title": "netSmooth: Network-smoothing based imputation for single cell RNA-seq", "url": "https://www.biorxiv.org/content/early/2017/12/13/234021", "abstract": "Single cell RNA-seq (scRNA-seq) experiments suffer from a range of characteristic technical biases, such as dropouts (zero or near zero counts) and high variance. Current analysis methods rely on imputing missing values by various means of local averaging or regression, often amplifying biases inherent in the data. We present netSmooth, a network-diffusion based method that uses priors for the covariance structure of gene expression profiles on scRNA-seq experiments in order to smooth expression values. We demonstrate that netSmooth improves clustering results of scRNA-seq experiments from distinct cell populations, time-course experiments, and cancer genomics. We provide an R package for our method, available at: https://github.com/BIMSBbioinfo/netSmooth.", "tag": "Bioinformatics"}, {"title": "nQuire: A Statistical Framework For Ploidy Estimation Using Next Generation Sequencing", "url": "https://www.biorxiv.org/content/early/2017/12/13/143537", "abstract": "Intraspecific variation in ploidy occurs in a wide range of species including pathogenic and nonpathogenic eukaryotes such as yeasts and oomycetes. Ploidy can be inferred indirectly - without measuring DNA content - from experiments using next-generation sequencing (NGS). We present nQuire, a statistical framework that distinguishes between diploids, triploids and tetraploids using NGS. The command-line tool models the distribution of base frequencies at variable sites using a Gaussian Mixture Model, and uses maximum likelihood to select the most plausible ploidy model. nQuire handles large genomes at high coverage efficiently and uses standard input file formats. We demonstrate the utility of nQuire analyzing individual samples of the pathogenic oomycete Phytophthora infestans and the Baker's yeast Saccharomyces cerevisiae. Using these organisms we show the dependence between reliability of the ploidy assignment and sequencing depth. Additionally, we employ normalized maximized log-likelihoods generated by nQuire to ascertain ploidy level in a population of samples with ploidy heterogeneity. Using these normalized values we cluster samples in three dimensions using multivariate Gaussian mixtures. The cluster assignments retrieved from a S. cerevisiae population recovered the true ploidy level in over 96% of samples. Finally, we show that nQuire can be used regionally to identify chromosomal aneuploidies. nQuire provides a statistical framework to study organisms with intraspecific variation in ploidy. nQuire is likely to be useful in epidemiological studies of pathogens, artificial selection experiments, and for historical or ancient samples where intact nuclei are not preserved. It is implemented as a stand-alone Linux command line tool in the C programming language and is available at github.com/clwgg/nQuire under the MIT license.", "tag": "Bioinformatics"}, {"title": "OCR-Stats: Robust estimation and statistical testing of mitochondrial respiration activities using Seahorse XF Analyzer", "url": "https://www.biorxiv.org/content/early/2017/12/13/231522", "abstract": "Accurate quantification of cellular and mitochondrial bioenergetic activity is of great interest in many medical and biological areas. Mitochondrial stress experiments performed with Seahorse Bioscience XF Analyzers allow estimating 6 bioenergetics measures by monitoring oxygen consumption rates (OCR) of living cells in multi-well plates. However, detailed statistical analyses of OCR measurements from XF Analyzers have been lacking so far. Here, we performed 126 mitochondrial stress experiments involving 203 fibroblast cell lines to understand how OCR behaves across different biosamples, wells, and plates; which allowed us to statistically model OCR behavior over time. We show that the noise of OCR is multiplicative and that outlier data points can concern individual measurements or all measurements of a well. Based on these insights, we developed a novel statistical method, OCR-Stats, that: i) models multiplicative noise, ii) automatically identifies outlier data points and outlier wells, and iii) takes into account replicates both within and between plates. This led to a significant reduction of the coefficient of variation across experiments of basal respiration by 36% (P = 0.004), and of maximal respiration by 32% (P = 0.023). Also, we propose an optimal experimental design with a minimum number of well replicates needed to obtain confident results. Finally, we use statistical testing taking into account the inter-plate variation to compare the bioenergetics measures of two samples.", "tag": "Bioinformatics"}, {"title": "Hercules: a profile HMM-based hybrid error correction algorithm for long reads", "url": "https://www.biorxiv.org/content/early/2017/12/13/233080", "abstract": "Motivation: Choosing whether to use second or third generation sequencing platforms can lead to trade-offs between accuracy and read length. Several studies require long and accurate reads including de novo assembly, fusion and structural variation detection. In such cases researchers often combine both technologies and the more erroneous long reads are corrected using the short reads. Current approaches rely on various graph based alignment techniques and do not take the error profile of the underlying technology into account. Memory- and time- efficient machine learning algorithms that address these shortcomings have the potential to achieve better and more accurate integration of these two technologies. Results: We designed and developed Hercules, the first machine learning-based long read error correction algorithm. The algorithm models every long read as a profile Hidden Markov Model with respect to the underlying platform's error profile. The algorithm learns a posterior transition/emission probability distribution for each long read and uses this to correct errors in these reads. Using datasets from two DNA-seq BAC clones (CH17-157L1 and CH17-227A2), and human brain cerebellum polyA RNA-seq, we show that Hercules-corrected reads have the highest mapping rate among all competing algorithms and highest accuracy when most of the basepairs of a long read are covered with short reads. Availability: Hercules source code is available at https://github.com/BilkentCompGen/Hercules", "tag": "Bioinformatics"}, {"title": "HebbPlot: An intelligent tool for learning and visualizing chromatin mark signatures", "url": "https://www.biorxiv.org/content/early/2017/12/12/207670", "abstract": "Histone modifications play important roles in gene regulation, heredity, imprinting, and many human diseases. The histone code is complex, consisting of about 100 marks. Biologists need computational tools for characterizing general signatures representing the distributions of tens of chromatin marks around thousands of regions. To this end, we developed a software tool called HebbPlot, which utilizes a Hebbian neural network to learn such signatures. HebbPlot presents a signature as a digitized image, which can be easily interpreted. We validated HebbPlot in six case studies. HebbPlot is applicable to a wide array of studies, facilitating the deciphering of the histone code.", "tag": "Bioinformatics"}, {"title": "Relative evolutionary rate inference in HyPhy with LEISR", "url": "https://www.biorxiv.org/content/early/2017/12/12/206011", "abstract": "We introduce LEISR (Likehood Estimation of Individual Site Rates, pronounced \"laser\"), a tool to infer relative evolutionary rates from protein and nucleotide data, implemented in HyPhy. LEISR is based on the popular Rate4Site (Pupko et al., 2002) approach for inferring relative site-wise evolutionary rates, primarily from protein data. We extend the original method for more general use in several key ways: i) We increase the support for nucleotide data with additional models, ii) We allow for datasets of arbitrary size, iii) We support analysis of site-partitioned datasets to correct for the presence of recombination breakpoints, and iv) We implemented LEISR as MPI-enabled to support rapid, high-throughput analysis. LEISR is available in HyPhy starting with version 2.3.8.", "tag": "Bioinformatics"}, {"title": "GEMBS \u2014 high through-put processing for DNA methylation data from Whole Genome Bisulfite Sequencing (WGBS)", "url": "https://www.biorxiv.org/content/early/2017/12/12/201988", "abstract": "DNA methylation is essential for normal embryogenesis and development in mammals. Currently, whole genome sequencing of bisulfite converted DNA (WGBS) represents the gold standard for studying DNA methylation at genomic level. Contrary to other techniques, it provides an unbiased view of the entire genome at single base pair resolution. However, in practice, due to its (until recently) comparatively high cost, its application for the analysis of large data sets (i.e. > 50 samples) has been lagging behind other more cost-efficient platforms, such as for example the Illumina microarrays (Infinium 27K, 450k and EPIC). Subsequently, despite the variety of software tools that exist for the analysis of WGBS, processing of large datasets still remains cumbersome. We present GEMBS, a bioinformatics pipeline specifically designed for the analysis of large WGBS data sets. GEMBS is based on two core modules: GEM3, a high performance read aligner, and BScall, a variant caller specifically for bisulfite sequencing data. Both components are embedded in a highly parallel workflow enabling highly efficient and reliable execution in a HPC environment. In this study, we benchmark GEMBS performance against other common analysis tools and show how GEMBS can be used for accurate variant calling from WGBS data.", "tag": "Bioinformatics"}, {"title": "clusterTools: Proximity Searches For Functional Elements To Identify Putative Biosynthetic Gene Clusters", "url": "https://www.biorxiv.org/content/early/2017/12/12/119214", "abstract": "Motivation: The low cost of DNA sequencing has accelerated research in natural product biosynthesis allowing us to rapidly link small molecules to the clusters that produce them. However, the large amount of data means that the number of putative biosynthetic gene clusters (BGCs) far exceeds our ability to experimentally characterize them. This necessitates the need for development of further tools to analyze putative BGCs to flag those of interest for further characterization. Results: clusterTools implements a framework to aid in the characterization of putative BGCs. It does this by or-ganizing genomic information on coding sequences in a way that enables directed, hypothesis-driven queries for functional elements in close physical proximity of each other. Genomic sequence databases can be constructed in clusterTools with an interface to the NCBI Genbank and Genomes databases, or from private sequence databases. clusterTools can be used either to identify interesting BGCs from a database of putative BGCs, or on databases of genomic sequences to identify and download regions of interest in the DNA for further processing and annotation in programs such as antiSMASH. We have used clusterTools to identify putative and known biosynthetic gene clus-ters involved in bacterial polyketide alkaoloid and tetronate biosynthesis. Availability and Implementation: clusterTools is implemented in Python and is available via the AGPL. Stand-alone versions of clusterTools are available for Macintosh, Windows, and Linux upon registration (https://goo.gl/forms/QRKTkpqiA0g31IWp1). The source-code is available at https://www.github.com/emzodls/clusterArch.", "tag": "Bioinformatics"}, {"title": "Identifying drug-gene interactions from CRISPR knockout screens with drugZ", "url": "https://www.biorxiv.org/content/early/2017/12/12/232736", "abstract": "Chemogenetic profiling enables the identification of gene mutations that enhance or suppress the activity of small molecules. This knowledge provides insights into drug mechanism-of-action, genetic vulnerabilities, and resistance mechanisms, all of which may help stratify patient populations. We present drugZ, an algorithm for identifying both synergistic and suppressor chemogenetic interactions from highly sensitive CRISPR screens, available at github.com/hart-lab/drugz. In screens for interactions with a poly(ADP-ribose) polymerase (PARP) inhibitor, DrugZ identifies a greater fraction of the homologous recombination repair pathway than contemporary methods, and confirms KEAP1 loss as a resistance factor for ERK inhibitors.", "tag": "Bioinformatics"}, {"title": "Pathway enrichment analysis of -omics data", "url": "https://www.biorxiv.org/content/early/2017/12/12/232835", "abstract": "Pathway enrichment analysis helps gain mechanistic insight into large gene lists typically resulting from genome scale (-omics) experiments. It identifies biological pathways that are enriched in the gene list more than expected by chance. We explain pathway enrichment analysis and present a practical step-by-step guide to help interpret gene lists resulting from RNA-seq and genome sequencing experiments. The protocol comprises three major steps: define a gene list from genome scale data, determine statistically enriched pathways, and visualize and interpret the results. We focus on differentially expressed genes and mutated cancer genes, however the described principles can be applied to diverse -omics data. The protocol is designed for biologists with no prior bioinformatics training and uses freely available software including g:Profiler, GSEA, Cytoscape and Enrichment Map.", "tag": "Bioinformatics"}, {"title": "Comparative Annotation Toolkit (CAT) - simultaneous clade and personal genome annotation", "url": "https://www.biorxiv.org/content/early/2017/12/11/231118", "abstract": "The recent introductions of low-cost, long-read, and read-cloud sequencing technologies coupled with intense efforts to develop efficient algorithms have made affordable, high-quality de novo sequence assembly a realistic proposition. The result is an explosion of new, ultra-contiguous genome assemblies. To compare these genomes we need robust methods for genome annotation. We describe the fully open source Comparative Annotation Toolkit (CAT), which provides a flexible way to simultaneously annotate entire clades and identify orthology relationships. We show that CAT can be used to improve annotations on the rat genome, annotate the great apes, annotate a diverse set of mammals, and annotate personal, diploid human genomes. We demonstrate the resulting discovery of novel genes, isoforms and structural variants, even in genomes as well studied as rat and great ape, and how these annotations improve cross-species RNA expression experiments.", "tag": "Bioinformatics"}, {"title": "BLAST-based validation of metagenomic sequence assignments", "url": "https://www.biorxiv.org/content/early/2017/12/11/181636", "abstract": "When performing bioforensic casework, it is important to be able to reliably detect the presence of a particular organism in a metagenomic sample, even if the organism is only present in a trace amount. For this task, it is common to use a sequence classification program that determines the taxonomic affiliation of individual sequence reads by comparing them to reference database sequences. As metagenomic data sets often consist of millions or billions of reads that need to be compared to reference databases containing millions of sequences, such sequence classification programs typically use search heuristics and databases with reduced sequence diversity to speed up the analysis, which can lead to incorrect assignments. Thus, in a bioforensic setting where correct assignments are paramount, assignments of interest made by \"first-pass\" classifiers should be confirmed using the most precise methods and comprehensive databases available. In this study we present a BLAST-based method for validating the assignments made by less precise sequence classification programs, with optimal parameters for filtering of BLAST results determined via simulation of sequence reads from genomes of interest, and we apply the method to the detection of four pathogenic organisms. The software implementing the method is open source and freely available.", "tag": "Bioinformatics"}, {"title": "GDCRNATools: an R/Bioconductor package for integrative analysis of lncRNA, miRNA, and mRNA data in GDC", "url": "https://www.biorxiv.org/content/early/2017/12/11/229799", "abstract": "The large-scale multidimensional omics data in the Genomic Data Commons (GDC) provides opportunities to investigate the crosstalk among different RNA species and their regulatory mechanisms in cancers. Easy-to-use bioinformatics pipelines are needed to facilitate such studies. We have developed a user-friendly R/Bioconductor package, named GDCRNATools, to facilitate downloading, organizing, and analyzing RNA data in GDC with an emphasis on deciphering the lncRNA-mRNA related competing endogenous RNAs (ceRNAs) regulatory network in cancers. Many widely used bioinformatics tools and databases are utilized in our package. Users can easily pack preferred downstream analysis pipelines or integrate their own pipelines into the workflow. Interactive shiny web apps built in GDCRNATools greatly improve visualization of results from the analysis. GDCRNATools is an R/Bioconductor package that is freely available at https://github.com/Jialab-UCR/GDCRNATools", "tag": "Bioinformatics"}, {"title": "Prometheus: omics portals for interkingdom comparative genomic analyses", "url": "https://www.biorxiv.org/content/early/2017/12/11/232298", "abstract": "Functional analyses of genes are crucial for unveiling biological responses, for genetic engineering, and for developing new medicines. However, functional analyses have largely been restricted to model organisms, representing a major hurdle for functional studies and industrial applications. To resolve this, comparative genome analyses can be used to provide clues to gene functions as well as their evolutionary history. To this end, we present Prometheus (http://prometheus.kobic.re.kr), web-based omics portal that contains more than 17,215 sequences from prokaryotic and eukaryotic genomes. This portal supports interkingdom comparative analyses via a domain architecture-based gene identification system, Gene Search, and users can easily and rapidly identify single or entire gene sets in specific pathways. Bioinformatics tools for further analyses are provided in Prometheus or through BioExpress, a cloud-based bioinformatics analysis platform. Prometheus suggests a new paradigm for comparative analyses with large amounts of genomic information.", "tag": "Bioinformatics"}, {"title": "Altered transcription factor binding events predict personalized gene expression and confer insight into functional cis-regulatory variants", "url": "https://www.biorxiv.org/content/early/2017/12/11/228155", "abstract": "Deciphering the functional roles of cis-regulatory variants is a critical challenge in genome analysis and interpretation. We hypothesize that altered transcription factor (TF) binding events are a central mechanism by which cis-regulatory variants impact gene expression. We present TF2Exp, the first gene-based framework (to our knowledge) to predict the impact of altered TF binding on personalized gene expression based on cis-regulatory variants. Using data from lymphoblastoid cell lines, TF2Exp models achieved suitable performance for 3,060 genes. Alterations within DNase I hypersensitive, CTCF-bound, and tissue-specific TF-bound regions were the greatest contributors to the models. Our cis-regulatory variant-based TF2Exp models performed as well as the state-of-the-art SNP-based models, both in cross-validation and external validation. In addition, unlike SNP-based models, our TF2Exp models have the unique advantages to evaluate impact of uncommon variants and distinguish the functional roles of variants in linkage disequilibrium, showing broader utility for future human genetic studies.", "tag": "Bioinformatics"}, {"title": "Large-scale structural variation detection in subterranean clover subtypes using optical mapping validated at nucleotide level", "url": "https://www.biorxiv.org/content/early/2017/12/11/232132", "abstract": "Whole genome sequencing has been widely used to detect structural variations (SVs). However, the limited single molecule size makes it difficult to characterize large-scale SVs in a genome because they cannot fully cover such vast and complex regions. Recently, optical mapping in nanochannels has provided novel resolution to detect large-scale SVs by comparing the physical location of the nickase recognition sequence in genomes. Other than in humans, SVs discovered in plants by optical mapping have not been validated. To assess the accuracy of SV calling in plants by optical mapping, we selected two genetically diverse subspecies of the Trifolium model species, subterranean clover cvs. Daliak and Yarloop. The SVs discovered by BioNano optical mapping (BOM) were validated using Illumina short reads. In the analysis, BOM identified 12 large-scale regions containing deletions and 19 containing insertions in Yarloop. The 12 large-scale regions contained 71 small deletions when validated by Illumina short reads. The results suggest that BOM could detect the total size of deletions and insertions, but it could not precisely report the location and actual quantity of SVs in the genome. Nucleotide-level validation is crucial to confirm and characterize SVs reported by optical mapping. The accuracy of SV detection by BOM is highly dependent on the quality of reference genomes and the density of selected nickases.", "tag": "Bioinformatics"}, {"title": "Computer-aided prediction of antigen presenting cell modulators for designing peptide-based vaccine adjuvants", "url": "https://www.biorxiv.org/content/early/2017/12/11/232025", "abstract": "Background: Evidences in literature strongly advocate the potential of immunomodulatory peptides for use as vaccine adjuvants. All the mechanisms of vaccine adjuvants ensuing immunostimulatory effects directly or indirectly stimulate Antigen Presenting Cells (APCs). While numerous methods have been developed in the past for predicting B-cell and T-cell epitopes; no method is available for predicting the peptides that can modulate the APCs. Methods: We named the peptides that can activate APCs as A-cell epitopes and developed methods for their prediction in this study. A dataset of experimentally validated A-cell epitopes was collected and compiled from various resources. To predict A-cell epitopes, we developed Support Vector Machine-based machine learning models using different sequence-based features. Results: A hybrid model developed on a combination of sequence-based features (dipeptide composition and motif occurrence), achieved the highest accuracy of 96.91% with Matthews Correlation Coefficient (MCC) value of 0.94 on the training dataset. We also evaluated the hybrid models on an independent dataset and achieved a comparable accuracy of 94.93% with MCC 0.90. Conclusion: The models developed in this study were implemented in a web-based platform VaxinPAD to predict and design immunomodulatory peptides or A-cell epitopes. This web server available at http://webs.iiitd.edu.in/raghava/vaxinpad/ and http://crdd.osdd.net/raghava/vaxinpad/ will facilitate researchers in designing peptide-based vaccine adjuvants.", "tag": "Bioinformatics"}, {"title": "Global optimization approach for circular and chloroplast genome assembly", "url": "https://www.biorxiv.org/content/early/2017/12/11/231324", "abstract": "We describe a global optimization approach for genome assembly where the steps of scaffolding, gap-filling, and scaffold extension are simultaneously solved in the framework of a common objective function. The approach is based on integer programming model for solving genome scaffolding as a problem of finding a long simple path in a specific graph that satisfies additional constraints encoding the insert-size information. The optimal solution of this problem allows one to obtain new kind of contigs that we call distance-based contig. We test the algorithm on a benchmark of chloroplasts and compare the quality of the results with recent scaffolders.", "tag": "Bioinformatics"}, {"title": "afpCOOL: An Accurate Tool for Antifreeze Protein Detection", "url": "https://www.biorxiv.org/content/early/2017/12/11/231761", "abstract": "Various cold-adapted organisms produce antifreeze proteins (AFPs), which prevent to freeze of cell fluids by resisting the growth of the ice crystal. AFPs are currently being recognized in various organisms that are living in extremely low temperatures. AFPs have several important applications in increasing freeze tolerance of plants; maintain the tissue in frozen conditions and producing cold-hardy plants using transgenic technology. Substantial differences in the sequence and structure of the AFPs, pose a challenge for researcher to identify these proteins. In this paper, we proposed a novel method for identifying AFPs using support vector machine (SVM) by incorporating 4 types of features. Results on two benchmark datasets revealed the strength of the proposed method in AFP prediction. Also, according to the results on an independent test set, our method outperformed the current state-of-the-art methods. The further analysis showed the non-satisfactory performance of the BLAST in AFP detection: more than 62% of the BLAST searches have specificity less than 10% and there is no any BLAST search with sensitivity higher than 10%. These results reveal the urgent need for an accurate tool for AFP detection. In addition, the comparison results of the discrimination power of different feature types disclosed that evolutionary features and amino acid composition are the most contributing features in AFP detection. This method has been implemented as a stand-alone tool, namely afpCOOL, for various operating systems to predict AFPs with a user friendly graphical interface. Availability: afpCOOL is freely available at http://bioinf.modares.ac.ir:8080/AFPCOOL/page/afpcool.jsp", "tag": "Bioinformatics"}, {"title": "Cysteine proteases of human hookworm Necator Americanus as virulence factors and implications for drug design with anti-heparin and heparin analogs: A bioinformatics study", "url": "https://www.biorxiv.org/content/early/2017/12/10/116921", "abstract": "Human hookworm Necator Americanus (NA) causes iron deficiency anemia, as the parasite ingests blood from the gastrointestinal tract of its human host. This bioinformatics-based study focuses on eight of the cathepsin B-like cysteine proteases (CPs) of the worm to explore their pathogenic potential. CP1 - CP6, which harbored the active site cysteine residue for enzymatic activity, were relevantly observed to have N-terminal signal peptide for extracellular localization. The secretory CPs could be releasing indigenous worm heparin at the host-pathogen interface for anticoagulation purposes. CP2 and CP3 showed a novel hemoglobinase motif that could be a prerequisite for hemoglobin degradation. CP1 and CP6 shared similar enzymatic-pocket features with cathepsin B and cruzain that cleave high molecular weight kininogen for blood-thinning activity. CP1, CP2, CP3, CP5 and CP6 were predicted to bind heparin, at their C terminal domain, like human cathepsin B and cruzain non-covalently bind heparin to enhance their activity. The actions of the NA CPs in concert with heparin, have implications for anti-heparin and heparin analog design against hookworm infection.", "tag": "Bioinformatics"}, {"title": "Statistical Approaches to Decreasing the Discrepancy of Non-detects in qPCR Data", "url": "https://www.biorxiv.org/content/early/2017/12/08/231621", "abstract": "Quantitative real-time PCR (qPCR) is one of the most widely used methods to measure gene expression. Despite extensive research in qPCR laboratory protocols, normalization, and statistical analysis, little attention has been given to qPCR non-detects -- those reactions failing to produce a minimum amount of signal. While most current software replaces these non-detects with a value representing the limit of detection, recent work suggests that this introduces substantial bias in estimation of both absolute and differential expression. Recently developed single imputation procedures, while better than previously used methods, underestimate residual variance, which can lead to anti-conservative inference. We propose to treat non-detects as non-random missing data, model the missing data mechanism, and use this model to impute missing values or obtain direct estimates of relevant model parameters. To account for the uncertainty inherent in the imputation, we propose a multiple imputation procedure, which provides a set of plausible values for each non-detect. In the proposed modeling framework, there are three sources of uncertainty: parameter estimation, the missing data mechanism, and measurement error. All three sources of variability are incorporated in the multiple imputation and direct estimation algorithms. We demonstrate the applicability of these methods on three real qPCR data sets and perform an extensive simulation study to assess model sensitivity to misspecification of the missing data mechanism, to the number of replicates within the sample, and to the overall size of the data set. The proposed methods result in unbiased estimates of the model parameters; therefore, these approaches may be beneficial when estimating both absolute and differential gene expression. The developed methods are implemented in the R/Bioconductor package nondetects. The statistical methods introduced here reduce discrepancies in gene expression values derived from qPCR experiments, providing more confidence in generating scientific hypotheses and performing downstream analysis.", "tag": "Bioinformatics"}, {"title": "Linear models enable powerful differential activity analysis in massively parallel reporter assays", "url": "https://www.biorxiv.org/content/early/2017/12/08/196394", "abstract": "Massively parallel reporter assays (MPRAs) have emerged as a popular means for understanding noncoding variation in a variety of conditions. However, development of statistical analysis methods has not kept pace with the use of this assay. We present a linear model framework, mpralm, for the differential analysis of activity measures from these experiments that we show is calibrated and powerful. We show that it outperforms statistical tests that are commonly used in the literature, in the first comprehensive evaluation of statistical methods on several datasets. We investigate the theoretical and real-data properties of barcode summarization methods, and show an unappreciated impact of summarization method for some datasets. Finally, we perform a power analysis and show substantial improvements in power by performing up to 6 replicates per condition, whereas sequencing depth has limited impact; we recommend to always use at least 4 replicates. These results inform recommendations for differential analysis, general group comparisons, and power analysis. Our contributions in investigating the functional dependence of statistical power on sample sizes and sequencing depth will help MPRA practitioners make informed choices in study design, and lead to improved inference.", "tag": "Bioinformatics"}, {"title": "Defining a landscape of molecular phenotypes using a simple single sample scoring method", "url": "https://www.biorxiv.org/content/early/2017/12/08/231217", "abstract": "Background: Gene set scoring provides a useful approach for quantifying concordance between sample transcriptomes and selected molecular signatures. Most methods use information from all samples to score an individual sample, leading to unstable scores in small data sets and introducing biases from sample composition across a data set (e.g. varying numbers of samples for different cancer subtypes). To address these issues we have developed a truly single sample scoring method, and associated R/Bioconductor package singscore. Results: We have developed a rank-based single sample scoring method, implemented as a Bioconductor package. We use multiple cancer data sets to compare it against widely-used scoring methods, including GSVA, z-scores, PLAGE, and ssGSEA. Our approach does not depend upon background samples and thus the scores are stable regardless of the composition and number of samples in the gene expression data set. In contrast, scores obtained by GSVA, z-score, PLAGE and ssGSEA can be unstable when less data are available (nSamples < 25). We show that the computational time for singscore is faster than current implementations of GSVA and ssGSEA, and is comparable with that of z-score and PLAGE. The singscore package also produces visualisations and interactive plots that enable exploration of molecular phenotypes. Conclusions: The single sample scoring method described here is independent of sample composition in gene expression data and thus it provides stable scores that are less likely to be influenced by unwanted variation across samples. These scores can be used for dimensional reduction of transcriptomic data and the phenotypic landscapes obtained by scoring samples against multiple molecular signatures may provide insights for sample stratification.", "tag": "Bioinformatics"}, {"title": "ProGeM: A framework for the prioritisation of candidate causal genes at molecular quantitative trait loci", "url": "https://www.biorxiv.org/content/early/2017/12/08/230094", "abstract": "Quantitative trait locus (QTL) mapping of molecular phenotypes such as metabolites, lipids and proteins through genome-wide association studies (GWAS) represents a powerful means of highlighting molecular mechanisms relevant to human diseases. However, a major challenge of this approach is to identify the causal gene(s) at the observed QTLs. Here we present an analysis framework for the 'Prioritisation of candidate causal Genes at Molecular QTLs' (ProGeM), which incorporates biological domain-specific annotation data alongside genome annotation data from multiple repositories. We assessed the performance of ProGeM using a reference set of 213 previously reported and extensively curated metabolite QTLs. For 98% of these loci (n=209), the expert-curated gene was one of the candidate causal genes prioritised by ProGeM. Systematic benchmarking analyses revealed that 70% (n=150) of the candidate causal genes were nearest to the sentinel variant at the investigated molecular QTLs, indicating that genomic proximity is the most reliable indicator of true positive causal genes. In contrast, cis-gene expression QTL data led to three false positive candidate causal gene assignments for every one true positive assignment. Finally, we provide evidence that these conclusions may also apply to cis-protein QTLs and loci associated with complex diseases. ProGeM is freely available via GitHub.", "tag": "Bioinformatics"}, {"title": "HumCFS: A database of fragile sites in human chromosomes", "url": "https://www.biorxiv.org/content/early/2017/12/08/231233", "abstract": "Genomic instability is the hallmark of cancer and several other pathologies, such as mental retardation; preferentially occur at specific loci in genome known as chromosomal fragile sites. HumCFS (http://webs.iiitd.edu.in/raghava/humcfs/) is a manually curated database provides comprehensive information on 118 experimentally characterized fragile sites present in human chromosomes. HumCFS comprises of 19068 entries with wide range of information such as nucleotide sequence of fragile sites, their length, coordinates on the chromosome, cytoband, their inducers and possibility of fragile site occurrence i.e. either rare or common etc. Each fragile region gene is further annotated to disease database DisGenNET, to understand its disease association. Protein coding genes are identified by annotating each fragile site to UCSC genome browser (GRCh38/hg38). To know the extent of miRNA lying in fragile site region, miRNA from miRBase has been mapped. Comprehensively, HumCFS encompasses mapping of 5010 genes with 19068 transcripts, 1104 miRNA and 3737 disease-associated genes on fragile sites. In order to facilitate users, we integrate standard web-based tools for easy data retrieval and analysis.", "tag": "Bioinformatics"}, {"title": "Long Read Annotation (LoReAn): automated eukaryotic genome annotation based on long-read cDNA sequencing", "url": "https://www.biorxiv.org/content/early/2017/12/08/230359", "abstract": "Single-molecule full-length cDNA sequencing can aid genome annotation by revealing transcript structure and alternative splice-forms, yet current annotation pipelines do not incorporate such information. Here we present LoReAn (Long Read Annotation) software, an automated annotation pipeline utilizing short- and long-read cDNA sequencing, protein evidence, and ab initio prediction to generate accurate genome annotations. Based on annotations of two fungal and two plant genomes, we show that LoReAn outperforms popular annotation pipelines by integrating single-molecule cDNA sequencing data generated from either the PacBio or MinION sequencing platforms, and correctly predicting gene structure and capturing genes missed by other annotation pipelines.", "tag": "Bioinformatics"}, {"title": "Comparative systems analysis of the secretome of the opportunistic pathogen Aspergillus fumigatus and other Aspergillus species", "url": "https://www.biorxiv.org/content/early/2017/12/08/230953", "abstract": "Aspergillus fumigatus and multiple other Aspergillus species cause a wide range of lung infections, collectively termed aspergillosis. Aspergilli are ubiquitous in environment with healthy immune systems routinely eliminating inhaled conidia, however, Aspergilli can become an opportunistic pathogen in immune-compromised patients. The aspergillosis mortality rate and emergence of drug-resistance reveals an urgent need to identify novel targets. Secreted and cell membrane proteins play a critical role in fungal-host interactions and pathogenesis. Using a computational pipeline integrating data from high-throughput experiments and bioinformatic predictions, we have identified secreted and cell membrane proteins in ten Aspergillus species known to cause aspergillosis. Small secreted and effector-like proteins similar to agents of fungal-plant pathogenesis were also identified within each secretome. A comparison with humans revealed that at least 70% of Aspergillus secretomes has no sequence similarity with the human proteome. An analysis of antigenic qualities of Aspergillus proteins revealed that the secretome is significantly more antigenic than cell membrane proteins or the complete proteome. Finally, overlaying an expression dataset, four A. fumigatus proteins upregulated during infection and with available structures, were found to be structurally similar to known drug target proteins in other organisms, and were able to dock in silico with the respective drug.", "tag": "Bioinformatics"}, {"title": "Show me your neighbours, and I'll tell you what you are - cellular microenvironment matters", "url": "https://www.biorxiv.org/content/early/2017/12/08/231282", "abstract": "To answer major questions of cell biology, it is essential to understand cellular complexity. Modern automated microscopes produce vast amounts of images routinely, making manual analysis nearly impossible. Due to their efficiency, machine learning-based analysis software have become essential tools to perform single-cell-level phenotypic analysis of large imaging datasets. However, an important limitation of such methods is that they do not use the information gained from the cellular micro- and macroenvironment: the algorithmic decision is based solely on the local properties of the cell of interest. Here, we present how various microenvironmental features contribute to identifying a cell and how such additional information can improve single-cell-level phenotypic image analysis. The proposed methodology was tested for different sizes of Euclidean and nearest neighbour-based cellular environments both on tissue sections and cell cultures. Our experimental data verify that the microenvironment of a cell largely determines its entity. This effect was found to be especially strong for established tissues, while it was somewhat weaker in the case of cell cultures. Our analysis shows that combining local cellular features with the properties of the cell's microenvironment significantly improves the accuracy of machine learning-based phenotyping.", "tag": "Bioinformatics"}, {"title": "Differential expression analysis of log-ratio transformed counts: benchmarking methods for RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2017/12/07/231175", "abstract": "Background: Count data generated by next-generation sequencing assays do not measure absolute transcript abundances. Instead, the data are constrained to an arbitrary \"library size\" by the sequencing depth of the assay, and typically must be normalized prior to statistical analysis. The constrained nature of these data means one could alternatively use a log-ratio transformation in lieu of normalization, as often done when testing for differential abundance (DA) of operational taxonomic units (OTUs) in 16S rRNA data. Therefore, we benchmark how well the ALDEx2 package, a transformation-based DA tool, detects differential expression in high-throughput RNA-sequencing data (RNA-Seq), compared to conventional RNA-Seq differential expression methods. Results: To evaluate the performance of log-ratio transformation-based tools, we apply the ALDEx2 package to two simulated, and one real, RNA-Seq data sets. The latter was previously used to benchmark dozens of conventional RNA-Seq differential expression methods, enabling us to directly compare transformation-based approaches. We show that ALDEx2, widely used in meta-genomics research, identifies differentially expressed genes (and transcripts) from RNA-Seq data with high precision and, given sufficient sample sizes, high recall too (regardless of the alignment and quantification procedure used). Although we show that the choice in log-ratio transformation can affect performance, ALDEx2 has high precision (i.e., few false positives) across all transformations. Finally, we present a novel, iterative log-ratio transformation (now implemented in ALDEx2) that further improves performance in simulations. Conclusions: Our results suggest that log-ratio transformation-based methods can work to measure differential expression from RNA-Seq data, provided that certain assumptions are met. Moreover, these methods have high precision (i.e., few false positives) in simulations and perform as good as, or better than, than conventional methods on real data. With previously demonstrated applicability to 16S rRNA data, ALDEx2 can work as a single tool for data from multiple sequencing modalities.", "tag": "Bioinformatics"}, {"title": "Full-Length Envelope Analyzer (FLEA): A tool for longitudinal analysis of viral amplicons", "url": "https://www.biorxiv.org/content/early/2017/12/07/230474", "abstract": "Next generation sequencing of viral populations has advanced our understanding of viral population dynamics, the development of drug resistance, and escape from host immune responses. Many applications require complete gene sequences, which can be impossible to reconstruct from short reads. HIV-1 env, the protein of interest for HIV vaccine studies, is exceptionally challenging for long-read sequencing and analysis due to its length, high substitution rate, and extensive indel variation. While long-read sequencing is attractive in this setting, the analysis of such data is not well handled by existing methods. To address this, we introduce FLEA (Full-Length Envelope Analyzer), which performs end-to-end analysis and visualization of long-read sequencing data. FLEA consists of both a pipeline (optionally run on a high-performance cluster), and a client-side web application that provides interactive results. The pipeline transforms FASTQ reads into high-quality consensus sequences (HQCSs) and uses them to build a codon-aware multiple sequence alignment. The resulting alignment is then used to infer phylogenies, selection pressure, and evolutionary dynamics. The web application provides publication-quality plots and interactive visualizations, including an annotated viral alignment browser, time series plots of evolutionary dynamics, visualizations of gene-wide selective pressures (such as dN/dS) across time and across protein structure, and a phylogenetic tree browser. We demonstrate how FLEA may be used to process Pacific Biosciences HIV-1 env data and describe recent examples of its use. Simulations show how FLEA dramatically reduces the error rate of this sequencing platform, providing an accurate portrait of complex and variable HIV-1 env populations. A public instance of FLEA is hosted at http://flea.datamonkey.org. The Python source code for the FLEA pipeline can be found at https://github.com/veg/flea-pipeline. The client-side application is available at https://github.com/veg/flea-web-app. A live demo of the P018 results can be found at http://flea.murrell.group/view/P018.", "tag": "Bioinformatics"}, {"title": "Improving the annotation of the Heterorhabditis bacteriophora genome", "url": "https://www.biorxiv.org/content/early/2017/12/07/225904", "abstract": "Genome assembly and annotation remains an exacting task. As the tools available for these tasks improve, it is useful to return to data produced with earlier instances to assess their credibility and correctness. The entomopathogenic nematode Heterorhabditis bacteriophora is widely used to control insect pests in horticulture. The genome sequence for this species was reported to encode an unusually high proportion of unique proteins and a paucity of secreted proteins compared to other related nematodes. We revisited the H. bacteriophora genome assembly and gene predictions to ask whether these unusual characteristics were biological or methodological in origin. We mapped an independent resequencing dataset to the genome and used the blobtools pipeline to identify potential contaminants. While present (0.2% of the genome span, 0.4% of predicted proteins), assembly contamination was not significant. Re-prediction of the gene set using BRAKER1 and published transcriptome data generated a predicted proteome that was very different from the published one. The new gene set had a much reduced complement of unique proteins, better completeness values that were in line with other related species' genomes, and an increased number of proteins predicted to be secreted. It is thus likely that methodological issues drove the apparent uniqueness of the initial H. bacteriophora genome annotation and that similar contamination and misannotation issues affect other published genome assemblies.", "tag": "Bioinformatics"}, {"title": "Testing hypotheses about the microbiome using an ordination-based linear decomposition model", "url": "https://www.biorxiv.org/content/early/2017/12/06/229831", "abstract": "Background: Data from a microbiome study is often analyzed by calculating a matrix of pairwise distances between observations; ordination using principal components or multidimensional scaling of the distance matrix is frequently successful in separating meaningful groups of observations (e.g., cases and controls). Although distance-based analyses like PERMANOVA can be used to test association using only the distance matrix, the connection between data on individual species (or operational taxonomic units, OTUs) and the information in the distance matrix is lost. This makes it hard to know which species contribute to the patterns seen in ordination for the high-dimensional data we gather in a microbiome study. Methods: We introduce a novel approach called the linear decomposition model (LDM) that provides a single analysis path that includes distance-based ordination, global tests of any effect of the microbiome, and tests of the effect of individual OTUs with false discovery rate (FDR)-based correction for multiple testing. The LDM accommodates both continuous and discrete variables (e.g., clinical outcomes, environmental factors) as well as interaction terms to be tested either singly or in combination, allows for adjustment of confounding covariates, and uses permutation-based p-values that can control for correlation (e.g., repeated measurements on the same individual). The LDM can also be applied to transformed data, and an `omnibus' test can easily combine results from analyses conducted on different transformation scales. Results: For global testing, our simulations indicate the LDM provides correct type I error, even with substantial confounding and/or correlations, and can have substantially higher power than existing distance-based methods. For testing individual OTUs, our simulations indicate the LDM generally controlled the FDR well. In contrast, DESeq2 had infated FDR in the presence of confounding and inferior sensitivity with sparse OTU data; MetagenomeSeq generally had the lowest sensitivity. The flexibility of the LDM for a variety of microbiome studies is illustrated by the analysis of data from two microbiome studies. Conclusions: We present the LDM, a method that integrates global testing of any microbiome effect and detection of differentially abundant OTUs. The LDM is generally more powerful than existing methods, and is capable of handling the confounders and correlated data that frequently occur in modern microbiome studies.", "tag": "Bioinformatics"}, {"title": "BrainImageR: Spatiotemporal gene set analysis referencing the human brain", "url": "https://www.biorxiv.org/content/early/2017/12/06/229302", "abstract": "RATIONALE: Neurological molecular analyses such as transcriptomics, epigenetics, and genome-wide association studies must be assessed in the context of the human brain in order to generate biologically meaningful inferences. It is often difficult to access primary human brain tissue; therefore, approximations are made using in vitro modeling or by identifying disease-associated genes from DNA extracted from blood. Gene sets from these studies are then compared to the post-mortem human brain to provide an assessment of the brain region and the developmental time point that a gene set is most closely associated with. However, most analyses of post-mortem datasets are achieved by building new computational tools each time in-house, which can cause discrepancies from study to study, indicating that the field is in need of a user-friendly suite of tools to examine spatiotemporal expression with respect to the postmortem brain. Such a tool will be of use to the molecular interrogation of neurological and psychiatric disorders, with direct advantages for the disease-modeling and human genetics communities. RESULTS: We have developed brainImageR, an R package that calculates both the spatial and temporal association of a dataset with post-mortem gene expression data from the Allen Brain Atlas. BrainImageR performs a robust analysis of gene set enrichment to identify enriched anatomical regions, provides a global high-resolution visualization of these enrichments across the human brain, and predicts when in developmental time the sample is most closely matched to, a task that has become increasingly important in the field of in vitro neuronal modeling. These functionalities of brainImageR enable a quick and efficient characterization of a given dataset across normal human brain development. AVAILABILITY AND IMPLEMENTATION: BrainImageR is released under the Creative Commons CC BY-SA 4.0 license and the source code can be downloaded through github at https://github.com/saralinker/brainImageR.", "tag": "Bioinformatics"}, {"title": "Succinct De Bruijn Graph Construction for Massive Populations Through Space-Efficient Merging", "url": "https://www.biorxiv.org/content/early/2017/12/06/229641", "abstract": "Recently, there has been significant amount of effort in developing space-efficient and succinct data structures for storing and building the traditional de Bruijn graph and its variants, including the colored de Bruijn graph. However, a problem not yet considered is developing a means to merge succinct representations of the de Bruijn graph---a challenge is necessary for constructing the de Bruijn graph on very-large datasets. We create VARIMERGE, for building the colored de Bruijn graph on a very-large dataset through partitioning the data into smaller subsets, building the colored de Bruijn graph using a FM-index based representation, and merging these representations in an iterative format. This last step is an algorithmic challenge for which we present an algorithm in this paper. Lastly, we demonstrate the utility of VARIMERGE by demonstrating: a four-fold reduction in working space when constructing an 8,000 color dataset, and the construction of population graph two orders of magnitude larger than previous reported methods.", "tag": "Bioinformatics"}, {"title": "TahcoRoll: An Efficient Approach for Signature Profiling in Genomic Data through Variable-Length k-mers", "url": "https://www.biorxiv.org/content/early/2017/12/06/229708", "abstract": "K-mer profiling has been one of the trending approaches to analyze read data generated by high-throughput sequencing technologies. The tasks of k-mer profiling include, but are not limited to, counting the frequencies and determining the occurrences of short sequences in a dataset. The notion of k-mer has been extensively used to build de Bruijn graphs in genome or transcriptome assembly, which requires examining all possible k-mers presented in the dataset. Recently, an alternative way of profiling has been proposed, which constructs a set of representative k-mers as genomic markers and profiles their occurrences in the sequencing data. This technique has been applied in both transcript quantification through RNA-Seq and taxonomic classification of metagenomic reads. Most of these applications use a set of fixed-size k-mers since the majority of existing k-mer counters are inadequate to process genomic sequences with variable-length k-mers. However, choosing the appropriate k is challenging, as it varies for different applications. As a pioneer work to profile a set of variable-length k-mers, we propose TahcoRoll in order to enhance the Aho-Corasick algorithm. More specifically, we use one bit to represent each nucleotide, and integrate the rolling hash technique to construct an efficient in-memory data structure for this task. Using both synthetic and real datasets, results show that TahcoRoll outperforms existing approaches in either or both time and memory efficiency without using any disk space. In addition, compared to the most efficient state-of-the-art k-mer counters, such as KMC and MSBWT, TahcoRoll is the only approach that can process long read data from both PacBio and Oxford Nanopore on a commodity desktop computer. The source code of TahcoRoll is implemented in C++14, and available at https://github.com/chelseaju/TahcoRoll.git.", "tag": "Bioinformatics"}, {"title": "Housekeeping genes, revisited at the single-cell level", "url": "https://www.biorxiv.org/content/early/2017/12/06/229815", "abstract": "Housekeeping genes are critical for understanding the core transcriptome and instrumental in data normalisation given their stable expression in different tissues and cells. Previous studies defined housekeeping genes using bulk transcriptome data. With recent advances in single-cell RNA-sequencing (scRNA-seq), it is now possible to identify steadily expressed genes across individual cells. Here we introduce the concept of housekeeping index and a framework for assessing housekeeping genes at the single-cell level using high-resolution scRNA-seq data. We apply our approach on two scRNA-seq datasets from early mammalian development and evaluate derived housekeeping genes on ten additional scRNA-seq datasets from diverse cell/tissue types.", "tag": "Bioinformatics"}, {"title": "Learning from mistakes: Accurate prediction of cell type-specific transcription factor binding", "url": "https://www.biorxiv.org/content/early/2017/12/06/230011", "abstract": "Computational prediction of cell type-specific, in-vivo transcription factor binding sites is still one of the central challenges in regulatory genomics, and a variety of approaches has been proposed for this purpose. Here, we present our approach that earned a shared first rank in the \"ENCODE-DREAM in vivo Transcription Factor Binding Site Prediction Challenge\" in 2017. This approach employs an extensive set of features derived from chromatin accessibility, binding motifs, gene expression, sequence and annotation to train classifiers using a supervised, discriminative learning principle. Two further key aspects of this approach is learning classifier parameters in an iterative training procedure that successively adds additional negative examples to the training set, and creating an ensemble prediction by averaging over classifiers obtained for different training cell types. In post-challenge analyses, we benchmark the influence of different feature sets and find that chromatin accessiblity and binding motifs are sufficient to yield state-of-the-art performance for in-vivo binding site predictions. We also show that the iterative training procedure and the ensemble prediction are pivotal for the final prediction performance. To make predictions of this approach readily accessible, we predict 682 peak lists for a total of 31 transcription factors in 22 primary cell types and tissues, which are available for download at https://www.synapse.org/#!Synapse:syn11526239, and we demonstrate that these predictions may help to yield biological conclusions.", "tag": "Bioinformatics"}, {"title": "Transfer RNA genes experience exceptionally elevated mutation rates", "url": "https://www.biorxiv.org/content/early/2017/12/06/229906", "abstract": "Transfer RNAs (tRNAs) are a central and necessary component for the biological synthesis of new proteins, and they are among the most highly conserved and most frequently transcribed sequences across all of life. Despite their clear significance for fundamental cellular processes, however, the forces governing tRNA evolution are poorly understood. Here, we present evidence that transcription-associated mutagenesis and strong purifying selection are key determinants of patterns of sequence polymorphism and divergence within and surrounding tRNA genes across several diverse model organisms. Remarkably, our results indicate that the mutation rate at broadly expressed tRNA loci is between 8.7 and 13.8 times greater than the genome-wide average. Furthermore, evolutionary analyses provide strong evidence that tRNA loci, but not their flanking sequences, experience strong purifying selection, acting in direct response to this elevated mutation rate. Finally, we also find a highly significant correlation between tRNA expression levels and the mutation rates in their immediate flanking regions, suggesting the possibility of predicting gene expression levels based on relative mutation rates and sequence variation data among tRNA gene loci. Our results provide novel insight into individual tRNA gene evolution, and imply that tRNA loci contribute disproportionately to mutational load in human populations.", "tag": "Bioinformatics"}, {"title": "pysster: Learning Sequence and Structure Motifs in DNA and RNA Sequences using Convolutional Neural Networks", "url": "https://www.biorxiv.org/content/early/2017/12/06/230086", "abstract": "Convolutional neural networks have been shown to perform exceptionally well in a variety of tasks, including biological sequence classification. Available implementations, however, are usually optimized for a particular task and difficult to reuse. To enable researchers to utilize these networks more easily we implemented pysster, a Python package for training and interpretation of convolutional neural networks. The package can be applied to both DNA and RNA to classify sets of sequences by learning sequence and secondary structure motifs. It offers an automated hyper-parameter optimization and options to visualize learned motifs along with information about their positional and class enrichment. The package runs seamlessly on CPU and GPU and provides a simple interface to train and evaluate a network with a handful lines of code. pysster is freely available at https://github.com/budach/pysster.", "tag": "Bioinformatics"}, {"title": "Barley Long Non-Coding RNAs and Their Tissue-Specific Co-expression Pattern with Coding-Transcripts", "url": "https://www.biorxiv.org/content/early/2017/12/05/229559", "abstract": "Long non-coding RNAs (lncRNA) with non-protein or small peptide-coding potential transcripts are emerging regulatory molecules. With the advent of next-generation sequencing technologies and novel bioinformatics tools, a tremendous number of lncRNAs has been identified in several plant species. Recent reports demonstrated roles of plant lncRNAs such as development and environmental response. Here, we reported a genome-wide discovery of ~8,000 barley lncRNAs and measured their expression pattern upon excessive boron (B) treatment. According to the tissue-based comparison, leaves have a greater number of B-responsive differentially expressed lncRNAs than the root. Functional annotation of the coding transcripts, which were co-expressed with lncRNAs, revealed that molecular function of the ion transport, establishment of localization, and response to stimulus significantly enriched only in the leaf. On the other hand, 32 barley endogenous target mimics (eTM) as lncRNAs, which potentially decoy the transcriptional suppression activity of 18 miRNAs, were obtained. Presented data including identification, expression measurement, and functional characterization of barley lncRNAs suggest that B-stress response might also be regulated by lncRNA expression via cooperative interaction of miRNA-eTM-coding target transcript modules.", "tag": "Bioinformatics"}, {"title": "SciRide Finder : citation-based paradigm in biomedical literature search.", "url": "https://www.biorxiv.org/content/early/2017/12/05/208959", "abstract": "There are more than 26 million peer-reviewed biomedical research items according to Medline/PubMed. This breadth of information is indicative of the progress in biomedical sciences on one hand, but an overload for scientists performing literature searches on the other. A major portion of scientific literature search is to find statements, numbers and protocols that can be cited to build an evidence-based narrative for a new manuscript. Because science builds on prior knowledge, such information has likely been written out and cited in an older manuscript. Thus, Cited Statements, pieces of text from scientific literature supported by citing other peer-reviewed publications, carry significant amount of condensed information on prior art. Based on this principle, we propose a literature search service, SciRide Finder (finder.sciride.org), which constrains the search corpus to such Cited Statements only. We demonstrate that Cited Statements can carry different information to this found in titles/abstracts and full text, giving access to alternative literature search results than traditional search engines. We further show how presenting search results as a list of Cited Statements allows researchers to easily find information to build an evidence-based narrative for their own manuscripts.", "tag": "Bioinformatics"}, {"title": "Predicting DNA accessibility in the pan-cancer tumor genome using RNA-seq, WGS, and deep learning", "url": "https://www.biorxiv.org/content/early/2017/12/05/229385", "abstract": "DNA accessibility, chromatin regulation, and genome methylation are key drivers of transcriptional events promoting tumor growth. However, understanding the impact of DNA sequence data on transcriptional regulation of gene expression is a challenge, particularly in noncoding regions of the genome. Recently, neural networks have been used to effectively predict DNA accessibility in multiple specific cell types. These models make it possible to explore the impact of mutations on DNA accessibility and transcriptional regulation. Our work first improved on prior cell-specific accessibility prediction, obtaining a mean receiver operating characteristic (ROC) area under the curve (AUC) = 0:910 and mean precision-recall (PR) AUC = 0:605, compared to the previous mean ROC AUC = 0:895 and mean PR AUC = 0:561. Our key contribution extended the model to enable accessibility predictions on any new sample for which RNA-seq data is available, without requiring cell-type-specific DNase-seq data for re-training. This new model obtained overall PR AUC = 0:621 and ROC AUC = 0:897 when applied across whole genomes of new samples whose biotypes were held out from training, and PR AUC = 0:725 and ROC AUC = 0:913 on randomly held out new samples whose biotypes were allowed to overlap with training. More significantly, we showed that for promoter and promoter flank regions of the genome our model predicts accessibility to high reliability, achieving PR AUC = 0:838 in held out biotypes and PR AUC = 0:908 in randomly held out samples. This performance is not sensitive to whether the promoter and flank regions fall within genes used in the input RNA-seq expression vector. Finally, we utilize this tool to investigate, for the first time, promoter accessibility patterns across several cohorts from The Cancer Genome Atlas (TCGA).", "tag": "Bioinformatics"}, {"title": "Bioinformatics analysis quantifies neighborhood preferences of cancer cells in Hodgkin lymphoma.", "url": "https://www.biorxiv.org/content/early/2017/12/04/228981", "abstract": "Motivation: Hodgkin lymphoma is a tumor of the lymphatic system and represents one of the most frequent lymphoma in the Western world. It is characterized by Hodgkin cells and Reed-Sternberg cells, which exhibit a broad morphological spectrum. The cells are visualized by immunohistochemical staining of tissue sections. In pathology, tissue images are mainly manually evaluated, relying on the expertise and experience of pathologists. Computational quantification methods become more and more essential to evaluate tissue images. In particular, the distribution of cancer cells is of great interest. Results: Here, we systematically quantified and investigated cancer cell properties and their spatial neighborhood relations by applying statistical analyses to whole slide images of Hodgkin lymphoma and lymphadenitis, which describes a non-cancerous inflammation of the lymph node. We differentiated cells by their morphology and studied the spatial neighborhood relation of more than 400,000 immunohistochemically stained cells. We found that, according to their morphological features, the cells exhibited significant preferences for and aversions to cells of specific profiles as nearest neighbor. We quantified differences between Hodgkin lymphoma and lymphadenitis concerning the neighborhood relations of cells and the sizes of cells. The approach can easily be applied to other cancer types.", "tag": "Bioinformatics"}, {"title": "A Multi-Species Functional Embedding Integrating Sequence and Network Structure", "url": "https://www.biorxiv.org/content/early/2017/12/04/229211", "abstract": "A key challenge to transferring knowledge between species is that different species have fundamentally different genetic architectures. Initial computational approaches to transfer knowledge across species have relied on measures of heredity such as genetic homology, but these approaches suffer from limitations. First, only a small subset of genes have homologs, limiting the amount of knowledge that can be transferred, and second, genes change or repurpose functions, complicating the transfer of knowledge. Many approaches address this problem by expanding the notion of homology by leveraging high-throughput genomic and proteomic measurements, such as through network alignment. In this work, we take a new approach to transferring knowledge across species by expanding the notion of homology through explicit measures of functional similarity between proteins in different species. Specifically, our kernel-based method, HANDL (Homology Assessment across Networks using Diffusion and Landmarks), integrates sequence and network structure to create a functional embedding in which proteins from different species are embedded in the same vector space. We show that inner products in this space and the vectors themselves capture functional similarity across species, and are useful for a variety of functional tasks. We perform the first whole-genome method for predicting phenologs, generating many that were previously identified, but also predicting new phenologs supported from the biological literature. We also demonstrate the HANDL embedding captures pairwise gene function, in that gene pairs with synthetic lethal interactions are significantly separated in HANDL space, and the direction of separation is conserved across species. (Software for the HANDL algorithm is available at http://bit.ly/lrgr-handl.)", "tag": "Bioinformatics"}, {"title": "RIFRAF: a frame-resolving consensus algorithm", "url": "https://www.biorxiv.org/content/early/2017/12/04/227520", "abstract": "Motivation: Protein coding genes can be studied using long-read next generation sequencing. However, high rates of indel sequencing errors are problematic, corrupting the reading frame. Even the consensus of multiple independent sequence reads retains indel errors. To solve this problem, we introduce RIFRAF, a sequence consensus algorithm that takes a set of error-prone reads and a reference sequence and infers an accurate in-frame consensus. RIFRAF uses a novel structure, analogous to a two-layer hidden Markov model: the consensus is optimized to maximize alignment scores with both the set of noisy reads and with a reference. The template-to-reads component of the model encodes the preponderance of indels, and is sensitive to the per-base quality scores, giving greater weight to more accurate bases. The reference-to-template component of the model penalizes frame-destroying indels. A local search algorithm proceeds in stages to find the best consensus sequence for both objectives. Results: Using Pacific Biosciences SMRT sequences of HIV envelope (NL4-3), we compare our approach to other consensus and frame correction methods. RIFRAF consistently finds a consensus sequence that is more accurate and in-frame, especially with small numbers of reads. It was able to perfectly reconstruct over 80% of consensus sequences from as few as three reads, whereas the best alternative required twice as many. RIFRAF is able to achieve these results and keep the consensus in-frame even with a distantly related reference sequence. Moreover, unlike other frame correction methods, RIFRAF can detect and keep true indels while removing erroneous ones. Availability: RIFRAF is implemented in Julia, and source code is publicly available at https://github.com/MurrellGroup/Rifraf.jl", "tag": "Bioinformatics"}, {"title": "Balances: a new perspective for microbiome analysis", "url": "https://www.biorxiv.org/content/early/2017/12/04/219386", "abstract": "High-throughput sequencing technologies have revolutionized microbiome research by allowing the relative quantification of microbiome composition and function in different environments. One of the main goals in microbiome analysis is the identification of microbial species that are differentially abundant among groups of samples, or whose abundance is associated with a variable of interest. Most available methods for microbiome abundance testing perform univariate tests for each microbial species or taxa separately, ignoring the compositional nature of microbiome data. We propose an alternative approach for microbiome abundance testing that consists on the identification of two groups of taxa whose relative abundance, or balance, is associated with the response variable of interest. This approach is appealing, since it has direct translation to the biological concept of ecological balance between species in an ecosystem. In this work, we present selbal, a greedy stepwise algorithm for balance selection. We illustrate the algorithm with 16s abundance data from an HIV-microbiome study and a Crohn-microbiome study. Importance: A more meaningful approach for microbiome abundance testing is presented. Instead of testing each taxon separately we propose to explore abundance balances among groups of taxa. This approach acknowledges the compositional nature of microbiome data.", "tag": "Bioinformatics"}, {"title": "Unsupervised clustering and epigenetic classification of single cells", "url": "https://www.biorxiv.org/content/early/2017/12/04/143701", "abstract": "Characterizing epigenetic heterogeneity at the cellular level is a critical problem in the modern genomics era. Assays such as single cell ATAC-seq (scATAC-seq) offer an opportunity to interrogate cellular level epigenetic heterogeneity through patterns of variability in open chromatin. However, these assays exhibit technical variability that complicates clear classification and cell type identification in heterogeneous populations. We present scABC, an R package for the unsupervised clustering of single cell epigenetic data, to classify scATAC-seq data and discover regions of open chromatin specific to cell identity.", "tag": "Bioinformatics"}, {"title": "CONFOLD2: Improved contact-driven ab initio protein structure modeling", "url": "https://www.biorxiv.org/content/early/2017/12/04/228460", "abstract": "Background: Contact-guided protein structure prediction methods are becoming more and more successful because of the latest advances in residue-residue contact prediction. To support the contact-driven structure prediction, effective tools that can quickly build tertiary structural models of good quality from predicted contacts need to be developed. Results: We develop an improved contact-driven protein modeling method, CONFOLD2, and study how it may be effectively used for ab initio protein structure prediction with predicted contacts as input. It builds models using various subsets of input contacts to explore the fold space under the guidance of a soft square energy function, and then clusters the models to obtain top five models. CONFOLD2 is benchmarked on various datasets including CASP11 and 12 datasets with publicly available predicted contacts and yields better performance than the popular CONFOLD method. Conclusion: CONFOLD2 allows to quickly generate top five structural models for a protein sequence, when its secondary structures and contacts predictions at hand. CONFOLD2 is publicly available at https://github.com/multicom-toolbox/CONFOLD2/.", "tag": "Bioinformatics"}, {"title": "Interpretation of biological experiments changes with evolution of Gene Ontology and its annotations", "url": "https://www.biorxiv.org/content/early/2017/12/03/228080", "abstract": "Gene Ontology (GO) enrichment analysis is ubiquitously used for interpreting high throughput molecular data and generating hypotheses about underlying biological phenomena of experiments. However, the two building blocks of this analysis - the ontology and the annotations - evolve rapidly. We used gene signatures derived from 104 disease analyses to systematically evaluate how enrichment analysis results were affected by evolution of the GO over a decade. We found low consistency between enrichment analyses results obtained with early and more recent GO versions. Furthermore, there continues to be strong annotation bias in the GO annotations where 58% of the annotations are for 16% of the human genes. Our analysis suggests that GO evolution may have affected the interpretation and possibly reproducibility of experiments over time. Hence, researchers must exercise caution when interpreting GO enrichment analyses and should reexamine previous analyses with the most recent GO version.", "tag": "Bioinformatics"}, {"title": "Unsupervised correction of gene-independent cell responses to CRISPR-Cas9 targeting", "url": "https://www.biorxiv.org/content/early/2017/12/03/228189", "abstract": "Genome editing by CRISPR-Cas9 technology allows large scale screening of gene essentiality in cancer. A confounding factor when interpreting CRISPR-Cas9 screens is the high false-positive rate in detecting essential genes, particularly for those that are within copy number amplified regions of the genome. We have developed the computational tool CRISPRcleanR which is capable of identifying and correcting gene-independent responses to CRISPR-Cas9 targeting. CRISPRcleanR uses an unsupervised approach based on the segmentation of single-guide RNA (sgRNA) fold change values across the genome, without making any assumption on the copy number status of the targeted genes. Applying our method to newly generated genome-wide essentiality profiles from 15 cancer cell lines, we demonstrate that CRISPRcleanR reduces false positives when calling essential genes, correcting biases within and outside of amplified regions, while maintaining true positive rates. Established cancer dependencies and essentiality signals of amplified cancer driver genes are detectable post-correction. CRISPRcleanR reports sgRNA fold changes and normalised sgRNA read counts, and is therefore compatible with downstream analysis tools, and works with multiple sgRNA libraries. CRISPRcleanR is a versatile, open-source tool for the analysis of CRISPR-Cas9 knockout screens to identify essential genes.", "tag": "Bioinformatics"}, {"title": "Does relaxing the infinite sites assumption give better tumor phylogenies? An ILP-based comparative approach", "url": "https://www.biorxiv.org/content/early/2017/12/03/227801", "abstract": "Most of the evolutionary history reconstruction approaches are based on the infinite site assumption, which is underlying the Perfect Phylogeny model and whose main consequence is that acquired mutation can never lost. This results in the clonal model used to explain cancer evolution. Some recent results gives a strong evidence that recurrent and back mutations are present in the evolutionary history of tumors, thus showing that more general models then the Perfect Phylogeny are required. We propose a new approach that incorporates the possibility of losing a previously acquired mutation, extending the Persistent Phylogeny model. We exploit our model to provide an ILP formulation of the problem of reconstructing trees on mixed populations, where the input data consists of the fraction of cells in a set of samples that have a certain mutation. This is a fundamental problem in cancer genomics, where the goal is to study the evolutionary history of a tumor. An experimental analysis shows the usefulness of allowing mutation losses, by studying some real and simulated datasets where our ILP approach provides a better interpretation than the one obtained under perfect phylogeny assumption. Finally, we show how to incorporate multiple back mutations and recurrent mutations in our model.", "tag": "Bioinformatics"}, {"title": "In silico and in vitro screening of FDA-approved drugs for potential repurposing against tuberculosis", "url": "https://www.biorxiv.org/content/early/2017/12/03/228171", "abstract": "Motivation: Repurposing of known drugs to newer clinical conditions is a promising avenue for finding novel therapeutic applications for tuberculosis. Methods: We performed docking-based virtual screening for 1554 known drugs against two of the potential drug targets, namely trpD and coaA of M. tuberculosis. In the first round of in silico screening we used rigid docking using Glide and AutoDock Vina. We subjected the consistently ranked drugs for induced-fit docking by these tools against the same target proteins. We performed luciferase reporter phage (LRP) assay to determine the biological activity of five selected drugs against M. tuberculosis. Results: We observed lymecycline and cefpodoxime to be active against drug susceptible and drug resistant strains of M. tuberculosis. In addition, lymecycline and cefpodoxime showed synergistic activity with rifampin and isoniazid against M. tuberculosis. Conclusion: Our results suggest that lymecycline and cefpodoxime have potential to be repurposed for the treatment of tuberculosis.", "tag": "Bioinformatics"}, {"title": "GrandPrix: Scaling up the Bayesian GPLVM for single-cell data", "url": "https://www.biorxiv.org/content/early/2017/12/03/227843", "abstract": "The Gaussian Process Latent Variable Model (GPLVM) is a popular approach for dimensionality reduction of single-cell data and has been used for pseudotime estimation with capture time information. However current implementations are computationally intensive and will not scale up to modern droplet-based single-cell datasets which routinely profile many tens of thousands of cells. We provide an efficient implementation which allows scaling up this approach to modern single-cell datasets. We also generalize the application of pseudotime inference to cases where there are other sources of variation, such as branching dynamics. We applied our method on microarray, nCounter, RNA-seq, qPCR and droplet-based datasets from different organisms. The model converges an order of magnitude faster compared to existing methods whilst achieving similar levels of estimation accuracy. Further, we demonstrate the flexibility of our approach by extending the model to higher-dimensional latent spaces that can be used to simultaneously infer pseudotime and other structure such as branching. Thus, the model has the capability of producing meaningful biological insights about cell ordering as well as cell fate regulation. Software available at github.com/ManchesterBioinference/GrandPrix", "tag": "Bioinformatics"}, {"title": "MetaRiPPquest: A Peptidogenomics Approach for the Discovery of Ribosomally Synthesized and Post-translationally Modified Peptides", "url": "https://www.biorxiv.org/content/early/2017/12/03/227504", "abstract": "Ribosomally synthesized and post-translationally modified peptides (RiPPs) are an important class of natural products that include many antibiotics and a variety of other bioactive compounds. While recent breakthroughs in RiPP discovery raised the challenge of developing new algorithms for their analysis, peptidogenomic-based identification of RiPPs by combining genome/metagenome mining with analysis of tandem mass spectra remains an open problem. We present here MetaRiPPquest, a software tool for addressing this challenge that is compatible with large-scale screening platforms for natural product discovery. After searching millions of spectra in the Global Natural Products Social (GNPS) molecular networking infrastructure against just six genomic and metagenomic datasets, MetaRiPPquest identified 27 known and discovered 5 novel RiPP natural products.", "tag": "Bioinformatics"}, {"title": "HOME: A histogram based machine learning approach for effective identification of differentially methylated regions", "url": "https://www.biorxiv.org/content/early/2017/12/02/228221", "abstract": "DNA methylation is a covalent modification of DNA that plays important role in regulating gene expression, cell identity, and organism development. Localized changes in DNA methylation are observed between different cell types, during development and aging, in various disease states, and under different stress conditions, and are often associated with functionally important genomic regions, including promoters and enhancers. The development of whole genome bisulfite sequencing has made it possible to identify methylation differences at single base resolution throughout an entire genome. A persistent challenge in DNA methylome analysis is the accurate identification of differentially methylated regions (DMRs) in the genome between samples. Sensitive and specific identification of DMRs between different conditions requires accurate and efficient algorithms, and while various tools have been developed to tackle this problem, they frequently suffer from limitations in sensitivity and accuracy. Here, we present a novel Histogram Of MEthylation (HOME) based method that exploits the inherent difference in distribution of methylation levels between DMRs and non-DMRs to robustly discriminate between the two via a linear Support Vector Machine. HOME produces accurate DMR boundaries, few spurious DMRs, and provides the ability to determine DMRs in time-series data. HOME can identify DMRs among any number of treatment groups in experiments with or without replicates at high accuracy. We demonstrate that HOME produces more accurate DMRs than the current state-of-the-art methods on both simulated and biological datasets, and provide a user-friendly implementation of the tool.", "tag": "Bioinformatics"}, {"title": "CytoGAN: Generative Modeling of Cell Images", "url": "https://www.biorxiv.org/content/early/2017/12/02/227645", "abstract": "We explore the application of Generative Adversarial Networks to the domain of morphological profiling of human cultured cells imaged by fluorescence microscopy. When evaluated for their ability to group cell images responding to treatment by chemicals of known classes, we find that adversarially learned representations are superior to autoencoder-based approaches. While currently inferior to classical computer vision and transfer learning, the adversarial framework enables useful visualization of the variation of cellular images due to their generative capabilities.", "tag": "Bioinformatics"}, {"title": "Visualizing Transitions and Structure for High Dimensional Data Exploration", "url": "https://www.biorxiv.org/content/early/2017/12/01/120378", "abstract": "In the era of 'Big Data' there is a pressing need for tools that provide human interpretable visualizations of emergent patterns in high-throughput high-dimensional data. Further, to enable insightful data exploration, such visualizations should faithfully capture and emphasize emergent structures and patterns without enforcing prior assumptions on the shape or form of the data. In this paper, we present PHATE (Potential of Heat-diffusion for Affinity-based Transition Embedding) - an unsupervised low-dimensional embedding for visualization of data that is aimed at solving these issues. Unlike previous methods that are commonly used for visualization, such as PCA and tSNE, PHATE is able to capture and highlight both local and global structure in the data. In particular, in addition to clustering patterns, PHATE also uncovers and emphasizes progression and transitions (when they exist) in the data, which are often missed in other visualization-capable methods. Such patterns are especially important in biological data that contain, for example, single-cell phenotypes at different phases of differentiation, patients at different stages of disease progression, and gut microbial compositions that vary gradually between individuals, even of the same enterotype. The embedding provided by PHATE is based on a novel informational distance that captures long-range nonlinear relations in the data by computing energy potentials of data-adaptive diffusion processes. We demonstrate the effectiveness of the produced visualization in revealing insights on a wide variety of biomedical data, including single-cell RNA-sequencing, mass cytometry, gut microbiome sequencing, human SNP data, Hi-C data, as well as non-biomedical data, such as facebook network and facial image data. In order to validate the capability of PHATE to enable exploratory analysis, we generate a new dataset of 31,000 single-cells from a human embryoid body differentiation system. Here, PHATE provides a comprehensive picture of the differentiation process, while visualizing major and minor branching trajectories in the data. We validate that all known cell types are recapitulated in the PHATE embedding in proper organization. Furthermore, the global picture of the system offered by PHATE allows us to connect parts of the developmental progression and characterize novel regulators associated with developmental lineages.", "tag": "Bioinformatics"}, {"title": "A linear-time algorithm to sample the dual-birth model", "url": "https://www.biorxiv.org/content/early/2017/12/01/226423", "abstract": "The ability to sample models of tree evolution is essential in the analysis and interpretation of phylogenetic trees. The dual-birth model is an extension of the traditional birth-only model and allows for sampling trees of varying degrees of balance. However, for a tree with n leaves, the tree sampling algorithm proposed in the original paper is O(n log n). I propose an algorithm to sample trees under the dual-birth model in O(n), and I provide a fast C++ implementation of the proposed algorithm.", "tag": "Bioinformatics"}, {"title": "Identification and characterization of functional modules reflecting transcriptome transition during human neuron maturation", "url": "https://www.biorxiv.org/content/early/2017/12/01/174748", "abstract": "Background: Neuron maturation is a critical process in neurogenesis, during which neurons gain their morphological, electrophysiological and molecular characteristics for their functions as the central components of the nervous system. Results: To better understand the molecular changes during this process, we combined the protein-protein interaction network and public single cell RNA-seq data of mature and immature neurons to identify functional modules relevant to the neuron maturation process in humans. The analysis resulted in 33 discriminable modules which participate in varied functions including energy consumption, synaptic functions and housekeeping functions such as translation and splicing. Based on the identified modules, we trained a neuron maturity index (NMI) model for the quantification of maturation states of single neurons or purified bulk neurons. Applied to multiple single neuron transcriptome data sets of neuron development in humans and mice, the NMI model made estimation of neuron maturity states which were significantly correlated with the neuron maturation trajectories in both species, implying the reproducibility and conservation of the identified transcriptome transition. Conclusion: We identified 33 functional modules whose activities were significantly correlated with single neuron maturity states, which may play important roles in the neuron maturation process.", "tag": "Bioinformatics"}, {"title": "A Parallel Multiobjective Metaheuristic for Multiple Sequence Alignment", "url": "https://www.biorxiv.org/content/early/2017/12/01/103101", "abstract": "The alignment among three or more nucleotides/amino-acids sequences at the same time is known as Multiple Sequence Alignment (MSA), an NP-hard optimization problem. The time complexity of finding an optimal alignment raises exponentially when the number of sequences to align increases. In this work, we deal with a multiobjective version of the MSA problem where the goal is to simultaneously optimize the accuracy and conservation of the alignment. A parallel version of the Hybrid Multiobjective Memetic Metaheuristics for Multiple Sequence Alignment is proposed. In order to evaluate the parallel performance of our proposal, we have selected a pull of datasets with different number of sequences (up to 1000 sequences) and study its parallel performance against other well-known parallel metaheuristics published in the literature, such as MSAProbs, T-Coffee, Clustal \u03a9, and MAFFT. The comparative study reveals that our parallel aligner is around 25 times faster than the sequential version with 32 cores, obtaining a parallel efficiency around 80%.", "tag": "Bioinformatics"}, {"title": "Classifying cells with Scasat - a tool to analyse single-cell ATAC-seq", "url": "https://www.biorxiv.org/content/early/2017/11/30/227397", "abstract": "Motivation: The assay for transposase-accessible chromatin using sequencing (ATAC-seq) reveals the landscape and principles of DNA regulatory mechanisms by identifying the accessible genome of mammalian cells. When done at single-cell resolution, it provides an insight into the cell-to-cell variability that emerges from identical DNA sequences by identifying the variability in the genomic location of open chromatin sites in each of the cells. Processing of single-cell ATAC-seq requires a number of steps and a simple pipeline to processes and analyse single-cell ATAC-seq is not yet available. Results: This paper presents ScAsAT (single-cell ATAC-seq analysis tool), a complete pipeline to process scATAC-seq data with simple steps. The pipeline is developed in a Jupyter notebook environment that holds the executable code along with the necessary description and results. For the initial sequence processing steps, the pipeline uses a number of well-known tools which it executes from a python environment for each of the fastq files. While functions for the data analysis part are mostly written in R, it is robust, flexible, interactive and easy to extend. The pipeline was applied to a single-cell ATAC-seq dataset in order to identify different cell-types from a complex cell mixture. The results from Scasat showed that open chromatin location corresponding to potential regulatory elements can account for cellular heterogeneity and can identify regulatory regions that separates cells from a complex population. Availability: The jupyter notebook with the complete pipeline applied to the dataset published with this paper are publicly available on the Github (https://github.com/ManchesterBioinference/Scasat). An additional notebook is also provided for analysis of a publicly available dataset. The fastq files are submitted at ArrayExpress database at EMBL-EBI (www.ebi.ac.uk/arrayexpress) under accession number E-MTAB- 6116.", "tag": "Bioinformatics"}, {"title": "SNPTB: nucleotide variant identification and annotation in Mycobacterium tuberculosis genomes", "url": "https://www.biorxiv.org/content/early/2017/11/30/227066", "abstract": "Whole genome sequencing (WGS) has become a mainstay in biomedical research. The continually decreasing cost of sequencing has resulted in a data deluge that underlines the need for easy-to-use bioinformatics pipelines that can mine meaningful information from WGS data. SNPTB is one such pipeline that analyzes WGS data originating from in vitro or clinical samples of Mycobacterium tuberculosis and outputs high-confidence single nucleotide polymorphisms in the bacterial genome. The name of the mutated gene and the functional consequence of the mutation on the gene product is also determined. SNPTB utilizes open source software for WGS data analyses and is written primarily for biologists with minimal computational skills.", "tag": "Bioinformatics"}, {"title": "Integrative DNA copy number detection and genotyping from sequencing and array-based platforms", "url": "https://www.biorxiv.org/content/early/2017/11/30/172700", "abstract": "Motivation: Copy number variations (CNVs) are gains and losses of DNA segments and have been associated with disease. Many large-scale genetic association studies are performing CNV analysis using whole exome sequencing (WES) and whole genome sequencing (WGS). In many of these studies, previous SNP-array data are available. An integrated cross-platform analysis is ex-pected to improve resolution and accuracy, yet there is no tool for effectively combining data from sequencing and array platforms. The detection of CNVs using sequencing data alone can also be further improved by the utilization of allele-specific reads. Results: We propose a statistical framework, integrated Copy Number Variation detection algo-rithm (iCNV), which can be applied to multiple study designs: WES only, WGS only, SNP array only, or any combination of SNP and sequencing data. iCNV applies platform specific normalization, utilizes allele specific reads from sequencing and integrates matched NGS and SNP-array data by a Hidden Markov Model (HMM). We compare integrated two-platform CNV detection using iCNV to naive intersection or union of the two platforms and show that iCNV increases sensitivity and ro-bustness. We also assess the accuracy of iCNV compared to existing methods on WGS data only, and show that the utilization of allele-specific reads improve CNV detection accuracy.", "tag": "Bioinformatics"}, {"title": "Comparative genomics approaches accurately predict deleterious variants in plants", "url": "https://www.biorxiv.org/content/early/2017/11/30/112318", "abstract": "Recent advances in genome resequencing have led to increased interest in prediction of the functional consequences of genetic variants. Variants at phylogenetically conserved sites are of particular interest, because they are more likely than variants at phylogenetically variable sites to have deleterious effects on fitness and contribute to phenotypic variation. Numerous comparative genomic approaches have been developed to predict deleterious variants, but the approaches are nearly always assessed based on their ability to identify known disease-causing mutations in humans. Determining the accuracy of deleterious variant predictions in nonhuman species is important to understanding evolution, domestication, and potentially to improving crop quality and yield. To examine our ability to predict deleterious variants in plants we generated a curated database of 2,910 Arabidopsis thaliana mutants with known phenotypes. We evaluated seven approaches and found that while all performed well, their relative ranking differed from prior benchmarks in humans. We conclude that deleterious mutations can be reliably predicted in A. thaliana and likely other plant species, but that the relative performance of various approaches does not necessarily translate from one species to another.", "tag": "Bioinformatics"}, {"title": "THiCweed: fast, sensitive detection of sequence features by clustering big data sets", "url": "https://www.biorxiv.org/content/early/2017/11/30/104109", "abstract": "We present THiCweed, a new approach to analyzing transcription factor binding data from high-throughput chromatin-immunoprecipitation-sequencing (ChIP-seq) experiments. THiCweed clusters bound regions based on sequence similarity using a divisive hierarchical clustering approach based on sequence similarity within sliding windows, while exploring both strands. ThiCweed is specially geared towards data containing mixtures of motifs, which present a challenge to traditional motif-finders. Our implementation is significantly faster than standard motif-finding programs, able to process 30,000 peaks in 1-2 hours, on a single CPU core of a desktop computer. On synthetic data containing mixtures of motifs it is as accurate or more accurate than all other tested programs. THiCweed performs best with large \"window\" sizes (\u2265 50bp), much longer than typical binding sites (7-15 base pairs). On real data it successfully recovers literature motifs, but also uncovers complex sequence characteristics in flanking DNA, variant motifs, and secondary motifs even when they occur in < 5% of the input, all of which appear biologically relevant. We also find recurring sequence patterns across diverse ChIP-seq data sets, possibly related to chromatin architecture and looping. THiCweed thus goes beyond traditional motif-finding to give new insights into genomic TF binding complexity.", "tag": "Bioinformatics"}, {"title": "Community-driven data analysis training for biology", "url": "https://www.biorxiv.org/content/early/2017/11/29/225680", "abstract": "The primary problem with the explosion of biomedical datasets is not the data itself, not computational resources, and not the required storage space, but the general lack of trained and skilled researchers to manipulate and analyze these data. Eliminating this problem requires development of comprehensive educational resources. Here we present a community-driven framework that enables modern, interactive teaching of data analytics in life sciences and facilitates the development of training materials. The key feature of our system is that it is not a static but a continuously improved collection of tutorials. By coupling tutorials with a web-based analysis framework, biomedical researchers can learn by performing computation themselves through a web-browser without the need to install software or search for example datasets. Our ultimate goal is to expand the breadth of training materials to include fundamental statistical and data science topics and to precipitate a complete re-engineering of undergraduate and graduate curricula in life sciences.", "tag": "Bioinformatics"}, {"title": "Module analysis captures pancancer (epi)genetically deregulated cancer driver genes for smoking and antiviral response", "url": "https://www.biorxiv.org/content/early/2017/11/29/216754", "abstract": "The availability of increasing volumes of multi-omics profiles across many cancers promises to improve our understanding of the regulatory mechanisms underlying cancer. The main challenge is to integrate these multiple levels of omics profiles and especially to analyze them across many cancers. Here we present AMARETTO, an algorithm that addresses both challenges in three steps. First, AMARETTO identifies potential cancer driver genes through integration of copy number, DNA methylation and gene expression data. Then AMARETTO connects these driver genes with co-expressed target genes that they control, defined as regulatory modules. Thirdly, we connect AMARETTO modules identified from different cancer sites into a pancancer network to identify cancer driver genes. Here we applied AMARETTO in a pancancer study comprising eleven cancer sites and confirmed that AMARETTO captures hallmarks of cancer. We also demonstrated that AMARETTO enables the identification of novel pancancer driver genes. In particular, our analysis led to the identification of pancancer driver genes of smoking-induced cancers and 'antiviral' interferon-modulated innate immune response.", "tag": "Bioinformatics"}, {"title": "scmap - A tool for unsupervised projection of single cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/11/29/150292", "abstract": "Single-cell RNA-seq (scRNA-seq) is widely used to investigate the composition of complex tissues since the technology allows researchers to define cell-types using unsupervised clustering of the transcriptome. However, due to differences in experimental methods and computational analyses, it is often challenging to directly compare the cells identified in two different experiments. Here, we present scmap (http://bioconductor.org/packages/scmap), a method for projecting cells from a scRNA-seq experiment onto the cell-types or individual cells identified in other experiments (the application can be run for free, without restrictions, from http://www.hemberg-lab.cloud/scmap).", "tag": "Bioinformatics"}, {"title": "Tumor-specific Causal Inference (TCI): A Bayesian Method for Identifying Causative Genome Alterations within Individual Tumors", "url": "https://www.biorxiv.org/content/early/2017/11/28/225631", "abstract": "Precision medicine for cancer involves identifying and targeting the somatic genome alterations (SGAs) that drive the development of an individual tumor. Much of current efforts at finding driver SGAs have involved identifying the genes that are mutated more frequently than expected among a collection of tumors. When these population-derived driver genes are altered (perhaps in particular ways) in a given tumor, they are posited as driver genes for that tumor. In this technical report, we introduce an alternative approach for identifying causative SGAs, also known as \"drivers\", by inferring causal relationships between SGAs and molecular phenotypes at the individual tumor level. Our tumor-specific causal inference (TCI) algorithm uses a Bayesian method to identify the SGAs in a given tumor that have a high probability of regulating transcriptomic changes observed in that specific tumor. Thus, the method is focused on identifying the tumor specific SGAs that are causing expression changes that are specific to the tumor. Those SGAs that have a high probability of regulating transcriptomic changes related to oncogenic processes are then designated to be the putative drivers of the tumor. In this paper, we describe in detail the TCI algorithm and its implementation.", "tag": "Bioinformatics"}, {"title": "The Plateau Method for Forensic DNA SNP Mixture Deconvolution", "url": "https://www.biorxiv.org/content/early/2017/11/27/225805", "abstract": "Identification of individuals in complex DNA mixtures remains a challenge for forensic analysts. Recent advances in high throughput sequencing (HTS) are enabling analysis of DNA mixtures with expanded panels of Short Tandem Repeats (STRs) and/or Single Nucleotide Polymorphisms (SNPs). We present the plateau method for direct SNP DNA mixture deconvolution into sub-profiles based on differences in contributors DNA concentrations in the mixtures in the absence of matching reference profiles. The Plateau method can detect profiles of individuals whose contribution is as low as 1/200 in a DNA mixture (patent pending).", "tag": "Bioinformatics"}, {"title": "Bioentities: a resource for entity recognition and relationship resolution in biomedical text mining", "url": "https://www.biorxiv.org/content/early/2017/11/27/225698", "abstract": "Background: For automated reading of scientific publications to extract useful information about molecular mechanisms it is critical that genes, proteins and other entities be correctly associated with uniform identifiers, a process known as named entity linking or \"grounding.\" Correct entity identification is essential for resolving relationships between mined information, curated interaction databases, and biological datasets. The accuracy of this process is largely dependent on the availability of resources allowing computers to link commonly-used abbreviations and synonyms found in literature to uniform identifiers. Results: In a task involving automated reading of ~215,000 articles using the REACH event extraction software we found that grounding was disproportionately inaccurate for multi-protein families (e.g., \"AKT\") and named complexes that involve multiple molecular entities (e.g. \"NF-kB\"). To address this problem we created Bioentities, a manually curated resource defining protein families and complexes as they are commonly referred to in text. In Bioentities the gene-level constituents of families and complexes are defined in a flexible format allowing for multi-level, hierarchical membership. To create Bioentities, text strings corresponding to entities were identified empirically from literature and linked manually to uniform identifiers; these identifiers were also mapped to equivalent entries in multiple related databases. Bioentities also includes curated prefix and suffix patterns that improve named entity recognition and event extraction. Evaluation on a distinct corpus of ~54,000 articles showed that incorporating Bioentities into the set of databases used for grounding significantly increased grounding accuracy for families and complexes, from 15 to 71%. The hierarchical organization of entities also made it possible to integrate otherwise unconnected mechanistic information across families, subfamilies, and individual proteins. Conclusion: Bioentities is an effective tool for improving named entity recognition, grounding, and relationship resolution in automated reading of biomedical text. The content in Bioentities is available in both tabular and Open Biomedical Ontology formats at https://github.com/sorgerlab/bioentities under the Creative Commons CC0 license and has been integrated into the TRIPS/DRUM and REACH reading systems.", "tag": "Bioinformatics"}, {"title": "Mining therapeutic insights from large scale drug screenings with transfer learning", "url": "https://www.biorxiv.org/content/early/2017/11/27/225573", "abstract": "Despite the abundance of large-scale molecular and drug-response data, our ability to extract the underlying mechanisms of diseases and treatment efficacy has been in general limited. Machine learning algorithms applied to those data sets most often are used to provide predictions without interpretation, or reveal single drug-gene association and fail to derive robust insights. We propose to use Macau, a bayesian multitask multi-relational algorithm to generalize from individual drugs and genes and explore the association between the drug targets and signaling pathway activations. A typical insight would be: Activation of pathway Y will confer sensitivity to any drug targeting protein X. We applied our methodology to the Genomics of Drug Sensitivity in Cancer (GDSC) screening, using signaling pathways' activities as cell line input and nominal targets as drug input. The interactions between the drug target and the pathway activity can guide a tissue specific treatment strategy by for example suggesting how to modulate a certain protein to maximize the drug response for a given tissue. We confirmed in literature drug combination strategies derived from our result for brain, skin and stomach tissues. Such an analysis of interactions across tissues might help drug repurposing and patient stratification strategies.", "tag": "Bioinformatics"}, {"title": "ProLego: tool for extracting and visualizing topological modules in protein structures", "url": "https://www.biorxiv.org/content/early/2017/11/27/225565", "abstract": "In protein design, correct use of topology is among the initial and most critical feature. Meticulous selection of backbone topology aids in drastically reducing the structure search space. With ProLego, we present a server application to explore the component aspect of protein structures and provide an intuitive and efficient way to scan the protein topology space. We have implemented in-house developed topological representation in an automated-pipeline to extract protein topology from given protein structure. Using the topology string, ProLego, compares topology against a non-redundant extensive topology database (ProLegoDB) as well as extracts constituent topological modules. The platform offers interactive topology visualization graphs. ProLego, provides an alternative but comprehensive way to scan and visualize protein topology along with an extensive database of protein topology. ProLego can be found at http://www.proteinlego.com", "tag": "Bioinformatics"}, {"title": "Reconstructing phylogeny from reduced-representation genome sequencing data without assembly or alignment", "url": "https://www.biorxiv.org/content/early/2017/11/27/225623", "abstract": "Although genome sequencing is becoming cheaper and faster, reducing the quantity of data by only sequencing part of the genome lowers both sequencing costs and computational burdens. One popular genome-reduction approach is restriction site associated DNA sequencing, or RADseq. RADseq was initially designed for studying genetic variation across genomes usually at the population level, and it has also proved to be suitable for interspecific phylogeny reconstruction. RADseq data pose challenges for standard phylogenomic methods, however, due to incomplete coverage of the genome and large amounts of missing data. Alignment-free methods are both efficient and accurate for phylogenetic reconstructions with whole genomes and are especially practical for non-model organisms; nonetheless, alignment-free methods have only been applied with whole genome sequences. Here, we test a full-genome assembly and alignment-free method, AAF, in application to RADseq data and propose two procedures for reads selection to remove missing data. We validate these methods using both simulations and a real dataset. Reads selection improved the accuracy of phylogenetic construction in every simulated scenario and the real dataset, making AAF comparable to or better than alignment-based method with much lower computation burdens. We also investigated the sources of missing data in RADseq and their effects on phylogeny reconstruction using AAF. The AAF pipeline modified for RADseq data, phyloRAD, is available on github (https://github.com/fanhuan/phyloRAD).", "tag": "Bioinformatics"}, {"title": "RecoverY: K-mer based read classification for Y-chromosome specific sequencing and assembly", "url": "https://www.biorxiv.org/content/early/2017/11/27/148114", "abstract": "Motivation: The haploid mammalian Y chromosome is usually under-represented in genome assemblies due to high repeat content and low depth due to its haploid nature. One strategy to ameliorate the low coverage of Y sequences is to experimentally enrich Y-specific material before assembly. Since the enrichment process is imperfect, algorithms are needed to identify putative Y-specific reads prior to downstream assembly. A strategy that uses k-mer abundances to identify such reads was used to assemble the gorilla Y (Tomaszkiewicz et al 2016). However, the strategy required the manual setting of key parameters, a time-consuming process leading to sub-optimal assemblies. Results: We develop a method, RecoverY, that selects Y-specific reads by automatically choosing the abundance level at which a k-mer is deemed to originate from the Y. This algorithm uses prior knowledge about the Y chromosome of a related species or known Y transcript sequences. We evaluate RecoverY on both simulated and real data, for human and gorilla, and investigate its robustness to important parameters. We show that RecoverY leads to a vastly superior assembly compared to alternate strategies of filtering the reads or contigs. Compared to the preliminary strategy used in Tomaszkiewicz et al (2016), we achieve a 33% improvement in assembly size and a 20% improvement in the NG50, demonstrating the power of automatic parameter selection. Availability: Our tool RecoverY is freely available at https://github.com/makovalab-psu/RecoverY Contact: kmakova@bx.psu.edu, pashadag@cse.psu.edu", "tag": "Bioinformatics"}, {"title": "High-throughput ANI Analysis of 90K Prokaryotic Genomes Reveals Clear Species Boundaries", "url": "https://www.biorxiv.org/content/early/2017/11/27/225342", "abstract": "A fundamental question in microbiology is whether there is a continuum of genetic diversity among genomes or clear species boundaries prevail instead. Answering this question requires robust measurement of whole-genome relatedness among thousands of genomes and from diverge phylogenetic lineages. Whole-genome similarity metrics such as Average Nucleotide Identity (ANI) can provide the resolution needed for this task, overcoming several limitations of traditional techniques used for the same purposes. Although the number of genomes currently available may be adequate, the associated bioinformatics tools for analysis are lagging behind these developments and cannot scale to large datasets. Here, we present a new method, FastANI, to compute ANI using alignment-free approximate sequence mapping. Our analyses demonstrate that FastANI produces an accurate ANI estimate and is up to three orders of magnitude faster when compared to an alignment (e.g., BLAST)-based approach. We leverage FastANI to compute pairwise ANI values among all prokaryotic genomes available in the NCBI database. Our results reveal a clear genetic discontinuity among the database genomes, with 99.8% of the total 8 billion genome pairs analyzed showing either >95% intra-species ANI or <83% inter-species ANI values. We further show that this discontinuity is recovered with or without the most frequently represented species in the database and is robust to historic additions in the public genome databases. Therefore, 95% ANI represents an accurate threshold for demarcating almost all currently named prokaryotic species, and wide species boundaries may exist for prokaryotes.", "tag": "Bioinformatics"}, {"title": "Clusterdv, a simple density-based clustering method that is robust, general and automatic.", "url": "https://www.biorxiv.org/content/early/2017/11/25/224840", "abstract": "How to partition a data set into a set of distinct clusters is a ubiquitous and challenging problem. The fact that data sets vary widely in features such as cluster shape, cluster number, density distribution, background noise, outliers and degree of overlap, makes it difficult to find a single algorithm that can be broadly applied. One recent method, based on search of density peaks, can be applied successfully to cluster many kinds of data, but it is not fully automatic, and fails on some simple data distributions. We propose an alternative approach, which estimates density dips between points, and allows robust determination of cluster number and distribution across a wide array of data, without any manual parameter adjustment. We show that this method is able to solve a range of synthetic and experimental data sets, where the underlying structure is known, and identifies consistent and meaningful clusters in new behavioural data.", "tag": "Bioinformatics"}, {"title": "Improving bioinformatics prediction of microRNA targets by ranks aggregation", "url": "https://www.biorxiv.org/content/early/2017/11/25/224915", "abstract": "microRNAs are non-coding RNAs which down-regulate a large number of target mRNAs and modulate cell activity. Despite continued progress, bioinformatics prediction of microRNA targets remains a challenge since available softwares still suffer from a lack of accuracy and sensitivity. Moreover, these tools show fairly inconsistent results from one another. Thus, in an attempt to circumvent these difficulties, we aggregated all human results of three important prediction algorithms (miRanda, PITA and SVmicrO) showing additional characteristics in order to rerank them into a single list. This database is freely available through a webtool called miRabel (http://bioinfo.univ-rouen.fr/mirabel/) which can take either a list of miRNAs, genes or signaling pathways as search inputs. Receiver Operating Characteristic curves and Precision-Recall curves analysis carried out using experimentally validated data and very large datasets show that miRabel significantly improves the prediction of miRNA targets compared to the three algorithms used separately. Moreover, using the same analytical methods, miRabel shows significantly better predictions than other popular algorithms such as MBSTAR and miRWalk. Interestingly, a F-score analysis revealed that miRabel also significantly improves the relevance of the top results. The aggregation of results from different databases is therefore a powerful and generalizable approach to many other species to improve miRNA target predictions. Thus, miRabel is an efficient tool to accurately identify miRNA targets and integrate them into a biological context.", "tag": "Bioinformatics"}, {"title": "Combining accurate tumour genome simulation with crowd sourcing to benchmark somatic structural variant detection", "url": "https://www.biorxiv.org/content/early/2017/11/25/224733", "abstract": "Background: The phenotypes of cancer cells are driven in part by somatic structural variants (SVs). SVs can initiate tumours, enhance their aggressiveness and provide unique therapeutic opportunities. Whole-genome sequencing of tumours can allow exhaustive identification of the specific SVs present in an individual cancer, facilitating both clinical diagnostics and the discovery of novel mutagenic mechanisms. A plethora of somatic SV detection algorithms have been created to enable these discoveries, however there are no systematic benchmarks of them. Rigorous performance evaluation of somatic SV detection methods has been challenged by the lack of gold-standards, extensive resource requirements and difficulties in sharing personal genomic information. Results: To facilitate SV detection algorithm evaluations, we created a robust simulation framework for somatic SVs by extending the BAMSurgeon algorithm. We then organized and enabled a crowd-sourced benchmarking within the ICGC-TCGA DREAM Somatic Mutation Calling Challenge (SMC-DNA). We report here the results of SV benchmarking on three different tumours, comprising 204 submissions from 15 teams. In addition to ranking methods, we identify characteristic error-profiles of individual algorithms and general trends across them. Surprisingly, we find that ensembles of analysis pipelines do not always outperform the best individual method, indicating a need for developing new ways to aggregate somatic SV detection approaches. Conclusions: The synthetic tumours and somatic SV detection leaderboards remain available as a community benchmarking resource, and BAMSurgeon is available at https://github.com/adamewing/bamsurgeon.", "tag": "Bioinformatics"}, {"title": "AptaBlocks: Accelerating the Design of RNA-based Drug Delivery Systems", "url": "https://www.biorxiv.org/content/early/2017/11/25/216465", "abstract": "Synthetic RNA molecules are increasingly used to alter cellular functions. These successful applications indicate that RNA-based therapeutics might be able to target currently undruggable genes. However, to achieve this promise, an effective method for delivering therapeutic RNAs into specific cells is required. Recently, RNA aptamers emerged as promising delivery agents due to their ability of binding specific cell receptors. Crucially, these aptamers can frequently be internalized into the cells expressing these receptors on their surfaces. This property is leveraged in aptamer based drug delivery systems by combining such receptor-specific aptamers with a therapeutic \"cargo\" such that the aptamer facilitates the internalization of the cargo into the cell. The advancement of this technology however is contingent on an efficient method to produce stable molecular complexes that include specific aptamers and cargoes. A recently proposed experimental procedure for obtaining such complexes relies on conjugating the aptamer and the cargo with complementary RNA strands so that when such modified molecules are incubated together, the complementary RNA strands hybridize to form a double-stranded \"sticky bridge\" connecting the aptamer with its cargo. However, designing appropriate sticky bridge sequences guaranteeing the formation and stability of the complex while simultaneously not interfering with the aptamer or the cargo as well as not causing spurious aggregation of the molecules during incubation has proven highly challenging. To fill this gap, we developed AptaBlocks, a computational method to design sticky bridges to connect RNA-based molecules (blocks). AptaBlocks relies on a biophysically inspired theoretical model capturing the complex objectives of the design and yet is simple enough to allow for efficient parameter estimation. Given this model, the sticky bridge sequence is optimized using a Monte Carlo algorithm based on heat-bath transitions. The effectiveness of the algorithm has been verified computationally and experimentally. AptaBlocks can be used in variety of experimental settings and its preliminary version has already been leveraged to design an aptamer based delivery system for a cytotoxic drug targeting Pancreatic ductal adenocarcinoma cells. It is thus expected that AptaBlocks will play a substantial role in accelerating RNA-based drug delivery design. AptaBlocks is available at https://github.com/wyjhxq/AptaBlocks.", "tag": "Bioinformatics"}, {"title": "VarExp: Estimating variance explained by Genome-Wide GxE summary statistics", "url": "https://www.biorxiv.org/content/early/2017/11/25/224634", "abstract": "Many genomic analyses, such as genome-wide association studies (GWAS) or genome-wide screening for Gene-Environment (GxE) interactions have been performed to elucidate the underlying mechanisms of human traits and diseases. When the analyzed outcome is quantitative, the overall contribution of identified genetic variants to the outcome is often expressed as the percentage of phenotypic variance explained. In practice, this is commonly estimated using individual genotype data. However, using individual-level data faces practical and ethical challenges when the GWAS results are derived in large consortia through meta-analysis of results from multiple cohorts. In this work, we present a R package, \"VarExp\", that allows for the estimation of the percentage of phenotypic variance explained by variants of interest using summary statistics only. Our package allows for a range of models to be evaluated, including marginal genetic effects, GxE interaction effects, and main genetic and interaction effects jointly. Its implementation integrates all recent methodological developments on the topic and does not need external data to be uploaded by users.", "tag": "Bioinformatics"}, {"title": "scImpute: Accurate And Robust Imputation For Single Cell RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/11/25/141598", "abstract": "The emerging single cell RNA sequencing (scRNA-seq) technologies enable the investigation of transcriptomic landscapes at single-cell resolution. The analysis of scRNA-seq data is complicated by excess zero or near zero counts, the so-called dropouts due to the low amounts of mRNA sequenced within individual cells. Downstream analysis of scRNA-seq would be severely biased if the dropout events are not properly corrected. We introduce scImpute, a statistical method to accurately and robustly impute the dropout values in scRNA-seq data. ScImpute automatically identifies gene expression values affected by dropout events, and only perform imputation on these values without introducing new bias to the rest data. ScImpute also detects outlier or rare cells and excludes them from imputation. Evaluation based on both simulated and real scRNA-seq data on mouse embryos, mouse brain cells, human blood cells, and human embryonic stem cells suggests that scImpute is an effective tool to recover transcriptome dynamics masked by dropout events. scImpute is shown to correct false zero counts, enhance the clustering of cell populations and subpopulations, improve the accuracy of differential expression analysis, and aid the study of gene expression dynamics.", "tag": "Bioinformatics"}, {"title": "Quaternary structure evaluation tool for protein assemblies", "url": "https://www.biorxiv.org/content/early/2017/11/24/224196", "abstract": "The Protein Data Bank (PDB) is the single worldwide archive of experimentally-determined three-dimensional (3D) structures of proteins and nucleic acids. As of January 2017, the PDB housed more than 125,000 structures and was growing by more than 11,000 structures annually. Since the 3D structure of a protein is vital to understand the mechanisms of biological processes, diseases, and drug design, correct oligomeric assembly information is of critical importance. For example, it makes a difference if the protein is normally a dimer and not a monomer or a trimer or a tetramer or a hexamer in nature. Unfortunately, the biologically relevant oligomeric form of a 3D structure is not directly obtainable by X-ray crystallography. Instead, this information may be provided by the PDB Depositor as metadata coming from additional experiments, be inferred by sequence-sequence comparisons with similar proteins of known oligomeric state, or predicted using software, such as PISA (Proteins, Interfaces, Structures and Assemblies) or EPPIC (Evolutionary Protein Protein Interface Classifier). Despite significant efforts by professional PDB Biocurators during data deposition, there remain a number of structures in the archive with incorrect quaternary structure descriptions (or annotations). Further investigation is, therefore, needed to evaluate the correctness of quaternary structure annotations. In this study, we aim to identify the most probable oligomeric states for proteins represented in the PDB. Our approach evaluated the performance of four independent prediction methods, including text mining of primary publications, inference from homologous protein structures, and two computational methods (PISA and EPPIC). Aggregating predictions to give consensus results outperformed all four of the independent prediction methods, yielding 86% correct, 9% incorrect, and 5% inconclusive predictions, when tested with a well-curated benchmark dataset. We have developed a freely-available web-based tool to make this approach accessible to researchers and PDB Biocurators (http://quatstruct.rcsb.org).", "tag": "Bioinformatics"}, {"title": "MEGAN-LR: New algorithms allow accurate binning and easy interactive exploration of metagenomic long reads and contigs", "url": "https://www.biorxiv.org/content/early/2017/11/24/224535", "abstract": "Background: There are numerous computational tools for taxonomic or functional analysis of microbiome samples, optimized to run on hundreds of millions of short, high quality sequencing reads. Programs such as MEGAN allow the user to interactively navigate these large datasets. Long read sequencing technologies continue to improve and produce increasing numbers of longer reads (of varying lengths in the range of 10k-1M bps, say), but of low quality. There is an increasing interest in using long reads in microbiome sequencing and there is a need to adapt short read tools to long read datasets. Methods: We describe a new LCA-based algorithm for taxonomic binning, and an interval-tree based algorithm for functional binning, that are explicitly designed for long reads and assembled contigs. We provide a new interactive tool for investigating the alignment of long reads against reference sequences. For taxonomic and functional binning, we propose to use LAST to compare long reads against the NCBI-nr protein reference database so as to obtain frame-shift aware alignments, and then to process the results using our new methods. Results: All presented methods are implemented in the open source edition of MEGAN and we refer to this new extension as MEGAN-LR (MEGAN long read). We evaluate the LAST+MEGAN-LR approach in a simulation study, and on a number of mock community datasets consisting of Nanopore reads, PacBio reads and assembled PacBio reads. We also illustrate the practical application on a Nanopore dataset that we sequenced from an anammox bio-rector community.", "tag": "Bioinformatics"}, {"title": "DHSpred: support-vector-machine-based human DNase I hypersensitive sites prediction using the optimal features selected by random forest", "url": "https://www.biorxiv.org/content/early/2017/11/24/224527", "abstract": "DNase I hypersensitive sites (DHSs) are genomic regions that provide important information regarding the presence of transcriptional regulatory elements and the state of chromatin. Therefore, identifying DHSs in uncharacterized DNA sequences is crucial for understanding their biological functions and mechanisms. Although many experimental methods have been proposed to identify DHSs, they have proven to be expensive for genome-wide application. Therefore, it is necessary to develop computational methods for DHS prediction. In this study, we proposed a support vector machine (SVM)-based method for predicting DHSs, called DHSpred (DNase I Hypersensitive Site predictor in human DNA sequences), which was trained with 174 optimal features. The optimal combination of features was identified from a large set that included nucleotide composition and di- and trinucleotide physicochemical properties, using a random forest algorithm. DHSpred achieved a Matthews correlation coefficient and accuracy of 0.660 and 0.871, respectively, which were 3% higher than those of control SVM predictors trained with non-optimized features, indicating the efficiency of the feature selection method. Furthermore, the performance of DHSpred was superior to that of state-of-the-art predictors. An online prediction server has been developed to assist the scientific community, and is freely available at: http://www.thegleelab.org/DHSpred.html.", "tag": "Bioinformatics"}, {"title": "Interoperable and scalable metabolomics data analysis with microservices", "url": "https://www.biorxiv.org/content/early/2017/11/24/213603", "abstract": "Developing a robust and performant data analysis workflow that integrates all necessary components whilst still being able to scale over multiple compute nodes is a challenging task. We introduce a generic method based on the microservice architecture, where software tools are encapsulated as Docker containers that can be connected into scientific workflows and executed in parallel using the Kubernetes container orchestrator. The access point is a virtual research environment which can be launched on-demand on cloud resources and desktop computers. IT-expertise requirements on the user side are kept to a minimum, and established workflows can be re-used effortlessly by any novice user. We validate our method in the field of metabolomics on two mass spectrometry studies, one nuclear magnetic resonance spectroscopy study and one fluxomics study, showing that the method scales dynamically with increasing availability of computational resources. We achieved a complete integration of the major software suites resulting in the first turn-key workflow encompassing all steps for mass-spectrometry-based metabolomics including preprocessing, multivariate statistics, and metabolite identification. Microservices is a generic methodology that can serve any scientific discipline and opens up for new types of large-scale integrative science.", "tag": "Bioinformatics"}, {"title": "CaDrA: A computational framework for performing candidate driver analyses using binary genomic features", "url": "https://www.biorxiv.org/content/early/2017/11/23/221846", "abstract": "Identifying complementary genetic drivers of a given phenotypic outcome is a challenging task that is important to gaining new biological insight and discovering targets for disease therapy. Existing methods aimed at achieving this task lack analytical flexibility. We developed Candidate Driver Analysis or CaDrA, a framework to identify functionally-relevant subsets of binary genomic features that, together, are associated with a specific outcome of interest. We evaluate CaDrA's sensitivity and specificity for typically-sized multi-omic datasets, and demonstrate CaDrA's ability to identify both known and novel drivers of oncogenic activity in cancer cell lines and primary tumors.", "tag": "Bioinformatics"}, {"title": "Non-coding Transcriptome Maps Across Twenty Tissues of the Korean Black Chicken, Yeonsan Ogye", "url": "https://www.biorxiv.org/content/early/2017/11/23/223644", "abstract": "The Yeonsan Ogye (Ogye) is a rare chicken breed that populates the Korean peninsula. The entire body of this bird, including its feathers and skin, has a unique black coloring. Although some protein-coding genes related to this unique feature have been examined, non-coding elements have not been globally investigated. In this study, high-throughput RNA sequencing (RNA-seq) and reduced representation bisulfite sequencing (RRBS) were performed to construct whole non-coding transcriptome maps across twenty different Ogye tissues. The resulting maps included 6900 long non-coding RNA (lncRNA) genes comprising 1290 known and 5610 novel lncRNA genes. Compared to lncRNAs previously annotated in the Galllus gallus red junglefowl, a considerable number were either fragments of protein-coding genes or not expressed in Ogye tissues. Newly annotated Ogye lncRNA genes showed tissue-specific expression and simple gene structures containing 2 or 3 exons. Systematic analyses of sequencing data and other genomic data demonstrated that about 39% of the tissue-specific lncRNAs displayed evidence of function. In particular, heat shock transcription factor 2 (HSF2)-associated lncRNAs were discovered to be functionally linked to protein-coding genes that are specifically expressed in black skin tissues, tended to be more syntenically conserved in mammals, and were differentially expressed in black tissues relative to white tissues. Our findings and resulting maps provide not only a comprehensive catalogue of lncRNAs but also a set of functional lncRNAs that will facilitate understanding how the non-coding genome regulates unique phenotypes. Furthermore, our results should be of use for future genomic breeding of chickens.", "tag": "Bioinformatics"}, {"title": "Modelling the Dynamics of Biological Systems with the Geometric Hidden Markov Model", "url": "https://www.biorxiv.org/content/early/2017/11/22/224063", "abstract": "Many biological processes can be described geometrically in a simple way: stem cell differentiation can be represented as a branching tree and cell division can be depicted as a cycle. In this paper we introduce the geometric hidden Markov model (GHMM), a dynamical model whose goal is to capture the low-dimensional characteristics of biological processes from multivariate time series data. The framework integrates a graph-theoretical algorithm for dimensionality reduction with a latent variable model for sequential data. We analyzed time series data generated by an in silico model of a biomolecular circuit, the represillator. The trained model has a simple structure: the latent Markov chain corresponds to a two-dimensional lattice. We show that the short-term and long-term predictions of the GHMM reflect the oscillatory behaviour of the genetic circuit. Analysis of the inferred model with a community detection methods leads to a coarse-grained representation of the process.", "tag": "Bioinformatics"}, {"title": "The Oyster River Protocol: A Multi Assembler and Kmer Approach For de novo Transcriptome Assembly", "url": "https://www.biorxiv.org/content/early/2017/11/22/177253", "abstract": "Characterizing transcriptomes in non-model organisms has resulted in a massive increase in our understanding of biological phenomena. This boon, largely made possible via high-throughput sequencing, means that studies of functional, evolutionary and population genomics are now being done by hundreds or even thousands of labs around the world. For many, these studies begin with a de novo transcriptome assembly, which is a technically complicated process involving several discrete steps. The Oyster River Protocol (ORP), described here, implements a standardized and benchmarked set of bioinformatic processes, resulting in an assembly with enhanced qualities over other standard assembly methods. Specifically, ORP produced assemblies have higher Detonate and TransRate scores and mapping rates, which is largely a product of the fact that it leverages a multi-assembler and kmer assembly process, thereby bypassing the shortcomings of any one approach. These improvements are important, as previously unassembled transcripts are included in ORP assemblies, resulting in a significant enhancement of the power of downstream analysis. Further, as part of this study, I show that assembly quality is unrelated with the number of reads generated, above 30 million reads. Code Availability: The version controlled open-source code is available at https://github.com/macmanes-lab/Oyster_River_Protocol. Instructions for software installation and use, and other details are available at http://oyster-river-protocol.rtfd.org/.", "tag": "Bioinformatics"}, {"title": "New synthetic-diploid benchmark for accurate variant calling evaluation", "url": "https://www.biorxiv.org/content/early/2017/11/22/223297", "abstract": "Constructed from the consensus of multiple variant callers based on short-read data, existing benchmark datasets for evaluating variant calling accuracy are biased toward easy regions accessible by known algorithms. We derived a new benchmark dataset from the de novo PacBio assemblies of two human cell lines that are homozygous across the whole genome. This benchmark provides a more accurate and less biased estimate of the error rate of small variant calls in a realistic context.", "tag": "Bioinformatics"}, {"title": "Pediatric Severe Sepsis Prediction Using Machine Learning", "url": "https://www.biorxiv.org/content/early/2017/11/22/223289", "abstract": "Early detection of pediatric severe sepsis is necessary in order to administer effective treatment. In this study, we assessed the efficacy of a machine-learning-based prediction algorithm applied to electronic healthcare record (EHR) data for the prediction of severe sepsis onset. The resulting prediction performance was compared with the Pediatric Logistic Organ Dysfunction score (PELOD-2) and pediatric Systemic Inflammatory Response Syndrome score (SIRS) using cross-validation and pairwise t-tests. EHR data were collected from a retrospective set of de-identified pediatric inpatient and emergency encounters drawn from the University of California San Francisco (UCSF) Medical Center, with encounter dates between June 2011 and March 2016. Patients (n = 11,127) were 2-17 years of age and 103 [0.93%] were labeled severely septic. In four-fold cross-validation evaluations, the machine learning algorithm achieved an AUROC of 0.912 for discrimination between severely septic and control pediatric patients at onset and AUROC of 0.727 four hours before onset. Under the same measure, the prediction algorithm also significantly outperformed PELOD-2 (p < 0.05) and SIRS (p < 0.05) in the prediction of severe sepsis four hours before onset. This machine learning algorithm has the potential to deliver high-performance severe sepsis detection and prediction for pediatric inpatients.", "tag": "Bioinformatics"}, {"title": "Improving Gene Regulatory Network Inference by Incorporating Rates of Transcriptional Change", "url": "https://www.biorxiv.org/content/early/2017/11/22/093807", "abstract": "Organisms respond to changes in their environment through transcriptional regulatory networks (TRNs). The regulatory hierarchy of these networks can be inferred from expression data. Computational approaches to identify TRNs can be applied in any species where quality RNA can be acquired, However, ChIP-Seq and similar validation methods are challenging to employ in non-model species. Improving the accuracy of computational inference methods can significantly reduce the cost and time of subsequent validation experiments. We have developed ExRANGES, an approach that improves the ability to computationally infer TRN from time series expression data. ExRANGES utilizes both the rate of change in expression and the absolute expression level to identify TRN connections. We evaluated ExRANGES in five data sets from different model systems. ExRANGES improved the identification of experimentally validated transcription factor targets for all species tested, even in unevenly spaced and sparse data sets. This improved ability to predict known regulator-target relationships enhances the utility of network inference approaches in non-model species where experimental validation is challenging. We integrated ExRANGES with two different network construction approaches and it has been implemented as an R package available here: http://github.com/DohertyLab/ExRANGES. To install the package type: devtools::install_github(\"DohertyLab/ExRANGES\")", "tag": "Bioinformatics"}, {"title": "Prediction of Acute Kidney Injury with a Machine Learning Algorithm using Electronic Health Record Data", "url": "https://www.biorxiv.org/content/early/2017/11/22/223354", "abstract": "Background: A major problem in treating acute kidney injury (AKI) is that clinical criteria for recognition are markers of established kidney damage or impaired function; treatment before such damage manifests is desirable. Clinicians could intervene during what may be a crucial stage for preventing permanent kidney injury if patients with incipient AKI and those at high risk of developing AKI could be identified. Methods: We used a machine learning technique, boosted ensembles of decision trees, to train an AKI prediction tool on retrospective data from inpatients at Stanford Medical Center and intensive care unit patients at Beth Israel Deaconess Medical Center. We tested the algorithm's ability to detect AKI at onset, and to predict AKI 12, 24, 48, and 72 hours before onset, and compared its 3-fold cross-validation performance to the SOFA score for AKI identification in terms of Area Under the Receiver Operating Characteristic (AUROC). Results: The prediction algorithm achieves AUROC of 0.872 (95% CI 0.867, 0.878) for AKI onset detection, superior to the SOFA score AUROC of 0.815 (P < 0.01). At 72 hours before onset, the algorithm achieves AUROC of 0.728 (95% CI 0.719, 0.737), compared to the SOFA score AUROC of 0.720 (P < 0.01). Conclusions: The results of these experiments suggest that a machine-learning-based AKI prediction tool may offer important prognostic capabilities for determining which patients are likely to suffer AKI, potentially allowing clinicians to intervene before kidney damage manifests.", "tag": "Bioinformatics"}, {"title": "q2-longitudinal: a QIIME 2 plugin for longitudinal and paired-sample analyses of microbiome data", "url": "https://www.biorxiv.org/content/early/2017/11/22/223974", "abstract": "Studies of host-associated and environmental microbiomes often incorporate longitudinal sampling or paired samples in their experimental design. Longitudinal sampling provides valuable information about temporal trends and subject/population heterogeneity, offering advantages over cross-sectional and pre/post study designs. To support the needs of microbiome researchers performing longitudinal studies, we developed q2-longitudinal, a software plugin for the QIIME 2 microbiome analysis platform (https://qiime2.org). The q2-longitudinal plugin incorporates multiple methods for analysis of longitudinal and paired-sample data, including paired differences and distances, linear mixed effects models, microbial interdependence test, first differencing, and volatility analyses. The q2-longitudinal package (https://github.com/qiime2/q2-longitudinal) is open source software released under a BSD-3-Clause license and is freely available, including for commercial use.", "tag": "Bioinformatics"}, {"title": "Pervasive correlated evolution in gene expression shapes cell type transcriptomes", "url": "https://www.biorxiv.org/content/early/2017/11/22/070060", "abstract": "The evolution and diversification of cell types is a key means by which animal complexity evolves. Recently, hierarchical clustering and phylogenetic methods have been applied to RNA-seq data to infer cell type evolutionary history and homology. A major challenge for interpreting this data is that cell type transcriptomes may not evolve independently due to correlated changes in gene expression. This non-independence can arise for several reasons, such as when different tissues share common regulatory sequences for regulating genes expressed in multiple tissues, i.e. pleiotropic effects of mutations. We develop a model to estimate the level of correlated transcriptome evolution (LCE) and apply it to different datasets. The results reveal pervasive correlated transcriptome evolution among different cell and tissue types. In general, tissues related by morphology or developmental lineage exhibit higher LCE than more distantly related tissues. Analyzing new data collected from bird skin appendages suggests that LCE decreases with the phylogenetic age of tissues compared, with recently evolved tissues exhibiting the highest LCE. Furthermore, we show correlated evolution can alter patterns of hierarchical clustering, causing different tissue types from the same species to cluster together. Using a dataset with sufficient taxon sampling, we performed a gene-wise estimation of LCE, identifying genes that most strongly contribute to the correlated evolution signal. Removing genes with high LCE allows for accurate reconstruction of evolutionary relationships among tissue types. Our study provides a statistical method to measure and account for correlated gene expression evolution when interpreting comparative transcriptome data.", "tag": "Bioinformatics"}, {"title": "quanTIseq: quantifying immune contexture of human tumors", "url": "https://www.biorxiv.org/content/early/2017/11/22/223180", "abstract": "We introduce quanTIseq, a method to quantify the tumor immune contexture, determined by the type and density of tumor-infiltrating immune cells. quanTIseq is based on a novel deconvolution algorithm for RNA sequencing data that was validated with independent data sets. Complementing the deconvolution output with image data from tissue slides enables in silico multiplexed immunodetection and provides an efficient method for the immunophenotyping of a large number of tumor samples.", "tag": "Bioinformatics"}, {"title": "Computational haplotype recovery and long-read validation identifies novel isoforms of industrially relevant enzymes from natural microbial communities", "url": "https://www.biorxiv.org/content/early/2017/11/22/223404", "abstract": "Population-level diversity of natural microbiomes represent a biotechnological resource for biomining, biorefining and synthetic biology but requires the recovery of the exact DNA sequence (or \"haplotype\") of the genes and genomes of every individual present. Computational haplotype reconstruction is extremely difficult, complicated by environmental sequencing data (metagenomics). Current approaches cannot choose between alternative haplotype reconstructions and fail to provide biological evidence of correct predictions. To overcome this, we present Hansel and Gretel: a novel probabilistic framework that reconstructs the most likely haplotypes from complex microbiomes, is robust to sequencing error and uses all available evidence from aligned reads, without altering or discarding observed variation. We provide the first formalisation of this problem and propose \"metahaplome\" as a definition for the set of haplotypes for any genomic region of interest within a metagenomic dataset. Finally, we demonstrate using long-read sequencing, biological evidence of novel haplotypes of industrially important enzymes computationally predicted from a natural microbiome.", "tag": "Bioinformatics"}, {"title": "Prediction of potential disease-associated microRNAs using structural perturbation method", "url": "https://www.biorxiv.org/content/early/2017/11/22/223693", "abstract": "Motivation: The identification of disease-related microRNAs(miRNAs) is an essential but challenging task in bioinformatics research. Similarity-based link prediction methods are often used to predict potential associa-tions between miRNAs and diseases. In these methods, all unobserved associations are ranked by their similari-ty scores. Higher score indicates higher probability of existence. However, most previous studies mainly focus on designing advanced methods to improve the prediction accuracy while neglect to investigate the link pre-dictability of the networks that present the miRNAs and diseases associations. In this work, we construct a bilayer network by integrating the miRNA-disease network, the miRNA similarity network and the disease simi-larity network. We use structural consistency as an indicator to estimate the link predictability of the related networks. On the basis of the indicator, a derivative algorithm, called structural perturbation method (SPM), is applied to predict potential associations between miRNAs and diseases. Results: The link predictability of bilayer network is higher than that of miRNA-disease network, indicating that the prediction of potential miRNAs-diseases associations on bilayer network can achieve higher accuracy than based merely on the miRNA-disease network. A comparison between the SPM and other algorithms reveals the reliable performance of SPM which performed well in a 5-fold cross-validation. We test fifteen networks. The AUC values of SPM are higher than some well-known methods, indicating that SPM could serve as a useful computational method for improving the identification accuracy of miRNA\u2012disease associations. Moreover, in a case study on breast neoplasm, 80% of the top-20 predicted miRNAs have been manually confirmed by pre-vious experimental studies.", "tag": "Bioinformatics"}, {"title": "Infino: a Bayesian hierarchical model improves estimates of immune infiltration into tumor microenvironment", "url": "https://www.biorxiv.org/content/early/2017/11/21/221671", "abstract": "Robust quantification of immune cell infiltration into the tumor microenvironment may shed light on why only a small proportion of patients benefit from checkpoint therapy. The immune cells surrounding a tumor have been suggested to mediate an effective response to immunotherapy. However, traditional measurement of immune cell content around a tumor by immunohistochemistry, flow cytometry, or mass cytometry allows measurement of only up to a few dozen markers at a time, limiting the number of immune cell types identified. Immune cell type abundances may instead be estimated in silico by deconvolving gene expression mixtures from bulk RNA sequencing of tumor tissue. By measuring tens of thousands of transcripts at once, bulk RNA-seq provides a rich input to algorithms that quantify cell type abundances in the tumor microenvironment, affording the potential to quantify the states of a greater number of immune cell types (given adequate training data). Here, we first review existing methods for deconvolution and evaluate their performance on synthetic mixtures. Then we develop a Bayesian inference approach, named infino, that learns to distinguish immune cell expression phenotypes and deconvolve mixtures. In contrast to earlier approaches, infino accepts RNA sequencing data, models transcript expression variability, and exploits the relationships between cell types to improve deconvolution accuracy and allow interrogation from the level of broad categories to the level of finest granularity. The resulting probability distributions of immune infiltration could be applied to numerous questions concerning the diverse ecology of immune cell types, including assessment of the association of immune infiltration with response to immunotherapy, and study of the expression profile and presence of elusive T cell subcompartments, such as T cell exhaustion.", "tag": "Bioinformatics"}, {"title": "K-nearest neighbor smoothing for high-throughput single-cell RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2017/11/21/217737", "abstract": "High-throughput single-cell RNA-Seq (scRNA-Seq) methods can efficiently generate expression profiles for thousands of cells, and promise to enable the comprehensive molecular characterization of all cell types and states present in heterogeneous tissues. However, compared to bulk RNA-Seq, single-cell expression profiles are extremely noisy and only capture a fraction of transcripts present in the cell. Here, we describe an algorithm to smooth scRNA-Seq data, with the goal of significantly improving the signal-to-noise ratio of each profile, while largely preserving biological expression heterogeneity. The algorithm is based on the observation that across platforms, the technical noise exhibited by UMI-filtered scRNA-Seq data closely follows Poisson statistics. Smoothing is performed by first identifying the nearest neighbors of each cell in a step-wise fashion, based on variance-stabilized and partially smoothed expression profiles, and then aggregating their UMI counts. For multiple datasets, the application of our algorithm resulted in more stable cell type-specific expression profiles, and recovered correlations between co-expressed genes. More generally, smoothing improved the results of commonly used dimensionality reduction and clustering methods, greatly facilitating the identification of cell subsets and clusters of co-expressed genes. Our work implies that there exists a quantitative relationship between the number of cells profiled and the potential accuracy with which individual cell types or states can be characterized, and helps unlock the full potential of scRNA-Seq to elucidate molecular processes in healthy and disease tissues.", "tag": "Bioinformatics"}, {"title": "Detecting cancer vulnerabilities through gene networks under purifying selection in 4,700 cancer genomes", "url": "https://www.biorxiv.org/content/early/2017/11/21/222687.1", "abstract": "Large-scale cancer sequencing studies have uncovered dozens of mutations critical to cancer initiation and progression. However, a significant proportion of genes linked to tumor propagation remain hidden, often due to noise in sequencing data confounding low frequency alterations. Further, genes in networks under purifying selection (NPS), or those that are mutated in cancers less frequently than would be expected by chance, may play crucial roles in sustaining cancers but have largely been overlooked. We describe here a statistical framework that identifies genes that have a first order protein interaction network significantly depleted for mutations, to elucidate key genetic contributors to cancers. Not reliant on and thus, unbiased by, the gene of interest's mutation rate, our approach has identified 685 putative genes linked to cancer development. Comparative analysis indicates statistically significant enrichment of NPS genes in previously validated cancer vulnerability gene sets, while further identifying novel cancer-specific candidate gene targets. As more tumor genomes are sequenced, integrating systems level mutation data through this network approach should become increasingly useful in pinpointing gene targets for cancer diagnosis and treatment.", "tag": "Bioinformatics"}, {"title": "viGEN: An open source pipeline for the detection and quantification of viral RNA in human tumors", "url": "https://www.biorxiv.org/content/early/2017/11/21/099788", "abstract": "An estimated 17% of cancers worldwide are associated with infectious causes. The extent and biological significance of viral presence/infection in actual tumor samples is generally unknown but could be measured using human transcriptome (RNA-seq) data from tumor samples. We present an open source bioinformatics pipeline viGEN, which combines existing well-known and novel RNA-seq tools for not only the detection and quantification of viral RNA, but also variants in the viral transcripts. The pipeline includes 4 major modules: The first module allows to align and filter out human RNA sequences; the second module maps and count (remaining un-aligned) reads against reference genomes of all known and sequenced human viruses; the third module quantifies read counts at the individual viral genes level thus allowing for downstream differential expression analysis of viral genes between experimental and controls groups. The fourth module calls variants in these viruses. To the best of our knowledge, there are no publicly available pipelines or packages that would provide this type of complete analysis in one open source package. In this paper, we applied the viGEN pipeline to two case studies. We first demonstrate the working of our pipeline on a large public dataset, the TCGA cervical cancer cohort. We also performed additional in-depth analyses on a small focused study of TCGA liver cancer patients. In this cohort, we perform viral-gene quantification, viral-variant extraction and survival analysis. This allowed us to find differentially expressed viral-transcripts and viral-variants between the groups of patients, and connect them to clinical outcome. From our analyses, we show that we were able to successfully detect the human papilloma virus among the TCGA cervical cancer patients. We compared the viGEN pipeline with two metagenomics tools and demonstrate similar sensitivity/specificity. We were also able to quantify viral-transcripts and extract viral-variants using the liver cancer dataset. The results presented corresponded with published literature in terms of rate of detection, viral gene expression patterns and impact of several known variants of HBV genome. Results also show novel information about distinct patterns of expression and co-expression in Hepatitis B and the Human Endogenous Retrovirus (HERV) K113 viruses. This pipeline is generalizable, and can be used to provide novel biological insights into the significance of viral and other microbial infections in complex diseases, tumorigeneses and cancer immunology. The source code, with example data and tutorial is available at: https://github.com/ICBI/viGEN/.", "tag": "Bioinformatics"}, {"title": "Heritable tumor cell division rate heterogeneity induces clonal dominance", "url": "https://www.biorxiv.org/content/early/2017/11/21/097683", "abstract": "Tumors consist of a hierarchical population of cells that differ in their phenotype and genotype. This hierarchical organization of cells means that a few clones (i.e., cells and several generations of offspring) are abundant while most are rare, which is called clonal dominance. Such dominance also occurred in published in vitro iterated growth and passage experiments with tumor cells in which genetic barcodes were used for lineage tracing. A potential source for such heterogeneity is that dominant clones derive from cancer stem cells with an unlimited self-renewal capacity. Furthermore, ongoing evolution within the growing population may also induce clonal dominance. To understand how clonal dominance developed in the iterated growth and passage experiments, we built a computational model that accurately simulates these experiments. The model simulations reproduced the clonal dominance that developed in in vitro iterated growth and passage experiments when the division rates vary between cells, due to a combination of initial variation and of ongoing mutational processes. In contrast, the experimental results can neither be reproduced with a model that considers random growth and passage, nor with a model based on cancer stem cells. Altogether, our model suggests that in vitro clonal dominance develops due to selection of fast-dividing clones.", "tag": "Bioinformatics"}, {"title": "Maize GO Annotation - Methods, Evaluation, and Review (maize-GAMER)", "url": "https://www.biorxiv.org/content/early/2017/11/21/222836", "abstract": "We created a new high-coverage, robust, and reproducible functional annotation of maize protein coding genes based on Gene Ontology (GO) term assignments. Whereas the existing Phytozome and Gramene maize GO annotation sets only cover 41% and 56% of maize protein coding genes, respectively, this study provides annotations for 100% of the genes. We also compared the quality of our newly-derived annotations with the existing Gramene and Phytozome functional annotation sets by comparing all three to a manually annotated gold standard set of 1,619 genes where annotations were primarily inferred from direct assay or mutant phenotype. Evaluations based on the gold standard indicate that our new annotation set is measurably more accurate than those from Phytozome and Gramene. To derive this new high-coverage, high-confidence annotation set we used sequence-similarity and protein-domain-presence methods as well as mixed-method pipelines that developed for the Critical Assessment of Function Annotation (CAFA) challenge. Our project to improve maize annotations is called maize-GAMER (GO Annotation Method, Evaluation, and Review) and the newly-derived annotations are accessible via MaizeGDB (http://download.maizegdb.org/maize-GAMER) and CyVerse (B73 RefGen_v3 5b+ at doi.org/10.7946/P2S62P and B73 RefGen_v4 Zm00001d.2 at doi.org/10.7946/P2M925).", "tag": "Bioinformatics"}, {"title": "Serum proteins can successfully predict self-reported ethnicity: Implications for precision-medicine", "url": "https://www.biorxiv.org/content/early/2017/11/21/221614", "abstract": "The effects of inter-individual variability on disease treatment and prevention are important to the goals of \"precision medicine\". In biomedical research, consideration of racial or ethnic differences allows generation and exploration of hypotheses about interactions among genetic and environmental factors responsible for differential medical outcomes. The US National Institutes of Health, therefore recommends adequate participation of subjects from ethnic minority groups in research studies. Nevertheless, considerable debate has focused on validity of race or ethnicity as biological construct. Inconsistent definition of race/ethnicity and insignificant genetic variations between ethnic groups have invited disregard to this construct. On the contrary, differences in prevalence, expression and outcomes of various diseases among ethnic groups argue for continued and focused attention to ethnicity as important predicting variable. In context of Alzheimer's disease (AD), we have previously reported that ethnicity does moderates the proteomic markers of dementia. Here, we attempted to classify and predict self-reported ethnicity (Hispanic or non-Hispanic white, [NHW]) using a limited serum profile of 107 proteins. Random Forest (RF) classification method was able to discriminate those two ethnicities with 95% accuracy and could successfully predict ethnicity in an independent test-set (Area under ROC curve: 0.97). Variable selection method led to a condensed set of six proteins which yielded comparable classification and prediction accuracy. Our results provide preliminary evidence for proteomic variability between ethnic groups, and biological validity of ethnicity construct. Moreover, they also offer an opportunity to exploit these differences towards the objectives of precision medicine.", "tag": "Bioinformatics"}, {"title": "DNCON2: Improved protein contact prediction using two-level deep convolutional neural networks", "url": "https://www.biorxiv.org/content/early/2017/11/21/222893", "abstract": "Motivation: Significant improvements in the prediction of protein residue-residue contacts are observed in the recent years. These contacts, predicted using a variety of coevolution-based and machine learning methods, are the key contributors to the recent progress in ab initio protein structure prediction, as demonstrated in the recent CASP experiments. Continuing the development of new methods to reliably predict contact maps is essential to further improve ab initio structure prediction. Results: In this paper we discuss DNCON2, an improved protein contact map predictor based on two-level deep convolutional neural networks. It consists of six convolutional neural networks - the first five predict contacts at 6, 7.5, 8, 8.5, and 10 angstrom distance thresholds, and the last one uses these five predictions as additional features to predict final contact maps. On the free-modeling datasets in CASP10, 11, and 12 experiments, DNCON2 achieves mean precisions of 35%, 50%, and 53.4%, respectively, higher than 30.6% by MetaPSICOV on CASP10 dataset, 34% by MetaPSICOV on CASP11 dataset, and 46.3% by Raptor-X on CASP12 dataset, when top L/5 long-range contacts are evaluated. We attribute the improved performance of DNCON2 to the inclusion of short- and medium-range contacts into training, two-level approach to prediction, use of the state-of-the-art optimization and activation functions, and a novel deep learning architecture that allows each filter in a convolutional layer to access all the input features of a protein of arbitrary length. Availability: The web server of DNCON2 is at http://sysbio.rnet.missouri.edu/dncon2/ where training and testing datasets as well as the predictions for CASP10, 11, and 12 free-modeling datasets can also be downloaded. Its source code is available at https://github.com/multicom-toolbox/DNCON2/.", "tag": "Bioinformatics"}, {"title": "A Web-resource for Nutrient Use Efficiency related Genes, QTLs, and microRNA in important cereals and model plants", "url": "https://www.biorxiv.org/content/early/2017/11/21/222992", "abstract": "Cereals are the key contributors to global food security. Genes involved in uptake (transport), assimilation and utilization of macro- and micro-nutrients are responsible for their content in grain and straw. Although many cereal genomic databases are available, currently there is no cohesive web-resource of manually curated nutrient use efficiency (NtUE) related genes and QTLs, etc. In this study, we present a web-resource containing information on NtUE related genes/QTLs and the corresponding available microRNAs for some of these genes in four major cereal crops [wheat (Triticum aestivum), rice (Oryza sativa), maize (Zea mays), barley (Hordeum vulgare)], two alien species (Triticum urartu and Aegilops tauschii) related to wheat, and two model species including Brachypodium distachyon and Arabidopsis thaliana. Gene annotations integrated in the current web-resource were collected from the existing databases and the available literature. The primary goal of developing this web-resource is to provide descriptions of the NtUE related genes and their functional annotation. MicroRNA targeting some of the NtUE related genes and the quantitative trait loci (QTLs) for NtUE related traits are also included. The available information in the web-resource should help the users to readily search the desired information. Web-resource URL: http://bioclues.org/NtUE/", "tag": "Bioinformatics"}, {"title": "YAMP: a framework enabling reproducibility in metagenomics research", "url": "https://www.biorxiv.org/content/early/2017/11/21/223016", "abstract": "YAMP is a user-friendly workflow that enables the analysis of whole shotgun metagenomics data while using containerisation to ensure computational reproducibility and facilitate collaborative research. YAMP can be executed on any UNIX-like system, and offers seamless support for multiple job schedulers as well as for Amazon AWS cloud. Although YAMP has been developed to be ready-to-use by non-experts, bioinformaticians will appreciate its flexibility, modularisation, and simple customisation. The YAMP script, parameters, and documentation are available at https://github.com/alesssia/YAMP.", "tag": "Bioinformatics"}, {"title": "CREAM: Clustering of genomic REgions Analysis Method", "url": "https://www.biorxiv.org/content/early/2017/11/21/222562", "abstract": "Cellular identity relies on cell type-specific gene expression profiles controlled by cis-regulatory elements (CREs), such as promoters, enhancers and anchors of chromatin interactions. CREs are unevenly distributed across the genome, giving rise to distinct subsets such as individual CREs and Clusters Of cis-Regulatory Elements (COREs), also known as super-enhancers. Identifying COREs is a challenge due to technical and biological features that entail variability in the distribution of distances between CREs within a given dataset. To address this issue, we developed a new unsupervised machine learning approach termed Clustering of genomic REgions Analysis Method (CREAM). We demonstrate that COREs identified by CREAM are predictive of cell identity, consists of CREs strongly bound by master transcription factors accordingly to ChIP-seq signal intensity and are proximal to highly expressed genes. We further show that COREs identified by CREAM are preferentially found near genes essential for growth. Overall, CREAM offers an improved method compared to the state-of-the-art to identify COREs of biological function. CREAM is available as an open source R package (https://CRAN.R-project.org/package=CREAM) to identify COREs from cis-regulatory annotation datasets from any biological samples.", "tag": "Bioinformatics"}, {"title": "WeSeqMiner: A Weka package for building machine-learning models for sequence data", "url": "https://www.biorxiv.org/content/early/2017/11/20/217802", "abstract": "The application of machine learning techniques to biological sequence data typically requires a vector representation of the sequences. Extracting the numerical features from sequence data can be time consuming, especially if the user lacks programming skills. To this end, we propose a Weka package called WeSeqMiner, which provides several useful filters for extracting numerical features from sequence data for use in the Weka machine learning workbench. Motivated with an example, we show that the WeSeqMiner package integrates well with the Weka API, allowing transformations to be incorporated into Weka workflows for predictive model generation. WeSeqMiner can be installed by pointing the Weka package manager to the URL github.com/djhogan/seqminer/raw/master/seqminer.zip. The Javadoc for WeSeqMiner classes can be accessed at djhogan.github.io/seqminer.", "tag": "Bioinformatics"}, {"title": "cycleX: multi-dimensional pseudotime reveals cell cycle and differentiation relationship of dendritic cell progenitors", "url": "https://www.biorxiv.org/content/early/2017/11/20/222372", "abstract": "Advances in single-cell RNA-sequencing have helped reveal the previously underappreciated level of cellular heterogeneity present during cellular differentiation. A static snapshot of single-cell transcriptomes provides a good representation of the various stages of differentiation as differentiation is rarely synchronized between cells. Data from numerous single-cell analyses has suggested that cellular differentiation and development can be conceptualized as continuous processes. Consequently, computational algorithms have been developed to infer pseudotimes and re-ordered cells along developmental trajectories. However, existing pseudotime inference methods generate one-dimensional pseudotime in an unsupervised manner, which is inadequate to elucidate the effects of individual biological processes such as cell cycle and differentiation and the links between them. Here we present a method called cycleX which infers multi-dimensional pseudotimes to reveal putative relationship between cell cycle and differentiation during dendritic cell development. cycleX can be also applied to generate multi-dimensional pseudotime for the relationship among cell cycle, differentiation, trafficking, activation, metabolism and etc.", "tag": "Bioinformatics"}, {"title": "Targeted Genotyping of Variable Number Tandem Repeats with adVNTR", "url": "https://www.biorxiv.org/content/early/2017/11/18/221754", "abstract": "Whole Genome Sequencing is increasingly used to identify Mendelian variants in clinical pipelines. These pipelines focus on single nucleotide variants (SNVs) and also structural variants, while ignoring more complex repeat sequence variants. We consider the problem of genotyping Variable Number Tandem Repeats, composed of inexact tandem duplication of shorter (6-100bp) repeating units. VNTRs span 3% of the human genome, may be located in UTRs and coding exons, and are involved in multiple Mendelian disorders. While existing tools recognize VNTR carrying sequence, genotyping VNTRs (determining repeat unit count, and sequence variation) from whole genome sequenced reads remains challenging. We describe a method, adVNTR, that uses HMMs to model each VNTR, count repeat units, and detect sequence variation. adVNTR models can be developed for short-read (Illumina) and single molecule (PacBio) whole genome and exome sequencing, and show good results on multiple simulated and real data sets. adVNTR is available at https://github.com/mehrdadbakhtiari/adVNTR", "tag": "Bioinformatics"}, {"title": "Wx: a neural network-based feature selection algorithm for next-generation sequencing data", "url": "https://www.biorxiv.org/content/early/2017/11/18/221911", "abstract": "Motivation: Next-generation sequencing (NGS), which allows the simultaneous sequencing of billions of DNA fragments simultaneously, has revolutionized how we study genomics and molecular biology by generating genome-wide molecular maps of molecules of interest. For example, an NGS-based transcriptomic assay called RNA-seq can be used to estimate the abundance of approximately 190,000 transcripts together. As the cost of next-generation sequencing sharply declines, researchers in many fields have been conducting research using NGS. The amount of information produced by NGS has made it difficult for researchers to choose the optimal set of target genes (or genomic loci). Results: We have sought to resolve this issue by developing a neural network-based feature (gene) selection algorithm called Wx. The Wx algorithm ranks genes based on the discriminative index (DI) score that represents the classification power for distinguishing given groups. With a gene list ranked by DI score, researchers can institutively select the optimal set of genes from the highest-ranking ones. We applied the Wx algorithm to a TCGA pan-cancer gene-expression cohort to identify an optimal set of gene-expression biomarker (universal gene-expression biomarkers) candidates that can distinguish cancer samples from normal samples for 12 different types of cancer. The 14 gene-expression biomarker candidates identified by Wx were comparable to or outperformed previously reported universal gene expression biomarkers, highlighting the usefulness of the Wx algorithm for next-generation sequencing data. Thus, we anticipate that the Wx algorithm can complement current state-of-the-art analytical applications for the identification of biomarker candidates as an alternative method.", "tag": "Bioinformatics"}, {"title": "Walking behavior in a circular arena modified by pulsed light stimulation in Drosophila melanogaster w1118 line", "url": "https://www.biorxiv.org/content/early/2017/11/17/119966", "abstract": "The Drosophila melanogaster white-eyed w1118 line serves as a blank control, allowing genetic recombination of any gene of interest along with a readily recognizable marker. w1118 flies display behavioral susceptibility to environmental stimulation such as light. It is of great importance to characterize the behavioral performance of w1118 flies because this would provide a baseline from which the effect of the gene of interest could be differentiated. Little work has been performed to characterize the walking behavior in adult w1118 flies. Here we show that pulsed light stimulation increased the regularity of walking trajectories of w1118 flies in circular arenas. We statistically modeled the distribution of distances to center and extracted the walking structures of w1118 flies. Pulsed light stimulation redistributed the time proportions for individual walking structures. Specifically, pulsed light stimulation reduced the episodes of crossing over the central region of the arena. An addition of four genomic copies of mini-white, a common marker gene for eye color, mimicked the effect of pulsed light stimulation in reducing crossing in a circular arena. The reducing effect of mini-white was copy-number-dependent. These findings highlight the rhythmic light stimulation-evoked modifications of walking behavior in w1118 flies and an unexpected behavioral consequence of mini-white in transgenic flies carrying w1118 isogenic background.", "tag": "Bioinformatics"}, {"title": "TKSA-MC: A Web Server for rational mutation through the optimization of protein charge interactions", "url": "https://www.biorxiv.org/content/early/2017/11/17/221556", "abstract": "The TKSAMC is a web server which calculates protein charge-charge interactions via the Tanford-Kirkwood Surface Accessibility model with the Monte Carlo method for sampling different protein protonation states. The optimization of charge-charge interactions via directed mutations has successfully enhanced the thermal stability of different proteins and could be a key to protein engineering improvement. The server presents the electrostatic free energy contribution of each polar-charged residue to protein native state stability. The server also indicates which residues contribute to destabilizing the protein native state with positive energy and the side chain exposed to solvent. This residue is a candidate for mutation to increase protein thermostability as a function of the chosen pH condition. The web server is freely available at UNESP (S\u00e3o Paulo State University - DF/IBILCE): http://tksamc.df.ibilce.unesp.br.", "tag": "Bioinformatics"}, {"title": "Clust: automatic extraction of optimal co-expressed gene clusters from gene expression data", "url": "https://www.biorxiv.org/content/early/2017/11/17/221309", "abstract": "Identification of co-expressed genes within a given experimental or biological context can provide evidence for genetic or physical interactions between genes. Thus, detection of co-expression has become a routine step in large-scale analyses of gene expression data. In this work, we show that application of the most commonly used methods to identify co-expressed gene clusters produce results that do not match the biological expectations of co-expressed gene clusters. Specifically, clusters generated using these methods are not discrete and can contain up to 50% unreliably assigned genes. Consequently, downstream analyses on these clusters, such as functional term enrichment analysis, suffer from high error rates. We present clust, an automated method that solves this problem by extracting clusters from gene expression datasets that match the biological expectations of co-expressed genes. Using 100 gene expression datasets from five model organisms we demonstrate that the statistical properties of clusters generated by clust are better than those produced by other methods. We further show that this improvement results in a concomitant improvement in detection of enriched functional terms.", "tag": "Bioinformatics"}, {"title": "CapsidMesh: atomic-detail structured mesh representation of icosahedral viral capsids for the study of their physical properties", "url": "https://www.biorxiv.org/content/early/2017/11/17/221663", "abstract": "Viruses are the most abundant pathogens affecting all forms of life. A major component of a virus is a protein shell, known as the viral capsid, that encapsulates the genomic material. The capsid has the fundamental functions to protect and transport the viral genome, and recognize the host cell. Descriptions of this macromolecular complex have been proposed at different scales of approximation, but little is known about the physical properties of the capsid. Here, we introduce a methodology to generate a structured volumetric mesh of icosahedral viral capsids, or CapsidMesh, based on their atomic information. The CapsidMesh models are suitable for numerical simulations and analysis, keeping control over all levels of protein structure (atoms, amino acids, chains, oligomers, capsid). Material properties of the capsid subunits can be set at every mesh element and then run a simulation of a physical process using a third-party package. Analysis can be made by extracting the information of interest. We validated our methodology by generating a CapsidMesh and simulating the nanoindentation through Finite Element analysis of several capsids previously characterized by Atomic Force Microscopy experiments. We estimated the elastic modulus consistently by reproducing the experimental linear elastic response. Our results show that the atomic detail of the CapsidMesh is sufficient to reproduce anisotropic properties of the particle. Supplementary Information available.", "tag": "Bioinformatics"}, {"title": "Simple statistical identification and removal of contaminant sequences in marker-gene and metagenomics data", "url": "https://www.biorxiv.org/content/early/2017/11/17/221499", "abstract": "The accuracy of microbial community surveys based on marker-gene and metagenomic sequencing (MGS) suffers from the presence of contaminants - DNA sequences not truly present in the sample. Contaminants come from a variety of sources, including reagents. Appropriate laboratory practices can reduce contamination in MGS data, but do not eliminate it. Here we introduce decontam (https://github.com/benjjneb/decontam), an open-source R package which implements a statistical classification procedure for identifying contaminants in MGS data. Contaminants are identified on the basis of two widely reproduced signatures: contaminants are more frequent in low-concentration samples, and are often found in negative controls. In a dataset from the human oral microbiome, the classification of amplicon sequence variants by decontam was strongly consistent with prior microscopic observations of microbial taxa in that environment. In both metagenomics and marker-gene measurements of a mock community dilution series, the removal of contaminants identified by decontam substantially reduced technical variation due to differences in reagents and sequencing centers. The application of decontam to two recently published datasets corroborated and extended their conclusions that little evidence existed for an indigenous placenta microbiome, and that some low-frequency taxa seemingly associated with preterm birth were run-specific contaminants. decontam integrates easily with existing MGS workflows, and allows researchers to generate more accurate profiles of microbial community composition at little to no additional cost.", "tag": "Bioinformatics"}, {"title": "CRISPys: Optimal sgRNA design for editing multiple members of a gene family using the CRISPR system", "url": "https://www.biorxiv.org/content/early/2017/11/17/221341", "abstract": "The discovery and development of the CRISPR-Cas9 system in the past few years has made eukaryotic genome editing, and specifically gene knockout for reverse genetics, a simpler, efficient, and effective task. The system is directed to the genomic target site by a programmed single-guide RNA (sgRNA) that base-pairs with the DNA target, subsequently leading to site-specific double-strand breaks. However, many gene families in eukaryotic genomes exhibit partially overlapping functions and, thus, the knockout of one gene might be concealed by the function of the other. In such cases, the reduced specificity of the CRISPR-Cas9 system, which may lead to the cleavage of genomic sites that are not identical to the sgRNA, can be harnessed for the simultaneous knockout of multiple homologous genes. Here, we introduce CRISPys, an algorithm for the optimal design of sgRNAs that would potentially target multiple members of a given gene family. CRISPys first clusters all the potential targets in the input sequences into a hierarchical tree structure that specifies the similarity among them. Then, sgRNAs are proposed in the internal nodes of the tree by embedding mismatches where needed, such that the cleavage efficiencies of the induced targets are maximized. We suggest several approaches for designing the optimal individual sgRNA, and an approach that provides a set of sgRNAs that also accounts for the homologous relationships among gene-family members. We further show by in-silico examination over all gene families in the Solanum lycopersicum genome that our suggested approach outperforms simpler alignment-based techniques.", "tag": "Bioinformatics"}, {"title": "SV2: Accurate Structural Variation Genotyping and De Novo Mutation Detection", "url": "https://www.biorxiv.org/content/early/2017/11/17/113498", "abstract": "Structural Variation (SV) detection from short-read whole genome sequencing is error prone, presenting significant challenges for population or family-based studies of disease. Here we describe SV2, a machine-learning algorithm for genotyping deletions and duplications from paired-end sequencing data. SV2 can rapidly integrate variant calls from multiple structural variant discovery algorithms into a unified call set with high genotyping accuracy and capability to detect de novo mutations.", "tag": "Bioinformatics"}, {"title": "Temporal epigenomic profiling identifies AHR as dynamic super-enhancer controlled regulator of mesenchymal multipotency", "url": "https://www.biorxiv.org/content/early/2017/11/17/183988", "abstract": "Temporal data on gene expression and context-specific open chromatin states can improve identification of key transcription factors (TFs) and the gene regulatory networks (GRNs) controlling cellular differentiation. However, their integration remains challenging. Here, we delineate a general approach for data-driven and unbiased identification of key TFs and dynamic GRNs, called EPIC-DREM. We generated time-series transcriptomic and epigenomic profiles during differentiation of mouse multipotent bone marrow stromal cells (MSCs) towards adipocytes and osteoblasts. Using our novel approach we constructed time-resolved GRNs for both lineages. To prioritize the identified shared regulators, we mapped dynamic super-enhancers in both lineages and associated them to target genes with correlated expression profiles. We identified aryl hydrocarbon receptor (AHR) as a mesenchymal key TF controlled by a dynamic cluster of MSC-specific super-enhancers that become repressed in both lineages. AHR represses differentiation-induced genes such as Notch3 and we propose AHR to function as a guardian of mesenchymal multipotency.", "tag": "Bioinformatics"}, {"title": "glactools: a command-line toolset for the management of genotype likelihoods and allele counts", "url": "https://www.biorxiv.org/content/early/2017/11/17/221127", "abstract": "Motivation: Research projects involving population genomics routinely need to store genotyping information, population allele frequencies, combine files from different samples, query the data and export it to various formats. This is often done using bespoke in-house scripts which cannot be easily adapted to new projects and seldom constitute reproducible workflows. Results: We introduce glactools, a set of command-line utilities which can import data from genotypes or population-wide allele frequencies into an intermediate representation, compute various operations on it and export the data to several file formats used by population genetics software. This intermediate format can take 2 forms, one to store per-individual genotype likelihoods and a second for allele counts from one or more individuals. glactools allows users to perform operations such as intersecting datasets, merging individuals into populations, creating subsets, perform queries (e.g. return sites where a given population does not share an allele with a second one) and compute summary statistics to answer biologically relevant questions. Availability: glactools is freely available for use under the GPL. It requires a C++ compiler and the htslib library. (https://grenaud.github.io/glactools/).", "tag": "Bioinformatics"}, {"title": "Newly identified relatives of botulinum neurotoxins shed light on their molecular evolution", "url": "https://www.biorxiv.org/content/early/2017/11/17/220806", "abstract": "The evolution of bacterial toxins is a central question to understanding the origins of human pathogens and infectious disease. Through genomic data mining, we traced the evolution of the deadliest known toxin family, clostridial neurotoxins, comprised of tetanus and botulinum neurotoxins (BoNT). We identified numerous uncharacterized lineages of BoNT-related genes in environmental species outside of Clostridium, revealing insights into their molecular ancestry. Phylogenetic analysis pinpointed a sister lineage of BoNT-like toxins in the gram-negative organism, Chryseobacterium piperi, that exhibit distant homology at the sequence level but preserve overall domain architecture. Resequencing and assembly of the C. piperi genome confirmed the presence of BoNT-like proteins encoded within two toxin-rich gene clusters. A C. piperi BoNT-like protein was validated as a novel toxin that induced necrotic cell death in human kidney cells. Mutagenesis of the putative active site abolished toxicity and indicated a zinc metalloprotease-dependent mechanism. The C. piperi toxin did not cleave common SNARE substrates of BoNTs, indicating that BoNTs have diverged from related families in substrate specificity. The new lineages of BoNT-like toxins identified by computational methods represent evolutionary missing links, and suggest an origin of clostridial neurotoxins from ancestral toxins present in environmental bacteria.", "tag": "Bioinformatics"}, {"title": "Efficient online-group-screening designs for agent identification", "url": "https://www.biorxiv.org/content/early/2017/11/17/220863", "abstract": "Identifying significant causal agents among a large number of candidates is challenging. When experimental resources are limited, exhaustively screening a large number of agents for the desired effect could incur a large cost and take a substantial amount of time. However, in many large scale experiments, such as high-throughput screening (HTS), the ratio of causal to non-causal agents is usually very low. In this paper, we introduce a group-screening strategy to efficiently screen causal agents by grouping them into treatments. Our analysis shows that when a large number of candidates factors are screened and true agent percentage is very low (less than 1%), even in the worst case we could save up to 80% of the experiment runs. In the case where experiments span many rounds, we provide an online version of the group-screening that can determine the best strategy automatically based on the existing results. We applied this method to a real HTS experiment with 50,000 candidates that would require 9 rounds to finish in an exhaustive case. Our analysis showed that by applying the online-group-screening method, in the worst case, we can use 3 rounds and 19.7% (9828/50000) total tests to identify all the agents. Finally, we show that with minor modifications, this framework extends to more complex agent discovery problems.", "tag": "Bioinformatics"}, {"title": "Fast ordered sampling of DNA sequence variants", "url": "https://www.biorxiv.org/content/early/2017/11/17/220871", "abstract": "Explosive growth in the amount of genomic data is matched by increasing power of consumer-grade computers. Even applications that require powerful servers can be quickly tested on desktop or laptop machines if we can generate representative samples from large data sets. I describe a fast and memory-efficient implementation of an on-line sampling method developed for tape drives 30 years ago. Focusing on genotype files, I test the performance of this technique on modern solid-state and spinning hard drives, and show that it performs well compared to a simple sampling scheme. I illustrate its utility by developing a method to quickly estimate genome-wide patterns of linkage disequilibrium (LD) decay with distance. I provide open-source software that samples loci from several variant format files, a separate program that performs LD decay estimates, and a C++ library that lets developers incorporate these methods into their own projects.", "tag": "Bioinformatics"}, {"title": "Systematic interrogation of diverse Omic data reveals interpretable, robust, and generalizable transcriptomic features of clinically successful therapeutic targets", "url": "https://www.biorxiv.org/content/early/2017/11/16/220848", "abstract": "Target selection is the first and pivotal step in drug discovery. An incorrect choice may not manifest itself for many years after hundreds of millions of research dollars have been spent. We collected a set of 332 targets that succeeded or failed in phase III clinical trials, and explored whether Omic features describing the target genes could predict clinical success. We obtained features from the recently published comprehensive resource: Harmonizome. Nineteen features appeared to be significantly correlated with phase III clinical trial outcomes, but only 4 passed validation schemes that used bootstrapping or modified permutation tests to assess feature robustness and generalizability while accounting for target class selection bias. We also used classifiers to perform multivariate feature selection and found that classifiers with a single feature performed as well in cross-validation as classifiers with more features (AUROC=0.57 and AUPR=0.81). The two predominantly selected features were mean mRNA expression across tissues and standard deviation of expression across tissues, where successful targets tended to have lower mean expression and higher expression variance than failed targets. This finding supports the conventional wisdom that it is favorable for a target to be present in the tissue(s) affected by a disease and absent from other tissues. Overall, our results suggest that it is feasible to construct a model integrating interpretable target features to inform target selection. We anticipate deeper insights and better models in the future, as researchers can reuse the data we have provided to improve methods for handling sample biases and learn more informative features. Code, documentation, and data for this study have been deposited on GitHub at https://github.com/arouillard/omic-features-successful-targets.", "tag": "Bioinformatics"}, {"title": "The fractured landscape of RNA-seq alignment: The default in our STARs", "url": "https://www.biorxiv.org/content/early/2017/11/16/220681", "abstract": "Many tools are available for RNA-seq alignment and expression quantification, with comparative value being hard to establish. Benchmarking assessments often show high performance, with occasional outliers, but are often focused on either model data or fail to explain variation in performance in detail. This leaves us to ask, what is the most meaningful way to assess different alignment choices? And importantly, where is there room for progress? In this work, we explore the answers to these two questions by performing an exhaustive assessment of the STAR aligner. We assess STAR's performance across a range of alignment parameters using common metrics, and then on biologically focused tasks. We find technical metrics such as fraction mapping or expression profile correlation are uninformative, capturing properties unlikely to have any role in biological discovery. Surprisingly, we find that changes in alignment parameters within a wide range have little impact on both technical and biological performance. Yet, when performance finally does break, it happens in difficult regions, such as X Y paralogs and MHC genes. We believe improved reporting by developers will help establish where results are likely to be robust or fragile, providing a better baseline to establish where methodological progress can still occur.", "tag": "Bioinformatics"}, {"title": "LSX: Automated reduction of gene-specific lineage evolutionary rate heterogeneity for multi-gene phylogeny inference", "url": "https://www.biorxiv.org/content/early/2017/11/16/220053", "abstract": "Motivation: LS3 is a recently published algorithm to reduce lineage evolutionary rate heterogeneity, a condition that can produce inference artifacts in molecular phylogenetics. The LS3 scripts are Linux-specific and the criterion to reduce lineage rate heterogeneity can be too stringent in datasets with both very long and very short branches. Results: LSX is a multi-platform user-friendly R script that performs the LS3 algorithm, and has added features in order to make better lineage rate calculations. In addition, we developed and implemented an alternative version of the algorithm, LS4, which reduces lineage rate heterogeneity not only by detecting branches that are too long but also branches that are too short, resulting in less stringent data filtering. Availability: The LSX script LSx_v.1.1.R and the user manual are available for download at: https://genev.unige.ch/research/laboratory/Juan-Montoya .", "tag": "Bioinformatics"}, {"title": "CPdock: The Complementarity Plot for Docking of Proteins: Implementing Multi-dielectric Continuum Electrostatics", "url": "https://www.biorxiv.org/content/early/2017/11/16/185686", "abstract": "The Complementarity plot (CP) is an established validation tool for protein structures, applicable to both, globular proteins (folding) as well as protein-protein complexes (binding). It computes the shape and electrostatic complementarities (Sm, Em) for amino acid side-chains buried within the protein interior or interface and plots them in a two-dimensional plot having knowledge-based probabilistic quality estimates for the residues as well as for the whole structure. The current report essentially presents an upgraded version of the plot with the implementation of the advanced multi-dielectric functionality (as in Delphi version 6.2 or higher) in the computation of electrostatic complementarity to make the validation tool physico-chemically more realistic. The two methods (single- and multi-dielectric) agrees decently in their resultant Em values and hence, provisions for both methods have been kept in the software suite. So to speak, the global electrostatic balance within a well-folded protein and / or a well-packed interface seems only marginally perturbed by the choice of different internal dielectric values. However, both from theoretical as well as practical grounds, the more advanced multi-dielectric version of the plot is certainly recommended for potentially producing more reliable results. The report also presents a new methodology and a variant plot, namely, CPdock, based on the same principles of complementarity, specifically designed to be used in the docking of proteins. The efficacy of the method to discriminate between good and bad docked protein complexes have been tested on a recent state-of-the-art docking benchmark. The results unambiguously indicate that CPdock can indeed be effective in the initial screening phase of a docking scoring pipeline before going into more sophisticated and computationally expensive scoring functions. CPdock has been made available at https://github.com/nemo8130/CPdock", "tag": "Bioinformatics"}, {"title": "miRAW: A deep learning approach to predict miRNA targets by analyzing whole miRNA transcripts", "url": "https://www.biorxiv.org/content/early/2017/11/16/220483", "abstract": "MicroRNAs (miRNAs) are small non-coding RNAs that regulate gene expression by binding to partially complementary regions within the 3'UTR of their target genes. Computational methods play an important role in target prediction and assume that the miRNA \"seed region\" (nt 2 to 8) is required for functional targeting, but typically only identify ~80% of known bindings. Recent studies have highlighted a role for the entire miRNA, suggesting that a more flexible methodology is needed. We present a novel approach for miRNA target prediction based on Deep Learning (DL) which, rather than incorporating any knowledge (such as seed regions), investigates the entire miRNA and 3'UTR mRNA nucleotides to learn a uninhibited set of feature descriptors related to the targeting process. We collected more than 150,000 experimentally validated homo sapiens miRNA:gene targets and cross referenced them with different CLIP-Seq, CLASH and iPAR-CLIP datasets to obtain ~20,000 validated miRNA:gene exact target sites. Using this data, we implemented and trained a deep neural network - composed of autoencoders and a feed-forward network - able to automatically learn features describing miRNA-mRNA interactions and assess functionality. Predictions were then refined using information such as site location or site accessibility energy. In a comparison using independent datasets, our DL approach consistently outperformed existing prediction methods, recognizing the seed region as a common feature in the targeting process, but also identifying the role of pairings outside this region. Thermodynamic analysis also suggests that site accessibility plays a role in targeting but that it cannot be used as a sole indicator for functionality.", "tag": "Bioinformatics"}, {"title": "Salt-bridge Dynamics in Intrinsically Disordered Proteins: A trade-off between electrostatic interactions and structural flexibility", "url": "https://www.biorxiv.org/content/early/2017/11/16/220392", "abstract": "Intrinsically Disordered Proteins (IDPs) are enriched in charged and polar residues; and, therefore, electrostatic interactions play a predominant role in their dynamics. In order to remain multi-functional and exhibit their characteristic binding promiscuity, they need to retain considerable dynamic flexibility. At the same time, they also need to accommodate a large number of oppositely charged residues, which eventually lead to the formation of salt-bridges, imparting local rigidity. The formation of salt-bridges therefore oppose the desired dynamic flexibility. Hence, there appears to be a meticulous trade-off between the two mechanisms which the current study attempts to unravel. With this objective, we identify and analyze salt-bridges, both as isolated as well as composite ionic bond motifs, in the molecular dynamic trajectories of a set of appropriately chosen IDPs. Dynamic structural properties of these salt-bridges like persistence, time evolution of the secondary structural 'order-disorder' transitions, correlated atomic movements, contribution in the overall electrostatic balance of the proteins and other essential features have been studied in necessary detail. The results suggest that the key to maintain such a trade-off over time is the continuous formation and dissolution of salt-bridges with a wide range of persistence. Also, the continuous dynamic interchange of charged-atom-pairs (coming from a variety of oppositely charged side-chains) in the transient ionic bonds supports a model of dynamic flexibility concomitant with the well characterized stochastic conformational switching in these proteins. Furthermore, the analyses of hydrophobic burial profiles present a simple and effective single-parameter-tool (namely, the rGb score) to characterize the instability of the IDPs in solution - which serves as the basis of their high reactivity. The results and conclusions should facilitate the future design of salt-bridges as a mean to further explore the disordered-globular interface in proteins.", "tag": "Bioinformatics"}, {"title": "What are the most influencing factors in reconstructing a reliable transcriptome assembly?", "url": "https://www.biorxiv.org/content/early/2017/11/16/220269", "abstract": "Reconstructing the genome and transcriptome for a new or extant species are essential steps in expanding our understanding of the organism's active RNA landscape and gene regulatory dynamics, as well as for developing therapeutic targets to fight disease. The advancement of sequencing technologies has paved the way to generate high-quality draft transcriptomes. With many possible approaches available to accomplish this task, there is a need for a closer investigation of the factors that influence the quality of the results. We carried out an extensive survey of variety of elements that are important in transcriptome assembly. We utilized the human RNA-Seq data from the Sequencing Quality Control Consortium (SEQC) as a well-characterized and comprehensive resource with an available, well-studied human reference genome. Our results indicate that the quality of the library construction significantly impacts the quality of the assembly. Higher coverage of the genome is not as important as the quality of the input RNA-Seq data. Thus, once a certain coverage is attained, the quality of the assembly is mainly dependent on the base-calling accuracy of the input sequencing reads; and it is important to avoid saturating the assembler with extra coverage.", "tag": "Bioinformatics"}, {"title": "Latent variable model for aligning barcoded short-reads improves downstream analyses", "url": "https://www.biorxiv.org/content/early/2017/11/16/220236", "abstract": "Recent years have seen the emergence of several \"third-generation\" sequencing platforms, each of which aims to address shortcomings of standard next-generation short-read sequencing by producing data that capture long-range information, thereby allowing us to access regions of the genome that are inaccessible with short-reads alone. These technologies either produce physically longer reads typically with higher error rates or instead capture long-range information at low error rates by virtue of read \"barcodes\" as in 10x Genomics' Chromium platform. As with virtually all sequencing data, sequence alignment for third-generation sequencing data is the foundation on which all downstream analyses are based. Here we introduce a latent variable model for improving barcoded read alignment, thereby enabling improved downstream genotyping and phasing. We demonstrate the feasibility of this approach through developing EMerAld -- or EMA for short -- and testing it on the barcoded short-reads produced by 10x's sequencing technologies. EMA not only produces more accurate alignments, but unlike other methods also assigns interpretable probabilities to the alignments it generates. We show that genotypes called from EMA's alignments contain over 30% fewer false positives than those called from Lariat's (the current 10x alignment tool), with a fewer number of false negatives, on datasets of NA12878 and NA24385 as compared to NIST GIAB gold standard variant calls. Moreover, we demonstrate that EMA is able to effectively resolve alignments in regions containing nearby homologous elements -- a particularly challenging problem in read mapping -- through the introduction of a novel statistical binning optimization framework, which allows us to find variants in the pharmacogenomically important CYP2D region that go undetected when using Lariat or BWA. Lastly, we show that EMA's alignments improve phasing performance compared to Lariat's in both NA12878 and NA24385, producing fewer switch/mismatch errors and larger phase blocks on average. EMA software and datasets used are available at http://ema.csail.mit.edu.", "tag": "Bioinformatics"}, {"title": "ChIPdig: a comprehensive user-friendly tool for mining multi-sample ChIP-seq data", "url": "https://www.biorxiv.org/content/early/2017/11/16/220079", "abstract": "Summary: In this article we present ChIPdig, a tool for analyzing and comparing multiple ChIP-seq data sets. ChIPdig is written in R and enables access to powerful R-based functions and packages through a simple user interface powered by the shiny package. It allows users to align reads to a reference genome, perform peak calling and differential enrichment analysis on regions of interest, annotate such regions based on available coordinates of transcription start and termination sites, exons, introns, 5\u2032 UTRs and 3\u2032 UTRs, and generate heatmaps and metaplots for visualizing coverage. Availability and implementation: ChIPdig is open source and available at https://github.com/rmesse/ChIPdig.", "tag": "Bioinformatics"}, {"title": "Per-sample immunoglobulin germline inference from B cell receptor deep sequencing data", "url": "https://www.biorxiv.org/content/early/2017/11/16/220285", "abstract": "The collection of immunoglobulin genes in an individual's germline, which gives rise to B cell receptors via recombination, is known to vary significantly across individuals. In humans, for example, each individual has only a fraction of the several hundred known V alleles. Furthermore, this set of known V alleles is both incomplete (particularly for non-European samples), and contains a significant number of spurious alleles. The resulting uncertainty as to which immunoglobulin alleles are present in any given sample results in inaccurate B cell receptor sequence annotations, and in particular inaccurate inferred naive ancestors. In this paper we first show that the currently widespread practice of aligning each sequence to its closest match in the full set of IMGT alleles results in a very large number of spurious alleles that are not in the sample's true set of germline V alleles. We then describe a new method for inferring each individual's germline gene set from deep sequencing data, and show that it improves upon existing methods by making a detailed comparison on a variety of simulated and real data samples. This new method has been integrated into the partis annotation and clonal family inference package, available at https://github.com/psathyrella/partis, and is run by default without affecting overall run time.", "tag": "Bioinformatics"}, {"title": "Extracting biological age from biomedical data via deep learning: too much of a good thing?", "url": "https://www.biorxiv.org/content/early/2017/11/16/219162", "abstract": "Aging-related physiological changes are systemic and, at least in humans, are linearly associated with age. Therefore, linear combinations of physiological measures trained to estimate chronological age have recently emerged as a practical way to quantify aging in the form of biological age. Aging acceleration, defined as the difference between the predicted and chronological age was found to be elevated in patients with major diseases and is predictive of mortality. In this work, we compare three increasingly accurate biological age models: metrics derived from unsupervised Principal Components Analysis (PCA), alongside two supervised biological age models; a multivariate linear regression and a state-of-the-art deep convolution neural network (CNN). All predictions were made using one-week long locomotor activity records from a 2003-2006 National Health and Nutrition Examination Survey (NHANES) dataset. We found that application of the supervised approaches improves the accuracy of the chronological age estimation at the expense of a loss of the association between the aging acceleration predicted by the model and all-cause mortality. Instead, we turned to the NHANES death register and introduced a novel way to train parametric proportional hazards models in a form suitable for out-of-the-box implementation with any modern machine learning software. Finally, we characterized a proof-of-concept example, a separate deep CNN trained to predict mortality risks that outperformed any of the biological age or simple linear proportional hazards models. Our findings demonstrate the emerging potential of combined wearable sensors and deep learning technologies for applications involving continuous health risk monitoring and real-time feedback to patients and care providers.", "tag": "Bioinformatics"}, {"title": "Exploring Applications of Crowdsourcing to Cryo-EM", "url": "https://www.biorxiv.org/content/early/2017/11/15/220145", "abstract": "Extraction of particles from cryo-electron microscopy (cryo-EM) micrographs is a crucial step in processing single-particle datasets. Although algorithms have been developed for automatic particle picking, these algorithms generally rely on two-dimensional templates for particle identification, which may exhibit biases that can propagate artifacts through the reconstruction pipeline. Manual picking is viewed as a gold-standard solution for particle selection, but it is too time-consuming to perform on data sets of thousands of images. In recent years, crowdsourcing has proven effective at leveraging the open web to manually curate datasets. In particular, citizen science projects such as Galaxy Zoo have shown the power of appealing to users' scientific interests to process enormous amounts of data. To this end, we explored the possible applications of crowdsourcing in cryo-EM particle picking, presenting a variety of novel experiments including the production of a fully annotated particle set from untrained citizen scientists. We show the possibilities and limitations of crowdsourcing particle selection tasks, and explore further options for crowdsourcing cryo-EM data processing.", "tag": "Bioinformatics"}, {"title": "Molecular characterization of breast and lung tumors by integration of multiple data types with sparse-factor analysis", "url": "https://www.biorxiv.org/content/early/2017/11/15/183582", "abstract": "Effective cancer treatment is crucially dependent on the identification of the biological processes that drive a tumor. However, multiple processes may be active simultaneously in a tumor. Clustering is inherently unsuitable to this task as it assigns a tumor to a single cluster. In addition, the wide availability of multiple data types per tumor provides the opportunity to profile the processes driving a tumor more comprehensively. Here we introduce Functional Sparse-Factor Analysis (funcSFA) to address these challenges. FuncSFA integrates multiple data types to define a lower dimensional space capturing the relevant variation. A tailor-made module associates biological processes with these factors. FuncSFA is inspired by iCluster, which we improve in several key aspects. First, we increase the convergence efficiency significantly, allowing the analysis of multiple molecular datasets that have not been pre-matched to contain only concordant features. Second, FuncSFA does not assign tumors to discrete clusters, but identifies the dominant driver processes active in each tumor. This is achieved by a regression of the factors on the RNA expression data followed by a functional enrichment analysis and manual curation step. We apply FuncSFA to the TCGA breast and lung datasets. We identify EMT and Immune processes common to both cancer types. In the breast cancer dataset we recover the known intrinsic subtypes and identify additional processes. These include immune infiltration and EMT, and processes driven by copy number gains on the 8q chromosome arm. In lung cancer we recover the major types (adenocarcinoma and squamous cell carcinoma) and processes active in both of these types. These include EMT, two immune processes, and the activity of the NFE2L2 transcription factor. In summary, FuncSFA is a robust method to perform discovery of key driver processes in a collection of tumors through unsupervised integration of multiple molecular data types and functional annotation.", "tag": "Bioinformatics"}, {"title": "The Bio::Phylo libraries for phylogenetic data analysis, version 2.0", "url": "https://www.biorxiv.org/content/early/2017/11/15/211334", "abstract": "Phylogenetic analysis is a broad and expanding field that requires versatile programming toolkits to manage the various data types, file formats, and needs for scalability, simulation, visualization, and data exploration. We present version 2.0 of the Bio::Phylo libraries for phylogenetic data analysis. This new release represents a rewrite of the architecture, allowing for extensions that improve speed and persistence, as well as increased functionality in terms of analysis, data reading and writing, and visualization. The package is released as open source software under the same terms as Perl itself and available from the comprehensive Perl archive network as well as directly from the source code repository.", "tag": "Bioinformatics"}, {"title": "Simultaneous Determination of Protein Structure and Dynamics Using Cryo-Electron Microscopy", "url": "https://www.biorxiv.org/content/early/2017/11/15/219972", "abstract": "Cryo-electron microscopy is rapidly emerging as a powerful technique to determine the structures of complex macromolecular systems elusive to other techniques. Since many of these systems are highly dynamical, characterising also their movements is a crucial step to unravel their biological functions. To achieve this goal, we report an integrative modelling approach to simultaneously determine structure and dynamics from cryo-electron microscopy density maps. By quantifying the level of noise in the data and dealing with their ensemble-averaged nature, this approach enables the integration of multiple sources of information to model ensembles of structures and infer their populations. We illustrate the method by characterising structure and dynamics of the integral membrane receptor STRA6, thus providing insights into the mechanisms by which it interacts with retinol binding protein and translocates retinol across the membrane.", "tag": "Bioinformatics"}, {"title": "Anisotropic Cellular Mechanoresponse for Radial Size Maintenance of Developing Epithelial Tubes", "url": "https://www.biorxiv.org/content/early/2017/11/15/172916", "abstract": "Cellular behaviors responding to mechanical forces control the size of multicellular tissues as demonstrated in isotropic size maintenance of developing tissues. However, how mechanoresponse systems work to maintain anisotropic tissue size including tube radial size remains unknown. Here we reveal the system underlying radial size maintenance of the murine epididymal tubule by combining quantitative imaging, mathematical modeling, and mechanical perturbations. We found that an oriented cell intercalation making the tubule radial size smaller counteracts a cell tension reduction due to neighbor cell division along the tubule circumferential axis. Moreover, we demonstrated that the tubule cells enhance actomyosin constriction driving the cell intercalation in response to mechanical forces anisotropically applied on the cells. Our results suggest that epididymal tubule cells have endogenous systems for responding as active cell movement to mechanical forces exclusively along the circumferential axis, and the anisotropic cellular mechanoresponse spontaneously controls the tubule radial size.", "tag": "Bioinformatics"}, {"title": "Privacy-preserving generative deep neural networks support clinical data sharing", "url": "https://www.biorxiv.org/content/early/2017/11/15/159756", "abstract": "Though it is widely recognized that data sharing enables faster scientific progress, the sensible need to protect participant privacy hampers this practice in medicine. We train deep neural networks that generate synthetic subjects closely resembling study participants. Using the SPRINT trial as an example, we show that machine-learning models built from simulated participants generalize to the original dataset. We incorporate differential privacy, which offers strong guarantees on the likelihood that a subject could be identified as a member of the trial. Investigators who have compiled a dataset can use our method to provide a freely accessible public version that enables other scientists to perform discovery-oriented analyses. Generated data can be released alongside analytical code to enable fully reproducible workflows, even when privacy is a concern. By addressing data sharing challenges, deep neural networks can facilitate the rigorous and reproducible investigation of clinical datasets.", "tag": "Bioinformatics"}, {"title": "Powerful Inference with the D-statistic on Low-Coverage Whole-Genome Data", "url": "https://www.biorxiv.org/content/early/2017/11/15/127852", "abstract": "The detection of ancient gene flow between human populations is an important issue in population genetics. A commonly used tool for detecting ancient admixture events is the D-statistic. The D-statistic is based on the hypothesis of a genetic relationship that involves four populations, whose correctness is assessed by evaluating specific coincidences of alleles between the groups. When working with high throughput sequencing data is it not always possible to accurately call genotypes. When genotype calling is not possible the D-statistic that is currently used samples a single base from the reads of one chosen individual per population. This method has the drawback of ignoring much of the information in the data. Those issues are especially striking in the case of ancient genomes, often characterized by low sequencing depth and high error rates for the sequenced bases. Here we provide a significant improvement to overcome the problems of the present-day D-statistic by considering all reads from multiple individuals in each population. Moreover we apply type-specific error correction to combat the problems of sequencing errors and show a way to correct for introgression from an external population that is not part of the supposed genetic relationship, and how this method leads to an estimate of the admixture rate. We prove that the improved D-statistic, as well as the traditional one, is approximated by a standard normal. Furthermore we show that our method overperforms the traditional D-statistic in detecting admixtures. The power gain is most pronounced for low/medium sequencing depth (1-10X) and performances are as good as with perfectly called genotypes at a sequencing depth of 2X. We also show the reliability of error correction on scenarios with simulated errors and ancient data, and correct for introgression in known scenarios to verify the correctness the estimation of the admixture rates.", "tag": "Bioinformatics"}, {"title": "BURRITO: An interactive multi-omic tool for visualizing taxa-function relationships in microbiome data", "url": "https://www.biorxiv.org/content/early/2017/11/15/217315", "abstract": "The abundance of both taxonomic groups and gene categories in microbiome samples can now be easily assayed via various sequencing technologies, and visualized using a variety of software tools. However, the assemblage of taxa in the microbiome and its gene content are clearly linked, and tools for visualizing the relationship between these two facets of microbiome composition and for facilitating exploratory analysis of their co-variation are lacking. Here we introduce BURRITO, a web tool for interactive visualization of microbiome multi-omic data with paired taxonomic and functional information. BURRITO simultaneously visualizes the taxonomic and functional compositions of multiple samples and dynamically highlights relationships between taxa and functions to capture the underlying structure of these data. Users can browse for taxa and functions of interest and interactively explore the share of each function attributed to each taxon across samples. BURRITO supports multiple input formats for taxonomic and metagenomic data, allows adjustment of data granularity, and can export generated visualizations as static publication-ready formatted figures. In this paper, we describe the functionality of BURRITO, and provide illustrative examples of its utility for visualizing various trends in the relationship between the composition of taxa and functions in complex microbiomes.", "tag": "Bioinformatics"}, {"title": "Metaproteomics of colonic microbiota unveils discrete protein functions among colitic mice and control groups", "url": "https://www.biorxiv.org/content/early/2017/11/15/219782", "abstract": "Metaproteomics can greatly assist established high-throughput sequencing methodologies to provide systems biological insights into the alterations of microbial protein functionalities correlated with disease-associated dysbiosis of the intestinal microbiota. Here, we utilized the well-characterized murine T cell transfer model of colitis to find specific changes within the intestinal luminal proteome associated with inflammation. MS proteomic analysis of colonic samples permitted the identification of ~10,000-12,000 unique peptides that corresponded to 5,610 protein clusters identified across three groups, including the colitic Rag1-/- T cell recipients, isogenic Rag1-/- controls, and wild-type mice. We demonstrate that the colitic mice exhibited a significant increase in Proteobacteria and Verrucomicrobia and show that such alterations in the microbial communities contributed to the enrichment of specific proteins with transcription and translation gene ontology terms. In combination with 16S sequencing, our metaproteomics-based microbiome studies provide a foundation for assessing alterations in intestinal luminal protein functionalities in a robust and well-characterized mouse model of colitis, and set the stage for future studies to further explore the functional mechanisms of altered protein functionalities associated with dysbiosis and inflammation.", "tag": "Bioinformatics"}, {"title": "Automated high throughput animal DNA metabarcode classification", "url": "https://www.biorxiv.org/content/early/2017/11/14/219675", "abstract": "Until now, there has been difficulty assigning names to animal barcode sequences isolated directly from eDNA in a rapid, high-throughput manner, providing a measure of confidence for each assignment. To address this gap, we have compiled nearly 1 million marker gene DNA barcode sequences appropriate for classifying chordates, arthropods, and flag members of other major eukaryote groups. We show that the RDP naive Bayesian classifier can assign the same number of queries 19 times faster than the popular BLAST top hit method and reduce the false positive rate by two-thirds. As reference databases become more representative of current species diversity, confidence in taxonomic assignments should continue to improve. We recommend that investigators can improve the performance of species-level assignments immediately by supplementing existing reference databases with full-length DNA barcode sequences from representatives of local fauna.", "tag": "Bioinformatics"}, {"title": "COLT-Viz: Interactive Visualization of Antibody Lineage Trees", "url": "https://www.biorxiv.org/content/early/2017/11/14/219592", "abstract": "Many tools have been developed to visualize phylogenetic trees, which is a traditional technique for evolutionary tree analysis. However, due to the unique characteristics of antibody lineage trees, the phylogenetic method cannot adequately construct proper tree structures for antibody lineages, and many other tools have been developed to address this problem. However, there still lacks of an adequate tool to visualize the resulted antibody lineage structures that are more complicated than phylogenetic trees. In addition, high-throughput sequencing-based antibody repertoire profiling enables the counting of the number of transcripts associated with individual antibody sequences, thus more dimensions need to be encoded in the tree structure visualization. Further, users may wish to manually adjust the tree structure for a special context. When doing so, they may wish to maintain some biological constraints that are applicable in antibody lineage tree structure, such as isotype switching constraints or different sampling constraints. Here, we report an interactive visualization tool (COLT-Viz) designed to display the number of RNA copies, number of somatic hypermutations, and sample collection time associated with each antibody sequence as well as the distance to neighboring sequences for each antibody sequence in the lineage. COLT-Viz also allows users to interactively visualize and edit antibody lineage structures while giving users the option to automatically check biological constraints on the edited structures to ensure accuracy. COLT-Viz takes JSON text format as input files and can easily be used to visualize networks with or without the biological constraints. We believe the amount of information that can be displayed for complex antibody lineages, the interactive interface, and the option of checking for biological constraints make COLT-Viz a versatile tool for antibody lineage tree visualization that will guide further biological discoveries.", "tag": "Bioinformatics"}, {"title": "Modeling Enhancer-Promoter Interactions with Attention-Based Neural Networks", "url": "https://www.biorxiv.org/content/early/2017/11/14/219667", "abstract": "Background: Gene regulatory sequences play critical roles in ensuring tightly controlled RNA expression patterns that are essential in a large variety of biological processes. Specifically, enhancer sequences drive expression of their target genes, and the availability of genome-wide maps of enhancer-promoter interactions has opened up the possibility to use machine learning approaches to extract and interpret features that define these interactions in different biological contexts. Methods: Inspired by machine translation models we develop an attention-based neural network model, EPIANN, to predict enhancer-promoter interactions based on DNA sequences. Codes and data are available at https://github.com/wgmao/EPIANN. Results: Our approach accurately predicts enhancer-promoter interactions across six cell lines. In addition, our method generates pairwise attention scores at the sequence level, which specify how short regions in the enhancer and promoter pair-up to drive the interaction prediction. This allows us to identify over-represented transcription factors (TF) binding sites and TF-pair interactions in the context of enhancer function.", "tag": "Bioinformatics"}, {"title": "High genomic diversity of multi-drug resistant wastewater Escherichia coli", "url": "https://www.biorxiv.org/content/early/2017/11/14/215210", "abstract": "Wastewater treatment plants play an important role in the release of antibiotic resistance into the environment. It has been shown that wastewater contains multi-drug resistant Escherichia coli, but information on strain diversity is surprisingly scarce. Here we present an exceptionally large dataset on multidrug resistant Escherichia coli, originating from wastewater, over a thousand isolates were phenotypically characterized for twenty antibiotics and for 103 isolates whole genomes were sequenced. To our knowledge this is the first study documenting such a comprehensive diversity of multi-drug resistant Escherichia coli in wastewater. The genomic diversity of the isolates was unexpectedly high and contained a high number of resistance and virulence genes. To illustrate the genomic diversity of the isolates we calculated the pan genome of the wastewater Escherichia coli and found it to contain over sixteen thousand genes. To analyse this diverse dataset, we devised a computational approach correlating genotypic variation and resistance phenotype, this way we were able to identify not only known, but also candidate resistance genes. Finally, we could verify that the effluent of a wastewater treatment plant will contain multi-drug resistant Escherichia coli belonging to clinically important clonal groups.", "tag": "Bioinformatics"}, {"title": "QuASAR: Quality Assessment of Spatial Arrangement Reproducibility in Hi-C Data", "url": "https://www.biorxiv.org/content/early/2017/11/14/204438", "abstract": "Hi-C has revolutionized global interrogation of chromosome conformation, however there are few tools to assess the reliability of individual experiments. Here we present a new approach, QuASAR, for measuring quality within and between Hi-C samples. We show that QuASAR can detect even tiny fractions of noise and estimate both return on additional sequencing and quality upper bounds. We also demonstrate QuASAR's utility in measuring replicate agreement across feature resolutions. Finally, QuASAR can estimate resolution limits based on both internal and replicate quality scores. QuASAR provides an objective means of Hi-C sample comparison while providing context and limits to these measures.", "tag": "Bioinformatics"}, {"title": "Beyond pseudotime: Following T-cell maturation in single-cell RNAseq time series", "url": "https://www.biorxiv.org/content/early/2017/11/14/219188", "abstract": "Cellular development has traditionally been described as a series of transitions between discrete cell states, such as the sequence of double negative, double positive and single positive stages in T-cell development. Recent advances in single cell transcriptomics suggest an alternative description of development, in which cells follow continuous transcriptomic trajectories. A cell's state along such a trajectory can be captured with pseudotemporal ordering, which however is not able to predict development of the system in real time. We present pseudodynamics, a mathematical framework that integrates time-series and genetic knock-out information with such transcriptome-based descriptions in order to describe and analyze the real-time evolution of the system. Pseudodynamics models the distribution of a cell population across a continuous cell state coordinate over time based on a stochastic differential equation along developmental trajectories and random switching between trajectories in branching regions. To illustrate feasibility, we use pseudodynamics to estimate cell-state-dependent growth and differentiation of thymic T-cell development. The model approximates a developmental potential function (Waddington's landscape) and suggests that thymic T-cell development is biphasic and not strictly deterministic before beta-selection. Pseudodynamics generalizes classical discrete population models to continuous states and thus opens possibilities such as probabilistic model selection to single cell genomics.", "tag": "Bioinformatics"}, {"title": "Genomes from uncultivated prokaryotes: a comparison of metagenome-assembled and single-amplified genomes", "url": "https://www.biorxiv.org/content/early/2017/11/14/219295", "abstract": "Background: Prokaryotes dominate the biosphere and regulate biogeochemical processes essential to all life. Yet, our knowledge about their biology is for the most part limited to the minority that has been successfully cultured. Molecular techniques now allow for obtaining genome sequences of uncultivated prokaryotic taxa, facilitating in-depth analyses that may ultimately improve our understanding of these key organisms. Results: We compared results from two culture-independent strategies for recovering bacterial genomes: single-amplified genomes and metagenome-assembled genomes. Single-amplified genomes were obtained from samples collected at an offshore station in the Baltic Sea Proper and compared to previously obtained metagenome-assembled genomes from a time series at the same station. Among 16 single-amplified genomes analyzed, seven were found to match metagenome-assembled genomes, affiliated with a diverse set of taxa. Notably, genome pairs between the two approaches were nearly identical (>98.7% identity) across overlapping regions (30-80% of each genome). Within matching pairs, the single-amplified genomes were consistently smaller and less complete, whereas the genetic functional profiles were maintained. For the metagenome-assembled genomes, only on average 3.6% of the bases were estimated to be missing from the genomes due to wrongly binned contigs; the metagenome assembly was found to cause incompleteness to a higher degree than the binning procedure. Conclusions: The strong agreement between the single-amplified and metagenome-assembled genomes emphasizes that both methods generate accurate genome information from uncultivated bacteria. Importantly, this implies that the research questions and the available resources are allowed to determine the selection of genomics approach for microbiome studies.", "tag": "Bioinformatics"}, {"title": "A systems approach to refine disease taxonomy by integrating phenotypic and molecular networks", "url": "https://www.biorxiv.org/content/early/2017/11/14/219089", "abstract": "The International Classification of Diseases (ICD) relies on clinical features and lags behind the current understanding of the molecular specificity of disease pathobiology, necessitating approaches that incorporate growing biomedical data for classifying diseases to meet the needs of precision medicine. Our analysis revealed that the heterogeneous molecular diversity of disease chapters and the blurred boundary between disease categories in ICD should be further investigated. Here, we propose a new classification of diseases (NCD) by developing an algorithm that predicts the additional categories of a disease by integrating multiple networks consisting of disease phenotypes and their molecular profiles. With statistical validations from phenotype-genotype associations and interactome networks, we demonstrate that NCD improves disease specificity owing to its overlapping categories and polyhierarchical structure. Furthermore, NCD captures the molecular diversity of diseases and defines clearer boundaries in terms of both phenotypic similarity and molecular associations, establishing a rational strategy to reform disease taxonomy.", "tag": "Bioinformatics"}, {"title": "Combining RNA-seq data and homology-based gene prediction for plants, animals and fungi", "url": "https://www.biorxiv.org/content/early/2017/11/14/219287", "abstract": "Motivation: Genome annotation is of key importance in many research questions. The identification of protein-coding genes is often based on transcriptome sequencing data, ab-initio or homology-based prediction. Recently, it was demonstrated that intron position conservation improves homology-based gene prediction, and that experimental data improves ab-initio gene prediction. Results: Here, we present an extension of the gene prediction tool GeMoMa that utilizes amino acid sequence conservation, intron position conservation and optionally RNA-seq data for homology-based gene prediction. We show on published benchmark data for plants, animals and fungi that GeMoMa performs better than the gene prediction programs BRAKER1, MAKER2, and CodingQuarry, and purely RNA-seq-based pipelines for transcript identification. In addition, we demonstrate that using multiple reference organisms may help to further improve the performance of GeMoMa. Finally, we apply GeMoMa to four nematode species and to the recently published barley reference genome indicating that current annotations of protein-coding genes may be refined using GeMoMa predictions. Availability: GeMoMa has been published under GNU GPL3 and is freely available at http://www.jstacs.de/index.php/GeMoMa.", "tag": "Bioinformatics"}, {"title": "A Deep Recurrent Neural Network Discovers Complex Biological Rules to Decipher RNA Protein-Coding Potential", "url": "https://www.biorxiv.org/content/early/2017/11/13/200758.1", "abstract": "The current deluge of newly identified RNA transcripts presents a singular opportunity for improved assessment of coding potential, a cornerstone of genome annotation, and for machine-driven discovery of biological knowledge. While traditional, feature-based methods for RNA classification are limited by current scientific knowledge, deep learning methods can independently discover complex biological rules in the data de novo. We trained a gated recurrent neural network (RNN) on human messenger RNA (mRNA) and long noncoding RNA (lncRNA) sequences. Our model, mRNA RNN (mRNN), surpasses state-of-the-art methods at predicting protein-coding potential. To understand what mRNN learned, we probed the network and uncovered several context-sensitive codons highly predictive of coding potential. Our results suggest that gated RNNs can learn complex and long-range patterns in full-length human transcripts, making them ideal for performing a wide range of difficult classification tasks and, most importantly, for harvesting new biological insights from the rising flood of sequencing data.", "tag": "Bioinformatics"}, {"title": "CODEX2: full-spectrum copy number variation detection by high-throughput DNA sequencing", "url": "https://www.biorxiv.org/content/early/2017/11/13/211698", "abstract": "High-throughput DNA sequencing enables detection of copy number variations (CNVs) on the genome-wide scale with finer resolution compared to array-based methods, but suffers from biases and artifacts that lead to false discoveries and low sensitivity. We describe CODEX2, a statistical framework for full-spectrum CNV profiling that is sensitive for variants with both common and rare population frequencies and that is applicable to study designs with and without negative control samples. We demonstrate and evaluate CODEX2 on whole-exome and targeted sequencing data, where biases are the most prominent. CODEX2 outperforms existing methods and, in particular, significantly improves sensitivity for common CNVs.", "tag": "Bioinformatics"}, {"title": "sigQC: A procedural approach for standardising the evaluation of gene signatures", "url": "https://www.biorxiv.org/content/early/2017/11/13/203729", "abstract": "With the increase in next generation sequencing generating large amounts of genomic data, gene expression signatures are becoming critically important tools, poised to make a large impact on the diagnosis, management and prognosis for a number of diseases. Increasingly, it is becoming necessary to determine whether a gene expression signature may apply to a dataset, but no standard quality control methodology exists. In this work, we introduce the first protocol, implemented in an R package sigQC, enabling a streamlined methodological and standardised approach for the quality control validation of gene signatures on independent data sets. The emphasis in this work is in showing the critical quality control steps involved in the generation of a clinically and biologically useful, transportable gene signature, including ensuring sufficient expression, variability, and autocorrelation of a signature. We demonstrate the application of the protocol in this work, showing how the outputs created from sigQC may be used for the evaluation of gene signatures on large-scale gene expression data in cancer.", "tag": "Bioinformatics"}, {"title": "AQMM: Enabling Absolute Quantification of Metagenome and Metatranscriptome", "url": "https://www.biorxiv.org/content/early/2017/11/13/218347.1", "abstract": "Metatranscriptome has become increasingly important along with the application of next-generation sequencing in the studies of microbial functional gene activity in environmental samples. However, the quantification of target active gene is hindered by the current relative quantification methods, especially when tracking the sharp environmental change. Great needs are here for an easy-to-perform method to obtain the absolute quantification. By borrowing information from the parallel metagenome, an absolute quantification method for both metagenomic and metatranscriptomic data to per gene/cell/volume/gram level was developed. The effectiveness of AQMM was validated by simulated experiments and was demonstrated with a real experimental design of comparing activated sludge with and without foaming. Our method provides a novel bioinformatic approach to fast and accurately conduct absolute quantification of metagenome and metatranscriptome in environmental samples. The AQMM can be accessed from https://github.com/biofuture/aqmm.", "tag": "Bioinformatics"}, {"title": "Protein structures as shapes: Analysing protein structure variation using geometric morphometrics", "url": "https://www.biorxiv.org/content/early/2017/11/13/219030", "abstract": "A phenotype is defined as an organism's physical traits. In the macroscopic world, an animal's shape is a phenotype. Geometric morphometrics (GM) can be used to analyze its shape. Let's pose protein structures as microscopic three dimensional shapes, and apply principles of GM to the analysis of macromolecules. In this paper we introduce a way to 1) abstract a structure as a shape; 2) align the shapes; and 3) perform statistical analysis to establish patterns of variation in the datasets. We show that general procrustes super-imposition (GPS) can be replaced by multiple structure alignment without changing the outcome of the test. We also show that estimating the deformation of the shape (structure) can be informative to analyze relative residue variations. Finally, we show an application of GM for two protein structure datasets: 1) in the \u03b1-amylase dataset we demonstrate the relationship between structure, function, and how the dependency of chloride has an important effect on the structure; and 2) in the Niemann-Pick disease, type C1 (NPC1) protein's molecular dynamic simulation dataset, we introduce a simple way to analyze the trajectory of the simulation by means of protein structure variation.", "tag": "Bioinformatics"}, {"title": "Similarity corpus on microbial transcriptional regulation", "url": "https://www.biorxiv.org/content/early/2017/11/13/219014", "abstract": "The ability to express the same meaning in different ways is a well known property of natural language. This amazing property is the source of major difficulties in natural language processing. Given the constant increase in published literature, its curation and information extraction would strongly benefit by efficient automatic processes, for which, corpora of sentences evaluated by experts is a valuable resource. Given our interest in applying such approaches to the benefit of curation of the biomedical literature, specifically about gene regulation in microbial organisms, we decided to build a corpus with graded textual similarity evaluated by curators, and designed specifically oriented to our purposes. Based on the predefined statistical power of future analyses, we defined features of the design including sampling, selection criteria, balance, and size among others. A non-fully crossed-design was performed for each pair of sentences by 3 evaluators from 7 different groups, adapting the SEMEVAL scale to our goals in four successive iterative sessions with a clear improvement in the consensuated guidelines and inter-rater-reliability results. Alternatives for the corpus evaluation are widely discussed. To the best of our knowledge this is the first similarity corpus in this domain of knowledge. We have initiated its incorporation in our research towards high throughput curation strategies based in natural language processing.", "tag": "Bioinformatics"}, {"title": "BayCount: A Bayesian Decomposition Method for Inferring Tumor Heterogeneity using RNA-Seq Counts", "url": "https://www.biorxiv.org/content/early/2017/11/13/218511", "abstract": "Tumors are heterogeneous - a tumor sample usually consists of a set of subclones with distinct transcriptional profiles and potentially different degrees of aggressiveness and responses to drugs. Understanding tumor heterogeneity is therefore critical for precise cancer prognosis and treatment. In this paper, we introduce BayCount, a Bayesian decomposition method to infer tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using negative binomial factor analysis, BayCount takes into account both the between-sample and gene-specific random effects on raw counts of sequencing reads mapped to each gene. For the posterior inference, we develop an efficient compound Poisson based blocked Gibbs sampler. Simulation studies show that BayCount is able to accurately estimate the subclonal inference, including number of subclones, the proportions of these subclones in each tumor sample, and the gene expression profiles in each subclone. For real-world data examples, we apply BayCount to The Cancer Genome Atlas lung cancer and kidney cancer RNA sequencing count data and obtain biologically interpretable results. Our method represents the first effort in characterizing tumor heterogeneity using RNA sequencing count data that simultaneously removes the need of normalizing the counts, achieves statistical robustness, and obtains biologically/clinically meaningful insights. The R package BayCount implementing our model and algorithm is available for download.", "tag": "Bioinformatics"}, {"title": "Clinker: visualising fusion genes detected in RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/11/13/218586", "abstract": "Genomic profiling efforts have revealed a rich diversity of oncogenic fusion genes, and many are emerging as important therapeutic targets. While there are many ways to identify fusion genes from RNA-seq data, visualising these transcripts and their supporting reads remains challenging. Clinker is a bioinformatics tool written in Python, R and Bpipe, that leverages the superTranscript method to visualise fusion genes. We demonstrate the use of Clinker to obtain interpretable visualisations of the RNA-seq data that lead to fusion calls. In addition, we use Clinker to explore multiple fusion transcripts with novel breakpoints within the P2RY8-CRLF2 fusion gene in B-cell Acute Lymphoblastic Leukaemia (B-ALL). Availability and Implementation: Clinker is freely available from Github https://github.com/Oshlack/Clinker under a MIT License.", "tag": "Bioinformatics"}, {"title": "TRUmiCount: Correctly counting absolute numbers of molecules using unique molecular identifiers", "url": "https://www.biorxiv.org/content/early/2017/11/13/217778", "abstract": "Counting DNA or RNA molecules using next-generation sequencing (NGS) suffers from amplification biases. Counting unique molecular identifiers (UMIs) instead of reads is still prone to over-estimation due to amplification and sequencing artifacts and under-estimation due to lost molecules. We present the algorithm TRUmiCount that corrects for these errors, based on a mechanistic model of the PCR and sequencing process whose parameters have an immediate physical interpretation and are easily estimated. We demonstrate that our algorithm outputs essentially unbiased counts with substantially improved accuracy.", "tag": "Bioinformatics"}, {"title": "PRS-on-Spark: a novel, efficient and flexible approach for generating polygenic risk scores", "url": "https://www.biorxiv.org/content/early/2017/11/13/209833", "abstract": "Motivation: Polygenic risk scores describe the genomic contribution to complex phenotypes and consistently account for a larger proportion of the variance than single nucleotide polymorphisms alone. However, there is little consensus on the optimal data input for generating polygenic risk scores and existing approaches largely preclude the use of imputed posterior probabilities and strand-ambiguous SNPs. Results: We developed PRS-on-Spark (PRSoS) a polygenic risk score software implemented in Apache Spark and Python that accommodates a variety of data input (e.g., observed genotypes, imputed genotypes or imputed dosage data) and strand-ambiguous SNPs. We show that PRSoS is flexible and efficient, accommodates strand-ambiguous SNP and computes polygenic risk scores at a range of p-value thresholds more quickly than existing software (PRSice). We also show that the use of imputed posterior probabilities and the inclusion of strand ambiguous SNPs increase the proportion of variance explained by a polygenic risk scores for major depression. Availability and Implementation: PRSoS is written in Apache Spark and Python and is freely available (see https://github.com/MeaneyLab/PRSoS).", "tag": "Bioinformatics"}, {"title": "Quadratic Programming Data Descriptors for Abnormal Beat Detection in ECG Recordings", "url": "https://www.biorxiv.org/content/early/2017/11/13/218008", "abstract": "This paper analyzes the efficacy of applying one class classifiers (OCCs) to the problem of abnormal beat detection in ECG. OCCs allow us to make patient-specific predictions with minimum training. Patient-specific techniques can result in more accurate predictions as they can compensate for inter-individual variations in ECG morphologies. The paper also proposes a novel OCC called Quadratic Programming Dissimilarity representation based Data Descriptor (QPDDD). A comparison of the proposed classification technique with existing classifiers over the MIT-BIH arrhythmia database is presented. Results show that OCCs coupled with wavelet domain features present a practical, robust and scalable solution for handling inter-individual variability in ECG patterns of different types of cardiac beats. An equal error rate of 90-95% was obtained for the MIT-BIH arrhythmia database depending upon the amount of training data used. A major advantage of the proposed scheme is that it requires only normal beats during its training. Another advantage is that it is able to handle inter-individual differences in ECG morphologies as the training takes place separately for each individual.", "tag": "Bioinformatics"}, {"title": "Saturating Single-Cell Datasets", "url": "https://www.biorxiv.org/content/early/2017/11/12/218370", "abstract": "High throughput methods for profiling the transcriptomes of single cells have recently emerged as transformative approaches for large-scale population surveys of cellular diversity in heterogeneous primary tissues. Efficient generation of such an atlas will depend on sufficient sampling of the diverse cell types while remaining cost-effective to enable a comprehensive examination of organs, developmental stages, and individuals. To examine the relationship between cell number and transcriptional heterogeneity in the context of unbiased cell type classification, we explicitly explored the population structure of a publically available 1.3 million cell dataset from the E18.5 mouse brain. We propose a computational framework for inferring the saturation point of cluster discovery in a single cell mRNA-seq experiment, centered around cluster preservation in downsampled datasets. In addition, we introduce a complexity index, which characterizes the heterogeneity of cells in a given dataset. Using Cajal-Retzius cells as an example of a limited complexity dataset, we explored whether biological distinctions relate to technical clustering. Surprisingly, we found that clustering distinctions carrying biologically interpretable meaning are achieved with far fewer cells (20,000). Together, these findings suggest that most of the biologically interpretable insights from the 1.3 million cells can be recapitulated by analyzing 50,000 randomly selected cells, indicating that instead of profiling few individuals at high cellular coverage, the much anticipated cell atlasing studies may instead benefit from profiling more individuals, or many time points at lower cellular coverage.", "tag": "Bioinformatics"}, {"title": "Using Optimal F-Measure and Random Resampling in Gene Ontology Enrichment Calculations", "url": "https://www.biorxiv.org/content/early/2017/11/12/218248", "abstract": "Background: A central question in bioinformatics is how to minimize arbitrariness and bias in analysis of patterns of enrichment in data. A prime example of such a question is enrichment of gene ontology (GO) classes in lists of genes. Our paper deals with two issues within this larger question. One is how to calculate the false discovery rate (FDR) within a set of apparently enriched ontologies, and the second how to set that FDR within the context of assessing significance for addressing biological questions, to answer these questions we compare a random resampling method with a commonly used method for assessing FDR, the Benjamini-Hochberg (BH) method. We further develop a heuristic method for evaluating Type II (false negative) errors to enable utilization of F-Measure binary classification theory for distinguishing significant from non-significant degrees of enrichment. Results: The results show the preferability and feasibility of random resampling assessment of FDR over the analytical methods with which we compare it. They also show that the reasonableness of any arbitrary threshold depends strongly on the structure of the dataset being tested, suggesting that the less arbitrary method of F-measure optimization to determine significance threshold is preferable. Conclusion: Therefore, we suggest using F-measure optimization instead of placing an arbitrary threshold to evaluate the signifi-cance of Gene Ontology Enrichment results, and using resampling to replace analytical methods.", "tag": "Bioinformatics"}, {"title": "MCO: towards an ontology and unified vocabulary for a framework-based annotation of microbial growth conditions", "url": "https://www.biorxiv.org/content/early/2017/11/12/218289", "abstract": "Motivation: A major component in our understanding of the biology of an organism is the mapping of its genotypic potential into the repertoire of its phenotypic expression profiles. This genotypic to phenotypic mapping is executed by the machinery of gene regulation that turns genes on and off, which in microorganisms is essentially studied by changes in growth conditions and genetic modifications. Although many efforts have been made to systematize the annotation of experimental conditions in microbiology, the available annotation is not based on a consistent and controlled vocabulary for the unambiguous description of growth conditions, making difficult the identification of biologically meaningful comparisons of knowledge generated in different experiments or laboratories, a task urgently needed given the massive amounts of data generated by high throughput (HT) technologies. Results: We curated terms related to experimental conditions that affect gene expression in E. coli K-12. Since this is the best-studied microorganism, the collected terms are the seed for the first version of the Microbial Conditions Ontology (MCO), a controlled and structured vocabulary that can be expanded to annotate microbial conditions in general. Moreover, we developed an annotation framework using the MCO terms to describe experimental conditions, providing the foundation to identify regulatory networks that operate under a particular condition. MCO supports comparisons of HT-derived data from different repositories. In this sense, we started to map common RegulonDB terms and Colombos bacterial expression compendia terms to MCO. Availability and Implementation: As far as we know, MCO is the first ontology for growth conditions of any bacterial organism and it is available at http://regulondb.ccg.unam.mx/. Furthermore, we will disseminate MCO throughout the Open Biomedical Ontology (OBO) Foundry in order to set a standard for the annotation of gene expression data derived from conventional as well as HT experiments in E. coli and other microbial organisms. This will enable the comparison of data from diverse data sources.", "tag": "Bioinformatics"}, {"title": "FateID infers cell fate bias in multipotent progenitors from single-cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/11/11/218115", "abstract": "Differentiation of multipotent cells is a complex process governed by interactions of thousands of genes subject to substantial expression fluctuations. Resolving cell state heterogeneity arising during this process requires quantification of gene expression within individual cells. However, computational methods linking this heterogeneity to biases towards distinct cell fates are not well established. Here, we perform deep single-cell transcriptome sequencing of ~2,000 bone-marrow derived mouse hematopoietic progenitors enriched for lymphoid lineages. To resolve subtle transcriptome priming indicative of distinct lineage biases, we developed FateID, an iterative supervised learning algorithm for the probabilistic quantification of cell fate bias. FateID delineates domains of fate bias within progenitor populations and permits the derivation of high-resolution differentiation trajectories, revealing a common progenitor population of B cells and plasmacytoid dendritic cells, which we validated by in vitro differentiation assays. We expect that FateID will enhance our understanding of the process of cell fate choice in complex multi-lineage differentiation systems.", "tag": "Bioinformatics"}, {"title": "The Generalized Data Model for Clinical Research", "url": "https://www.biorxiv.org/content/early/2017/11/10/194597", "abstract": "Background: Most healthcare data sources store information within their own unique schemas, making reliable and reproducible research challenging. Consequently, researchers have adopted various data models to improve the efficiency of research. Transforming and loading data into these models is a labor-intensive process that can alter the original semantics of the original data. Therefore, we created a data model with a hierarchical structure that simplifies the transformation process and minimizes data alteration. Methods: There were two design goals in constructing the tables and table relationships for the Generalized Data Model (GDM). The first was to use a single table for storing clinical codes in their original vocabularies to retain the original semantic representation of the data. The second was to retain hierarchical information present in the original data while retaining provenance. The model was tested by transforming synthetic Medicare data; Surveillance, Epidemiology, and End Results data linked to Medicare claims; and electronic health records from the Clinical Practice Research Datalink. We also tested a subsequent transformation from the GDM into the Sentinel data model. Results: The resulting data model contains 19 tables, with the Clinical Codes, Contexts, and Collections tables serving as the core of the model, and containing most of the clinical, provenance, and hierarchical information. In addition, a Mapping table allows users to apply an arbitrarily complex set of relationships among vocabulary elements to facilitate automated analyses. Conclusions: The GDM offers researchers a simple process for loading data, clear data provenance, and a clear path for users to transform their data into other data models. The GDM is designed to retain hierarchical relationships among data elements as well as the original semantic representation of the data, ensuring consistency in study building as part of a complete data pipeline for researchers.", "tag": "Bioinformatics"}, {"title": "A substrate for modular, extensible data-visualization", "url": "https://www.biorxiv.org/content/early/2017/11/10/217349", "abstract": "As scientific questions grow in scope and datasets grow larger, collaborative research teams and data dissemination have emerged as core research enablers. However, simply visualizing datasets is challenging, especially when sharing information across research groups or to the broader scientific community. We present substrate, a data-visualization platform designed to enable communication and code reuse across diverse research teams. Written in three.js, our platform provides a rigid and simple, yet powerful interface for scientists to rapidly build tools and effective visualizations.", "tag": "Bioinformatics"}, {"title": "Mantis: A Fast, Small, and Exact Large-Scale Sequence Search Index", "url": "https://www.biorxiv.org/content/early/2017/11/10/217372", "abstract": "Motivation: Sequence-level searches on large collections of RNA-seq experiments, such as the NIH Sequence Read Archive (SRA), would enable one to ask many questions about the expression or variation of a given transcript in a population. Bloom filter-based indexes and variants, such as the Sequence Bloom Tree, have been proposed in the past to solve this problem. However, these approaches suffer from fundamental limitations of the Bloom filter, resulting in slow build and query times, less-than-optimal space usage, and large numbers of false positives. Results: This paper introduces Mantis, a space-efficient data structure that can be used to index thousands of raw-read experiments and facilitate large-scale sequence searches on those experiments. Mantis uses counting quotient filters instead of Bloom filters, enabling rapid index builds and queries, small indexes, and exact results, i.e., no false positives or negatives. Furthermore, Mantis is also a colored de Bruijn graph representation, so it supports fast graph traversal and other topological analyses in addition to large-scale sequence-level searches. In our performance evaluation, index construction with Mantis is 4.4x faster and yields a 20% smaller index than the state-of-the-art split sequence Bloom tree (SSBT). For queries, Mantis is 6x-108x faster than SSBT and has no false positives or false negatives. For example, Mantis was able to search for all 200,400 known human transcripts in an index of 2652 human blood, breast, and brain RNA-seq experiments in one hour and 22 minutes; SBT took close to 4 days and AllSomeSBT took about eight hours. Mantis is written in C++11 and is available at https://github.com/splatlab/mantis.", "tag": "Bioinformatics"}, {"title": "The Block Object Storage Service (bossDB): A Cloud-Native Approach for Petascale Neuroscience Discovery", "url": "https://www.biorxiv.org/content/early/2017/11/10/217745", "abstract": "Large volumetric neuroimaging datasets have grown in size over the past ten years from gigabytes to terabytes, with petascale data becoming available and more common over the next few years. Current approaches to store and analyze these emerging datasets are insufficient in their ability to scale in both cost-effectiveness and performance. Additionally, enabling large-scale processing and annotation is critical as these data grow too large for manual inspection. We propose a new cloud-native managed service for large and multi-modal experiments, providing support for data ingest, storage, visualization, and sharing through a RESTful Application Programming Interface (API) and web-based user interface. Our project is open source and can be easily and cost-effectively used for a variety of modalities and applications.", "tag": "Bioinformatics"}, {"title": "Efficient and accurate detection of splice junctions from RNAseq with Portcullis", "url": "https://www.biorxiv.org/content/early/2017/11/10/217620", "abstract": "Next generation sequencing (NGS) technologies enable rapid and cheap genome-wide transcriptome analysis, providing vital information about gene structure, transcript expression and alternative splicing. Key to this is the the accurate identification of exon-exon junctions from RNA sequenced (RNA-seq) reads. A number of RNA-seq aligners capable of splitting reads across these splice junctions (SJs) have been developed, however, it has been shown that while they correctly identify most genuine SJs available in a given sample, they also often produce large numbers of incorrect SJs. Herein we describe the extent of this problem using popular RNA-seq mapping tools, and present a new method, called Portcullis, to rapidly filter false SJs junctions from spliced alignments produced by any RNA-seq mapper capable of creating SAM/BAM files. We show that Portcullis distinguishes between genuine and false positive junctions to a high-degree of accuracy across different species, samples, expression levels, error profiles and read lengths. Portcullis makes efficient use of memory and threading and, to our knowledge, is currently the only SJ prediction tool that reliably scales for use with large RNAseq datasets and large highly fragmented genomes, whilst delivering highly accurate SJs.", "tag": "Bioinformatics"}, {"title": "Mining the forest: uncovering biological mechanisms by interpreting Random Forests", "url": "https://www.biorxiv.org/content/early/2017/11/10/217695", "abstract": "Biological datasets are large and complex. Machine learning models are therefore essential to capture relationships in the data. Unfortunately, the inferred complex models are often difficult to understand and interpretation is limited to a list of features ranked on their importance in the model. We propose a computational approach, called Foresight, which enables interpretation of the patterns uncovered by Random Forest models trained on biological datasets. Foresight exploits the correlation structure in the data to uncover relevant groups of features and the interactions between them. This facilitates interpretation of the computational model and can provide more detailed insight in the underlying biological relationships than simply ranking features. We demonstrate Foresight on both an artificial dataset and a large gene expression dataset of breast cancer patients. Using the latter dataset we show that our approach retrieves biologically relevant features and provides a rich description of the interactions and correlation structure between these features.", "tag": "Bioinformatics"}, {"title": "Minerva: An Alignment and Reference Free Approach to Deconvolve Linked-Reads for Metagenomics", "url": "https://www.biorxiv.org/content/early/2017/11/10/217869", "abstract": "Emerging linked-read technologies (aka read-cloud or barcoded short-reads) have revived interest in standard short-read technology as a viable way to understand large scale structure in genomes and metagenomes. Linked-read technologies, such as the 10X Chromium system, use a microfluidic system and a set of specially designed 3 prime barcodes (aka UIDs) to tag short DNA reads which were originally sourced from the same long fragment of DNA; subsequently these specially barcoded reads are sequenced on standard short read platforms. This approach results in interesting compromises. Each long fragment of DNA is covered only sparsely by short reads, no information about the relative ordering of reads from the same fragment is preserved, and typically each 3 prime barcode matches reads from 5-20 long fragments of DNA. However, the cost per base to sequence is far lower than single molecule long read sequencing systems, far less input DNA is required, and the error rate is that of standard short-reads. Linked-reads represent a new set of algorithmic challenges. In this paper we formally describe one particular issue common to all applications of linked-read technology: the deconvolution of reads with a single barcode into clusters that correspond to a single long fragment of DNA. We introduce Minerva, A graph-based algorithm which approximately solves the barcode deconvolution problem for metagenomic data (where reference genomes may be incomplete or unavailable). Additionally, we demonstrate that deconvolved barcoded reads significantly improve downstream results by improving the specificity of taxonomic assignments, and by improving the ability of topic models to identify clusters of related sequences.", "tag": "Bioinformatics"}, {"title": "Multi-Omics factor analysis disentangles heterogeneity in blood cancer", "url": "https://www.biorxiv.org/content/early/2017/11/10/217554", "abstract": "Multi-omic studies in large cohorts promise to characterize biological processes across molecular layers including genome, transcriptome, epigenome, proteome and perturbation phenotypes. However, methods for integrating multi-omic datasets are lacking. We present Multi-Omics Factor Analysis (MOFA), an unsupervised dimensionality reduction method for discovering the driving sources of variation in multi-omics data. Our model infers a set of (hidden) factors that capture biological and technical sources of variability across data modalities. We applied MOFA to data from 200 patient samples of chronic lymphocytic leukemia (CLL) profiled for somatic mutations, RNA expression, DNA methylation and ex-vivo responses to a panel of drugs. MOFA automatically discovered the known dimensions of disease heterogeneity, including immunoglobulin heavy chain variable region (IGHV) status and trisomy of chromosome 12, as well as previously underappreciated drivers of variation, such as response to oxidative stress. These factors capture key dimensions of patient heterogeneity, including those linked to clinical outcomes. Finally, MOFA handles missing data modalities in subsets of samples, enabling imputation, and the model can identify outlier samples.", "tag": "Bioinformatics"}, {"title": "Predicting Transcription Factor Binding Sites with Convolutional Kernel Networks", "url": "https://www.biorxiv.org/content/early/2017/11/10/217257", "abstract": "The growing amount of biological sequences available makes it possible to learn genotype-phenotype relationships from data with increasingly high accuracy. By exploiting large sets of sequences with known phenotypes, machine learning methods can be used to build functions that predict the phenotype of new, unannotated sequences. In particular, deep neural networks have recently obtained good performances on such prediction tasks, but are notoriously difficult to analyze or interpret. Here, we introduce a hybrid approach between kernel methods and convolutional neural networks for sequences, which retains the ability of neural networks to learn good representations for a learning problem at hand, while defining a well characterized Hilbert space to describe prediction functions. Our method outperforms state-of-the-art convolutional neural networks on a transcription factor binding prediction task while being much faster to train and yielding more stable and interpretable results. Source code is freely available at https://gitlab.inria.fr/dchen/CKN-seq.", "tag": "Bioinformatics"}, {"title": "MODE-TASK: Large-scale protein motion tools", "url": "https://www.biorxiv.org/content/early/2017/11/10/217505", "abstract": "Summary: MODE-TASK, a novel software suite, comprises Principle Component Analysis, Multidi-mensional Scaling, and t-Distributed Stochastic Neighbor Embedding techniques using molecular dynamics trajectories. MODE-TASK also includes a Normal Mode Analysis tool based on Anisotropic Network Model so as to provide a variety of ways to analyse and compare large-scale motions of protein complexes for which long MD simulations are prohibitive. Availability and Implementation: MODE-TASK has been open-sourced, and is available for down-load from https://github.com/RUBi-ZA/MODE-TASK, implemented in Python and C++. Supplementary information: Documentation available at http://mode-task.readthedocs.io.", "tag": "Bioinformatics"}, {"title": "Deep2Full: Predictive model for complementing phenotypic outcomes in a deep mutational scan using protein sequence and structure information", "url": "https://www.biorxiv.org/content/early/2017/11/10/217158", "abstract": "Large scale mutagenesis experiments are becoming possible owing to the advancement in the sequencing technologies and high throughput screening. Deep mutational scans perform exhaustive single-point mutations on a protein and probe their phenotypic affects. Performing a full scan with site-directed mutations of all the amino acid residues in a protein may not be practical, and may not even be required, especially if predictive computational models can be developed. Computational models are however naive to cellular response in the myriads of assay-conditions. In order to develop the realistic paradigm of assay context-aware predictive hybrid models, we combine minimal deep mutational studies with computational models and predict the phenotypic outcomes quantitatively. Structural, sequence and co-evolutionary information along with partial deep mutational scan data was included to capture the phenotypic relevance of the mutations to the specific screening criterion. The model reliably predicts the fitness outcomes of hundreds of randomly selected amino acid mutations in \u03b2-lactamase, when the phenotypic fitness data from as few as 15% of the full mutation is available. Interestingly, the predictive capabilities are better with a random set of mutations rather than with a systematic substitution of all amino acids to alanine, asparagine and histidine (ANH). The model can potentially be extended for predicting the phenotypic outcomes at other concentrations of the stressor by carefully analyzing the dose-response curves of a representative set of mutations.", "tag": "Bioinformatics"}, {"title": "Accurate detection of HIV transmission clusters from phylogenetic trees using a multi-state birth-death model", "url": "https://www.biorxiv.org/content/early/2017/11/10/215491", "abstract": "HIV transmission networks are highly clustered, and accurate identification of these clusters is essential for effective targeting of public health interventions. This clustering affects the transmission dynamics of the HIV epidemic, which affects the pathogen phylogenies reconstructed from patient samples. We present a new method for identifying transmission clusters by detecting the changes in transmission rate provoked by the introduction of the epidemic into a new cluster. The method employs a multi-state birth-death (MSBD) model where each state represents a cluster. Transmission rates in each cluster decrease exponentially over time, simulating susceptible depletion in the cluster. This model is fitted to the pathogen phylogeny using a Maximum Likelihood approach. Using simulated datasets we show that the MSBD method is able to reliably infer both the cluster repartition and the transmission parameters from a pathogen phylogeny. In contrast to existing cutpoint-based methods for cluster identification, which are dependent on a parameter set by the user, the MSBD method is consistently reliable. It also performs better on phylogenies containing nested clusters. We present an application of our method to the inference of transmission clusters using sequences obtained from the Swiss HIV Cohort Study. The MSBD method is available as an R package.", "tag": "Bioinformatics"}, {"title": "Improving the calling of non-invasive prenatal testing on 13-/18-/21-trisomy by support vector machine discrimination", "url": "https://www.biorxiv.org/content/early/2017/11/10/216689", "abstract": "With the advance of next-generation sequencing technologies, non-invasive prenatal testing (NIPT) has been developed and employed in fetal aneuploidy screening on 13-/18-/21-trisomies through detecting cell-free fetal DNA (cffDNA) in maternal blood. Although Z test is widely used in NIPT nowadays, there is still necessity to improve its accuracy for removing a) false negatives and false positives, and b) the ratio of unclassified data, so as to reduce the potential harm to patients caused by these inaccuracies as well as the induced cost of retests. Employing multiple Z tests with machine-learning algorithm could provide a better prediction on NIPT data. Combining the multiple Z values with indexes of clinical signs and quality control, features were collected from the known samples and scaled for model training in support vector machine (SVM) discrimination. The trained model was applied to predict the unknown samples, which showed significant improvement. In 4752 qualified NIPT data, our method reached 100% accuracies on all three chromosomes, including 151 data that were grouped as unclassified by one-Z-value based method. Moreover, four false positives and four false negatives were corrected by using this machine-learning model. To our knowledge, this is the first study to employ support vector machine in NIPT data analysis. It is expected to replace the current one-Z-value based NIPT analysis in clinical use.", "tag": "Bioinformatics"}, {"title": "Ennet: construction of potential cancer-driving networks based on somatic enhancer mutations only", "url": "https://www.biorxiv.org/content/early/2017/11/10/216226", "abstract": "Whole genome sequencing technology has facilitated the discovery of a large number of somatic mutations in enhancers (SMEs), whereas the utility of SMEs in tumorigenesis has not been fully explored. Here we present Ennet, a method to comprehensively investigate SMEs enriched networks (SME-networks) in cancer by integrating SMEs, enhancer-gene interactions and gene-gene interactions. Using Ennet, we performed a pan-cancer analysis in 2004 samples from 8 cancer types and found many well-known cancer drivers were involved in the SME-networks, including ESR1, SMAD3, MYC, EGFR, BCL2 and PAX5. Meanwhile, Ennet also identified many new networks with less characterization but have potentially important roles in cancer, including a large SME-network in medulloblastoma (MB), which contains genes enriched in the glutamate receptor and neural development pathways. Interestingly, SME-networks are specific across cancer types, and the vast majority of the genes identified by Ennet have few mutations in gene bodies. Collectively, our work suggests that using enhancer-only somatic mutations can be an effective way to discover potential cancer-driving networks. Ennet provides a new perspective to explore new mechanisms for tumor progression from SMEs.", "tag": "Bioinformatics"}, {"title": "A tangled tale of convergence and divergence: archaeal chromosomal proteins and Chromo-like domains in bacteria and eukaryotes", "url": "https://www.biorxiv.org/content/early/2017/11/10/217323", "abstract": "The Chromo-like superfamily of SH3-fold \u03b2-barrel domains recognize epigenetic marks in eukaryotic proteins. Their provenance has been placed either in archaea, based on apparent structural similarity to chromatin-compacting Sul7d and Cren7 proteins, or in bacteria based on the presence of sequence homologs. Using sequence and structural evidence we establish that the archaeal Cren7/Sul7 proteins emerged from a zinc ribbon (ZnR) ancestor. Further, we show that the ancestral eukaryotic Chromo-like domains evolved from bacterial precursors acquired from early endosymbioses, which already possessed an aromatic cage for recognition of modified amino-groups. These bacterial versions are part of a radiation of secreted SH3-fold domains, which spawned both chromo-like domains and classical SH3 domains in the context of peptide-recognition in the peptidoglycan. This establishes that Cren7/Sul7 converged to a SH3-like state from a ZnR precursor via the loss of metal-chelation and acquisition of stronger hydrophobic interactions; it is unlikely to have participated in the evolution of the chromo-like domains. We show that archaea possess several Cren7/Sul7-related proteins with intact Zn-chelating ligands, which we predict to play previously unstudied roles in cell-division comparable to the PRC barrel.", "tag": "Bioinformatics"}, {"title": "netDx: Interpretable patient classification using integrated patient similarity networks", "url": "https://www.biorxiv.org/content/early/2017/11/10/084418", "abstract": "Patient classification has widespread biomedical and clinical applications, including diagnosis, prognosis and treatment response prediction. A clinically useful prediction algorithm should be accurate, generalizable, be able to integrate diverse data types, and handle sparse data. Importantly, the resulting model should be easily interpretable, as clinicians are unlikely to trust black box statistical models. We describe netDx, the first supervised patient classification framework based on patient similarity networks. netDx meets the above criteria and particularly excels at data integration and model interpretability. We demonstrate the features of this framework by integrating up to six heterogeneous datatypes, including clinical variables, DNA methylation, somatic mutations, mRNA, miRNA and protein expression profiles, for survival prediction in kidney, lung, ovarian and brain cancer. We benchmarked netDx performance as a machine-learning method by predicting binary survival in four tumour types. netDx ranks at the top for two tumours and within the top 20th percentile for all four, demonstrating consistently good performance. In comparison to traditional machine learning-based patient classifiers, netDx results are more interpretable, visualizing the decision boundary in the context of patient similarity space and identifying biological pathways and other features important for prediction. By defining patient similarity using pathway-level gene expression, netDx identifies known molecular correlates of poor survival in kidney cancer, and identifies potentially novel pathways and biomarkers. Thus, netDx can serve both as a useful classifier and as a tool for discovery of biological features characteristic of disease. An open-source R/Java implementation of netDx is available along with sample files and automation workflows packaged as vignettes.", "tag": "Bioinformatics"}, {"title": "MRIQC Web-API: Crowdsourcing image quality metrics and expert quality ratings of structural and functional MRI", "url": "https://www.biorxiv.org/content/early/2017/11/09/216671", "abstract": "The MRIQC Web-API is a resource for scientists to train new automatic quality classifiers. The MRIQC Web-API has collected more than 30K sets of image quality measures automatically extracted from BOLD and T1-weighted scans using MRIQC. MRIQC is an automated MRI Quality Control tool, and here we present an extension to crowdsource these quality metrics along with anonymized metadata and manual quality ratings. This new resource will allow a better understanding of the normative values and distributions of these quality metrics, help determine the relationships between image quality and metadata such as acquisition parameters and finally, provide a cost-effective, easy way to annotate the quality of a large number of cross-site MR scans.", "tag": "Bioinformatics"}, {"title": "SgTiler: a fast method to design tiling sgRNAs for CRISPR/Cas9 mediated screening", "url": "https://www.biorxiv.org/content/early/2017/11/09/217166", "abstract": "Summary: Screening of genomic regions of interest using CRISPR/Cas9 is getting increasingly popular. The system requires designing of single guide RNAs (sgRNAs) that can efficiently guide the Cas9 endonuclease to the targeted region with minimal off-target effects. Tiling sgRNAs is the most effective way to perturb regulatory regions, such as promoters and enhancers. sgTiler is the first tool that provides a fast method for designing tiling sgRNAs Availability and Implementation: sgTiler is a command line tool that requires only one command to execute. Its source code is freely available on the web at https://github.com/HansenHeLab/sgTiler. sgTiler is implemented in Python and supported on any platform with Python and Bowtie.", "tag": "Bioinformatics"}, {"title": "Accurate Reconstruction of Microbial Strains from Metagenomic Sequencing Using Representative Reference Genomes", "url": "https://www.biorxiv.org/content/early/2017/11/09/215707", "abstract": "Exploring the genetic diversity of microbes within the environment through metagenomic sequencing first requires classifying these reads into taxonomic groups. Current methods compare these sequencing data with existing biased and limited reference databases. Several recent evaluation studies demonstrate that current methods either lack sufficient sensitivity for species-level assignments or suffer from false positives, overestimating the number of species in the metagenome. Both are especially problematic for the identification of low-abundance microbial species, e.g. detecting pathogens in ancient metagenomic samples. We present a new method, SPARSE, which improves taxonomic assignments of metagenomic reads. SPARSE balances existing biased reference databases by grouping reference genomes into similarity-based hierarchical clusters, implemented as an efficient incremental data structure. SPARSE assigns reads to these clusters using a probabilistic model, which specifically penalizes non-specific mappings of reads from unknown sources and hence reduces false-positive assignments. Our evaluation on simulated datasets from two recent evaluation studies demonstrated the improved precision of SPARSE in comparison to other methods for species-level classification. In a third simulation, our method successfully differentiated multiple co-existing Escherichia coli strains from the same sample. In real archaeological datasets, SPARSE identified ancient pathogens with \u22640.02% abundance, consistent with published findings that required additional sequencing data. In these datasets, other methods either missed targeted pathogens or reported non-existent ones.", "tag": "Bioinformatics"}, {"title": "TF2Network: predicting transcription factor regulators and gene regulatory networks in Arabidopsis using publicly available binding site information", "url": "https://www.biorxiv.org/content/early/2017/11/09/173559", "abstract": "A gene regulatory network (GRN) is a collection of regulatory interactions between transcription factors (TFs) and their target genes. GRNs control different biological processes and have been instrumental to understand the organization and complexity of gene regulation. Although various experimental methods have been used to map GRNs in Arabidopsis thaliana, their limited throughput combined with the large number of TFs makes that for many genes our knowledge about regulating TFs is incomplete. We introduce TF2Network, a tool that exploits the vast amount of TF binding site information and enables the delineation of GRNs by detecting potential regulators for a set of co-expressed or functionally related genes. Validation using two experimental benchmarks reveals that TF2Network predicts the correct regulator in 75-92% of the test sets. Furthermore, our tool is robust to noise in the input gene sets, has a low false discovery rate, and shows a better performance to recover correct regulators compared to other plant tools. TF2Network is accessible through a web interface where GRNs are interactively visualized and annotated with various types of experimental functional information. TF2Network was used to perform systematic functional and regulatory gene annotations, identifying new TFs involved in circadian rhythm and stress response.", "tag": "Bioinformatics"}, {"title": "GrapeTree: Visualization of core genomic relationships among 100,000 bacterial pathogens", "url": "https://www.biorxiv.org/content/early/2017/11/09/216788", "abstract": "Current methods struggle to reconstruct and visualise the genomic relationships of \u2265100,000 bacterial genomes. GrapeTree facilitates the analyses of allelic profiles from 10,000's of core genomes within a web browser window. GrapeTree implements a novel minimum spanning tree algorithm to reconstruct genetic relationships despite missing data together with a static \"GrapeTree Layout\" algorithm to render interactive visualisations of large trees. GrapeTree is a stand-along package for investigating Newick trees plus associated metadata and is also integrated into EnteroBase to facilitate cutting edge navigation of genomic relationships among >160,000 genomes from bacterial pathogens. The GrapeTree package was released under the GPL v3.0 Licence.", "tag": "Bioinformatics"}, {"title": "DiscoSnp-RAD: de novo detection of small variants for population genomics", "url": "https://www.biorxiv.org/content/early/2017/11/09/216747", "abstract": "We present an original method to de novo call variants for Restriction site associated DNA Sequencing (RAD-Seq). RAD-Seq is a technique characterized by the sequencing of specific loci along the genome, that is widely employed in the field of evolutionary biology since it allows to exploit variants (mainly SNPs) information from entire populations at a reduced cost. Common RAD dedicated tools, as STACKS or IPyRAD, are based on all-versus-all read comparisons, which require consequent time and computing resources. Based on the variant caller DiscoSnp, initially designed for shotgun sequencing, DiscoSnpRad avoids this pitfall as variants are detected by exploring the De Bruijn Graph built from all the read datasets. We tested the implementation on RAD data from 259 specimens of Chiastocheta flies, morphologically assigned to 7 species. All individuals were successfully assigned to their species using both STRUCTURE and Maximum Likelihood phylogenetic reconstruction. Moreover, identified variants succeeded to reveal a within species structuration and the existence of two populations linked to their geographic distributions. Furthermore, our results show that DiscoSnpRad is at least one order of magnitude faster than state-of-the-art tools. The overall results show that DiscoSnpRad is suitable to identify variants from RAD data, and stands out from other tools due to his completely different principle, making it significantly faster, in particular on large datasets.", "tag": "Bioinformatics"}, {"title": "De novo Clustering of Gene Expressed Variants in Transcriptomic Long Reads Data Sets", "url": "https://www.biorxiv.org/content/early/2017/11/08/170035", "abstract": "This work addresses the problem of grouping by genes long reads expressed in a whole transcriptome sequencing data set. Long read sequencing produces several thousands base-pair long sequences, although showing high error rate in comparison to short reads. Long reads can cover full-length RNA transcripts and thus are of high interest to complete references. However, the literature is lacking tools to cluster such data de novo, in particular for Oxford Nanopore Technologies reads. As a consequence, we propose a novel algorithm based on community detection and its implementation. Since solution is meant to be reference-free (de novo), it is especially well-tailored for non model species. We demonstrate it performs well on a real mouse data set. When a reference is available, we show that it stands as an alternative to mapping. In addition, we show that quick assessment of gene's expression is a straightforward use case of our solution.", "tag": "Bioinformatics"}, {"title": "Identifying Crohns disease signal from variome analysis", "url": "https://www.biorxiv.org/content/early/2017/11/08/216432", "abstract": "Background: After many years of concentrated research efforts, the exact cause of Crohns disease remains unknown. Its accurate diagnosis, however, helps in management and even preventing the onset of disease. Genome wide association studies have identified 140 loci associated with CD, but these carry very small log odds ratios and are uninformative for diagnoses. Results: Here we describe a machine learning method, AVA,Dx (Analysis of Variation for Association with Disease), that uses whole exome sequencing data to make predictions of CD status. Using the person-specific variation in these genes from a panel of only 111 individuals, we built disease prediction models informative of previously undiscovered disease genes. In this panel, our models differentiate CD patients from healthy controls with 71% precision and 73% recall at the default cutoff. By additionally accounting for batch effects, we are also able to predict individual CD status for previously unseen individuals from a separate CD study (84% precision, 73% recall). Conclusions: Larger training panels and additional features, including regulatory variants and environmental factors, e.g. human associated microbiota, are expected to improve model performance. However, current results already position AVA,Dx as both an effective method for highlighting pathogenesis pathways and as a simple Crohns disease risk analysis tool, which can improve clinical diagnostic time and accuracy.", "tag": "Bioinformatics"}, {"title": "PAIRUP-MS: Pathway Analysis and Imputation to Relate Unknowns in Profiles from Mass Spectrometry-based metabolite data", "url": "https://www.biorxiv.org/content/early/2017/11/08/209577", "abstract": "Metabolomics is a powerful approach for discovering biomarkers and metabolic quantitative trait loci. While untargeted profiling methods can measure up to thousands of metabolite signals in a single experiment, many signals cannot be readily identified as known metabolites or compared across datasets, making it difficult to infer biology and to conduct well-powered meta-analyses across studies. To deal with these challenges, we developed a suite of computational methods, PAIRUP-MS, to match metabolite signals across mass spectrometry-based profiling datasets using an imputation-based approach and to generate pathway annotations for these signals. We performed meta and pathway analyses for both known and unknown signals in multiple datasets and then validated the results using genetic associations. Finally, we applied the methods to detect metabolite signals and pathways associated with body mass index, demonstrating that our framework is useful for analyzing unknown signals in a robust and biologically meaningful manner and for improving the power of untargeted metabolomics studies.", "tag": "Bioinformatics"}, {"title": "Aligning sequences to general graphs in O(V + mE) time", "url": "https://www.biorxiv.org/content/early/2017/11/08/216127", "abstract": "Graphs are commonly used to represent sets of sequences. Either edges or nodes can be labeled by sequences, so that each path in the graph spells a concatenated sequence. Examples include graphs to represent genome assemblies, such as string graphs and de Bruijn graphs, and graphs to represent a pan-genome and hence the genetic variation present in a population. Being able to align sequencing reads to such graphs is a key step for many analyses and its applications include genome assembly, read error correction, and variant calling with respect to a variation graph. Given the wide range of applications of this basic problem, it is surprising that algorithms with optimal runtime are, to the best of our knowledge, yet unknown. In particular, aligning sequences to cyclic graphs currently represents a challenge both in theory and practice. Here, we introduce an algorithm to compute the minimum edit distance of a sequence of length m to any path in a node-labeled directed graph (V,E) in O(V+m|E|) time and O(|V|) space. The corresponding alignment can be obtained in the same runtime using O(\u221am|V|) space. The time complexity depends only on the length of the sequence and the size of the graph. In particular, it does not depend on the cyclicity of the graph, or any other topological features.", "tag": "Bioinformatics"}, {"title": "HapCHAT: Adaptive haplotype assembly for efficiently leveraging high coverage in long reads", "url": "https://www.biorxiv.org/content/early/2017/11/08/170225", "abstract": "Background: Haplotype assembly is the process of assigning the different alleles of the Single Nucleotide Polymorphisms (SNPs) covered by mapped sequencing reads to the two haplotypes of the genome of a human individual. Long reads, which are nowadays cheaper to produce and more widely available, have been used to improve the accuracy of the assembled haplotypes since their ability to span several SNPs along the genome. These long read are also characterized by a high error rate, an issue which may be mitigated, however, with larger sets of reads, since this error rate is uniform across genome positions. Unfortunately, current state-of-the-art methods that have been designed for long reads deal only with limited coverages. Results: Here, we propose a new method for assembling haplotypes which combines and extends the features of previous approaches to deal with long reads and higher coverages. In particular, our algorithm is able to dynamically adapt the estimated number of errors at each variant site, while minimizing the total number of error corrections necessary for finding a feasible solution. This allows our method to significantly reduce the required computational resources, allowing to consider datasets composed of higher coverages across genome positions. The algorithm has been implemented in a freely available tool, HapCHAT: _Hap_lotype Assembly _C_overage _H_andling by _A_dapting _T_hresholds. An experimental analysis on sequencing reads with up to 60x coverage reveals accuracy improvements achieved by considering a higher coverage with lower runtimes. Conclusions: Our method leverages the long-range information of sequencing reads that allows to obtain assembled haplotypes fragmented in a lower number of unphased haplotype blocks. At the same time, our method is also able to deal with higher coverages to better correct the errors in the original reads and to obtain more accurate haplotypes as a result.", "tag": "Bioinformatics"}, {"title": "Guanosine monophosphate reductase 1 is a potential therapeutic target for Alzheimer\u2032s disease", "url": "https://www.biorxiv.org/content/early/2017/11/08/215947", "abstract": "Alzheimer\u2032s disease (AD) is a severe neurodegenerative disorder. Identification of differentially expressed genes in AD would help to find biomarker and therapeutic target. Here, we carried out an analysis to identify the age-independent and AD-specific genes. We found that genes MET, WIF1 and NPTX2 are down regulated in AD. WIF1 and MET are in signaling of WNT and MET, regulating the activity of GSK3B, thus in AD. Importantly, we found gene GMPR shows a gradual increase in AD progress. A logistic model based on GMPR exhibits a good capacity in classifying AD cases. GMPR\u2032s product GMPR1 links with AMPK and adenosine receptor pathways, thus associating phosphorylation of Tau in AD. This allows GMPR1 to be a therapeutic target. Therefore, we screened five possible inhibitors to GMPR1 by docking GMPR1 with 1174 approved drugs. Among them, lumacaftor is ideal due to its high affinity and light molecular weight. We then tested the effect of lumacaftor on AD model mice. After twenty days of oral administration, Amyloid-beta accumulation is slowed down and phosphorylation of Tau is almost eliminated in the treated mice, showing a satisfying effect. In conclusion, the elevated expression level of GMPR tightly associates with AD progress and leads to AD phenotype probably through AMPK and adenosine receptor pathways; and one of therapeutic strategies is to inhibit GMPR\u2032s product with lumacaftor.", "tag": "Bioinformatics"}, {"title": "TraRECo: A Greedy Approach based de novo Transcriptome Assembler with Read Error Correction using Consensus Matrix", "url": "https://www.biorxiv.org/content/early/2017/11/08/216077", "abstract": "Background: Challenges in developing a good de novo transcriptome assembler include how to deal with read errors and sequence repeats. Almost all de novo assemblers utilize de Bruijn graph, which has a complexity linearly growing with data size while suffers from errors and repeat. Although one can correct errors by inspecting topological structure of the graph, it is an uneasy task when there are too many branches. There are two research directions: improving either graph reliability or path search precision. We focused on improving the reliability. Results: We present TraRECo, a greedy approach to de novo assembly employing error-aware graph construction. The idea is similar to overlap-layout-consensus approach used for genome assembly, but is different in that consensus is made through the entire graph construction step. Basically, we built contigs by direct read alignment within a distance margin and performed junction search to construct splicing graphs. While doing so, however, a contig of length l was represented by 4xl matrix (called consensus matrix), of which each element was the base count of aligned reads so far. A representative sequence is obtained, by taking majority in each column of the consensus matrix, to be used for further read alignment. Once splicing graphs were obtained, we used IsoLasso to find paths with noticeable read depth. The experiments using real and simulated reads showed that the method provides considerable improvements in sensitivity and reasonably better performances when comparing both sensitivity and precision. This could be achieved by making more erroneous reads to be participated in graph construction, which, in turn, improved the depth information quality used for the subsequent path search step. The results for simulated reads showed also challenges are still remaining since non-negligible percentage of transcripts with high abundance were not recovered by the assemblers we considered. Conclusion: de novo assembly is mainly to explore not-yet-discovered isoforms and must be able to represent as much reads as possible in an efficient way. In this sense, TraRECo provides us a potential alternative to improve graph reliability, even though the computational burden can be much higher than single k-mer de Bruijn graph approach.", "tag": "Bioinformatics"}, {"title": "Panaconda: Application of pan-synteny graph models to genome content analysis", "url": "https://www.biorxiv.org/content/early/2017/11/08/215988", "abstract": "Motivation: Whole-genome alignment and pan-genome analysis are useful tools in understanding the similarities and differences of many genomes in an evolutionary context. Here we introduce the concept of pan-synteny graphs, an analysis method that combines elements of both to represent conservation and change of multiple prokaryotic genomes at an architectural level. Pan-synteny graphs represent a reference free approach for the comparison of many genomes and allows for the identification of synteny, insertion, deletion, replacement, inversion, recombination, missed assembly joins, evolutionary hotspots, and reference based scaffolding. Results: We present an algorithm for creating whole genome multiple sequence comparisons and a model for representing the similarities and differences among sequences as a graph of syntenic gene families. As part of the pan-synteny graph creation, we first create a de Bruijn graph. Instead of the alphabet of nucleotides commonly used in genome assembly, we use an alphabet of gene families. This de Bruijn graph is then processed to create the pan-synteny graph. Our approach is novel in that it explicitly controls how regions from the same sequence and genome are aligned and generates a graph in which all sequences are fully represented as paths. This method harnesses previous computation involved in protein family calculation to speed up the creation of whole genome alignment for many genomes. We provide the software suite Panaconda, for the calculation of pan-synteny graphs given annotation input, and an implementation of methods for their layout and visualization.", "tag": "Bioinformatics"}, {"title": "SAFE-clustering: Single-cell Aggregated (From Ensemble) Clustering for Single-cell RNA-seq Data", "url": "https://www.biorxiv.org/content/early/2017/11/07/215723", "abstract": "We present SAFE-clustering, a flexible and efficient cluster ensemble method for single-cell RNA-seq (scRNA-seq) data. Taking as input results from multiple clustering methods, SAFE-clustering generates more accurate and robust consensus clustering results. Assessment across 14 datasets with the number of clusters ranging from 3 to 14, and the number of single cells ranging from 49 to >32,000 showcases the advantages of SAFE-clustering (http://yunliweb.its.unc.edu/safe/) both computationally and in terms of performance.", "tag": "Bioinformatics"}, {"title": "Biomarker identification for statin sensitivity of cancer cell lines", "url": "https://www.biorxiv.org/content/early/2017/11/07/215756", "abstract": "Statins are potent cholesterol reducing drugs that have been shown to reduce tumor cell proliferation in vitro and tumor growth in animal models. Moreover, retrospective human cohort studies demonstrated decreased cancer-specific mortality in patients taking statins. We previously implicated membrane E-cadherin expression as both a marker and mechanism for resistance to atorvastatin-mediated growth suppression of cancer cells; however, a transcriptome-profile-based biomarker signature for statin sensitivity has not yet been reported. Here, we utilized transcriptome data from fourteen NCI-60 cancer cell lines and their statin dose-response data to produce gene expression signatures that identify statin sensitive and resistant cell lines. We experimentally confirmed the validity of the identified biomarker signature in an independent set of cell lines and extended this signature to generate a proposed statin-sensitive subset of tumors listed in the TCGA database. Finally, we predicted drugs that would synergize with statins and found several predicted combination therapies to be experimentally confirmed. The combined bioinformatics-experimental approach described here can be used to generate an initial biomarker sensitivity for statin therapy.", "tag": "Bioinformatics"}, {"title": "Modeling Spatial Genomic Interactions with the Hawkes model", "url": "https://www.biorxiv.org/content/early/2017/11/07/214874", "abstract": "The spatial localization of many DNA-protein interactions is now available thanks to the development of ChIP-Seq, and their investigation calls for adapted statistical methods. Many methods were developped for peak calling, but few were proposed for the downstream analysis of peak-like data, whereas the spatial structure of such data may contain relevant biological information, like binding constraints for instance. Associations between the occurrences of two genomic features are usually assessed by overlaps, but here we propose a statistical model to precisely quantify the spatial interactions between the location of binding events. Our methodology relies on a multivariate spatial process, the Hawkes model, that can also be interpreted in terms of a graphical model to highlight spatial dependencies between genomic features. Using our method, we explore the chromatinian landscape of replication origins, and we highlight attractive and repulsive patterns that can be related to the regulation of the spatial program of replication.", "tag": "Bioinformatics"}, {"title": "A practical tool for Maximal Information Coefficient analysis", "url": "https://www.biorxiv.org/content/early/2017/11/07/215855", "abstract": "Background: The ability of finding complex associations in large omics datasets, assessing their significance, and prioritizing them according to their strength can be of great help in the data exploration phase. Mutual Information based measures of association are particularly promising, in particular after the recent introduction of the TICe and MICe estimators, which combine computational efficiency with good bias/variance properties. Despite that, a complete software implementation of these two measures and of a statistical procedure to test the significance of each association is still missing. Findings: In this paper we present MICtools, a comprehensive and effective pipeline which combines TICe and MICe into a multi-step procedure that allows the identification of relationships of various degrees of complexity. MICtools calculates their strength assessing statistical significance using a permutation-based strategy. The performances of the proposed approach are assessed by an extensive investigation in synthetic datasets and an example of a potential application on a metagenomic dataset is also illustrated. Conclusions: We show that MICtools, combining TICe and MICe, is able to highlight associations that would not be captured by conventional strategies. MICtools is implemented in Python, and is available for download at https://github.com/minepy/mictools.", "tag": "Bioinformatics"}, {"title": "DHX36 binding at G-rich sites in mRNA untranslated regions promotes translation", "url": "https://www.biorxiv.org/content/early/2017/11/07/215541", "abstract": "Translation efficiency can be affected by mRNA stability and secondary structures, including so-called G-quadruplex (G4) structures. The highly conserved and essential DEAH-box helicase DHX36/RHAU is able to resolve G4 structures on DNA and RNA in vitro, however a system-wide analysis of DHX36 targets and function is lacking. We globally mapped DHX36 occupancy in human cell lines and found that it preferentially binds to G-rich sequences in the coding sequences (CDS) and 5\u2032 and 3\u2032 untranslated regions (UTR) of more than 4,500 mRNAs. Functional analyses, including RNA sequencing, ribosome footprinting, and quantitative mass spectrometry revealed that DHX36 decreased target mRNA stability. However, target mRNA accumulation in DHX36 KO cells did not lead to a significant increase in ribosome footprints or protein output indicating that they were translationally incompetent. We hypothesize that DHX36 resolves G4 and other structures that interfere with efficient translation initiation.", "tag": "Bioinformatics"}, {"title": "Resolving outbreak dynamics using Approximate Bayesian Computation for stochastic birth-death models", "url": "https://www.biorxiv.org/content/early/2017/11/07/215533", "abstract": "Earlier research has suggested that Approximate Bayesian Computation (ABC) makes it possible to fit intractable simulator-based stochastic birth-death models to investigate communicable disease outbreak dynamics and that the accuracy of ABC inference can be comparable to that of exact Bayesian inference based on for example particle-filtering Markov Chain Monte Carlo. However, recent findings have indicated that key parameters such as the reproductive number, R, may remain poorly identifiable from data generated under an infinite alleles model. Here we show that the identifiability issue can be resolved by taking into account disease-specific characteristics of the transmission process in closer detail in the birth-death model. Using tuberculosis (TB) in the San Francisco Bay area as a case-study, we consider the situation where the genotype data are generated as a mixture of two stochastic processes, each with their distinct dynamics and clear epidemiological interpretation. ABC inference based on the ELFI software yields stable and accurate posterior inferences about outbreak dynamics from aggregated annual case data with genotype information. We also show that under the proposed model the infectious population size can be reliably inferred and that it is approximately two orders of magnitude smaller than considered in the previous ABC studies focusing on the same data, which is much better aligned with epidemiological knowledge about active TB prevalence. Similarly, the reproductive number R related to the primary underlying transmission process is estimated to be nearly three-fold compared with the previous estimates, which has a substantial impact on the interpretation of the fitted outbreak model. Our Python codes implementing the simulator model and the inference algorithm are freely available for further research and use at GitHub.", "tag": "Bioinformatics"}, {"title": "Bioinformatics Workflow Management With The Wobidisco Ecosystem", "url": "https://www.biorxiv.org/content/early/2017/11/07/213884", "abstract": "To conduct our computational experiments, our team developed a set of workflow-management-related projects: Ketrew, Biokepi, and Coclobas. The family of tools and libraries are designed with reliability and flexibility as main guiding principles. We describe the components of the software stack and explain the choices we made. Every piece of software is free and open-source; the umbrella documentation project is available at https://github.com/hammerlab/wobidisco.", "tag": "Bioinformatics"}, {"title": "Predicting Cancer Drug Response Using a Recommender System", "url": "https://www.biorxiv.org/content/early/2017/11/07/215327", "abstract": "Motivation: As we move towards an era of precision medicine, the ability to predict patient-specific drug responses in cancer based on molecular information such as gene expression data represents both an opportunity and a challenge. In particular, methods are needed that can accommodate the high-dimensionality of data to learn interpretable models capturing drug response mechanisms, as well as providing robust predictions across datasets. Results: We propose a method based on ideas from \"recommender systems\" (CaDRReS) that predicts cancer drug responses for unseen cell-lines/patients based on learning projections for drugs and cell-lines into a latent \"pharmacogenomic\" space. Comparisons with other proposed approaches for this problem based on large public datasets (CCLE, GDSC) shows that CaDRReS provides consistently good models and robust predictions even across unseen patient-derived cell-line datasets. Analysis of the pharmacogenomic spaces inferred by CaDRReS also suggests that they can be used to understand drug mechanisms, identify cellular subtypes, and further characterize drug-pathway associations.", "tag": "Bioinformatics"}, {"title": "ReprDB and panDB: minimalist databases with maximal microbial representation", "url": "https://www.biorxiv.org/content/early/2017/11/07/214916", "abstract": "Background: Profiling of shotgun metagenomic samples is hindered by a lack of unified microbial reference genome databases that i) assemble genomic information from all open access microbial genomes, ii) have relatively small sizes, and iii) are compatible to various metagenomic read mapping tools. Moreover, computational tools to rapidly compile and update such databases to accommodate the rapid increase in new reference genomes do not exist. As a result, database-guided analyses often fail to profile a substantial fraction of metagenomic shotgun sequencing reads from complex microbiomes. Results: We report pipelines that efficiently traverse all open access microbial genomes and assemble non-redundant genomic information. The pipelines result in two species-resolution microbial reference databases of relatively small sizes: reprDB, which assembles microbial representative or reference genomes, and panDB, for which we developed a novel iterative alignment algorithm to identify and assemble non-redundant genomic regions in multiple sequenced strains. With the databases, we managed to assign taxonomic labels and genome positions to the majority of metagenomic reads from human skin and gut microbiomes, demonstrating a significant improvement over a previous database-guided analysis on the same datasets. Conclusions: reprDB and panDB leverage the rapid increases in the number of open access microbial genomes to more fully profile metagenomic samples. Additionally, the databases exclude redundant sequence information to avoid inflated storage or memory space and indexing or analyses time. Finally, the novel iterative alignment algorithm significantly increases efficiency in pan-genome identification and can be useful in comparative genomic analyses.", "tag": "Bioinformatics"}, {"title": "Towards a reliable, automated method of individual alpha frequency (IAF) quantification", "url": "https://www.biorxiv.org/content/early/2017/11/07/176792", "abstract": "Individual alpha frequency (IAF) is a promising electrophysiological marker of interindividual differences in cognitive function. IAF has been linked with trait-like differences in information processing and general intelligence, and provides an empirical basis for the definition of individualised frequency bands. Despite its widespread application, however, there is little consensus on the optimal method for estimating IAF, and many common approaches are prone to bias and inconsistency. Here, we describe an automated strategy for deriving two of the most prevalent IAF estimators in the literature: peak alpha frequency (PAF) and centre of gravity (CoG). These indices are calculated from resting-state power spectra that have been smoothed using a Savitzky-Golay filter (SGF). We evaluate the performance characteristics of this analysis procedure in both empirical and simulated EEG datasets. Applying the SGF technique to resting-state data from n = 63 healthy adults furnished 61 PAF, and 62 CoG estimates. The statistical properties of these estimates were consistent with previous reports. Simulation analyses revealed that the SGF routine was able to reliably extract target alpha components, even under relatively noisy spectral conditions. The routine consistently outperformed a simpler method of automated peak detection that did not involve spectral smoothing. The SGF technique is fast, open-source, and available in two popular programming languages (MATLAB and Python), and thus can easily be integrated within the most popular M/EEG toolsets (EEGLAB, FieldTrip and MNE-Python). As such, it affords a convenient tool for improving the reliability and replicability of future IAF-related research.", "tag": "Bioinformatics"}, {"title": "Arteria: An automation system for a sequencing core facility", "url": "https://www.biorxiv.org/content/early/2017/11/06/214858", "abstract": "Arteria is an automation system aimed at sequencing core facilities. It is built on existing open source technologies, with a modular design allowing for a community-driven effort to create plug-and-play micro-services. Herein we describe the Arteria system and elaborate on the underlying conceptual framework. The Arteria system breaks down into three conceptual levels; orchestration, process and execution. At the orchestration level it utilizes an event-based model of automation. It models processes, e.g. the steps involved in processing sequencing data, as workflows and executes these in a micro-service based environment. This creates a system which is both flexible and scalable. The Arteria Project code is available as open source software at http://www.github.com/arteria-project.", "tag": "Bioinformatics"}, {"title": "DriverPower: Combined burden and functional impact tests for cancer driver discovery", "url": "https://www.biorxiv.org/content/early/2017/11/06/215244", "abstract": "We describe DriverPower, a software package that uses mutational burden and functional impact evidence to identify cancer driver mutations in coding and non-coding sites within cancer whole genomes. Using a total of 1,373 genomic features derived from public sources, DriverPower's background mutation model explains up to 93% of the regional variance in the mutation rate across a variety of tumour types. By incorporating functional impact scores, we are able to further increase the accuracy of driver discovery. Testing across a collection of 2,583 cancer genomes from the Pan-Cancer Analysis of Whole Genomes (PCAWG) project, DriverPower identifies 217 coding and 95 non-coding driver candidates. Comparing to six published methods used by the PCAWG Drivers and Functional Interpretation Group, DriverPower has the highest F1-score for both coding and non-coding driver discovery. This demonstrates that DriverPower is an effective framework for computational driver discovery.", "tag": "Bioinformatics"}, {"title": "Exploring biological networks in 3D, stereoscopic 3D and immersive 3D with iCAVE", "url": "https://www.biorxiv.org/content/early/2017/11/06/212126", "abstract": "Biological networks are becoming increasingly large and complex, pushing the limits of existing 2D tools. iCAVE is an open source software tool for interactive visual explorations of large and complex networks in 3D, stereoscopic 3D or immersive 3D. It introduces new 3D network layout algorithms and 3D-extensions of popular 2D network layout, clustering and edge bundling algorithms to assists researchers in understanding the underlying patterns in large, multi-layered, clustered or complex networks. This protocol aims to guide new users on the basic functions of iCAVE for loading data, laying out networks (single or multi-layered), bundling edges, clustering networks, visualizing clusters, visualizing data attributes and saving output images or videos. It also provides examples on visualizing networks constrained in physical 3D space (e.g. proteins; neurons; brain). It is accompanied with a new version of iCAVE with an enhanced user interface and highlights new features useful for existing users.", "tag": "Bioinformatics"}, {"title": "CalR: A Web-based Analysis Tool for Indirect Calorimetry Experiments", "url": "https://www.biorxiv.org/content/early/2017/11/06/213967", "abstract": "We report a web-based tool for analysis of indirect calorimetry experiments which measure physiological energy balance. CalR easily imports raw data files, generates plots, and determines the most appropriate statistical tests for interpretation. Analysis with the general linear model (which includes ANOVA and ANCOVA) allows for flexibility to interpret experiments of obesity and thermogenesis. Users may also produce standardized output files of an experiment which can be shared and subsequently re-evaluated using CalR. This framework will provide the transparency necessary to enhance consistency and reproducibility in experiments of energy expenditure. CalR analysis software will greatly increase the speed and efficiency with which metabolic experiments can be organized, analyzed according to accepted norms, and reproduced, and will likely become a standard tool for the field. CalR is accessible at https://CalR.bwh.harvard.edu.", "tag": "Bioinformatics"}, {"title": "Community assessment of cancer drug combination screens identifies strategies for synergy prediction", "url": "https://www.biorxiv.org/content/early/2017/11/06/200451", "abstract": "In the last decade advances in genomics, uptake of targeted therapies, and the advent of personalized treatments have fueled a dramatic change in cancer care. However, the effectiveness of most targeted therapies is short lived, as tumors evolve and develop resistance. Combinations of drugs offer the potential to overcome resistance. The space of possible combinations is vast, and significant advances are required to effectively find optimal treatment regimens tailored to a patient's tumor. DREAM and AstraZeneca hosted a Challenge open to the scientific community aimed at computational prediction of synergistic drug combinations and predictive biomarkers associated to these combinations. We released a data set comprising ~11,500 experimentally tested drug combinations, coupled to deep molecular characterization of the respective 85 cancer cell lines. Among 150 submitted approaches, those that incorporated prior knowledge of putative drug targets showed superior performance predicting drug synergy across independent data. Genomic features of best-performing models revealed putative mechanisms of drug synergy for multiple drugs in combination with PI3K/AKT pathway inhibitors.", "tag": "Bioinformatics"}, {"title": "Integrated molecular and clinical analysis for understanding human disease relationships", "url": "https://www.biorxiv.org/content/early/2017/11/06/214833", "abstract": "We jointly examined gene-expression and electronic health record data for 104 diseases to identify unbiased clusters of molecularly and clinically related diseases. We performed gene expression meta-analysis of 41,000 samples and computed diseases' clinical profile similarity using 2 million patient records. Based on molecular data, we observed autoimmune diseases clustering with their specific infectious triggers and brain disorders clustering by disease class. In contrast, the electronic health records based clinical profiles clustered diseases according to the similarity of their initial manifestation and later complications. Our integrated molecular and clinical analysis identified diseases with under-appreciated, therapeutically actionable relationships, such as between myositis and interstitial cystitis. This global understanding of relationships between diseases has potential to identify disease causing mechanisms and offer novel therapeutic targets.", "tag": "Bioinformatics"}, {"title": "Quetzal - an open source C++ template library for coalescence-based environmental demogenetic models inference", "url": "https://www.biorxiv.org/content/early/2017/11/06/214767", "abstract": "The purpose of this article is to introduce an implementation framework enabling us, using available genetic samples, to understand and foresee the behavior of species living in a fragmented and temporally changing environment. To this aim, we first present a model of coalescence which is conditioned to environment, through an explicit modeling of population growth and migration. The parameters of this model can be infered using Approximate Bayesian Computation techniques, which supposes that the considered model can be efficiently simulated. We next present Quetzal, a C++ library composed of generic components designed to efficiently implement a wide range of coalescence-based environmental demogenetic models.", "tag": "Bioinformatics"}, {"title": "Deep learning accurately predicts estrogen receptor status in breast cancer metabolomics data", "url": "https://www.biorxiv.org/content/early/2017/11/06/214254", "abstract": "Metabolomics holds the promise as a new technology to diagnose highly heterogeneous diseases. Conventionally, metabolomics data analysis for diagnosis is done using various statistical and machine learning based classification methods. However, it remains unknown if deep neural network, a class of increasingly popular machine learning methods, is suitable to classify metabolomics data. Here we use a cohort of 271 breast cancer tissues, 204 positive estrogen receptor (ER+) and 67 negative estrogen receptor (ER-), to test the accuracies of autoencoder, a deep learning (DL) framework, as well as six widely used machine learning models, namely Random Forest (RF), Support Vector Machines (SVM), Recursive Partitioning and Regression Trees (RPART), Linear Discriminant Analysis (LDA), Prediction Analysis for Microarrays (PAM), and Generalized Boosted Models (GBM). DL framework has the highest area under the curve (AUC) of 0.93 in classifying ER+/ER- patients, compared to the other six machine learning algorithms. Furthermore, the biological interpretation of the first hidden layer reveals eight commonly enriched significant metabolomics pathways (adjusted P-value<0.05) that cannot be discovered by other machine learning methods. Among them, protein digestion & absorption and ATP-binding cassette (ABC) transporters pathways are also confirmed in integrated analysis between metabolomics and gene expression data in these samples. In summary, deep learning method shows advantages for metabolomics based breast cancer ER status classification, with both the highest prediction accurcy (AUC=0.93) and better revelation of disease biology. We encourage the adoption of autoencoder based deep learning method in the metabolomics research community for classification.", "tag": "Bioinformatics"}, {"title": "Identification of residue pairing in interacting \u03b2-strands from a predicted residue contact map", "url": "https://www.biorxiv.org/content/early/2017/11/06/214643", "abstract": "Despite the rapid progress of protein residue contact prediction, predicted residue contact maps frequently contain many errors. However, information of residue pairing in \u03b2 strands could be extracted from a noisy contact map, due to the presence of characteristic contact patterns in \u03b2-\u03b2 interactions. This information may benefit the tertiary structure prediction of mainly \u03b2 proteins. In this work, we introduce a novel ridge-detection-based \u03b2-\u03b2 contact predictor, RDb2C, to identify residue pairing in \u03b2 strands from any predicted residue contact map. The algorithm adopts ridge detection, a well-developed technique in computer image processing, to capture consecutive residue contacts, and then utilizes a novel multi-stage random forest framework to integrate the ridge information and additional features for prediction. Starting from the predicted contact map of CCMpred, RDb2C remarkably outperforms all state-of-the-art methods on two conventional test sets of \u03b2 proteins (BetaSheet916 and BetaSheet1452), and achieves F1-scores of ~62% and ~76% at the residue level and strand level, respectively. Taking the prediction of the more advanced RaptorX-Contact as input, RDb2C achieves impressively higher performance, with F1-scores reaching ~76% and ~86% at the residue level and strand level, respectively. According to our tests on 61 mainly \u03b2 proteins, improvement in the \u03b2-\u03b2 contact prediction can further ameliorate the structural prediction. Availability: All source data and codes are available at http://166.111.152.91/Downloads.html or at the GitHub address of https://github.com/wzmao/RDb2C.", "tag": "Bioinformatics"}, {"title": "Binless normalization of Hi-C data provides significant interaction and difference detection independently of resolution", "url": "https://www.biorxiv.org/content/early/2017/11/05/214403", "abstract": "3C-like experiments, such as 4C or Hi-C, have been fundamental in understanding genome organization. Thanks to these technologies, it is now known, for example, that Topologically Associating Domains (TADs) and chromatin loops are implicated in the dynamic interplay of gene activation and repression, and their disruption can have dramatic effects on embryonic development. To make their detection easier, scientists have endeavored into deeper sequencing to mechanically increase the chances to detect weaker signals such as chromatin loops. Part of this mindset can be attributed to the limitations of existing software: the analysis of Hi-C experiments is both statistically and computationally demanding. Here, we devise a new way to represent Hi-C data, which leads to a more detailed classification of paired-end reads and, ultimately, to a new normalization and interaction detection method. Unlike any other, Binless is resolution-agnostic, and adapts to the quality and quantity of available data. We demonstrate its capacities to call interactions and differences and make the software freely available.", "tag": "Bioinformatics"}, {"title": "TaxAss: Leveraging Custom Databases Achieves Fine-Scale Taxonomic Resolution", "url": "https://www.biorxiv.org/content/early/2017/11/05/214288", "abstract": "Taxonomy assignment for microbial community composition studies can be limited by a lack of relevant reference organisms in large taxonomy databases. TaxAss is a taxonomy assignment workflow to classify 16S rRNA gene amplicon data using two taxonomy reference databases: a large comprehensive database such as Greengenes or SILVA, and a small ecosystem-specific database curated by scientists within a field. To test TaxAss performance, we classified five different freshwater datasets using the comprehensive Greengenes database and the freshwater-specific FreshTrain database. TaxAss increased the percent of the dataset classified compared to using only Greengenes. The increase in classifications was highest at fine-resolution taxa levels, where across the freshwater test-datasets the classifications at species-level increased by 24-40 percent reads. A similar increase in classifications was not observed in a control mouse gut dataset, which was not expected to contain freshwater bacteria. TaxAss maintained taxonomic richness compared to using only the FreshTrain. Richness was maintained across all taxa-levels from phylum to species. Without TaxAss, the majority of organisms not represented in the FreshTrain were unclassified, but at finer taxa levels incorrect classifications were also significant. TaxAss splits a dataset's sequences into two groups based on their percent identity to reference sequences in the ecosystem-specific database. Highly similar sequences are classified using the ecosystem-specific database and the others are classified using the comprehensive database. TaxAss metrics help users choose a percent identity cutoff appropriate for their data. TaxAss is free and open source, and available at www.github.com/McMahonLab/TaxAss.", "tag": "Bioinformatics"}, {"title": "TCGA-Assembler 2: Software Pipeline for Retrieval and Processing of TCGA/CPTAC Data", "url": "https://www.biorxiv.org/content/early/2017/11/05/214320", "abstract": "Motivation: The Cancer Genome Atlas (TCGA) program has produced huge amounts of cancer genomics data providing unprecedented opportunities for research. In 2014, we developed TCGA-Assembler (Zhu et al, 2014), a software pipeline for retrieval and processing of public TCGA data. In 2016, TCGA data were transferred from the TCGA data portal to the Genomic Data Commons (GDC), which is supported by a different set of data storage and retrieval mechanisms. In addition, new proteomics data of TCGA samples have been generated by the Clinical Proteomic Tumor Analysis Consortium (CPTAC) program, which were not available for downloading through TCGA-Assembler. It is desirable to acquire and integrate data from both GDC and CPTAC. Results: We develop TCGA-Assembler 2 (TA2) to automatically download and integrate data from GDC and CPTAC. We make substantial improvement on the functionality of TA2 to enhance user experience and software performance. TA2 together with its previous version have helped more than 2,000 researchers from 64 countries to access and utilize TCGA and CPTAC data in their research. Availability of TA2 will continue to allow existing and new users to conduct reproducible research based on TCGA and CPTAC data. Availability: http://www.compgenome.org/TCGA-Assembler/ .", "tag": "Bioinformatics"}, {"title": "Boosting Gene Expression Clustering with System-Wide Biological Information: A Robust Autoencoder Approach", "url": "https://www.biorxiv.org/content/early/2017/11/05/214122", "abstract": "Gene expression analysis provides genome-wide insights into the transcriptional activity of a cell. One of the first computational steps in exploration and analysis of the gene expression data is clustering. With a number of standard clustering methods routinely used, most of the methods do not take prior biological information into account. In this paper, we propose a new approach for gene expression clustering analysis. The approach benefits from a new deep learning architecture, Robust Autoencoder, which provides a more accurate high-level representation of the feature sets, and from incorporating prior biological information into the clustering process. We tested our approach on two distinct gene expression datasets and compared the performance with two widely used clustering methods, hierarchical clustering and k-means, as well as with a recent deep learning clustering approach. As a result, our approach outperformed all other clustering methods on the labeled yeast gene expression dataset. Furthermore we showed that it is better in identifying the functionally common clusters than k-means on the unlabeled human gene expression dataset. The results demonstrate that our new deep learning architecture could generalize well the specific properties of gene expression profiles. Furthermore, the results confirm our hypothesis that the prior biological network knowledge could be helpful in the gene expression clustering task.", "tag": "Bioinformatics"}, {"title": "TNER: A Novel Bayesian Background Error Suppression Method for Mutation Detection in Circulating Tumor DNA", "url": "https://www.biorxiv.org/content/early/2017/11/05/214379", "abstract": "The use of ultra-deep, next generation sequencing of circulating tumor DNA (ctDNA) holds great promise for early detection of cancer as well as a tool for monitoring disease progression and therapeutic responses. However, the low abundance of ctDNA in the bloodstream coupled with technical errors introduced during library construction and sequencing, complicates mutation detection. To achieve high accuracy of variant calling via better distinguishing low frequency ctDNA mutations from technical errors, we introduce TNER (Tri-Nucleotide Error Reducer), a novel Bayesian background error suppression method that provides a robust estimation of background noise to enhance the specificity for ctDNA mutation detection without sacrificing sensitivity. Results on both simulated and real healthy subjects data demonstrate that the proposed algorithm consistently outperforms current position-specific models, particularly when the sample of is small. TNER is publicly available at https://github.com/ctDNA/TNER.", "tag": "Bioinformatics"}, {"title": "Pydigree: a python library for manipulation and forward-time simulation and of genetic datasets", "url": "https://www.biorxiv.org/content/early/2017/11/05/213413", "tag": "Bioinformatics", "abstract": "The development of software for working with data from population genetics or genetic epidemiology often requires substantial time spent implementing common procedures. Pydigree is a cross-platform Python 3 library that contains efficient, user friendly implementations for many of these common functions, and support for input from common file formats. Developers can combine the functions and data structures to rapidly implement programs handling genetic data. Pydigree presents a useful environment for development of applications for genetic data or rapid prototyping before reimplementation in a higher-performance language. Pydigree is freely available under an open source license. Stable sources can be found in the Python Package Index at https://pypi.python.org/pypi/pydigree/, and development sources can be downloaded at https://github.com/jameshicks/pydigree/ ."}, {"title": "Creating ethnicity-specific reference intervals for lab tests from EHR data", "url": "https://www.biorxiv.org/content/early/2017/11/04/213892", "tag": "Bioinformatics", "abstract": "The results of clinical lab tests are an essential component of medical decision-making. To guide interpretation, test results are returned with reference intervals defined by the range in which 95% of values occur in healthy individuals. Clinical laboratories often set their own reference intervals to accommodate local population and instruments variations. This approach is costly and can be biased. We describe a novel data-driven method for using electronic health record data to extract healthy patients' information to define reference intervals. We found that the distributions of many clinical lab tests differ among self-identified racial and ethnic groups (SIREs) in healthy patients. Finally, we derived SIRE-specific reference intervals and provide evidence that these intervals have clinical prognostic value. Specifically, we show that for two lab tests, serum creatinine level and hemoglobin A1C, SIRE-specific reference intervals are more predictive for need for dialysis and development type 2 diabetes than existing reference intervals."}, {"title": "Loter: A software package to infer local ancestry for a wide range of species", "url": "https://www.biorxiv.org/content/early/2017/11/03/213728", "tag": "Bioinformatics", "abstract": "Admixture between populations provides opportunity to study biological adaptation and phenotypic variation. Admixture studies can rely on local ancestry inference for admixed individuals, which consists of computing at each locus the number of copies that originate from ancestral source populations. Existing software packages for local ancestry inference are tuned to provide accurate results on human data and recent admixture events. Here, we introduce Loter, an open-source software package that does not require any biological parameter besides haplotype data in order to make local ancestry inference available for a wide range of species. Using simulations, we compare the performance of Loter to HAPMIX, LAMP-LD, and RFMix. HAPMIX is the only software severely impacted by imperfect haplotype reconstruction. Loter is the less impacted software by increasing admixture time when considering simulated and admixed human genotypes. LAMP-LD and RFMIX are the most accurate method when admixture took place 20 generations ago or less; Loter accuracy is comparable or better than RFMix accuracy when admixture took place of 50 or more generations; and its accuracy is the largest when admixture is more ancient than 150 generations. For simulations of admixed Populus genotypes, Loter and LAMP-LD are robust to increasing admixture times by contrast to RFMix. When comparing length of reconstructed and true ancestry tracts, Loter and LAMP-LD provide results whose accuracy is again more robust than RFMix to increasing admixture times. We apply Loter to admixed Populus individuals and lengths of ancestry tracts indicate that admixture took place around 100 generations ago."}, {"title": "Modelling G\u00d7E with historical weather information improves genomic prediction in new environments", "url": "https://www.biorxiv.org/content/early/2017/11/03/213231", "tag": "Bioinformatics", "abstract": "Interaction between the genotype and the environment (G\u00d7E) has a strong impact on the yield of major crop plants. Although influential, taking G\u00d7E explictily into account in plant breeding has remained difficult. Recently G\u00d7E has been predicted from environmental and genomic covariates, but existing works have not shown that generalization to new environments and years without access to in-season data is possible and practical applicability remains unclear. Using data from a Barley breeding program in Finland, we construct an in-silico experiment to study the viability of G\u00d7E prediction under practical constraints. We show that the response to the environment of a new generation of untested Barley cultivars can be predicted in new locations and years using genomic data, machine learning and historical weather observations for the new locations. Our results highlight the need for models of G\u00d7E: non-linear effects clearly dominate linear ones and the interaction between the soil type and daily rain is identified as the main driver for G\u00d7E for Barley in Finland. Our study implies that genomic selection can be used to capture the yield potential in G\u00d7E effects for future growth seasons, providing a possible means to achieve yield improvements, needed for feeding the growing population."}, {"title": "GrigoraSNPs: Optimized HTS DNA Forensic SNP Analysis", "url": "https://www.biorxiv.org/content/early/2017/11/03/173716", "tag": "Bioinformatics", "abstract": "High throughput DNA sequencing technologies enable improved characterization of forensic DNA samples enabling greater insights into DNA contributor(s). Current DNA forensics techniques rely upon allele sizing of short tandem repeats by capillary electrophoresis. High throughput sequencing enables forensic sample characterizations for large numbers of single nucleotide polymorphism loci. The slowest computational component of the DNA forensics analysis pipeline is the characterization of raw sequence data. This paper optimizes the SNP calling module of the DNA analysis pipeline with runtime results that scale linearly with the number of HTS sequences (patent pending). GrigoraSNPs can analyze 100 million reads in less than 5 minutes using 3 threads on a 4.0 GHz Intel i7-6700K laptop CPU. Compared to standard bioinformatics pipelines that run for hours on servers, GrigoraSNPs enables rapid sample analysis."}, {"title": "FireCloud, a scalable cloud-based platform for collaborative genome analysis: Strategies for reducing and controlling costs", "url": "https://www.biorxiv.org/content/early/2017/11/03/209494", "tag": "Bioinformatics", "abstract": "FireCloud, one of three NCI Cloud Pilots, is a collaborative genome analysis platform built on a cloud computing infrastructure. FireCloud aims to solve the many challenges presented by the increasingly large data sets and computing requirements employed in cancer research. However, cost uncertainty associated with cloud computing's pay-as-you-go model is proving to be a barrier to adoption of cloud computing. In this paper we present guidelines for optimizing workflows to minimize cost and reduce latency. Our guidelines include: (i) dynamic disk sizing to efficiently utilize virtual disks; (ii) tuned provisioning of virtual machines (VMs) using a performance monitoring tool; (iii) taking advantage of steep price discounts of preemptible VMs; and (iv) utilizing the optimal parallelization of a task's workload."}, {"title": "A descriptive marker gene approach to single-cell pseudotime inference", "url": "https://www.biorxiv.org/content/early/2017/11/02/060442", "tag": "Bioinformatics", "abstract": "Pseudotime estimation from single-cell gene expression allows the recovery of temporal information from otherwise static profiles of individual cells. This pseudotemporal information can be used to characterise transient events in temporally evolving biological systems. Conventional algorithms typically emphasise an unsupervised transcriptome-wide approach and use retrospective analysis to evaluate the behaviour of individual genes. Here we introduce an orthogonal approach termed \"Ouija\" that learns pseudotimes from a small set of marker genes that might ordinarily be used to retrospectively confirm the accuracy of unsupervised pseudotime algorithms. Crucially, we model these genes in terms of switch-like or transient behaviour along the trajectory, allowing us to understand why the pseudotimes have been inferred and learn informative parameters about the behaviour of each gene. Since each gene is associated with a switch or peak time the genes are effectively ordered along with the cells, allowing each part of the trajectory to be understood in terms of the behaviour of certain genes. In the following we introduce our model and demonstrate that in many instances a small panel of marker genes can recover pseudotimes that are consistent with those obtained using the entire transcriptome. Furthermore, we show that our method can detect differences in the regulation timings between two genes and identify \"metastable\" states - discrete cell types along the continuous trajectories - that recapitulate known cell types. Ouija therefore provides a powerful complimentary approach to existing whole transcriptome based pseudotime estimation methods. An open source implementation is available at http://www.github.com/kieranrcampbell/ouija as an R package and at http://www.github.com/kieranrcampbell/ouijaflow as a Python/TensorFlow package."}, {"title": "ZINB-WaVE: A general and flexible method for signal extraction from single-cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/11/02/125112", "tag": "Bioinformatics", "abstract": "Single-cell RNA sequencing (scRNA-seq) is a powerful high-throughput technique that enables researchers to measure genome-wide transcription levels at the resolution of single cells. Because of the low amount of RNA present in a single cell, some genes may fail to be detected even though they are expressed; these genes are usually referred to as dropouts. Here, we present a general and flexible zero-inflated negative binomial model (ZINB-WaVE), which leads to low-dimensional representations of the data that account for zero inflation (dropouts), over-dispersion, and the count nature of the data. We demonstrate, with simulated and real data, that the model and its associated estimation procedure are able to give a more stable and accurate low-dimensional representation of the data than principal component analysis (PCA) and zero-inflated factor analysis (ZIFA), without the need for a preliminary normalization step."}, {"title": "Interplay between folding and binding modulates protein sequences, structures, functions and regulation", "url": "https://www.biorxiv.org/content/early/2017/11/02/211524", "tag": "Bioinformatics", "abstract": "Intrinsically Disordered Proteins (IDPs) fulfill critical biological roles without having the potential to fold on their own. While lacking inherent structure, the majority of IDPs do reach a folded state via interaction with a protein partner, presenting a deep entanglement of the folding and binding process. Protein disorder has been recognized as a major determinant in several properties of proteins; yet the way the binding process is reflected in these features in general lacks this detail of description. Recent advances in database development enabled us to identify three basic scenarios of the interplay between folding and binding in unprecedented detail. These scenarios have fundamentally different properties in terms of protein sequence, structure, function and regulation, depending on the structural properties of the interacting partners. Strikingly, the existence of a binding partner and its structural properties influence all analyzed properties of proteins to the same extent as the divide between inherent order or disorder. The appreciation of this interplay between folding and binding is the basis for the successful charting of unknown territories in the protein interactome, the understanding of how different binding modes assemble regulatory networks, and the development of future pharmaceutical applications."}, {"title": "Genome-Wide Mining, Characterization and Development of miRNA-SSRs in Arabidopsis thaliana", "url": "https://www.biorxiv.org/content/early/2017/11/02/203851", "tag": "Bioinformatics", "abstract": "Simple Sequence Repeats (SSRs), also known as microsatellites are short tandem repeats of DNA sequences that are 1-6 bp long. In plants, SSRs serve as a source of important class of molecular markers because of their hypervariabile and co-dominant nature, making them useful both for the genetic studies and marker-assisted breeding. The SSRs are widespread throughout the genome of an organism, so that a large number of SSR datasets are available, most of them from either protein-coding regions or untranslated regions. It is only recently, that their occurrence within microRNAs (miRNA) genes has received attention. As is widely known, miRNA themselves are a class of non-coding RNAs (ncRNAs) with varying length of 19-22 nucleotides (nts), which play an important role in regulating gene expression in plants under different biotic and abiotic stresses. In this communication, we describe the results of a study, where miRNA-SSRs in full length pre-miRNA sequences of Arabidopsis thaliana were mined. The sequences were retrieved by annotations available at EnsemblPlants using BatchPrimer3 server with miRNA-SSR flanking primers found to be well distributed. Our analysis shows that miRNA-SSRs are relatively rare in protein-coding regions but abundant in non-coding region. All the observed 147 di-, tri-, tetra-, penta- and hexanucleotide SSRs were located in non-coding regions of all the 5 chromosomes of A. thaliana. While we confirm that miRNA-SSRs were commonly spread across the full length pre-miRNAs, we envisage that such studies would allow us to identify newly discovered markers for breeding studies."}, {"title": "An efficient gene regulatory network inference algorithm for early Drosophila melanogaster embryogenesis", "url": "https://www.biorxiv.org/content/early/2017/11/02/213025", "tag": "Bioinformatics", "abstract": "The spatial patterns of gene expression in early Drosophila melanogaster embryogenesis have been studied experimentally and theoretically to reveal the molecular basis of morphogenesis. In particular, the gene regulatory network (GRN) of gap genes has been investigated through mathematical modeling and simulation. Although these simulation-based approaches are useful for describing complex dynamics and have revealed several important regulations in spatial patterning, they are computationally intensive because they optimize GRN with iterative simulation. Recently, the advance of experimental technologies is enabling the acquisition of comprehensive spatial expression data, and an efficient algorithm will be necessary to analyze such large-scale data. In this research, we developed an efficient algorithm to infer the GRN based on a linear reaction-diffusion model. First, we qualitatively analyzed the GRNs of gap genes and pair-rule genes based on our algorithm and showed that two mutual repressions are fundamental regulations. Then, we inferred the GRN from gap gene data, and identified asymmetric regulations in addition to the two mutual repressions. We analyzed the effect of these asymmetric regulations on spatial patterns, and showed that they have the potential to adjust peak position. Our algorithm runs in sub-second time, which is significantly smaller than the runtime of simulation-based approaches (between 8 and 160 h, for example). Nevertheless, our inferred GRN was highly correlated with the simulation-based GRNs. We also analyzed the gap gene network of Clogmia albipunctata and showed that different mutual repression regulations might be important in comparison with those of Drosophila melanogaster. As our algorithm can infer GRNs efficiently and can be applied to several different network analysis, it will be a valuable approach for analyzing large-scale data."}, {"title": "Head-Direction drift in rat pups is consistent with an angular path-integration process", "url": "https://www.biorxiv.org/content/early/2017/11/01/212852", "tag": "Bioinformatics", "abstract": "The sense of direction is a vital computation, whose neural basis is considered to be carried out by head-direction cells. One way to estimate head-direction is by integrating head angular-velocity over time. However, this process results in error accumulation resembling a random walk, proportional to , which constitutes a mark for a path integration process. In the present study we analyzed previously recorded data to quantify the drift in head-direction cells of rat pups before and after eye-opening. We found that in rat pups before eye-opening the drift propagated as a random walk, while in rats after eye-opening the drift was lower. This suggests that a path-integration process underlies the estimation of head-direction, such that before eye-opening the head-direction system runs in an open-loop manner and accumulates error. After eye-opening, visual-input, such as arena shape, helps to correct errors and thus compute the sense of direction accurately."}, {"title": "Active deep learning reduces annotation burden in automatic cell segmentation", "url": "https://www.biorxiv.org/content/early/2017/11/01/211060", "tag": "Bioinformatics", "abstract": "The relationship between cellular architecture and cellular state and function is apparent, but not yet completely understood. Precise characterization of cellular state is important in many fields, from pathology to synthetic biology. High-content high-throughput microscopy is now more than ever accessible to researchers. This allows for collection of large amount of cellular images. Naturally, the analysis of this data cannot be left to manual investigation and needs to resort to the use of efficient computing algorithms for cellular detection, segmentation, and tracking. Annotation is required for building high quality algorithms. Medical professionals and researchers spend a lot of effort and time in annotating cells. This task has proved to be very repetitive and time consuming. The expert's time is valuable and should be used effectively. Our hypothesis is that active deep learning will help to share some of the burden that researchers face in their everyday work. In this paper, we focus specifically on the problem of cellular segmentation. We approach the segmentation task using a classification framework. Each pixel in the image is classified based on whether the patch around it resides on the interior, boundary or exterior of the cell. Deep convolutional neural networks (CNN) are used to perform the classification task. Active learning is the method used to reduce the annotation burden. Uncertainty sampling, a popular active learning framework is used in conjunction with CNN to segment the cells in the image. Three datasets of mammalian nuclei and cytoplasm are used for this work. We show that active deep learning significantly reduces the number of training samples required and also improves the quality of segmentation."}, {"title": "Ontology-based similarity calculations with an improved annotation model", "url": "https://www.biorxiv.org/content/early/2017/11/01/199554", "tag": "Bioinformatics", "abstract": "A typical use case of ontologies is the calculation of similarity scores between items that are annotated with classes of the ontology. For example, in differential diagnostics and disease gene prioritisation, the Human Phenotype Ontology (HPO) is often used to compare a query phenotype profile against gold-standard phenotype profiles of diseases or genes. The latter have long been constructed as flat lists of ontology classes, which, as we show in this work, can be improved by exploiting existing structure and information in annotation datasets or full text disease descriptions. We derive a study-wise annotation model of diseases and genes and show that this can improve the performance of semantic similarity measures. Inferred weights of individual annotations are one reason for this improvement, but more importantly using the study-wise structure further boosts the results of the algorithms according to precision-recall analyses. We test the study-wise annotation model for diseases annotated with classes from the HPO and for genes annotated with Gene Ontology (GO) classes. We incorporate this annotation model into similarity algorithms and show how this leads to improved performance. This work adds weight to the need for enhancing simple list-based representations of disease or gene annotations. We show how study-wise annotations can be automatically derived from full text summaries of disease descriptions and from the annotation data provided by the GO Consortium and how semantic similarity measure can utilise this extended annotation model."}, {"title": "Stratification of TAD boundaries identified in reproducible Hi-C contact matrices reveals preferential insulation of super-enhancers by strong boundaries", "url": "https://www.biorxiv.org/content/early/2017/11/01/141481", "tag": "Bioinformatics", "abstract": "The metazoan genome is compartmentalized in megabase-scale areas of highly interacting chromatin known as topologically associating domains (TADs), typically identified by computational analyses of Hi-C sequencing data. TADs are demarcated by boundaries that are largely conserved across cell types and even across species, although, increasing evidence suggests that the seemingly invariant TAD boundaries may exhibit plasticity and their insulating strength can vary. However, a genome-wide characterization of TAD boundary strength in mammals is still lacking. A systematic classification and characterization of TAD boundaries may generate new insights into their function. In this study, we first use fused two-dimensional lasso as a machine learning method to improve Hi-C contact matrix reproducibility, and, subsequently, we categorize TAD boundaries based on their insulation score. We demonstrate that higher TAD boundary insulation scores are associated with elevated CTCF levels and that they may differ across cell types. Intriguingly, we observe that super-enhancer elements are preferentially insulated by strong boundaries, i.e. boundaries of higher insulation score. Furthermore, we perform a pan-cancer analysis to demonstrate that strong TAD boundaries and super-enhancer elements are frequently co-duplicated in cancer patients. Taken together, our findings suggest that super-enhancers insulated by strong TAD boundaries may be exploited, as a functional unit, by cancer cells to promote oncogenesis."}, {"title": "Compositional Mediation Analysis for Microbiome Studies", "url": "https://www.biorxiv.org/content/early/2017/11/01/149419", "tag": "Bioinformatics", "abstract": "Motivated by recent advances in causal mediation analysis and problems in the analysis of microbiome data, we consider the setting where the effect of a treatment on an outcome is transmitted through perturbing the microbial communities or compositional mediators. Compositional and high-dimensional nature of such mediators makes the standard mediation analysis not directly applicable to our setting. We propose a sparse compositional mediation model that can be used to estimate the causal direct and indirect (or mediation) effects utilizing the algebra for compositional data in the simplex space. We also propose tests of total and component-wise mediation effects using bootstrap. We conduct extensive simulation studies to assess the performance of the proposed method and apply the method to a real metagenomic dataset to investigate the effect of fat intake on body mass index mediated through the gut microbiome composition."}, {"title": "Functional Connectomics from Data: Probabilistic Graphical Models for Neuronal Network of C. elegans", "url": "https://www.biorxiv.org/content/early/2017/11/01/212423", "tag": "Bioinformatics", "abstract": "We propose a data-driven approach to represent neuronal network dynamics as a Probabilistic Graphical Model (PGM). Our approach learns the PGM structure by employing dimension reduction to network response dynamics evoked by stimuli applied to each neuron separately. The outcome model captures how stimuli propagate through the network and thus represents functional dependencies between neurons, i.e., functional connectome. The benefit of using a PGM as the functional connectome is that posterior inference can be done efficiently and circumvent the complexities in direct inference of response pathways in dynamic neuronal networks. In particular, posterior inference reveals the relations between known stimuli and downstream neurons or allows to query which stimuli are associated with downstream neurons. For validation and as an example for our approach we apply our methodology to a model of Caenorhabiditis elegans nervous system which structure and dynamics are well-studied. From its dynamical model we collect time series of the network response and use singular value decomposition to obtain a low-dimensional projection of the time series data. We then extract dominant patterns in each data matrix to get pairwise dependency information and create a graphical model for the full somatic nervous system. The PGM enables us to obtain and verify underlying neuronal pathways dominant for known behavioral scenarios and to detect possible pathways for novel scenarios."}, {"title": "Evaluation of protein-ligand docking methods on peptide-ligand complexes for docking small ligands to peptides", "url": "https://www.biorxiv.org/content/early/2017/11/01/212514", "tag": "Bioinformatics", "abstract": "In the past, many benchmarking studies have been performed on protein-protein and protein-ligand docking however there is no study on peptide-ligand docking. In this study, we evaluated the performance of seven widely used docking methods (AutoDock, AutoDock Vina, DOCK 6, PLANTS, rDock, GEMDOCK and GOLD) on a dataset of 57 peptide-ligand complexes. Though these methods have been developed for docking ligands to proteins but we evaluate their ability to dock ligands to peptides. First, we compared TOP docking pose of these methods with original complex and achieved average RMSD from 4.74\u00c5 for AutoDock to 12.63\u00c5 for GEMDOCK. Next we evaluated BEST docking pose of these methods and achieved average RMSD from 3.82\u00c5 for AutoDock to 10.83\u00c5 for rDock. It has been observed that ranking of docking poses by these methods is not suitable for peptide-ligand docking as performance of their TOP pose is much inferior to their BEST pose. AutoDock clearly shows better performance compared to the other six docking methods based on their TOP docking poses. On the other hand, difference in performance of different docking methods (AutoDock, AutoDock Vina, PLANTS and DOCK 6) was marginal when evaluation was based on their BEST docking pose. Similar trend has been observed when performance is measured in terms of success rate at different cut-off values. In order to facilitate scientific community a web server PLDbench has been developed (http://webs.iiitd.edu.in/raghava/pldbench/)."}, {"title": "MetaCompass: Reference-guided Assembly of Metagenomes", "url": "https://www.biorxiv.org/content/early/2017/11/01/212506", "tag": "Bioinformatics", "abstract": "Metagenomic studies have primarily relied on de novo approaches for reconstructing genes and genomes from microbial mixtures. While database driven approaches have been employed in certain analyses, they have not been used in the assembly of metagenomes. Here we describe the first effective approach for reference-guided metagenomic assembly of low-abundance bacterial genomes that can complement and improve upon de novo metagenomic assembly methods. When combined with de novo assembly approaches, we show that MetaCompass can generate more complete assemblies than can be obtained by de novo assembly alone, and improve on assemblies from the Human Microbiome Project (over 2,000 samples)."}, {"title": "Clingen Cancer Somatic Working Group: standardizing and democratizing access to cancer molecular diagnostic data to drive translational research", "url": "https://www.biorxiv.org/content/early/2017/11/01/212225", "tag": "Bioinformatics", "abstract": "A growing number of academic and community clinics are conducting genomic testing to inform treatment decisions for cancer patients. In the last 3-5 years, there has been a rapid increase in clinical use of next generation sequencing (NGS) based cancer molecular diagnostic (MolDx) testing. The increasing availability and decreasing cost of tumor genomic profiling means that physicians can now make treatment decisions armed with patient-specific genetic information. Accumulating research in the cancer biology field indicates that there is significant potential to improve cancer patient outcomes by effectively leveraging this rich source of genomic data in treatment planning. To achieve truly personalized medicine in oncology, it is critical to catalog cancer sequence variants from MolDx testing for their clinical relevance along with treatment information and patient outcomes, and to do so in a way that supports large-scale data aggregation and new hypothesis generation. One critical challenge to encoding variant data is adopting a standard of annotation of those variants that are clinically actionable. Through the NIH-funded Clinical Genome Resource (ClinGen), in collaboration with NLM ClinVar database and >50 academic and industry based cancer research organizations, we developed the Minimal Variant Level Data (MVLD) framework to standardize reporting and interpretation of drug associated alterations. We are currently involved in collaborative efforts to align the MVLD framework with parallel, complementary sequence variants interpretation clinical guidelines from the Association of Molecular Pathologists (AMP) for clinical labs. In order to truly democratize access to MolDx data for care and research needs, these standards must be harmonized to support sharing of clinical cancer variants. Here we describe the processes and methods developed within the ClinGen Somatic WG in collaboration with over 60 cancer care and research organizations as well as CLIA-certified, CAP-accredited clinical testing labs to develop standards for cancer variant interpretation and sharing."}, {"title": "Experimenting with reproducibility in bioinformatics", "url": "https://www.biorxiv.org/content/early/2017/10/31/143503", "tag": "Bioinformatics", "abstract": "Reproducibility has been shown to be limited in many scientific fields. This question is a fundamental tenet of the scien-tific activity, but the related issues of reusability of scientific data are poorly documented. Here, we present a case study of our attempt to reproduce a promising bioinformatics method [1] and illustrate the challenges to use a published method for which code and data were available. First, we tried to re-run the analysis with the code and data provided by the au-thors. Second, we reimplemented the method in Python to avoid dependency on a MATLAB licence and ease the execu-tion of the code on HPCC (High-Performance Computing Cluster). Third, we assessed reusability of our reimplementation and the quality of our documentation. Then, we experimented with our own software and tested how easy it would be to start from our implementation to reproduce the results, hence attempting to estimate the robustness of the reproducibility. Finally, in a second part, we propose solutions from this case study and other observations to improve reproducibility and research efficiency at the individual and collective level."}, {"title": "Genetic Locus Modulating IOP", "url": "https://www.biorxiv.org/content/early/2017/10/31/204073", "tag": "Bioinformatics", "abstract": "Ipso facto"}, {"title": "ITHANET: Information and database community portal for haemoglobinopathies", "url": "https://www.biorxiv.org/content/early/2017/10/31/209361", "tag": "Bioinformatics", "abstract": "Haemoglobinopathies are the commonest monogenic diseases, with millions of carriers and patients worldwide. Online resources for haemoglobinopathies are largely divided into specialised sites catering for patients, researchers and clinicians separately. However, the severity, ubiquity and surprising genetic complexity of the haemoglobinopathies call for an integrated website to serve as a free and comprehensive repository and tool for patients, scientists and health professionals alike. This paper presents the ITHANET community portal, an expanding resource for clinicians and researchers dealing with haemoglobinopathies. It integrates information on news, events, publications, clinical trials and haemoglobinopathy-related organisations and experts and, most importantly, databases of variations, epidemiology and diagnostic and clinical data. Specifically, ITHANET provides annotation for 2690 haemoglobinopathy-related variations, epidemiological data for more than 180 countries and information for more than 600 HPLC diagnostic reports. The ITHANET portal accepts and incorporates contributions to its content by local experts from any country in the world and is freely available for the public at http://www.ithanet.eu."}, {"title": "accuMUlate: A mutation caller designed for mutation accumulation experiments", "url": "https://www.biorxiv.org/content/early/2017/10/31/182956", "tag": "Bioinformatics", "abstract": "Motivation: Mutation accumulation (MA) is the most widely used method for directly studying the effects of mutation. Modern sequencing technologies have led to an increased interest in MA experiments. By sequencing whole genomes from MA lines, researchers can directly study the rate and molecular spectra of spontaneous mutations and use these results to understand how mu- tation contributes to biological processes. At present there is no software designed specifically for identifying mutations from MA lines. Studies that combine MA with whole genome sequencing use custom bioinformatic pipelines that implement heuristic rules to identify putative mutations. Results: Here we describe accuMUlate, a program that is designed to detect mutations from MA experiments. accuMUlate implements a probabilistic model that reflects the design of a typical MA experiments while being flexible enough to accommodate properties unique to any particular experiment. For each putative mutation identified from this model accuMUlate calculates a set of summary statistics that can be used to filter sites that may be false positives. A companion tool, denominate, can be used to apply filtering rules based on these statistics to simulated mutations and thus identify the number of callable sites per sample. Availability: Source code and releases available from https://github.com/dwinter/accuMUlate."}, {"title": "WorMachine: Machine Learning-Based Phenotypic Analysis Tool for Worms", "url": "https://www.biorxiv.org/content/early/2017/10/31/211359", "tag": "Bioinformatics", "abstract": "While Caenorhabditis elegans nematodes are powerful model organisms, quantification of visible phenotypes is still often labor-intensive, biased, and error-prone. We developed 'WorMachine', a three-step MATLAB-based image analysis software that allows automated identification of C. elegans worms, extraction of morphological features, and quantification of fluorescent signals. The program offers machine learning techniques which should aid in studying a large variety of research questions. We demonstrate the power of WorMachine using five separate assays: scoring binary and continuous sexual phenotypes, quantifying the effects of different RNAi treatments, and measuring intercellular protein aggregation. Thus, WorMachine is a 'quick and easy', high-throughput, automated, and unbiased analysis tool for measuring phenotypes."}, {"title": "GPseudoRank: MCMC for sampling from posterior distributions of pseudo-orderings using Gaussian processes", "url": "https://www.biorxiv.org/content/early/2017/10/31/211417", "tag": "Bioinformatics", "abstract": "A number of previous approaches to pseudotime estimation have provided point estimates of the ordering of cells for scRNA-seq data, while more recently, Gaussian process latent variable models and MCMC methods have been applied to understand the uncertainty associated with the pseudotemporal ordering. We present a new type of Gaussian process latent variable model for pseudotemporal ordering, which samples a distribution on the probability space of the orderings, that is on the group of permutations, rather than on the hugely high-dimensional vector space of possible pseudotimes, as done by previous models. We determine the best proposal distribution for our Metropolis-Hastings sampler for different types of data in an extensive simulation study, and show on a microarray data set that it is both able to capture complicated posterior distributions with modes close to pseudotime estimates found by state-of-the-art methods for point estimation of pseudotime orderings, and identify a global maximum of the distribution close to the true order. Finally, in an application to scRNA-seq data we demonstrate the particular potential of our method to identify phases of lower and higher pseudotime uncertainty during a biological process."}, {"title": "Probabilistic Count Matrix Factorization for Single Cell Expression Data Analysis", "url": "https://www.biorxiv.org/content/early/2017/10/31/211938", "tag": "Bioinformatics", "abstract": "The development of high throughput single-cell technologies now allows the investigation of the genome-wide diversity of transcription. This diversity has shown two faces: the expression dynamics (gene to gene variability) can be quantified more accurately, thanks to the measurement of lowly-expressed genes. Second, the cell-to-cell variability is high, with a low proportion of cells expressing the same gene at the same time/level. Those emerging patterns appear to be very challenging from the statistical point of view, especially to represent and to provide a summarized view of single-cell expression data. PCA is one of the most powerful framework to provide a suitable representation of high dimensional datasets, by searching for new axis catching the most variability in the data. Unfortunately, classical PCA is based on Euclidean distances and projections that work poorly in presence of over-dispersed counts that show zero-inflation. We propose a probabilistic Count Matrix Factorization (pCMF) approach for single-cell expression data analysis, that relies on a sparse Gamma-Poisson factor model. This hierarchical model is inferred using a variational EM algorithm. We show how this probabilistic framework induces a geometry that is suitable for single-cell data, and produces a compression of the data that is very powerful for clustering purposes. Our method is competed to other standard representation methods like t-SNE, and we illustrate its performance for the representation of single-cell data. We especially focus on a publicly available data set, being single-cell expression profile of neural stem cells."}, {"title": "Protective role of the vulture facial and gut microbiomes aid adaptation to scavenging", "url": "https://www.biorxiv.org/content/early/2017/10/30/211490", "tag": "Bioinformatics", "abstract": "Background: Vultures have adapted the remarkable ability to feed on carcasses that may contain microorganisms that would be pathogenic to most other animals. The holobiont concept suggests that the genetic basis of such adaptation may not only lie within their genomes, but additionally in their associated microbes. To explore this, we generated shotgun DNA sequencing datasets of the facial and gut microbiomes from the black and turkey vultures. We characterized i) the functional potential and taxonomic diversity of their microbiomes, ii) the potential pathogenic challenges they face, and iii) elements in the microbiome that could play a protective role to the vulture's face and gut. Results: We found elements involved in diseases, such as periodontitis and pneumonia (more abundant in the face), and gas gangrene and food poisoning (more abundant in the gut). Interestingly, we found taxa and functions with potential for playing health beneficial roles, such as antilisterial bacteria in the gut, and genes for the production of antiparasites and antiinsectisides in the face. Based on the identified phages, we suggest that phages aid in the control, and possibly elimination as in phage therapy, of microbes reported as pathogenic to a variety of species. Interestingly, we also identified Adineta vaga in the gut, an invertebrate that feeds on dead bacteria and protozoans, suggesting a defensive predatory mechanism. Finally, we suggest a colonization resistance role though biofilm formation played by Fusobacteria and Clostridia in the gut. Conclusions: Our results highlight the importance of complementing genomic analyses with metagenomics in order to obtain a clearer understanding of the host-microbial alliance and show the importance of microbiome-mediated health protection for adaptation to extreme diets, such as scavenging."}, {"title": "Extracting active modules from multilayer PPI network: a continuous optimization approach", "url": "https://www.biorxiv.org/content/early/2017/10/30/211433", "tag": "Bioinformatics", "abstract": "Active modules identification has received much attention due to its ability to reveal regulatory and signaling mechanisms of a given cellular response. Most existing algorithms identify active modules by extracting connected nodes with high activity scores from a graph. These algorithms do not consider other topological properties such as community structure, which may correspond to functional units. In this paper, we propose an active module identification algorithm based on a novel objective function, which considers both and network topology and nodes activity. This objective is formulated as a constrained quadratic programming problem, which is convex and can be solved by iterative methods. Furthermore, the framework is extended to the multilayer dynamic PPI networks. Empirical results on the single layer and multilayer PPI networks show the effectiveness of proposed algorithms. Availability: The package and code for reproducing all results and figures are available at https://github.com/fairmiracle/ModuleExtraction."}, {"title": "INFERNO - INFERring the molecular mechanisms of NOncoding genetic variants", "url": "https://www.biorxiv.org/content/early/2017/10/30/211599", "tag": "Bioinformatics", "abstract": "The majority of variants identified by genome-wide association studies (GWAS) reside in the noncoding genome, where they affect regulatory elements including transcriptional enhancers. We propose INFERNO (INFERring the molecular mechanisms of NOncoding genetic variants), a novel method which integrates hundreds of diverse functional genomics data sources with GWAS summary statistics to identify putatively causal noncoding variants underlying association signals. INFERNO comprehensively infers the relevant tissue contexts, target genes, and downstream biological processes affected by causal variants. We apply INFERNO to schizophrenia GWAS data, recapitulating known schizophrenia-associated genes including CACNA1C and discovering novel signals related to transmembrane cellular processes."}, {"title": "HiGlass: Web-based Visual Exploration and Analysis of Genome Interaction Maps", "url": "https://www.biorxiv.org/content/early/2017/10/30/121889", "tag": "Bioinformatics", "abstract": "In the face of new multidimensional genomic data, visual tools are essential for exploring data, discovering patterns, and rationalizing and disseminating results. Within the field of genomics, this pursuit is often confounded by not only the size of the data but the variety of scales at which features are visible. Genome-wide interaction maps produced by Chromosome Conformation Capture techniques such as Hi-C are particularly challenging as patterns emerge at a range of scales from hundreds of kilobases to hundreds of megabases. Since automatic detection and classification of these features gives ambiguous results, visual exploration and comparison is essential for analysis of Hi-C data and its comparison with other genomic tracks, requiring navigation and comparison across multiple loci, multiple resolutions and multiple samples. Here we present HiGlass, an open source visualization tool built on web technologies that provides a rich interface for rapid, multiplex, and multiscale navigation of 2D genomic maps alongside 1D genomic tracks, allowing users to combine various data types, synchronize multiple visualization modalities, and effortlessly share fully customizable views with others. We demonstrate its utility in exploring different experimental conditions, comparing the results of analyses, and creating interactive snapshots to share with collaborators and the broader public. HiGlass is accessible online at http://higlass.io and is also available as a containerized application that can be run on any platform."}, {"title": "Linked-read analysis identifies mutations in single-cell DNA sequencing data", "url": "https://www.biorxiv.org/content/early/2017/10/30/211169", "tag": "Bioinformatics", "abstract": "Whole-genome sequencing of DNA from single cells has the potential to reshape our understanding of the mutational heterogeneity in normal and disease tissues. A major difficulty, however, is distinguishing artifactual mutations that arise from DNA isolation and amplification from true mutations. Here, we describe linked-read analysis (LiRA), a method that utilizes phasing of somatic single nucleotide variants with nearby germline variants to identify true mutations, thereby allowing accurate estimation of somatic mutation rates at the single-cell level."}, {"title": "Identification and prioritisation of causal variants in human genetic disorders from exome or whole genome sequencing data", "url": "https://www.biorxiv.org/content/early/2017/10/29/209882", "tag": "Bioinformatics", "abstract": "With genome sequencing entering the clinics as a diagnostic tool to study genetic disorders, there is an increasing need for bioinformatics solutions that enable precise causal variant identification in a timely manner. Background: Workflows for the identification of candidate disease-causing variants perform usually the following tasks: i) identification of variants; ii) filtering of variants to remove polymorphisms and technical artifacts; and iii) prioritization of the remaining variants to provide a small set of candidates for further analysis. Methods: Here, we present a pipeline designed to identify variants and prioritize the variants and genes from trio sequencing or pedigree-based sequencing data into different tiers. Results: We show how this pipeline was applied in a study of patients with neurodevelopmental disorders of unknown cause, where it helped to identify the causal variants in more than 35% of the cases. Conclusions: Classification and prioritization of variants into different tiers helps to select a small set of variants for downstream analysis."}, {"title": "ACEseq - allele specific copy number estimation from whole genome sequencing", "url": "https://www.biorxiv.org/content/early/2017/10/29/210807", "tag": "Bioinformatics", "abstract": "ACEseq is a computational tool for allele-specific copy number estimation in tumor genomes based on whole genome sequencing. In contrast to other tools it features GC-bias correction, unique replication timing-bias correction and integration of structural variant (SV) breakpoints for improved genome segmentation. ACEseq clearly outperforms widely used state-of-the art methods, provides a fully automated estimation of tumor cell content and ploidy, and additionally computes homologous recombination deficiency scores."}, {"title": "Contextual Regression: An Accurate and Conveniently Interpretable Nonlinear Model for Mining Discovery from Scientific Data", "url": "https://www.biorxiv.org/content/early/2017/10/29/210997", "tag": "Bioinformatics", "abstract": "Machine learning algorithms such as linear regression, SVM and neural network have played an increasingly important role in the process of scientific discovery. However, none of them is both interpretable and accurate on nonlinear datasets. Here we present contextual regression, a method that joins these two desirable properties together using a hybrid architecture of neural network embedding and dot product layer. We demonstrate its high prediction accuracy and sensitivity through the task of predictive feature selection on a simulated dataset and the application of predicting open chromatin sites in the human genome. On the simulated data, our method achieved high fidelity recovery of feature contributions under random noise levels up to 200%. On the open chromatin dataset, the application of our method not only outperformed the state of the art method in terms of accuracy, but also unveiled two previously unfound open chromatin related histone marks. Our method fills in the gap of accurate and interpretable nonlinear modeling in scientific data mining tasks."}, {"title": "A Doubly Stochastic Change Point Detection Algorithm for Noisy Biological Signals", "url": "https://www.biorxiv.org/content/early/2017/10/29/106088", "tag": "Bioinformatics", "abstract": "Experimentally and clinically collected time series data are often contaminated with significant confounding noise, creating short, noisy time series. This noise, due to natural variability and measurement error, poses a challenge to conventional change point detection methods. We propose a novel and robust statistical method for change point detection for noisy biological time sequences. Our method is a significant improvement over traditional change point detection methods, which only examine a potential anomaly at a single time point. In contrast, our method considers all suspected anomaly points and considers the joint probability distribution of the number of change points and the elapsed time between two consecutive anomalies. We validate our method with three simulated time series, a widely accepted benchmark data set, two geological time series, a data set of ECG recordings, and a physiological data set of heart rate variability measurements of fetal sheep model of human labour, comparing it to three existing methods. Our method demonstrates significantly improved performance over the existing pointwise detection methods."}, {"title": "runibic: a Bioconductor package for parallel row-based biclustering of gene expression data", "url": "https://www.biorxiv.org/content/early/2017/10/28/210682", "tag": "Bioinformatics", "abstract": "Biclustering (called also co-clustering) is an unsupervised technique of simultaneous analysis of rows and columns of input matrix. From the first application to gene expression data, multiple algorithms have been proposed. Only a handful of them were able to provide accurate results and were fast enough to be suitable for large-scale genomic datasets. In this paper we introduce a Bioconductor package with parallel version of UniBic biclustering algorithm: one of the most accurate biclustering methods that have been developed so far. For the convenience of usage, we have wrapped the algorithm in an R package called runibic. The package includes: (1) a couple of times faster parallel version of the original sequential algorithm, (2) much more efficient memory management, (3) modularity which allows to build new methods on top of the provided one, (4) integration with the modern Bioconductor packages such as SummarizedExperiment, ExpressionSet and biclust. The package is implemented in R (3.4) and will be available in the new release of Bioconductor (3.6). Currently it could be downloaded from the following URL: http://github.com/athril/runibic/ ."}, {"title": "Effects of Spatial Heterogeneity on Transmission Potential in Vectorial-Contact Networks: A Comparison of Three Aedes aegypti Control Strategies", "url": "https://www.biorxiv.org/content/early/2017/10/28/210450", "tag": "Bioinformatics", "abstract": "Dengue, chikungunya and zika are all transmitted by the Aedes aegypti mosquito. Despite the strong influence of host spatial distribution and movement patterns on the ability of mosquito vectors to transmit pathogens, there is little understanding how these complex interactions modify the spread of disease in spatially heterogeneous populations. In light of present fears of a worldwide zika epidemic, and failures to eradicate dengue and chikungunya; there is a pressing need to get a better picture of how high-resolution details such as human movement in a small landscape, modify the patterns of transmission of these diseases and how different mosquito-control interventions could be affected by these movements. In this work we use a computational agent-based model (ABM) to simulate mosquito-human interactions in two different levels of spatial heterogeneity, with human movement, and in the presence of three mosquito-control interventions (spatial spraying, the release of Wolbachia-infected mosquitoes and release of insects with dominant lethal gene). To analyse the results from each of these experiments we examined mosquito population dynamics and host to host contact networks that emerged from the distribution of consecutive bites across humans. We then compared results across experiments to understand the differential effectiveness of different interventions in both the presence and absence of spatial heterogeneities, and analysed network measures of epidemiological relevance (degree probability distributions, mean path length, network density and small-worldness). From our experiments we conclude that spatial heterogeneity greatly influences how a pathogen may spread in a host population when mediated by a mosquito vector, and that these important heterogeneities also strongly affect effectiveness of interventions. Finally, we demonstrate that these host to host vectorial-contact networks can provide operationally important information to inform selection of optimal vector-control strategies."}, {"title": "Deep-RBPPred: Predicting RNA binding proteins in the proteome scale based on deep learning", "url": "https://www.biorxiv.org/content/early/2017/10/27/210153", "tag": "Bioinformatics", "abstract": "RNA binding protein (RBP) plays an important role in cell processes. Identifying RBPs by computation and experiment are both essential. Recently, RBPPred is proposed in our group to predict RBP with a high performance. However, RBPPred is too slow for that it will generate PSSM matrix as its feature. Herein, we develop a deep learning model called Deep-RBPPred. The model has three advantages comparing to previous models. 1. Deep-RBPPred only needs few physicochemical properties. 2. Deep-RBPPred runs much faster. 3. Deep-RBPPred has a good generalization ability. In the meantime, the performance is still as good as the stats-of-the-art method. In the testing in A. thaliana, S. cerevisiae and H. sapiens proteomics, MCC (AUC) are 0.6077 (0.9421), 0.573 (0.9034) and 0.8141(0.9515) respectively when the score cutoff is set to 0.5. In the verifying in Gerstberger-1538, the SN of our model is 90.38%. The running times are 9s, 7s, 8s and 10s, respectively, for H.sapiens, A.thaliana, S.cerevisiae and Gerstberger-1538 when it is tested in GPU. Deep-RBPPred forecasts 94.65% of 299 new RBP and about 8% higher sensitivity than RBPPred. We also apply deep-RBPPred in 19 eukaryotes proteomics and 11 bacteria proteomics downloaded from Uniprot. The result shows that rate of RBPs in eukaryotes proteome are much higher than bacteria proteome. Testing in 6 proteomics shows the many RBPs may be still undiscovered so far."}, {"title": "Computational Tools for the Identification and Interpretation of Sequence Motifs in Immunopeptidomes", "url": "https://www.biorxiv.org/content/early/2017/10/27/210336", "tag": "Bioinformatics", "abstract": "Recent advances in proteomics and mass-spectrometry have widely expanded the detectable peptide repertoire presented by major histocompatibility complex (MHC) molecules on the cell surface, collectively known as the immunopeptidome. Finely characterizing the immunopeptidome brings about important basic insights into the mechanisms of antigen presentation, but can also reveal promising targets for vaccine development and cancer immunotherapy. In this report, we describe a number of practical and efficient approaches to analyze immunopeptidomics data, discussing the identification of meaningful sequence motifs in various scenarios and considering current limitations. We address the issue of filtering false hits and contaminants, and the problem of motif deconvolution in cell lines expressing multiple MHC alleles, both for the MHC class I and class II systems. Finally, we demonstrate how machine learning can be readily employed by non-expert users to generate accurate prediction models directly from mass-spectrometry eluted ligand data sets."}, {"title": "Knowledge of the Neighborhood of the Reactive Site up to Three Atoms Can Predict Biochemistry and Protein Sequences", "url": "https://www.biorxiv.org/content/early/2017/10/27/210039", "tag": "Bioinformatics", "abstract": "Thousands of biochemical reactions with characterized biochemical activities are still orphan. Novel reactions predicted by pathway generation tools also lack associated protein sequences and genes. Mapping orphan and novel reactions back to the known biochemistry and proposing genes for their catalytic functions is a daunting problem. We propose a new method, BridgIT, to identify candidate genes and protein sequences for orphan and novel enzymatic reactions. BridgIT introduces, for the first time, the information of the enzyme binding pocket into reaction similarity comparisons. It ascertains the similarity of two reactions by comparing the reactive sites of their substrates and their surrounding structures, along with the structures of the generated products. BridgIT compares orphan and novel reactions to enzymatic reactions with known protein sequences, and then, it proposes protein sequences and genes of the most similar non-orphan reactions as candidates for catalyzing the novel or orphan reactions. We performed BridgIT analysis of orphan reactions from KEGG 2011 (Kyoto Encyclopedia of Genes and Genomes, published in 2011) that became non-orphan in KEGG 2016, and BridgIT correctly predicted enzymes with identical third- and fourth-level EC numbers for 91% and 56% of these reactions, respectively. BridgIT results revealed that it is sufficient to know information about six atoms together with their connecting bonds around the reactive sites of the substrates to match a protein sequence to the catalytic activity of enzymatic reactions with maximal accuracy. Moreover, the same information about only three atoms around the reactive site allowed us to correctly match 87% of the analyzed enzymatic reactions. Finally, we used BridgIT to provide candidate protein sequences for 137,000 novel enzymatic reactions from the recently introduced ATLAS of Biochemistry. A web-tool of BridgIT can be consulted at http://lcsb-databases.epfl.ch/BridgIT/."}, {"title": "Genotype fingerprints enable fast and private comparison of genetic testing results for research and direct-to-consumer applications", "url": "https://www.biorxiv.org/content/early/2017/10/27/208025", "tag": "Bioinformatics", "abstract": "As genetic testing expands out of the research laboratory into medical practice as well as the direct-to-consumer market, the efficiency with which the resulting genotype data can be compared between individuals is of increasing importance. We present a method for summarizing personal genotypes, yielding 'genotype fingerprints' that can be derived from any single nucleotide polymorphism (SNP)-based assay and readily compared to estimate relatedness. The resulting fingerprints remain comparable as chip designs evolve to higher marker densities. We demonstrate that they support applications including distinguishing genotypes of closely related individuals by relationship type, distinguishing closely related individuals from individuals from the same background population, identification of individuals in known background populations, and de novo identification of subpopulations within a large cohort in a high-throughput manner. An important feature of genotype fingerprints is that, while fingerprints do not preserve anonymity, they summarize individual marker data in a way that prevents phenotype prediction. Genotype fingerprints are therefore well-suited to public sharing for ancestry determination purposes, without revealing personal health risk status."}, {"title": "DiscoSnp++: de novo detection of small variants from raw unassembled read set(s)", "url": "https://www.biorxiv.org/content/early/2017/10/27/209965", "tag": "Bioinformatics", "abstract": "Motivation: Next Generation Sequencing (NGS) data provide an unprecedented access to life mechanisms. In particular, these data enable to detect polymorphisms such as SNPs and indels. As these polymorphisms represent a fundamental source of information in agronomy, environment or medicine, their detection in NGS data is now a routine task. The main methods for their prediction usually need a reference genome. However, non-model organisms and highly divergent genomes such as in cancer studies are extensively investigated. Results: We propose DiscoSnp++, in which we revisit the DiscoSnp algorithm. DiscoSnp++ is designed for detecting and ranking all kinds of SNPs and small indels from raw read set(s). It outputs files in fasta and VCF formats. In particular, predicted variants can be automatically localized afterward on a reference genome if available. Its usage is extremely simple and its low resource requirements make it usable on common desktop computers. Results show that DiscoSnp++ performs better than state-of-the-art methods in terms of computational resources and in terms of results quality. An important novelty is the de novo detection of indels, for which we obtained 99% precision when calling indels on simulated human datasets and 90% recall on high confident indels from the Platinum dataset. License: GNU Affero general public license. Availability: https://github.com/GATB/DiscoSnp ."}, {"title": "Bioconda: A sustainable and comprehensive software distribution for the life sciences", "url": "https://www.biorxiv.org/content/early/2017/10/27/207092", "tag": "Bioinformatics", "abstract": "We present Bioconda (https://bioconda.github.io), a distribution of bioinformatics software for the lightweight, multi-platform and language-agnostic package manager Conda. Currently, Bioconda offers a collection of over 3000 software packages, which is continuously maintained, updated, and extended by a growing global community of more than 200 contributors. Bioconda improves analysis reproducibility by allowing users to define isolated environments with defined software versions, all of which are easily installed and managed without administrative privileges."}, {"title": "Assessment of batch-correction methods for scRNA-seq data with a new test metric", "url": "https://www.biorxiv.org/content/early/2017/10/27/200345", "tag": "Bioinformatics", "abstract": "Single-cell transcriptomics is a versatile tool for exploring heterogeneous cell populations. As with all genomics experiments, batch effects can hamper data integration and interpretation. The success of batch effect correction is often evaluated by visual inspection of dimension reduced representations such as principal component analysis. This is inherently imprecise due to the high number of genes and non-normal distribution of gene expression. Here, we present a k-nearest neighbour batch effect test (kBET, https://github.com/theislab/kBET) to quantitatively measure batch effects. kBET is easier to interpret, more sensitive and more robust than visual evaluation and other measures of batch effects. We use kBET to assess commonly used batch regression and normalisation approaches, and quantify the extent to which they remove batch effects while preserving biological variability. Our results illustrate that batch correction based on log-transformation or scran pooling followed by ComBat reduced the batch effect while preserving structure across data sets. Finally we show that kBET can pinpoint successful data integration methods across multiple data sets, in this case from different publications all charting mouse embryonic development. This has important implications for future data integration efforts, which will be central to projects such as the Human Cell Atlas where data for the same tissue may be generated in multiple locations around the world."}, {"title": "Prediction and interpretation of deleterious coding variants in terms of protein structural stability", "url": "https://www.biorxiv.org/content/early/2017/10/27/210120", "tag": "Bioinformatics", "abstract": "The classification of human genetic variants into deleterious and neutral is a challenging issue, whose complexity is rooted in the large variety of biophysical mechanisms that can be responsible for disease conditions. For non-synonymous mutations in structured proteins, one of these is the protein stability change, which can lead to functionality loss. We developed a stability-driven knowledge-based classifier that uses protein structure, artificial neural networks and solvent accessibility-dependent combinations of statistical potentials to predict whether destabilizing or stabilizing mutations are disease-causing. Our predictor yields a balanced accuracy of 71% in cross validation. As expected, it has a very high positive predictive value of 89%: it predicts with high accuracy the subset of mutations that are deleterious because of stability issues, but is by construction unable of classifying variants that are deleterious for other reasons. Its combination with an evolutionary-based predictor increases the balanced accuracy up to 75%, and allowed predicting more than 1/4 of the deleterious variants with 95% positive predictive value. Our method, called SNPMuSiC, can be used with both experimental and structural models and compares favorably with other prediction tools on several independent test sets. It constitutes a step towards interpreting variant effects at the molecular scale."}, {"title": "miCloud: a plug and play, on-premises bioinformatics cloud, providing seamless integration with Illumina genome sequencers", "url": "https://www.biorxiv.org/content/early/2017/10/27/209734", "tag": "Bioinformatics", "abstract": "Benchtop genome sequencers such as the Illumina MiSeq or MiniSeq are revolutionizing genomics research for smaller, independent laboratories, by enabling access to low-cost Next Generation Sequencing (NGS) technology in-house. However, post-sequencing bioinformatics data analysis still presents a significant bottleneck. We developed miCloud, a bioinformatics platform for NGS data analysis, as a solution to fill the gap between the low-cost, widely available computational resources and lack of user-friendly bioinformatics software. The miCloud is highly modular and is based on Docker virtual machine containers for its components. There are three pipelines ready to execute with the miCloud upon installation, two for single and paired-end ChIP-Seq data, in addition to one more for paired-end RNA-Seq data. Additionally, we have integrated the Visual Omics Explorer (VOE) with the miCloud, to provide users with access to rich, interactive visualizations and publication-ready graphics from the pipeline outputs. Laboratories lacking NGS data analysis expertise can easily deploy miCloud in order to process data generated from in-house sequencing instruments, on a computer with 6-10 GigaBytes (GB) of memory and 1 TeraByte (TB) or less of storage."}, {"title": "A novel feature selection for RNA-seq analysis", "url": "https://www.biorxiv.org/content/early/2017/10/27/209841", "tag": "Bioinformatics", "abstract": "RNA-seq data are challenging existing omics data analytics for its volume and complexity. Although quite a few computational models were proposed from different standing points to conduct differential expression (D.E.) analysis, almost all these methods do not provide a rigorous feature selection for high-dimensional RNA-seq count data. Instead, most or even all genes are invited into differential calls no matter they have real contributions to data variations or not. Thus, it would inevitably affect the robustness of D.E. analysis and lead to the increase of false positive ratios. In this study, we presented a novel feature selection method: nonnegative singular value approximation (NSVA) to enhance RNA-seq differential expression analysis by taking advantage of RNA-seq count data's non-negativity. As a variance-based feature selection method, it selects genes according to its contribution to the first singular value direction of input data in a data-driven approach. It demonstrates robustness to depth bias and gene length bias in feature selection in comparison with its five peer methods. Combining with state-of-the-art RNA-seq differential expression analysis, it contributes to enhancing differential expression analysis by lowering false discovery rates caused by the biases. Furthermore, we demonstrated the effectiveness of the proposed feature selection by proposing a data-driven differential expression analysis: NSVA-seq, besides conducting network marker discovery."}, {"title": "Synthesizing Signaling Pathways from Temporal Phosphoproteomic Data", "url": "https://www.biorxiv.org/content/early/2017/10/26/209676", "tag": "Bioinformatics", "abstract": "Advances in proteomics reveal that pathway databases fail to capture the majority of cellular signaling activity. Our mass spectrometry study of the dynamic epidermal growth factor (EGF) response demonstrates that over 89% of significantly (de)phosphorylated proteins are excluded from individual EGF signaling maps, and 63% are absent from all annotated pathways. We present a computational method, the Temporal Pathway Synthesizer (TPS), to discover missing pathway elements by modeling temporal phosphoproteomic data. TPS uses constraint solving to exhaustively explore all possible structures for a signaling pathway, eliminating structures that are inconsistent with protein-protein interactions or the observed phosphorylation event timing. Applied to our EGF response data, TPS connects 83% of the responding proteins to receptors and signaling proteins in EGF pathway maps. Inhibiting predicted active kinases supports the TPS pathway model. The TPS algorithm is broadly applicable and also recovers an accurate model of the yeast osmotic stress response."}, {"title": "FAST-SG: An alignment-free algorithm for hybrid assembly", "url": "https://www.biorxiv.org/content/early/2017/10/26/209122", "tag": "Bioinformatics", "abstract": "Long read sequencing technologies are the ultimate solution for genome repeats, allowing near reference level reconstructions of large genomes. However, long read de novo assembly pipelines are computationally intense and require a considerable amount of coverage, thereby hindering their broad application to the assembly of large genomes. Alternatively, hybrid assembly methods which combine short and long read sequencing technologies can reduce the time and cost required to produce de novo assemblies of large genomes. In this paper, we propose a new method, called FAST-SG, which uses a new ultra-fast alignment- free algorithm specifically designed for constructing a scaffolding graph using light-weight data structures. FAST-SG can construct the graph from either short or long reads. This allows the reuse of efficient algorithms designed for short read data and permits the definition of novel modular hybrid assembly pipelines. Using comprehensive standard datasets and benchmarks, we show how FAST-SG outperforms the state-of-the-art short read aligners when building the scaffolding graph, and can be used to extract linking information from either raw or error-corrected long reads. We also show how a hybrid assembly approach using FAST-SG with shallow long read coverage (5X) and moderate computational resources can produce long-range and accurate reconstructions of the genomes of Arabidopsis thaliana (Ler-0) and human (NA12878)."}, {"title": "MutationalPatterns: comprehensive genome-wide analysis of mutational processes", "url": "https://www.biorxiv.org/content/early/2017/10/26/071761", "tag": "Bioinformatics", "abstract": "Base substitution catalogs represent historical records of mutational processes that have been active in a system. Such processes can be distinguished by typical characteristics, like mutation type, sequence context, transcriptional and replicative strand bias, and distribution throughout the genome. MutationalPatterns is an R/Bioconductor package that characterizes this broad range of mutational patterns and potential relations with (epi-)genomic features. Furthermore, it offers an efficient method to quantify the contribution of known mutational signatures. Such analyses can be used to determine whether certain DNA repair mechanisms are perturbed and to further characterize the processes underlying known mutational signatures. Keywords: R, Base substitutions, Somatic mutations, Mutational signatures, Mutational processes, Transcriptional strand bias. Availability and implementation: The MutationalPatterns R package is freely available for download at https://www.bioconductor.org/packages/release/bioc/html/MutationalPatterns.html. The package documentation provides a detailed description of typical analysis workflows."}, {"title": "HPViewer: Sensitive and specific genotyping of human papillomavirus in metagenomic DNA", "url": "https://www.biorxiv.org/content/early/2017/10/25/208926", "tag": "Bioinformatics", "abstract": "Background: Shotgun DNA sequencing provides sensitive detection of all 182 HPV types in tissue and body fluid. However, existing computational methods either produce false positives misidentifying HPV types due to shared sequences among HPV, human, and prokaryotes, or produce false negative since they identify HPV by assembled contigs requiring large abundant of HPV reads. Results: We show that HPV shares extensive simple repeats with human and prokaryotes and homologous sequences among different HPV types. The shared sequences caused errors in HPV genotyping and the repeats of human origin caused false positives in HPVDetector. Programs, such as VirusTAP and Vipie, which require de novo assembly of shotgun reads into contigs, eliminated false positives at a cost of substantial reduction in sensitivity. Here, we designed HPViewer with two custom HPV reference databases masking simple repeats and homology sequences respectively and one homology distance matrix to hybridize these two databases. It directly identified HPV from short DNA reads rather than assembled contigs. Using 100,100 simulated samples, we revealed that HPViewer was robust for samples containing either high or low number of HPV reads. Using 12 shotgun sequencing samples from respiratory papillomatosis, HPViewer was equal to VirusTAP, and Vipie and better than HPVDetector with the respect to specificity and was the most sensitive method in the detection of HPV types 6 and 11. We demonstrated that contigs-based approaches had disadvantages of detection of HPV. In 1,573 sets of metagenomic data from 18 human body sites, HPViewer identified 104 types of HPV in a body-site associated pattern and 89 types of HPV co-occurring in one sample with other types of HPV at least once. Conclusions: We demonstrated HPViewer was sensitive and specific for HPV detection in metagenomic data. It was also suggested that masking shared sequences is an effective approach to avoid false positive detection and identifying HPV from short metagenomic reads is more sensitive than assembled contigs. The innovative homology distance matrix connecting two HPV databases, repeat-mask and homology-mask, optimized the balance of sensitivity and specificity. HPViewer can be accessed at https://github.com/yuhanH/HPViewer/."}, {"title": "Computational discovery of tissue morphology biomarkers in very long-term survivors with pancreatic ductal adenocarcinoma", "url": "https://www.biorxiv.org/content/early/2017/10/25/207969", "tag": "Bioinformatics", "abstract": "Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest forms of cancer, with an average 5-year survival rate of only 8%. Within PDAC patients, however, there is a small subset of patients who survive >10 years. Deciphering underlying reasons behind prolonged survival could potentially provide new opportunities to treat PDAC; however, no genomic, transcriptomic, proteomic, or clinical signatures have been found to robustly separate this subset of patients. Digital pathology, in combination with machine learning, provides an opportunity to computationally search for tissue morphology patterns associated with disease outcomes. Here, we developed a computational framework to analyze whole-slide images (WSI) of PDAC patient tissue and identify tissue-morphology signatures for very long term surviving patients. Our results indicate that less tissue morphology heterogeneity is significantly linked to better patient survival and that the extra-tumoral space encodes prognostic information for survival. Based on information from morphological heterogeneity in the tumor and its adjacent area, we established a machine learning model with an AUC of 0.94. Our analysis workflow highlighted a quantitative visual-based tissue phenotype analysis that also allows direct interaction with pathology. This study demonstrates a pathway to accelerate the discovery of undetermined tissue morphology associated with pathogenesis states and prognosis and diagnosis of patients by utilizing new computational approaches."}, {"title": "Continuous Biomarker Assessment by Exhaustive Survival Analysis", "url": "https://www.biorxiv.org/content/early/2017/10/25/208660", "tag": "Bioinformatics", "abstract": "Publicly available high-throughput molecular data can facilitate biomarker identification and evaluation, with common practice being to divide a dataset into good or poor prognosis groups at the median value of a quantitative marker. However, this approach ignores the biomarker's underlying distribution and potential confounding factors associated with outcome that will inevitably vary between cohorts. Here we present the survivALL R package (https://CRAN.R-project.org/package=survivALL) and associated web app survivAPP (https://survivapp.shinyapps.io/survivapp/), allowing researchers to generate visual and numerical comparisons of all possible points-of-separation and enable quantitative biomarkers to be reliably evaluated within and across datasets, independent of compositional variation. To compensate for multiple-testing a non-parametric bootstrap is applied, defining reliable confidence intervals to assess how a biomarker performs relative to chance. This approach represents a significant improvement over existing methodologies in stratifying patients using quantitative biomarker(s)."}, {"title": "Evaluation of Convolutionary Neural Networks Modeling of DNA Sequences using Ordinal versus one-hot Encoding Method", "url": "https://www.biorxiv.org/content/early/2017/10/25/186965", "tag": "Bioinformatics", "abstract": "Convolutionary neural network (CNN) is a popular choice for supervised DNA motif prediction due to its excellent performances. To employ CNN, the input DNA sequences are required to be encoded as numerical values and represented as either vectors or multi-dimensional matrices. This paper evaluates a simple and more compact ordinal encoding method versus the popular one-hot encoding for DNA sequences. We compare the performances of both encoding methods using three sets of datasets enriched with DNA motifs. We found that the ordinal encoding performs comparable to the one-hot method but with significant reduction in training time. In addition, the one-hot encoding performances are rather consistent across various datasets but would require suitable CNN configuration to perform well. The ordinal encoding with matrix representation performs best in some of the evaluated datasets. This study implies that the performances of CNN for DNA motif discovery depends on the suitable design of the sequence encoding and representation. The good performances of the ordinal encoding method demonstrates that there are still rooms for improvement for the one-hot encoding method."}, {"title": "Data Science Issues in Understanding Protein-RNA Interactions", "url": "https://www.biorxiv.org/content/early/2017/10/25/208124", "tag": "Bioinformatics", "abstract": "An interplay of experimental and computational methods is required to achieve a comprehensive understanding of protein-RNA interactions. Crosslinking and immunoprecipitation (CLIP) identifies endogenous interactions by sequencing RNA fragments that co-purify with a selected RBP under stringent conditions. Here we focus on approaches for the analysis of resulting data and appraise the methods for peak calling, visualisation, analysis and computational modelling of protein-RNA binding sites. We advocate a combined assessment of cDNA complexity and specificity for data quality control. Moreover, we demonstrate the value of analysing sequence motif enrichment in peaks assigned from CLIP data, and of visualising RNA maps, which examine the positional distribution of peaks around regulated landmarks in transcripts. We use these to assess how variations in CLIP data quality, and in different peak calling methods, affect the insights into regulatory mechanisms. We conclude by discussing future opportunities for the computational analysis of protein-RNA interaction experiments."}, {"title": "Image-based methods for phenotyping growth dynamics and fitness in large plant populations", "url": "https://www.biorxiv.org/content/early/2017/10/25/208512", "tag": "Bioinformatics", "abstract": "With the development of next-generation sequencing technologies, high-throughput phenotyping has become the new bottleneck of quantitative genetics analyses. The model species Arabidopsis thaliana offers extensive resources to investigate intraspecific trait variability and the genetic bases of ecologically relevant traits, such as growth dynamics and reproductive allocation. However, reproducible and cost-effective methods need to be developed for the measurement of growth and especially fitness related traits in large populations. Here we describe image-based methods that can be adapted to a wide range of laboratory conditions, and enable the reliable estimation of biomass accumulation and fruit production in thousands of A. thaliana individuals. We propose a semi-invasive approach, where part of a population is used to predict plant biomass from image analysis. The other part of the population is daily imaged during three weeks, then harvested at the end of the life cycle where rosette and inflorescence are separately imaged. We developed ImageJ macros and R codes for image segmentation, 2D skeletonization and subsequent statistical analysis. First, ontogenetic growth is modelled from estimated and measured dry mass for all individuals with non-linear regressions, from which the dynamics of absolute growth rate (GR) and relative growth rate (RGR) are calculated. Second, analysis of the 2D inflorescence skeleton allows the estimation of fruit production, an important component of plant fitness. Our method was evaluated across 451 natural accessions of A. thaliana. Cross-validation revealed that our image-based method allows predicting approximately 90% of biomass variation and 70% of fruit production. Furthermore, estimated traits - like measured traits - showed high heritabilities and inter-experiment reproducibility. We propose a flexible toolkit for the measurement of growth and fitness related traits in large plant populations. It is based on simple imaging, making the method reproducible at low cost in different facilities. However, as manual imaging of large plant populations can quickly become a limiting factor, we also describe an automated high-throughput imaging coupled with micro-computers that enables large phenotypic screening for genome-wide association studies and stress experiments."}, {"title": "Graph abstraction reconciles clustering with trajectory inference through a topology preserving map of single cells", "url": "https://www.biorxiv.org/content/early/2017/10/25/208819", "tag": "Bioinformatics", "abstract": "Single-cell RNA-seq allows quantification of biological heterogeneity across both discrete cell types and continuous cell differentiation transitions. We present approximate graph abstraction (AGA), an algorithm that reconciles the computational analysis strategies of clustering and trajectory inference by explaining cell-to-cell variation both in terms of discrete and continuous latent variables (https://github.com/theislab/graph_abstraction). This enables to generate cellular maps of differentiation manifolds with complex topologies --- efficiently and robustly across different datasets. Approximate graph abstraction quantifies the connectivity of partitions of a neighborhood graph of single cells, thereby generating a much simpler abstracted graph whose nodes label the partitions. Together with a random walk-based distance measure, this generates a topology preserving map of single cells --- a partial coordinatization of data useful for exploring and explaining its variation. We use the abstracted graph to assess which subsets of data are better explained by discrete clusters than by a continuous variable, to trace gene expression changes along aggregated single-cell paths through data and to infer abstracted trees that best explain the global topology of data. We demonstrate the power of the method by reconstructing differentiation processes with high numbers of branchings from single-cell gene expression datasets and by identifying biological trajectories from single-cell imaging data using a deep-learning based distance metric. Along with the method, we introduce measures for the connectivity of graph partitions, generalize random-walk based distance measures to disconnected graphs and introduce a path-based measure for topological similarity between graphs. Graph abstraction is computationally efficient and provides speedups of at least 30 times when compared to algorithms for the inference of lineage trees."}, {"title": "3D cell nuclear morphology: microscopy imaging dataset and voxel-based morphometry classification results", "url": "https://www.biorxiv.org/content/early/2017/10/25/208207", "tag": "Bioinformatics", "abstract": "Cell deformation is regulated by complex underlying biological mechanisms associated with spatial and temporal morphological changes in the nucleus. Quantitative analysis of changes in size and shape of nuclear structures in 3D microscopic images is important not only for investigating nuclear organization, but also for detecting and treating pathological conditions such as cancer. Multiple methods have been proposed to classify cell and nuclear morphological phenotypes in 3D, however, there is a lack of publicly available 3D data for the evaluation and comparison of such algorithms. To address this problem, we present a dataset containing a of total of 1,433 segmented nuclear and 3,282 nucleolar binary masks. We also provide a baseline evaluation of a number of popular classification algorithms using voxel-based morphometric measures. Original and derived imaging data are made publicly available for downloading on the project web-page: http://www.socr.umich.edu/projects/3d-cell-morphometry/data.html."}, {"title": "Lancet: genome-wide somatic variant calling using localized colored DeBruijn graphs", "url": "https://www.biorxiv.org/content/early/2017/10/24/196311", "tag": "Bioinformatics", "abstract": "Reliable detection of somatic variations is of critical importance in cancer research. Lancet is an accurate and sensitive somatic variant caller which detects SNVs and indels by jointly analyzing reads from tumor and matched normal samples using colored DeBruijn graphs. Extensive experimental comparison on synthetic and real whole-genome sequencing datasets demonstrates that Lancet has better accuracy, especially for indel detection, than widely used somatic callers, such as MuTect, MuTect2, LoFreq, Strelka, and Strelka2. Lancet features a reliable variant scoring system which is essential for variant prioritization and detects low frequency mutations without sacrificing the sensitivity to call longer insertions and deletions empowered by the local assembly engine. In addition to genome-wide analysis, Lancet allows inspection of somatic variants in graph space, which augments the traditional read alignment visualization to help confirm a variant of interest. Lancet is available as an open-source program at https://github.com/nygenome/lancet."}, {"title": "Recentrifuge: robust comparative analysis and contamination removal for metagenomic data", "url": "https://www.biorxiv.org/content/early/2017/10/24/190934", "tag": "Bioinformatics", "abstract": "Metagenomics sequencing is becoming more and more widespread in biomedical and biological research, and the pace is increasing thanks to nanopore sequencing. With a rising number of samples and data per sample, the challenge of efficiently comparing results within a sample and between samples arises. Such analysis is complicated by laboratory and host contaminants which demands a reliable method for the removal of negative control samples from the set of samples. This is critical in samples with low microbial biomass, where contamination can comprise most of a sample if not all. With Recentrifuge, researchers can analyze results from the Centrifuge classifier using interactive hierarchical pie charts with special emphasis on the score level of the classifications. In addition to control-substracted samples, shared taxa and exclusive taxa per sample are also provided with scores for each taxonomic level of interest, thus enabling robust comparative analysis of multiple samples in low biomass metagenomics studies."}, {"title": "Distribution of Purines and Pyrimidines over miRNAs of Human, Gorilla and Chimpanzee", "url": "https://www.biorxiv.org/content/early/2017/10/24/208405", "tag": "Bioinformatics", "abstract": "Meaningful words in English need vowels to break up the sounds that consonants make. The Nature has encoded her messages in RNA molecules using only four alphabets A, U, C and G in which the nine member double-ring bases (adenine (A) and Guanine (G)) are purines, while the six member single-ring bases (cytosine (C) and uracil (U)) are pyrimidines. Four bases A, U, C and G of RNA sequences are divided into three kinds of classifications according to their chemical properties. One of the three classifications, the purine-pyrimidine class is important. In understanding the distribution (organization) of purines and pyrimidines over some of the non-coding regions of RNA, all miRNAs from three species of Family Hominidae (namely human, gorilla and chimpanzee) are considered. The distribution of purines and pyrimidines over miRNA shows deviation from randomness. Based on the quantitative metrics (fractal dimension, Hurst exponent, Hamming distance, distance pattern of purine-pyrimidine, purine-pyrimidine frequency distribution and Shannon entropy) five different clusters have been made. It is identified that there exists only one miRNA in human hsa-miR-6124 which is purely made of purine bases only."}, {"title": "A probabilistic framework to dissect functional cell-type-specific regulatory elements and risk loci underlying the genetics of complex traits", "url": "https://www.biorxiv.org/content/early/2017/10/24/059345", "tag": "Bioinformatics", "abstract": "Dissecting the physiological circuitry underlying diverse human complex traits associated with heritable common mutations is an ongoing effort. The primary challenge involves identifying the relevant cell types and the causal variants among the vast majority of the associated mutations in the noncoding regions. To address this challenge, we developed an efficient probabilistic framework. First, we propose a sparse group-guided learning algorithm to infer cell-type-specific enrichments. Second, we propose a fine-mapping Bayesian model that incorporates as Bayesian priors the sparse enrichments to infer risk variants. Using the proposed framework to analyze 32 complex human traits revealed meaningful tissue-specific epigenomic enrichments indicative of the relevant disease pathologies. The prioritized variants exhibit prominent tissue-specific epigenomic signatures and significant enrichments for eQTL and conserved elements. Together, we demonstrate the general benefits of the proposed integrative framework in elucidating meaningful tissue-specific epigenomic elements from large-scale correlated annotations and the implicated functional variants for future experimental interrogation."}, {"title": "Scaling read aligners to hundreds of threads on general-purpose processors", "url": "https://www.biorxiv.org/content/early/2017/10/24/205328", "tag": "Bioinformatics", "abstract": "General-purpose processors now support hundreds of simultaneous threads of execution. To make best use of these threads, genomics software must contend with new and subtle computer architecture issues. We discuss some of these and propose methods for improving threadscaling in tools that analyze each read independently, such as read aligners. We implement these methods in new versions of Bowtie, Bowtie 2 and HISAT. We greatly improve thread scaling in many scenarios, including on the recent Intel Xeon Phi architecture. Finally, we highlight how scalability bottlenecks are exacerbated by variable-record-length file formats like FASTQ, and suggest modest format changes that enable superior scaling."}, {"title": "chewBBACA: A complete suite for gene-by-gene schema creation and strain identification", "url": "https://www.biorxiv.org/content/early/2017/10/23/173146", "tag": "Bioinformatics", "abstract": "Motivation: Gene-by-gene (GbG) approaches are becoming increasingly popular in bacterial genomic epidemiology and outbreak detection. However, there is a lack of open source software for schema definition and allele calling. Results: The chewBBACA suite was designed to assist users in the creation and evaluation of novel whole-genome or core-genome GbG schemas and allele calling in bacterial strains of interest. The alleles called by chewBBACA are potential coding sequences, allowing the user to evaluate the possible consequences of the observed diversity. The software can run in a laptop or in high performance clusters making it useful for both small laboratories and large reference centers. Availability: https://github.com/B-UMMI/chewBBACA/ ."}, {"title": "geck: trio-based comparative benchmarking of variant calls", "url": "https://www.biorxiv.org/content/early/2017/10/23/208116", "tag": "Bioinformatics", "abstract": "Motivation: Classical methods of comparing the accuracies of variant calling pipelines are based on truth sets of variants whose genotypes are previously determined with high confidence. An alternative way of performing benchmarking is based on Mendelian constraints between related individuals. Statistical analysis of Mendelian violations can provide truth set-independent benchmarking information, and enable benchmarking less-studied variants and diverse populations. Results: We introduce a statistical mixture model for comparing two variant calling pipelines from genotype data they produce after running on individual members of a trio. We determine the accuracy of our model by comparing the precision and recall of GATK Unified Genotyper and Haplotype Caller on the high-confidence SNPs of the NIST Ashkenazim trio and the two independent Platinum Genome trios. We show that our method is able to estimate differential precision and recall between the two pipelines with 10-3 uncertainty."}, {"title": "Profiling G protein-coupled receptors of Fasciola hepatica identifies orphan rhodopsins unique to phylum Platyhelminthes", "url": "https://www.biorxiv.org/content/early/2017/10/23/207316", "tag": "Bioinformatics", "abstract": "G protein-coupled receptors (GPCRs) are established drug targets. Despite their considerable appeal as targets for next-generation anthelmintics, poor understanding of their diversity and function in parasitic helminths has thwarted progress towards GPCR-targeted anti-parasite drugs. This study facilitates GPCR research in the liver fluke, Fasciola hepatica, by generating the first profile of GPCRs from the F. hepatica genome. Our dataset describes 146 high confidence GPCRs, representing the largest cohort of GPCRs, and the most complete set of in silico ligand-receptor predictions, yet reported in any parasitic helminth. All GPCRs fall within the established GRAFS nomenclature; comprising three glutamate, 135 rhodopsin, two adhesion, five frizzled and one smoothened GPCR. Stringent annotation pipelines identified 18 highly diverged rhodopsins in F. hepatica that maintained core rhodopsin signatures, but lacked significant similarity with non-flatworm sequences, providing a new sub-group of potential flukicide targets. These facilitated identification of a larger cohort of 76 related sequences from available flatworm genomes, representing new members of existing groups of flatworm-specific rhodopsins. These receptors imply flatworm specific GPCR functions, and/or co-evolution with unique flatworm ligands, and could facilitate development of exquisitely selective anthelminthics. Ligand binding domain sequence conservation relative to deorphanised rhodopsins enabled high confidence ligand-receptor matching of seventeen receptors activated by acetylcholine, neuropeptide F/Y, octopamine or serotonin. RNA-Seq analyses showed expression of 101 GPCRs across various developmental stages, with the majority expressed most highly in the pathogenic intra-mammalian juvenile parasites. These data identify a broad complement of GPCRs in F. hepatica, including rhodopsins likely to have key functions in neuromuscular control and sensory perception, as well as frizzled and adhesion families implicated, in other species, in growth, development and reproduction. This catalogue of liver fluke GPCRs provides a platform for new avenues into our understanding of flatworm biology and anthelmintic discovery."}, {"title": "Elucidation of dose-dependent transcriptional events immediately following ionizing radiation exposure", "url": "https://www.biorxiv.org/content/early/2017/10/23/207951", "tag": "Bioinformatics", "abstract": "Long duration space missions expose astronauts to ionizing radiation events associated with highly energetic and charged heavy particles. Such exposure can result in chromosomal aberrations increasing the likelihood of the development of cancer. Early detection and mitigation of these events is critical in providing positive outcomes. In order to aid in the development of portable devices used to measure radiation exposure, we constructed a genome-wide screen to detect transcriptional changes in peripheral blood lymphocytes shortly after (approximately 1 hour) radiation exposure at low (0.3 Gy), medium (1.5 Gy) and high (3.0 Gy) doses compared to control (0.0 Gy) using Affymetrix\u00ae Human Gene 1.0 ST v1 microarrays. Our results indicate a number of sensitive and specific transcriptional profiles induced by radiation exposure that can potentially be implemented as biomarkers for radiation exposure as well as dose effect. For overall immediate radiation exposure, KDELC1, MRPS30, RARS, and HEXIM1 were determined to be effective biomarkers while PRDM9, CHST4, and SLC26A10 were determined to be biomarkers specific to 0.3 Gy exposure; RPH, CCDC96, WDYHV1, and IFNA16 were identified for 1.5 Gy exposure; and CWC15, CHCHD7, and DNAAF2 were determined to be sensitive and specific to 3.0 Gy exposure. The resulting raw and analyzed data are publicly available through NCBI's Gene Expression Ominibus via accession GSE64375."}, {"title": "ascend: R package for analysis of single cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/10/23/207704", "tag": "Bioinformatics", "abstract": "Abstract Summary: ascend is an R package comprised of fast, streamlined analysis functions optimized to address the statistical challenges of single cell RNA-seq. The package incorporates novel and established methods to provide a flexible framework to perform filtering, quality control, normalization, dimension reduction, clustering, differential expression and a wide-range of plotting. ascend is designed to work with scRNA-seq data generated by any high-throughput platform, and includes functions to convert data objects between software packages. Availability: The R package and associated vignettes are freely available at https://github.com/IMB-Computational-Genomics-Lab/ascend. Contact: joseph.powell@uq.edu.au Supplementary information: An example dataset is available at ArrayExpress, accession number E-MTAB-6108"}, {"title": "MeShClust: an intelligent tool for clustering DNA sequences", "url": "https://www.biorxiv.org/content/early/2017/10/23/207720", "tag": "Bioinformatics", "abstract": "Sequence clustering is a fundamental step in analyzing DNA sequences. Widely-used software tools for sequence clustering, such as CD-HIT and UCLUST, utilize greedy approaches that are not guaranteed to produce the best results. These tools are sensitive to one parameter that determines the sequence similarity between sequences in a cluster. Often times, a biologist does not know the exact sequence similarity. Therefore, clusters produced by these tools do not likely match the real clusters comprising the data if the provided parameter is inaccurate. To overcome this limitation, we adapted the mean shift algorithm, which has been used successfully thousands of times in fields such as image processing and computer vision. Here we describe the first application of the mean shift algorithm to clustering DNA sequences. Further, we applied machine learning to devising a novel alignment-free method for measuring sequence similarity. These two innovations represent the core of our software tool, MeShClust. We demonstrate MeShClust's outstanding ability to cluster DNA sequences with high accuracy even when the sequence similarity parameter provided by the user is not very accurate."}, {"title": "A reliable and unbiased human protein network with the disparity filter", "url": "https://www.biorxiv.org/content/early/2017/10/23/207761", "tag": "Bioinformatics", "abstract": "The living cell operates thanks to an intricate network of protein interactions. Proteins activate, transport, degrade, stabilise and participate in the production of other proteins. As a result, a reliable and systematically generated protein wiring diagram is crucial for a deeper understanding of cellular functions. Unfortunately, current human protein networks are noisy and incomplete. Also, they suffer from both study and technical biases: heavily studied proteins (e.g. those of pharmaceutical interest) are known to be involved in more interactions than proteins described in only a few publications. Here, we use the experimental evidence supporting the interaction between proteins, in conjunction with the so-called disparity filter, to construct a reliable and unbiased proteome-scale human interactome. The application of a global filter, i.e. only considering interactions with multiple pieces of evidence, would result in an excessively pruned network. In contrast, the disparity filter preserves interactions supported by a statistically significant number of studies and does not overlook small-scale protein associations. The resulting disparity-filtered protein network covers 67% of the human proteome and retains most of the network's weight and connectivity properties."}, {"title": "MatchMiner: An open source computational platform for real-time matching of cancer patients to precision medicine clinical trials using genomic and clinical criteria", "url": "https://www.biorxiv.org/content/early/2017/10/23/199489", "tag": "Bioinformatics", "abstract": "Background: Molecular profiling of cancers is now routine at many cancer centers, and the number of precision cancer medicine clinical trials, which are informed by profiling, is steadily rising. Additionally, these trials are becoming increasingly complex, often having multiple arms and many genomic eligibility criteria. Currently, it is a challenging for physicians to match patients to relevant clinical trials using the patient's genomic profile, which can lead to missed opportunities. Automated matching against uniformly structured and encoded genomic eligibility criteria is essential to keep pace with the complex landscape of precision medicine clinical trials. Results: To meet these needs, we built and deployed an automated clinical trial matching platform called MatchMiner at the Dana-Farber Cancer Institute (DFCI). The platform has been integrated with Profile, DFCI's enterprise genomic profiling project, which contains tumor profile data for >20,000 patients, and has been made available to physicians across the Institute. As no current standard exists for encoding clinical trial eligibility criteria, a new language called Clinical Trial Markup Language (CTML) was developed, and over 178 genomically-driven clinical trials were encoded using this language. The platform is open source and freely available for adoption by other institutions. Conclusion: MatchMiner is the first open platform developed to enable computational matching of patient-specific genomic profiles to precision cancer medicine clinical trials. Creating MatchMiner required developing clinical trial eligibility standards to support genome-driven matching and developing intuitive interfaces to support practical use-cases. Given the complexity of tumor profiling and the rapidly changing multi-site nature of genome-driven clinical trials, open source software is the most efficient, scalable, and economical option for matching cancer patients to clinical trials."}, {"title": "eGARD: Extracting associations between genomic anomalies and drug responses from text", "url": "https://www.biorxiv.org/content/early/2017/10/23/148833", "tag": "Bioinformatics", "abstract": "Tumor molecular profiling plays an integral role in identifying genomic anomalies which may help in personalizing cancer treatments, improving patient outcomes and minimizing risks associated with different therapies. However, critical information regarding the evidence of clinical utility of such anomalies is largely buried in biomedical literature. It is becoming prohibitive for biocurators, clinical researchers and oncologists to keep up with the rapidly growing volume and breadth of information, especially those that describe therapeutic implications of biomarkers and therefore relevant for treatment selection. In an effort to improve and speed up the process of manually reviewing and extracting relevant information from literature, we have developed a natural language processing (NLP)-based text mining (TM) system called eGARD (extracting Genomic Anomalies association with Response to Drugs). This system relies on the syntactic nature of sentences coupled with various textual features to extract relations between genomic anomalies and drug response from MEDLINE abstracts. Our system achieved high precision, recall and F-measure of up to 0.95, 0.86 and 0.90, respectively, on annotated evaluation datasets created in-house and obtained externally from PharmGKB. Additionally, the system extracted information that helps determine the confidence level of extraction to support prioritization of curation. Such a system will enable clinical researchers to explore the use of published markers to stratify patients upfront for \"best-fit\" therapies and readily generate hypotheses for new clinical trials."}, {"title": "RGBM: Regularized Gradient Boosting Machines For The Identification of Transcriptional Regulators Of Discrete Glioma Subtypes", "url": "https://www.biorxiv.org/content/early/2017/10/23/132670", "tag": "Bioinformatics", "abstract": "The transcription factors (TF) which regulate gene expressions are key determinants of cellular phenotypes. Reconstructing large-scale genome-wide networks which capture the influence of TFs on target genes are essential for understanding and accurate modelling of living cells. We propose RGBM: a gene regulatory network (GRN) inference algorithm, which can handle data from heterogeneous information sources including dynamic time-series, gene knockout, gene knockdown, DNA microarrays and RNA-Seq expression profiles. RGBM allows to use an a priori mechanistic of active biding network consisting of TFs and corresponding target genes. RGBM is evaluated on the DREAM challenge datasets where it surpasses the winners of the competitions and other established methods for two evaluation metrics by about 10-15%. We use RGBM to identify the main regulators of the molecular subtypes of brain tumors. Our analysis reveals the identity and corresponding biological activities of the master regulators driving transformation of the G-CIMP-high into the G-CIMP-low subtype of glioma and PA-like into LGm6-GBM, thus, providing a clue to the yet undetermined nature of the transcriptional events driving the evolution among these novel glioma subtypes. RGBM is available for download on CRAN at https://cran.rproject.org/web/packages/RGBM/index.html"}, {"title": "A Deep Learning Model for Predicting Tumor Suppressor Genes and Oncogenes from PDB Structure", "url": "https://www.biorxiv.org/content/early/2017/10/22/177378", "tag": "Bioinformatics", "abstract": "While cancer is a heterogeneous complex of distinct diseases, the common underlying mechanism for uncontrolled tumor growth is due to mutations in proto-oncogenes and the loss of the regulatory function of tumor suppression genes. In this paper we propose a novel deep learning model for predicting tumor suppression genes (TSGs) and proto-oncogenes (OGs) from their Protein Data Bank (PDB) three dimensional structures. Specifically, we develop a convolutional neural network (CNN) to classify the feature map sets extracted from the tertiary protein structures. Each feature map set represents particular biochemical properties associated with the atomic coordinates appearing on the outer surface of protein's three dimensional structure. The experimental results on the collected dataset for classifying TSGs and OGs demonstrate promising performance with 82.57% accuracy and 0.89 area under the ROC curve. The initial success of the proposed model warrants further study to develop a comprehensive model to identify the cancer driver genes or events using TSG and OG as the basis to track the causal chain."}, {"title": "IMPPAT: A curated database of Indian Medicinal Plants, Phytochemistry And Therapeutics", "url": "https://www.biorxiv.org/content/early/2017/10/22/206995", "tag": "Bioinformatics", "abstract": "Phytochemical constituents of medicinal plants encompass a diverse space of chemical scaffolds which can be used for rational design of novel drugs. India is rich with a flora of indigenous medicinal plants that have been used for centuries in traditional Indian medicine to treat human maladies. A comprehensive online database on the phytochemistry of Indian medicinal plants will enable the application of systems biology and cheminformatic approaches towards natural product based drug discovery. In this direction, we here present, IMPPAT, a manually curated database of Indian Medicinal Plants, Phytochemistry, And Therapeutics. IMPPAT contains 1742 Indian medicinal plants, 9596 phytochemicals and 1124 therapeutic uses which span across 27074 plant-phytochemical associations and 11514 plant-therapeutic associations. Notably, the curation effort led to a non-redundant in silico chemical library of 9596 phytochemicals with standard chemical identifiers and structure information. Using cheminformatic approaches, we have computed the physicochemical properties and drug-likeliness of the phytochemicals in IMPPAT which led to a filtered subset of 960 potential druggable phytochemicals. Moreover, a comparative analysis against FDA approved drugs suggests that majority of the druggable phytochemicals in IMPPAT are good candidates for novel prospective drugs as they have little or no structural similarity with existing drugs. The IMPPAT database is openly accessible at: https://www.imsc.res.in/~asamal/resources/imppat/home."}, {"title": "Explainable machine learning predictions to help anesthesiologists prevent hypoxemia during surgery", "url": "https://www.biorxiv.org/content/early/2017/10/21/206540", "tag": "Bioinformatics", "abstract": "Hypoxemia causes serious patient harm, and while anesthesiologists strive to avoid hypoxemia during surgery, anesthesiologists are not reliably able to predict which patients will have intraoperative hypoxemia. Using minute by minute EMR data from fifty thousand surgeries we developed and tested a machine learning based system called Prescience that predicts real-time hypoxemia risk and presents an explanation of factors contributing to that risk during general anesthesia. Prescience improved anesthesiologists' performance when providing interpretable hypoxemia risks with contributing factors. The results suggest that if anesthesiologists currently anticipate 15% of events, then with Prescience assistance they could anticipate 30% of events or an estimated additional 2.4 million annually in the US, a large portion of which may be preventable because they are attributable to modifiable factors. The prediction explanations are broadly consistent with the literature and anesthesiologists' prior knowledge. Prescience can also improve clinical understanding of hypoxemia risk during anesthesia by providing general insights into the exact changes in risk induced by certain patient or procedure characteristics. Making predictions of complex medical machine learning models (such as Prescience) interpretable has broad applicability to other data-driven prediction tasks in medicine."}, {"title": "Nucleosome rotational positioning is influenced by cis- and trans-acting factors", "url": "https://www.biorxiv.org/content/early/2017/10/21/207167", "tag": "Bioinformatics", "abstract": "The rotational positioning of nucleosomes is associated with certain periodic DNA patterns, with the well-known preference of WW dinucleotides (where W is A or T) and SS dinucleotides (where S is G or C) to occur at the sites where nucleosomeal DNA bends into the minor and major grooves, respectively. However, cis- and trans-acting factors governing this nucleosome positioning sequence (NPS) pattern remain unclear. To identify these factors, we performed a systematic analysis of WW/SS patterns within 147-bp nucleosome core particle fragments in different eukaryotes under multiple cellular conditions. We find that >50% of nucleosomes in the genomes do not have this NPS pattern and >20% of nucleosomes take the inverse, anti-NPS pattern. The fraction of anti-NPS nucleosomes is increased in cells lacking one or more chromatin remodelers and in genes with high RNA polymerase II (Pol II) transcription frequencies. Remarkably, anti-NPS nucleosomes exhibit distinctive distributions across the yeast and human genomes: the nucleosomes are distributed uniformly in yeast, while in humans they are more enriched at promoters and coding regions than at repetitive DNA elements. Such differences can be explained by intrinsic DNA sequence patterns. Thus, our results suggest that the rotational positioning of nucleosomes can be influenced, in a synergic manner, by cis and trans determinants including DNA sequence, ATP-dependent nucleosome remodeling enzymes and RNA Pol II."}, {"title": "So you think you can PLS-DA?", "url": "https://www.biorxiv.org/content/early/2017/10/21/207225", "tag": "Bioinformatics", "abstract": "Partial Least-Squares Discriminant Analysis (PLS-DA) is a popular machine learning tool that is gaining increasing attention as a useful feature selector and classifier. In an effort to understand its strengths and weaknesses, we performed a series of experiments with synthetic data and compared its performance to its close relative from which it was initially invented, namely Principal Component Analysis (PCA). We demonstrate that even though PCA ignores the information regarding the class labels of the samples, this unsupervised tool can be remarkably effective as a dimensionality reducer and a feature selector. In some cases, it outperforms PLS-DA, which is made aware of the class labels in its input. Our experiments range from looking at the signal-to-noise ratio in the feature selection task, to considering many practical distributions and models for the synthetic data sets used. Our experiments consider many useful distributions encountered when analyzing bioinformatics and clinical data, especially in the context of machine learning, where it is hoped that the program automatically extracts and/or learns the hidden relationships."}, {"title": "Leveraging heterogeneity across multiple data sets increases accuracy of cell-mixture deconvolution and reduces biological and technical biases", "url": "https://www.biorxiv.org/content/early/2017/10/20/206466", "tag": "Bioinformatics", "abstract": "In silico quantification of cell proportions from mixed-cell transcriptomics data (deconvolution) requires a reference expression matrix, called basis matrix. We hypothesized that matrices created using only healthy samples from a single microarray platform would introduce biological and technical biases in deconvolution. We show presence of such biases in two existing matrices, IRIS and LM22, irrespective of the deconvolution method used. Here, we present immunoStates, a basis matrix built using 6160 samples with different disease states across 42 microarray platforms. We found that immunoStates significantly reduced biological and technical biases. We further show that cellular proportion estimates using immunoStates are consistently more correlated with measured proportions than IRIS and LM22, across all methods. Importantly, we found that different methods have virtually no effect once the basis matrix is chosen. Our results demonstrate the need and importance of incorporating biological and technical heterogeneity in a basis matrix for achieving consistently high accuracy."}, {"title": "Blind estimation and correction of microarray batch effect", "url": "https://www.biorxiv.org/content/early/2017/10/20/202952", "tag": "Bioinformatics", "abstract": "Microarray batch effect (BE) has been the primary bottleneck for large-scale integration of data from multiple experiments. Current BE correction methods either need known batch identities (ComBat) or have the potential to overcorrect, by removing true but unknown biological differences (SVA). Even though the effects of technical differences on measured expression have been published, there are no BE correction algorithms that take the approach of predicting technical effects from parameters computed from a fixed reference sample set. We show that a set of signatures, each of which is a vector the length of the number of probes, calculated on a Reference set of microarray samples can predict much of the batch effect in other Validation sets. We present a rationale of selecting a Reference set of samples designed to estimate technical differences without removing biological differences. Putting both together, we introduce the Batch Effect Signature Correction (BESC) algorithm that uses the BES calculated on the Reference set to efficiently predict and remove BE. Using two independent Validation sets, we show that BESC is capable of removing batch effect without removing unknown but true biological differences. Much of the variations due to batch effect is shared between different microarray datasets. That shared information can be used to predict signatures (i.e. directions of perturbation) due to batch effect in new datasets. The correction is blind (without needing to recompute the parameters on new samples to be corrected), single sample, (each sample is corrected independently of each other) and conservative (only those perturbations known to be likely to be due to technical differences are removed ensuring that unknown but important biological differences are maintained). Those three characteristics make it ideal for high-throughput correction of samples for a microarray data repository. An R Package besc implementing the algorithm is available from http://explainbio.com."}, {"title": "Online resources for PCAWG data exploration, visualization, and discovery", "url": "https://www.biorxiv.org/content/early/2017/10/20/163907", "tag": "Bioinformatics", "abstract": "The Pan-Cancer Analysis of Whole Genomes (PCAWG) cohort provides a large, uniformly-analyzed, whole-genome dataset. The PCAWG Landing Page (http://docs.icgc.org/pcawg) focuses on four biologist-friendly, publicly-available web tools for exploring this data: The ICGC Data Portal, UCSC Xena, Expression Atlas, and PCAWG-Scout. They enable researchers to dynamically query the complex genomics data, explore tumors' molecular landscapes, and include external information to facilitate interpretation."}, {"title": "SATORI: A System for Ontology-Guided Visual Exploration of Biomedical Data Repositories", "url": "https://www.biorxiv.org/content/early/2017/10/20/046755", "tag": "Bioinformatics", "abstract": "The ever-increasing number of biomedical data sets provides tremendous opportunities for re-use but current data repositories provide limited means of exploration apart from text-based search. Ontological metadata annotations provide context by semantically relating data sets. Visualizing this rich network of relationships can improve the explorability of large data repositories and help researchers find data sets of interest. We developed SATORI--an integrative search and visual exploration interface for the exploration of biomedical data repositories. The design is informed by a requirements analysis through a series of semi-structured interviews. We evaluated the implementation of SATORI in a field study on a real-world data collection. SATORI enables researchers to seamlessly search, browse, and semantically query data repositories via two visualizations that are highly interconnected with a powerful search interface. SATORI is an open-source web application, which is freely available at http://satori.refinery-platform.org and integrated into the Refinery Platform."}, {"title": "Exploring the single-cell RNA-seq analysis landscape with the scRNA-tools database", "url": "https://www.biorxiv.org/content/early/2017/10/20/206573", "tag": "Bioinformatics", "abstract": "As single-cell RNA-sequencing (scRNA-seq) datasets have become more widespread the number of tools designed to analyse these data has dramatically increased. Navigating the vast sea of tools now available is becoming increasingly challenging for researchers. In order to better facilitate selection of appropriate analysis tools we have been cataloguing and curating new analysis tools, as they become available, in the scRNA-tools database (www.scRNA-tools.org). Our database collects a range of information on each scRNA-seq analysis tool and categorises them according to the analysis tasks they perform. Exploration of this database gives insights into the areas of rapid development of analysis methods for scRNA-seq data. We see that many tools are developed to perform tasks specific to scRNA-seq analysis, particularly clustering and ordering of cells. We also find that the scRNA-seq community embraces an open-source approach, with most tools available under open-source licenses and preprints being extensively used as a means to describe methods. The scRNA-tools database provides a valuable resource for researchers embarking on scRNA-seq analysis and as a record of the growth of the field over time."}, {"title": "Understanding sequencing data as compositions: an outlook and review", "url": "https://www.biorxiv.org/content/early/2017/10/19/206425", "tag": "Bioinformatics", "abstract": "Motivation: Although seldom acknowledged explicitly, count data generated by sequencing platforms exist as compositions for which the abundance of each component (e.g., gene or transcript) is only coherently interpretable relative to other components within that sample. This property arises from the assay technology itself, whereby the number of counts recorded for each sample is constrained by an arbitrary total sum (i.e., library size). Consequently, sequencing data, as compositional data, exist in a non-Euclidean space that renders invalid many conventional analyses, including distance measures, correlation coefficients, and multivariate statistical models. Results: The purpose of this review is to summarize the principles of compositional data analysis (CoDA), provide evidence for why sequencing data are compositional, discuss compositionally valid methods available for analyzing sequencing data, and highlight future directions with regard to this field of study."}, {"title": "optSelect: using agent-based modeling and binary PSO techniques for ensemble feature selection and stability assessment", "url": "https://www.biorxiv.org/content/early/2017/10/19/206185", "tag": "Bioinformatics", "abstract": "Motivation: Recent studies have shown that the ensemble feature selection approaches are essential for generating robust classifiers. Existing methods for aggregating feature lists from different methods re-quire use of arbitrary thresholds for selecting the top ranked features and do not account for classification accuracy while selecting the optimal set. Here we present a two-stage ensemble feature selection framework for finding the optimal set of features without compromising on classification accuracy. Methods and Results: We present herein optSelect, a multi agent-based stochastic optimization approach for nested ensemble feature selection. Stage one involves function perturbation, where ranked list of features are generated using different methods and stage two involves data perturbation, where feature selection is performed within randomly selected subsets of the training data and the optimal set of features is selected within each set using the optSelect. The agents are assigned to different behavior states and move according to a binary PSO algorithm. A multi-objective fitness function is used to evaluate the classification accuracy of the agents. We evaluate the system performance using the random probe method and using five publicly available microarray datasets. The performance of optSelect is compared with single feature selection techniques and existing aggregation methods. The results show that the optSelect algorithm improves the classification accuracy compared to both individual and existing rank aggregation methods. The algorithm is incorporated into an R package, optSelect."}, {"title": "SEACOIN2.0: an interactive mining and visualization tool for information retrieval, summarization, and knowledge discovery", "url": "https://www.biorxiv.org/content/early/2017/10/19/206193", "tag": "Bioinformatics", "abstract": "The rapidly increasing size of biomedical databases such as Medline requires the use of intelligent data mining methods for information extraction and summarization. Existing biomedical text-mining tools have limited capabilities for incorporating citation information during document ranking and for inferring topological and network relationships between biomedical terms. Often too much is returned during summarization leading to information overload. Furthermore, literature based discoveries could be hard to interpret if the network is too complex. SEACOIN2.0 can incorporate citation information during document ranking and uses a unique association rule mining algorithm to generate multi-level k-ary trees. The multi-level trees facilitate efficient information retrieval, visual data exploration, summarization, and hypothesis generation. The system presents graphical summarization via multiple dynamic visualization panels and an interactive word cloud. LexRank algorithm is used to identify salient sentences in top abstracts related to the query. An average F-measure of 94% was achieved for document retrieval, and an average precision of 88% was obtained for identification of top co-occurrence terms. SEACOIN2.0 was also used to replicate previously published findings using the literature-based discovery and EMR-based PheWAS approaches. We present herein SEACOIN2.0, an interactive visual mining tool for improved information retrieval, automated multi-level summarization of Medline abstracts, and literature-based discovery. SEACOIN2.0 addresses the problem of information overload and allows clinicians and biomedical researchers to meet their information needs."}, {"title": "The response to selection in Glycoside Hydrolase Family 13 structures: A comparative quantitative genetics approach", "url": "https://www.biorxiv.org/content/early/2017/10/19/205542.1", "tag": "Bioinformatics", "abstract": "The Glycoside Hydrolase Family 13 (GH13) proteins have been extensively studied given their importance in industry and evolutionary studies. Despite of being a multicatalytic family, they mainly performed the hydrolysis of starch into smaller carbohydrates. Evolutionarily, they are of interest because of the TIM-barrel fold that all these proteins share. Industrially, they have been subjected to many bioengineering process to improve their function. Here, we introduce a framework to analyse the response to selection of GH13 protein structures given some phylogenetic and dynamic information. We found that the TIM-barrel is not selectable (for thermodynamic stability) since it has been under purifying selection during its recent evolution. We also show a method to rank important residues capable of altering towards an optimum. Overall, this paper demonstrates the feasibility of a framework for the analysis of protein structures in a given fitness landscape."}, {"title": "A machine learning approach to predicting short-term mortality risk in patients starting chemotherapy", "url": "https://www.biorxiv.org/content/early/2017/10/19/204081", "tag": "Bioinformatics", "abstract": "Background: Cancer patients who die soon after starting chemotherapy incur costs of treatment without benefits. Accurately predicting mortality risk from chemotherapy is important, but few patient data-driven tools exist. We sought to create and validate a machine learning model predicting mortality for patients starting new chemotherapy. Methods: We obtained electronic health records for patients treated at a large cancer center (26,946 patients; 51,774 new regimens) over 2004-14, linked to Social Security data for date of death. The model was derived using 2004-11 data, and performance measured on non-overlapping 2012-14 data. Findings: 30-day mortality from chemotherapy start was 2.1%. Common cancers included breast (21.1%), colorectal (19.3%), and lung (18.0%). Model predictions were accurate for all patients (AUC 0.94). Predictions for patients starting palliative chemotherapy (46.6% of regimens), for whom prognosis is particularly important, remained highly accurate (AUC 0.92). To illustrate model discrimination, we ranked patients initiating palliative chemotherapy by model-predicted mortality risk, and calculated observed mortality by risk decile. 30-day mortality in the highest-risk decile was 22.6%; in the lowest-risk decile, no patients died. Predictions remained accurate across all primary cancers, stages, and chemotherapies--even for clinical trial regimens that first appeared in years after the model was trained (AUC 0.94). The model also performed well for prediction of 180-day mortality (AUC 0.87; mortality 74.8% in the highest risk decile vs. 0.2% in the lowest). Predictions were more accurate than data from randomized trials of individual chemotherapies, or SEER estimates. Interpretation: A machine learning algorithm accurately predicted short-term mortality in patients starting chemotherapy using EHR data. Further research is necessary to determine generalizability and the feasibility of applying this algorithm in clinical settings."}, {"title": "Cluster Headache: Comparing Clustering Tools for 10X Single Cell Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/10/19/203752", "tag": "Bioinformatics", "abstract": "The commercially available 10X Genomics protocol to generate droplet-based single cell RNA-seq (scRNA-seq) data is enjoying growing popularity among researchers. Fundamental to the analysis of such scRNA-seq data is the ability to cluster similar or same cells into non-overlapping groups. Many competing methods have been proposed for this task, but there is currently little guidance with regards to which method offers most accuracy. Answering this question is complicated by the fact that 10X Genomics data lack cell labels that would allow a direct performance evaluation. Thus in this review, we focused on comparing clustering solutions of a dozen methods for three datasets on human peripheral mononuclear cells generated with the 10X Genomics technology. While clustering solutions appeared robust, we found that solutions produced by different methods have little in common with each other. They also failed to replicate cell type assignment generated with supervised labeling approaches. Furthermore, we demonstrate that all clustering methods tested clustered cells to a large degree according to the amount of genes coding for ribosomal protein genes in each cell."}, {"title": "Modeling Codon Rate Variation: Robust Inference of Protein and Nucleotide Selection", "url": "https://www.biorxiv.org/content/early/2017/10/19/174839", "tag": "Bioinformatics", "abstract": "There are numerous sources of variation in the rate of synonymous substitutions inside genes, such as direct selection on the nucleotide sequence, or mutation rate variation. However the majority of the codon models which are developed and widely used today still incorporate an assumption of effectively neutral synonymous substitution rate, constant between sites of each gene. Here we propose a simple yet effective extension to codon models, which incorporates codon substitution rate variation along the gene sequence. We find strong effects of substitution rate variation on positive selection inference. We also demonstrate that the computational load of our approach remains tractable, and therefore we are able to apply it to genome scale positive selection scans. We apply our new method to two datasets: 767 vertebrate orthologs and 8,606 orthologs from twelve Drosophila species. Our new model is strongly favored by the data, and the support of the model increases with the amount of information. Moreover, it is able to capture signatures of nucleotide level selection acting on translation initiation and on splicing sites within the coding region. Finally, we show that rate variation is highest in the highly recombining regions, and we hypothesize that recombination and mutation rate variation, such as high CpG mutation rate, are the two main sources of nucleotide rate variation. Overall, rate variation in substitutions is an important feature to capture, both to detect positive selection and to understand gene evolution, and the approach that we propose allows to do this in genome-wide scans."}, {"title": "Juicebox.js provides a cloud-based visualization system for Hi-C data", "url": "https://www.biorxiv.org/content/early/2017/10/19/205740", "tag": "Bioinformatics", "abstract": "Contact mapping experiments such as Hi-C explore how genomes fold in 3D. Here, we introduce Juicebox.js, a cloud-based web application for exploring the resulting datasets. Like the original Juicebox application, Juicebox.js allows users to zoom in and out of such datasets using an interface similar to Google Earth. Furthermore, Juicebox.js encodes the exact state of the browser in a shareable URL. Creating a public browser for a new Hi-C dataset does not require coding and can be accomplished in under a minute."}, {"title": "A unified computational framework for modeling genome-wide nucleosome landscape", "url": "https://www.biorxiv.org/content/early/2017/10/18/202580", "tag": "Bioinformatics", "abstract": "Nucleosomes form the fundamental building blocks of eukaryotic chromatin, and previous attempts to understand the principles governing their genome-wide distribution have spurred much interest and debate in biology. In particular, the precise role of DNA sequence in shaping local chromatin structure has been controversial. This paper rigorously quantifies of the contribution of hitherto-debated sequence features -- including G+C content, 10.5-bp periodicity, and poly(dA:dT) tracts -- to three distinct aspects of genome-wide nucleosome landscape: occupancy, translational positioning and rotational positioning. Our computational framework simultaneously learns nucleosome number and nucleosome-positioning energy from genome-wide nucleosome maps. In contrast to other previous studies, our model can predict both in-vitro and in-vivo nucleosome maps in S. cerevisiae. We find that although G+C content is the primary determinant of MNase-derived nucleosome occupancy, MNase digestion biases may substantially influence this GC dependence. By contrast, poly(dA:dT) tracts are seen to deter nucleosome formation, regardless of the experimental method used. We further show that the 10.5-bp nucleotide periodicity facilitates rotational but not translational positioning. Applying our method to in-vivo nucleosome maps demonstrates that, for a subset of genes, the regularly-spaced nucleosome arrays observed around transcription start sites can be partially recapitulated by DNA sequence alone. Finally, in-vivo nucleosome occupancy derived from MNase-seq experiments around transcription termination sites can be mostly explained by the genomic sequence. Implications of these results and potential extensions of the proposed computational framework are discussed."}, {"title": "Analytic combinatorics for bioinformatics I: seeding methods", "url": "https://www.biorxiv.org/content/early/2017/10/18/205427", "tag": "Bioinformatics", "abstract": "Seeding heuristics are the most widely used strategies to speed up sequence alignment in bioinformatics. Such strategies are most successful if they are calibrated, so that the speed-versus-accuracy trade-off can be properly tuned. In the widely used case of read mapping, it has been so far impossible to predict the success rate of competing seeding strategies for lack of a theoretical framework. Here I present an approach to estimate such quantities based on the theory of analytic combinatorics. In a nutshell, the strategy is to specify a combinatorial construction of reads where the seeding heuristic fails, translate this specification into a generating function using formal rules, and finally extract the probabilities of interest from the singularities of the generating function. I use this approach to construct simple estimators of the success rate of the seeding heuristic under different types of sequencing errors. I also show how the analytic combinatorics strategy can be used to compute the associated type I and type II error rates (mapping the read to the wrong location, or being unable to map the read). Finally, I show how analytic combinatorics can be used to estimate average quantities such as the expected number of errors in reads where the seeding heuristic fails. Overall, this work introduces a theoretical and practical framework to find the success rate of seeding heuristics and related problems in bioinformatics."}, {"title": "zUMIs: A fast and flexible pipeline to process RNA sequencing data with UMIs", "url": "https://www.biorxiv.org/content/early/2017/10/18/153940", "tag": "Bioinformatics", "abstract": "Single cell RNA-seq (scRNA-seq) experiments typically analyze hundreds or thousands of cells after amplification of the cDNA. The high throughput is made possible by the early introduction of sample-specific barcodes (BCs) and the amplification bias is alleviated by unique molecular identifiers (UMIs). Thus the ideal analysis pipeline for scRNA-seq data needs to efficiently tabulate reads according to both BC and UMI. zUMIs is such a pipeline, it can handle both known and random BCs and also efficiently collapses UMIs, either just for exon mapping reads or for both exon and intron mapping reads. Another unique feature of zUMIs is the adaptive downsampling function, that facilitates dealing with hugely varying library sizes, but also allows to evaluate whether the library has been sequenced to saturation. zUMIs flexibility allows to accommodate data generated with any of the major scRNA-seq protocols that use BCs and UMIs. To illustrate the utility of zUMIs, we analysed a single-nucleus RNA-seq dataset and show that more than 35% of all reads map to introns. We furthermore show that these intronic reads are informative about expression levels, significantly increasing the number of detected genes and improving the cluster resolution."}, {"title": "Evolutionary variance analysis of the Glycoside Hydrolase Family 13: Structural evidence in classification and evolution", "url": "https://www.biorxiv.org/content/early/2017/10/18/201251", "tag": "Bioinformatics", "abstract": "Glycoside Hydrolase Family 13 (GH13) structures are responsible for the hydrolysis of starch into smaller carbohydrates. They important in industrial applications and evolutionary studies. This family has been thoroughly documented in the the Carbohydrate-Active enZYmes Database (CAZY), and divided into subfamilies based mainly in sequence information. Here we give structural evidence into GH13 classification and evolution using structural information. Here we proposed a novel method that is sensitive enough to identify miss-classifications, or to provide evidence for further partition that can be of interests to bio-engineers and evolutionary biologists. We also introduced a method to explore the relative importance of residues with respect to the overall deformation that it causes to the overall structure in an evolutionary time scale. We found that the GH13 family can be classified into three main structural groups. There is a hierarchical structure within these clusters that can be use to inform other classification schemes. We also found that by using structural information, subtle structural shifts can be identified and that can be missed in sequence/phylogeny-only based classifications. When each structural group is explored, we found that identifying the most structurally variable sites can lead to identification of functionally (both catalytically and structurally) important residues."}, {"title": "Leveraging uncertainty information from deep neural networks for disease detection", "url": "https://www.biorxiv.org/content/early/2017/10/18/084210", "tag": "Bioinformatics", "abstract": "Deep learning (DL) has revolutionized the field of computer vision and image processing. In medical imaging, algorithmic solutions based on DL have been shown to achieve high performance on tasks that previously required medical experts. However, DL-based solutions for disease detection have been proposed without methods to quantify and control their uncertainty in a decision. In contrast, a physician knows whether she is uncertain about a case and will consult more experienced colleagues if needed. Here we evaluate drop-out based Bayesian uncertainty measures for DL in diagnosing diabetic retinopathy (DR) from fundus images and show that it captures uncertainty better than straightforward alternatives. Furthermore, we show that uncertainty informed decision referral can improve diagnostic performance. Experiments across different networks, tasks and datasets show robust generalization. Depending on network capacity and task/dataset difficulty, we surpass 85% sensitivity and 80% specificity as recommended by the NHS when referring 0%-20% of the most uncertain decisions for further inspection. We analyse causes of uncertainty by relating intuitions from 2D visualizations to the high-dimensional image space. While uncertainty is sensitive to clinically relevant cases, sensitivity to unfamiliar data samples is task dependent, but can be rendered more robust."}, {"title": "A max-margin training of RNA secondary structure prediction integrated with the thermodynamic model", "url": "https://www.biorxiv.org/content/early/2017/10/18/205047", "tag": "Bioinformatics", "abstract": "Motivation: A popular approach for predicting RNA secondary structure is the thermodynamic nearest neighbor model that finds a thermodynamically most stable secondary structure with the minimum free energy (MFE). For further improvement, an alternative approach that is based on machine learning techniques has been developed. The machine learning based approach can employ a fine-grained model that includes much richer feature representations with the ability to fit the training data. Although a machine learning based fine-grained model achieved extremely high performance in prediction accuracy, a possibility of the risk of overfitting for such model has been reported. Results: In this paper, we propose a novel algorithm for RNA secondary structure prediction that integrates the thermodynamic approach and the machine learning based weighted approach. Our fine-grained model combines the experimentally determined thermodynamic parameters with a large number of scoring parameters for detailed contexts of features that are trained by the structured support vector machine (SSVM) with the L1 regularization to avoid overfitting. Our benchmark shows that our algorithm achieves the best prediction accuracy compared with existing methods, and heavy overfitting cannot be observed. Availability: The implementation of our algorithm is available at https://github.com/keio-bioinformatics/mxfold."}, {"title": "StateHub-StatePaintR: rapid and reproducible chromatin state evaluation for custom genome annotation.", "url": "https://www.biorxiv.org/content/early/2017/10/17/127720", "tag": "Bioinformatics", "abstract": "Genome annotation is critical to understand the function of disease variants, especially for clinical applications. To meet this need there are segmentations available from public consortia reflecting varying unsupervised approaches to functional annotation based on epigenetics data, but there remains a need for transparent, reproducible, and easily interpreted genomic maps of the functional biology of chromatin. We introduce a new methodological framework for defining a combinatorial epigenomic model of chromatin state on a web database, StateHub. In addition, we created an annotation tool for bioconductor, StatePaintR, which accesses these models and uses them to rapidly (on the order of seconds) produce chromatin state segmentations in standard genome browser formats. Annotations are fully documented with change history and versioning, authorship information, and original source files. StatePaintR calculates ranks for each state from next-gen sequencing peak statistics, facilitating variant prioritization, enrichment testing, and other types of quantitative analysis. StateHub hosts annotation tracks for major public consortia as a resource, and allows users to submit their own alternative models."}, {"title": "MAGI: A Bayesian-like method for metabolite, annotation, and gene integration", "url": "https://www.biorxiv.org/content/early/2017/10/17/204362", "tag": "Bioinformatics", "abstract": "Metabolomics is a widely used technology for obtaining direct measures of metabolic activities from diverse biological systems. However, it is limited by ambiguous metabolite identifications. Furthermore, interpretation is limited by incomplete and inaccurate genome-based predictions of enzyme activities (i.e. gene annotations). Metabolite, Annotation, and Gene Integration (MAGI) addresses these challenges by generating metabolite-gene associations via biochemical reactions based on a score between probable metabolite identifications and probable gene annotations. This is calculated by a Bayesian-like method and emphasizes consensus between metabolites and genes. We applied MAGI to sequence data and metabolomics data collected from Streptomyces coelicolor A3(2), an extensively characterized bacterium that produces diverse secondary metabolites. We found that coupling metabolomics and genomics data by scoring consensus between the two increases the quality of both metabolite identifications and gene annotations. Moreover, MAGI was found to make correct biochemical predictions for poorly annotated genes that were readily validated by literature searches. As metabolomics data become more widely available for sequenced organisms, this approach has the potential to improve our understanding of microbial metabolism while also providing testable hypotheses for specific biochemical functions. MAGI is freely available for academic use both as an online tool at https://magi.nersc.gov and with source code available at https://github.com/biorack/magi"}, {"title": "A simple, consistent estimator of heritability for genome-wide association studies", "url": "https://www.biorxiv.org/content/early/2017/10/17/204446", "tag": "Bioinformatics", "abstract": "Analysis of genome-wide association studies (GWAS) is characterized by a large number of univariate regressions where an outcome, a quantitative trait, is regressed on hundreds of thousands to millions of genomic markers, i.e. single-nucleotide polymorphism (SNP) counts, one marker at a time. Assuming a linear model linking the markers to the outcome, this article proposes an estimator of the heritability of the trait, defined here as the fraction of the variance of the trait explained by the genomic markers in the study. The estimator, called GWAS heritability (GWASH) estimator, is easy to compute, highly interpretable, and is consistent as the number of markers and the sample size increase. More importantly, it can be computed from summary statistics typically reported in GWAS, not requiring access to the original data. The estimator takes full account of the linkage disequilibrium (LD) or correlation between the SNPs in the study through moments of the LD matrix, estimable from auxiliary datasets. Unlike other proposed estimators in the literature, the precision of the estimate is obtainable analytically, allowing for power and sample size calculations for heritability estimates."}, {"title": "TASEP modelling provides a parsimonious explanation for the ability of a single uORF to upregulate downstream ORF translation during the Integrated Stress Response", "url": "https://www.biorxiv.org/content/early/2017/10/17/204784", "tag": "Bioinformatics", "abstract": "Translation initiation is the rate limiting step of protein synthesis that is downregulated during Integrated Stress Response (ISR). In our previous work (Andreev, O'Connor et al. 2015), we demonstrated that most human mRNAs resistant to this inhibition possess translated uORFs and in some cases a single uORF is sufficient for the resistance. Here we developed a computational model of Initiation Complexes Interference with Elongating Ribosomes (ICIER) to gain insight into the mechanism. We explored the relationship between the flux of scanning ribosomes upstream and downstream of a single uORF depending on uORF features. Paradoxically our analysis predicts that reducing ribosome flux upstream of certain uORFs increases initiation downstream. The model reveals derepression of downstream translation as general mechanism of uORF-mediated stress resistance. It predicts that stress resistance can be achieved with long or slowly translated uORFs that do not favor high levels of translation re-initiation  and start with non-leaky initiators."}, {"title": "KaryoScan: abnormal karyotype detection from whole-exome sequence", "url": "https://www.biorxiv.org/content/early/2017/10/17/204719", "tag": "Bioinformatics", "abstract": "Motivation: Detection of abnormal karyotypes from whole-exome sequencing has significant clinical potential, enabling a primary screen for chromosomal anomalies among samples undergoing short-read sequencing for nucleotide resolution genomic characterization. Results: We present KaryoScan, a high-throughput method for detecting chromosomal anomalies within large cohort exome sequencing studies. We detect and validate autosomal and sex chromosomal aneuploidies in a large exome sequencing cohort, and demonstrate detection of smaller and complex events (partial chromosome, mosaic, copy neutral, and complex rearrangements), representing the range of anomalies that can be uncovered from the exome. Availability: https://github.com/rgcgithub/karyoscan"}, {"title": "Hot-starting software containers for bioinformatics analyses", "url": "https://www.biorxiv.org/content/early/2017/10/17/204495", "tag": "Bioinformatics", "abstract": "Using software containers has become standard practice to reproducibly deploy and execute biomedical workflows on the cloud. We demonstrate that hot-starting, from containers that have been frozen after the application has already begun execution, reduces the costs of cloud computing by avoiding repetitive initialization steps. The method is widely applicable and can provide substantial savings both for small jobs and for large-scale deployments using automated schedulers."}, {"title": "Meta-analysis of Cytometry Data Reveals Racial Differences in Immune Cells", "url": "https://www.biorxiv.org/content/early/2017/10/17/130948", "tag": "Bioinformatics", "abstract": "While meta-analysis has demonstrated increased statistical power and more robust estimations in studies, the application of this commonly accepted methodology to cytometry data has been challenging. Different cytometry studies often involve diverse sets of markers. Moreover, the detected values of the same marker are inconsistent between studies due to different experimental designs and cytometer configurations. As a result, the cell subsets identified by existing auto-gating methods cannot be directly compared across studies. We developed MetaCyto for automated meta-analysis of both flow and mass cytometry (CyTOF) data. By combining clustering methods with a silhouette scanning method, MetaCyto is able to identify commonly labeled cell subsets across studies, thus enabling meta-analysis. Applying MetaCyto across a set of 10 heterogeneous cytometry studies totaling 2926 samples enabled us to identify multiple cell populations exhibiting differences in abundance between White and Asian adults. Software is released to the public through GitHub (github.com/hzc363/MetaCyto)."}, {"title": "The Emergent Connectome in Caenorhabditis elegans Embryogenesis", "url": "https://www.biorxiv.org/content/early/2017/10/17/146035", "tag": "Bioinformatics", "abstract": "The relatively new field of connectomics provides us with a unique window into nervous system function. In the model organism Caenorhabditis elegans, this promise is even greater due to the relatively small cellular size (302 cells) of the nervous system. While the adult C. elegans connectome has been characterized, the emergence of these networks in development has yet to be established. In this paper, we approach this problem using secondary data describing the birth times of terminally-differentiated cells as they appear in the embryo and connectomics data for pharyngeal nervous system cells in the adult hermaphrodite. By combining these two sources of data, we can better understand patterns that emerge in an incipient connectome. This includes identifying at what point in embryogenesis the cells of a connectome first comes into being, observing some of the earliest neuron-neuron interactions, and making comparisons between the formally-defined connectome and the embryogenetic interactome. An analysis is also conducted to root terminally-differentiated pharyngeal cells in their developmental cell lineage. This analysis reveals subnetworks with different properties at 300 minutes of embryogenesis. Overall, this analysis reveals important information about the birth order of specific cells, key building blocks of global connectivity, and how these structures corresponds to various embryogenetic stages during the emergence of a connectome."}, {"title": "HTX: a tool for the exploration and visualization of high-throughput image assays", "url": "https://www.biorxiv.org/content/early/2017/10/16/204016", "tag": "Bioinformatics", "abstract": "High-throughput screening (HTS) techniques have enabled large scale image-based studies, but extracting biological insights from the imaging data in an exploratory setting remains a challenge. Existing packages for this task either require expert annotations, which can bias the outcome of the study, or are completely unsupervised, failing to leverage the information present in the assay design. We present HTX, an interactive tool to aid in the exploration of large microscopy data sets by allowing the visualization of entire image-based assays according to visual similarities between the samples in an intuitive and navigable manner. Underlying HTX are a collection of novel algorithmic techniques for deep texture descriptor learning, 2D data visualization, adversarial suppression of batch effects, and backprop-based image saliency estimation. We demonstrate that HTX can exploit the screen metadata in order to learn screen-specific image descriptors, which are then used to quantify the visual similarity between samples in the assay. Given these similarities and the different visualization resources of HTX, it is shown that screens of small-molecule libraries on cell data can be easily explored, reproducing the results of previous studies where highly-specific domain knowledge was required."}, {"title": "DirtyGenes: significance testing for gene or bacterial population composition changes using the Dirichlet distribution", "url": "https://www.biorxiv.org/content/early/2017/10/16/204321", "tag": "Bioinformatics", "abstract": "High throughput sequencing, and quantitative polymerase chain reaction (qPCR), can detect changes in bacterial communities or the genes that they carry, between different environments or treatments. These methods are applied widely to microbiomes in humans, animals, soil and water; an important application is for changes in antimicrobial resistance genes (ARGs). However, at present, there is no statistical method to determine whether observed changes in the overall composition are significant, or result from random variations between samples. Therefore researchers are limited to graphical descriptions. We describe a novel statistical method to determine whether or not observed differences in bacterial populations or their genes are significant. It can be used with data from shotgun metagenomics, 16S characterisation or qPCR. It can also be used for experimental design, to calculate the number of samples needed in future experiments. We show its application to two example data sets. The first is published data on bacterial communities and ARGs in the environment, in which we show that there are significant changes in both ARG and community composition. The second is a new data set on seasonality in bacterial communities and ARGs in hooves from four sheep. While the observed differences are not significant, we show that a minimum group size of eight sheep in a future experiment would provide sufficient power to observe significant changes, should the already observed changes be true. This method has broad uses for statistical testing and experimental design in experiments on changing microbiomes, including for studies on antimicrobial resistance."}, {"title": "Integrative pipeline for profiling DNA copy number and inferring tumor phylogeny", "url": "https://www.biorxiv.org/content/early/2017/10/16/195230", "tag": "Bioinformatics", "abstract": "Copy number variation is an important and abundant source of variation in the human genome, which has been associated with a number of diseases, especially cancer. Massively parallel next-generation sequencing allows copy number profiling with fine resolution. Such efforts, however, have met with mixed successes, with setbacks arising partly from the lack of reliable analytical methods to meet the diverse and unique challenges arising from the myriad experimental designs and study goals in genetic studies. In cancer genomics, detection of somatic copy number changes and profiling of allele-specific copy number (ASCN) are complicated by experimental biases and artifacts as well as normal cell contamination and cancer subclone admixture. Furthermore, careful statistical modeling is warranted to reconstruct tumor phylogeny by both somatic ASCN changes and single nucleotide variants. Here we describe a flexible computational pipeline, MARATHON, which integrates multiple related statistical software for copy number profiling and downstream analyses in disease genetic studies."}, {"title": "Sequence fingerprints distinguish erroneous from correct predictions of Intrinsically Disordered Protein Regions", "url": "https://www.biorxiv.org/content/early/2017/10/15/203547", "tag": "Bioinformatics", "abstract": "More than sixty prediction methods for intrinsically disordered proteins (IDPs) have been developed over the years, many of which are accessible on the world-wide web. Nearly, all of these predictors give balanced accuracies in the ~65% to ~80% range. Since predictors are not perfect, further studies are required to uncover the role of amino acid residues in native IDP as compared to predicted IDP regions. In the present work, we make use of sequences of 100% predicted IDP regions, false positive disorder predictions, and experimentally determined IDP regions to distinguish the characteristics of native versus predicted IDP regions. A higher occurrence of asparagine is observed in sequences of native IDP regions but not in sequences of false positive predictions of IDP regions. The occurrences of certain combinations of amino acids at the pentapeptide level provide a distinguishing feature in the IDPs with respect to globular proteins. The distinguishing features presented in this paper provide insights into the sequence fingerprints of amino acid residues in experimentally-determined as compared to predicted IDP regions. These observations and additional work along these lines should enable the development of improvements in the accuracy of disorder prediction algorithm."}, {"title": "Transcriptome Deconvolution of Heterogeneous Tumor Samples with Immune Infiltration", "url": "https://www.biorxiv.org/content/early/2017/10/14/146795", "tag": "Bioinformatics", "abstract": "Transcriptomic deconvolution in cancer and other heterogeneous tissues remains challenging. Available methods lack the ability to estimate both component-specific proportions and expression profiles for individual samples. We present DeMixT, a new tool to deconvolve high dimensional data from mixtures of more than two components. DeMixT implements an iterated conditional mode algorithm and a novel gene-set-based component merging approach to improve accuracy. In a series of experimental validation studies and application to TCGA data, DeMixT showed high accuracy. Improved deconvolution is an important step towards linking tumor transcriptomic data with clinical outcomes. An R package, scripts and data are available: https://github.com/wwylab/DeMixT/."}, {"title": "The Genetic Chain Rule for Probabilistic Kinship Estimation", "url": "https://www.biorxiv.org/content/early/2017/10/13/202879.1", "tag": "Bioinformatics", "abstract": "Accurate kinship predictions using DNA forensic samples is limited to first degree relatives. High throughput sequencing of single nucleotide polymorphisms and short tandem repeats (STRs) can be used to expand DNA forensics kinship prediction capabilities. Current kinship identification models incorporate STR size profiles to statistical models that do not adequately depict genetic inheritance beyond the first degree, or machine learning algorithms that are prone to over optimization and requiring similar training data. This work presents an alternative approach using a com- putational framework that incorporates the inheritance of single nucleotide polymorphisms (SNPs) between specific relationships(patent pending)[1]. The impact of SNP panel size on predictions is visualized in terms of the distribution of allelic differences between individuals. The confidence of predictions is made by calculating log likelihood ratios. With a panel of 39108 SNPs evaluated on an in silico dataset, this method can resolve parents from siblings and distinguish 1st, 2nd, 3rd, and 4th degree relatives from each other and unrelated individuals."}, {"title": "FACSanadu: Graphical user interface for rapid visualization and quantification of flow cytometry data", "url": "https://www.biorxiv.org/content/early/2017/10/13/201897", "tag": "Bioinformatics", "abstract": "Flow cytometry is a fundamental technique in cell biology, yet few open source packages are available to analyse these data. Here we describe FACSanadu, an interactive package for rapid visualization and measurement of flow cytometry data. It is the first open source package that can read length profile data from the COPAS Biosorter."}, {"title": "HYPO: A database of hypothetical human proteins", "url": "https://www.biorxiv.org/content/early/2017/10/13/202887", "tag": "Bioinformatics", "abstract": "All annotated genes were once hypothetical or uncharacterized. Keeping this as an epilogue, we have enhanced our former database of hypothetical proteins (HP) in human (HypoDB) with added annotation, application programming interfaces and descriptive features. The database hosts 1000+ manually curated records of the known unknown regions in the human genome. The new updated version of HypoDB with functionalities (Blast, Match) is freely accessible at http://www.bioclues.org/hypo2."}, {"title": "Detecting T cell receptor rearrangements in silico from non-targeted DNA-sequencing (WGS/WES)", "url": "https://www.biorxiv.org/content/early/2017/10/13/201947", "tag": "Bioinformatics", "abstract": "To better understand the composition of heterogeneous tissue samples used in generating large genomic datasets, we developed a method for estimating the abundance of T cells within the cellular population. Somatic recombination of chromosomal DNA in T cells creates a vast repertoire of structurally divergent T cell receptors (TCRs) that recognize an array of non-self proteins. It also generates a genomic signature by which TCR sequences can be distinguished from other cell types in non-targeted NGS genomic data. Here we leverage this signature to extract reads with rearranged TCR sequences from a non-targeted population, such as whole genome sequencing (WGS) or whole exome sequencing (WES) datasets. We isolate and confirm T cell rearranged reads from the remainder of the genome (99.9%), accurately estimate relative T cell abundance within a cellular population, and provide a snapshot of the T cell receptor repertoire. This approach is unique from available TCR software options that focus on examining the overall diversity of the TCR repertoire and require prior amplification or selection of this region before sequencing, and has particular utility in immunoscoring clinical patient samples in situations where genomic data exists and other approaches are unavailable."}, {"title": "SPECTRE: a Suite of PhylogEnetiC Tools for Reticulate Evolution", "url": "https://www.biorxiv.org/content/early/2017/10/13/169177", "tag": "Bioinformatics", "abstract": "Split-networks are a generalization of phylogenetic trees that have proven to be a powerful tool in phylogenetics. Various ways have been developed for computing such networks, including split-decomposition, NeighborNet, QNet and FlatNJ. Some of these approaches are implemented in the user-friendly SplitsTree software package. However, to give the user the option to adjust and extend these approaches and to facilitate their integration into analysis pipelines, there is a need for robust, open-source implementations of associated data structures and algorithms. Here we present SPECTRE, a readily available, open-source library of data structures written in Java, that comes complete with new implementations of several pre-published algorithms and a basic interactive graphical interface for visualizing planar split networks. SPECTRE also supports the use of longer running algorithms by providing command line interfaces, which can be executed on servers or in High Performance Computing (HPC) environments."}, {"title": "Identifying accurate metagenome and amplicon software via a meta-analysis of benchmarking studies", "url": "https://www.biorxiv.org/content/early/2017/10/12/202077", "tag": "Bioinformatics", "abstract": "Environmental DNA sequencing has rapidly become a widely-used technique for investigating a range of questions, particularly related to health and environmental monitoring. There has also been a proliferation of bioinformatic methods for analysing metagenomic and amplicon datasets, which makes selecting adequate methods a significant challenge. A number of benchmark studies have been undertaken; however, these often present conflicting results. We have applied a network meta-analysis method to identify software methods that are generally accurate for mapping DNA sequences to taxonomic hierarchies. Based upon these results we have identified some methods and computational strategies that produce robust predictions."}, {"title": "The optimization of mRNA expression level by its intrinsic properties \u2014 insights from codon usage pattern and structural stability of mRNA", "url": "https://www.biorxiv.org/content/early/2017/10/12/202200", "tag": "Bioinformatics", "abstract": "The deviation from the uniform usage of synonymous codons is termed codon usage bias. A lot has been explained from the translational viewpoint for the observed phenomenon. To understand codon usage bias from the transcriptional perspective, we present here a holistic picture of this phenomenon in Saccharomyces cerevisiae, using both wild type and computationally mutated mRNAs. Although in wild type, both codon usage bias and mRNA stability positively regulate the gene (mRNA) expression level and are positively correlated with each other, any deviation from natural situation breaks such equilibrium. Computational examination of mRNA sequences with different sets of synonymous codon composition reveals that in mutated condition, the mRNA expression becomes reduced. Furthermore, constraining codon usage pattern to wild type and carrying out randomization of codons resulted in less stable mRNA. Further, we realized a Boolean Expression explaining the gene expression under various conditions of bias and mRNA stability. These studies suggest that selection of codons is favored for regulation of gene expression through potential formation of messenger RNA structures which contribute to folding stability. The naturally occurring codon composition is responsible for optimization of gene expression, and under such composition, the mRNA structure having highest stability is selected by nature."}, {"title": "Revising transcriptome assemblies with phylogenetic information in Agalma1.0", "url": "https://www.biorxiv.org/content/early/2017/10/12/202416", "tag": "Bioinformatics", "abstract": "One of the most common transcriptome assembly errors is to mistake different transcripts of the same gene as transcripts from multiple closely related genes. It is difficult to identify these errors during assembly, but in a phylogenetic analysis these errors can be diagnosed from gene trees containing clades of tips from the same species with improbably short branch lengths. treeinform is a module implemented in Agalma1.0 that uses phylogenetic analyses across species to refine transcriptome assemblies. It identifies transcripts of the same gene that were incorrectly assigned to multiple genes and reassign them as transcripts of the same gene. treeinform is implemented in Agalma1.0, available at https://bitbucket.org/caseywdunn/agalma. Supplementary information is available at bioRxiv."}, {"title": "GROOLS: reactive graph reasoning for genome annotation through biological processes", "url": "https://www.biorxiv.org/content/early/2017/10/12/117994", "tag": "Bioinformatics", "abstract": "Background: High quality functional annotation is essential for understanding the phenotypic consequences encoded in a genome. Despite improvements in bioinformatics methods, millions of sequences in databanks are not assigned reliable functions. The curation of protein functions in the context of biological processes is a way to evaluate and improve their annotation. Results: We developed an expert system using paraconsistent logic, named GROOLS (Genomic Rule Object-Oriented Logic System), that evaluates the completeness and the consistency of predicted functions through biological processes like metabolic pathways. Using a generic and hierarchical representation of knowledge, biological processes are modeled in a graph from which observations (i.e. predictions and expectations) are propagated by rules. At the end of the reasoning, conclusions are assigned to biological process components and highlight uncertainties and inconsistencies. Results on 14 microbial organisms are presented. Conclusions: GROOLS software is designed to evaluate the overall accuracy of functional unit and pathway predictions according to organism experimental data like growth phenotypes. It assists biocurators in the functional annotation of proteins by focusing on missing or contradictory observations."}, {"title": "A Bioinformatics Pipeline for Whole Exome Sequencing: Overview of the Processing and Steps from Raw Data to Downstream Analysis", "url": "https://www.biorxiv.org/content/early/2017/10/11/201145", "tag": "Bioinformatics", "abstract": "Recent advances in next generation sequencing (NGS) technologies have given an impetus to find causality for rare genetic disorders. Since 2005 and aftermath of the human genome project, efforts have been made to understand the rare variants of genetic disorders. Benchmarking the bioinformatics pipeline for whole exome sequencing (WES) has always been a challenge. In this protocol, we discuss detailed steps from quality check to analysis of the variants using a WES pipeline comparing them with reposited public NGS data and survey different techniques, algorithms and software tools used during each step. We observed that variant calling performed on exome and whole genome datasets have different metrics generated when compared to variant callers, GATK and VarScan with different parameters. Furthermore, we found that VarScan with strict parameters could recover 80-85% of high quality GATK SNPs with decreased sensitivity from NGS data. We believe our protocol in the form of pipeline can be used by researchers interested in performing WES analysis for genetic diseases and by large any clinical phenotypes."}, {"title": "An alignment method for nucleic acid sequences against annotated genomes", "url": "https://www.biorxiv.org/content/early/2017/10/11/200394", "tag": "Bioinformatics", "abstract": "Motivation: Biological sequence alignment is fundamental to their further interpretation. Current alignment algorithms typically align either nucleic acid or amino acid sequences. Using only nucleic acid sequence similarity, divergent sequences cannot be aligned reliably because of the limited alphabet and genetic saturation. To align divergent coding nucleic acid sequences, one can align using the translated amino acid sequences. This requires the detection of the correct open reading frame, is prone to eventual frame shift errors, and typically requires the treatment of genes separately. It was our motivation to design a nucleic acid sequence alignment algorithm to align a nucleic acid sequence against a (reference) genome sequence, that works equally well for similar and divergent sequences, and produces an optimal alignment considering simultaneously the alignment of all annotated coding sequences. Results: We define a genome alignment score for evaluating the quality of an alignment of a nucleic acid query sequence against a reference genome sequence, for which coding sequence features have been annotated (for example in a GenBank record). The genome alignment score combines the affine gap score for the nucleic acid sequence with an affine gap score for all amino acid alignments resulting from coding sequences in open reading frames contained within the query sequence. We present a Dynamic Programming algorithm to compute the optimal global or local alignment using this genomic alignment score and provide a formal proof of correctness. This algorithm allows the alignment of nucleic acid sequences from closely related and highly divergent sequences within the same software and using the same parameters, automatically correcting any eventual frame shift errors and produces at the same time the aligned translated amino acid sequences of all relevant coding sequence features. Availability: The software is available as a web application at http://www.genomedetective.com/app/aga and as command-line application at https://github.com/emweb/aga"}, {"title": "Evolving better RNAfold C source code", "url": "https://www.biorxiv.org/content/early/2017/10/11/201640", "tag": "Bioinformatics", "abstract": "Grow and graft genetic programming (GGGP) can automatically evolve an existing state-of-the art program to give more accurate predictions of the secondary structures adapted by RNA molecules using their base sequence alone. That is, genetic improvement (GI) can make functional as well as non-functional source code changes."}, {"title": "Practical computational reproducibility in the life sciences", "url": "https://www.biorxiv.org/content/early/2017/10/11/200683", "tag": "Bioinformatics", "abstract": "Many areas of research suffer from poor reproducibility. This problem is particularly acute in computationally intensive domains where results rely on a series of complex methodological decisions that are not well captured by traditional publication approaches. Various guidelines have emerged for achieving reproducibility, but practical implementation of these practices remains difficult. This is because reproducing published computational analyses requires installing many software tools plus associated libraries, connecting tools together into the complete pipeline, and specifying parameters. Here we present a suite of recently emerged technologies which make computational reproducibility not just possible, but, finally, practical in both time and effort. By combining a system for building highly portable packages of bioinformatics software, containerization and virtualization technologies for isolating reusable execution environments for these packages, and an integrated workflow system that automatically orchestrates the composition of these packages for entire pipelines, an unprecedented level of computational reproducibility can be achieved."}, {"title": "Methods in Description and Validation of Local Metagenetic Microbial Communities", "url": "https://www.biorxiv.org/content/early/2017/10/11/198614", "tag": "Bioinformatics", "abstract": "1. We propose MinHash (as implemented by MASH) and NMF as alternative methods to estimate similarity between metagenetic samples. We further describe these results with cluster analysis and correlations with independent ecological metadata. 2. Using sample to sample similarities based on MinHash similarities we use hierarchal clustering to generate clusters, simultaneously we generate groups based on NMF, we compare groups generated from the MinHash similarity derived clusters and from NMF to those determined by the environment, looking to Silhouette Width for an assessment of the quality of the cluster. 3. We analyze existing data from the Atacama desert to determine the relationship between ecological factors and group membership, using the generated groups from MASH and NMF we run an ANOVA to uncover links between metagenetic samples and the known environmental variables, such as pH and Soil Conductivity."}, {"title": "InfoTrim: A DNA Read Quality Trimmer Using Entropy", "url": "https://www.biorxiv.org/content/early/2017/10/11/201442", "tag": "Bioinformatics", "abstract": "Biological DNA reads are often trimmed before mapping, genome assembly, and other tasks to improve the quality of the results. Biological sequence complexity relates to alignment quality as low complexity regions can align poorly. There are many read trimmers, but many do not use sequence complexity for trimming. Alignment of reads generated from whole genome bisulfite sequencing is especially challenging since bisulfite treated reads tend to reduce sequence complexity. InfoTrim, a new read trimmer, was created to explore these issues. It is evaluated against five other trimmers using four read mappers on real and simulated bisulfite treated DNA data. InfoTrim has reasonable results that are consistent with other read mappers."}, {"title": "Bamgineer: Introduction of simulated allele-specific copy number variants into exome and targeted sequence data sets", "url": "https://www.biorxiv.org/content/early/2017/10/10/119636", "tag": "Bioinformatics", "abstract": "Somatic copy number variations (CNVs) play a crucial role in development of many human cancers. The broad availability of next-generation sequencing data has enabled the development of algorithms to computationally infer CNV profiles from a variety of data types including exome and targeted sequence data; currently the most prevalent types of cancer genomics data. However, systemic evaluation and comparison of these tools remains challenging due to a lack of ground truth reference sets. To address this need, we have developed Bamgineer, a tool written in Python to introduce user-defined haplotype-phased allele-specific copy number events into an existing Binary Alignment Mapping (BAM) file, with a focus on targeted and exome sequencing experiments. As input, this tool requires a read alignment file (BAM format), lists of non-overlapping genome coordinates for introduction of gains and losses (bed file), and an optional file defining known haplotypes (vcf format). To improve runtime performance, Bamgineer introduces the desired CNVs in parallel using queuing and parallel processing on a local machine or on a high-performance computing cluster. As proof-of-principle, we applied Bamgineer to a single high-coverage (mean: 220X) exome sequence file from a blood sample to simulate copy number profiles of 3 exemplar tumors from each of 10 tumor types at 5 tumor cellularity levels (20-100%, 150 BAM files in total). To demonstrate feasibility beyond exome data, we introduced read alignments to a targeted 5-gene cell-free DNA sequencing library to simulate EGFR amplifications at frequencies consistent with circulating tumor DNA (10, 1, 0.1 and 0.01%) while retaining the multimodal insert size distribution of the original data. We expect Bamgineer to be of use for development and systematic benchmarking of CNV calling algorithms by users using locally-generated data for a variety of applications. The source code is freely available at http://github.com/pughlab/bamgineer."}, {"title": "Biopipe: A Lightweight System Enabling Comparison of Bioinformatics Tools and Workflows", "url": "https://www.biorxiv.org/content/early/2017/10/10/201186", "tag": "Bioinformatics", "abstract": "Analyzing next generation sequencing data always requires researchers to install many tools, prepare input data compliant to the required data format, and execute the tools in specific orders. Such tool installation and workflow execution process is tedious and error-prone, and becomes very challenging when researchers need to compare multiple alternative tool chains. To mitigate this problem, we developed a new lightweight and portable system, Biopipe, to simplify the creation and execution of bioinformatics tools and workflows, and to further enable the comparison between alternative tools or workflows. Biopipe allows users to create and edit workflows with user-friendly web interfaces, and automates tool installation as well as workflow synthesis by downloading and executing predefined Docker images. With Biopipe, biologists can easily experiment with and compare different bioinformatics tools and workflows without much computer science knowledge. There are mainly two parts in Biopipe: a web application and a standalone Java application. They are freely available at http://bench.cs.vt.edu:8282/Biopipe-Workflow-Editor-0.0.1/index.xhtml and https://code.vt.edu/saima5/Biopipe-Run-Workflow."}, {"title": "Enhancer Linking by Methylation/Expression Relationships with the R package ELMER version 2", "url": "https://www.biorxiv.org/content/early/2017/10/10/148726", "tag": "Bioinformatics", "abstract": "Recent studies indicate that DNA methylation can be used to identify changes at transcriptional enhancers and other cis-regulatory elements in primary human samples. A systematic approach to inferring gene regulatory networks has been provided by the R/Bioconductor package ELMER (Enhancer Linking by Methylation/Expression Relationships), which first identifies DNA methylation changes in distal regulatory elements and correlates these with the expression of nearby genes to identify direct transcriptional targets. Next, ELMER performs a transcription factor binding motif analysis and integrates with expression profiling of all human transcription factors, to identify master regulatory TFs and place each differentially methylated regulatory element into the context of an altered gene regulatory network (GRN). Here we present a completely updated version of the package (ELMER v. 2.0), which uses the latest Bioconductor data structures including the popular MultiAsssayExperiment, supports multiple reference genome assemblies as well as the DNA methylation platforms Infinium MethylationEPIC and Infinium HumanMethylation450, and provides a 'Supervised' analysis mode for paired sample study designs (such as treated vs. untreated replicate samples). It also supports data import from the new NCI Genomic Data Commons (GDC) database. The new version is substantially re-written, improving stability, performance, and extensibility. It also uses improved databases for transcription factor binding domain families and binding motif specificities, and has newly designed output plots for publication-quality figures. Below, we describe the methods and new features of ELMER v. 2.0 and present two use case demonstrating how the tool can be used to analyze TCGA data in either Unsupervised or Supervised mode. ELMER (v2.0.0) is available as an R/Bioconductor package at https://github.com/tiagochst/ELMER. Also, ELMER.data (v2.0.0), which provides auxiliary data required to perform the analysis, is available at https://github.com/tiagochst/ELMER.data."}, {"title": "Touching proteins with virtual bare hands: how to visualize protein-drug complexes and their dynamics in virtual reality", "url": "https://www.biorxiv.org/content/early/2017/10/10/201152.1", "tag": "Bioinformatics", "abstract": "The ability to precisely visualize the atomic geometry of the interactions between a drug and its protein target in structural models is critical in predicting the correct modifications in previously identified inhibitors to create more effective next generation drugs. It is currently common practice among medicinal chemists while attempting the above to access the information contained in three-dimensional structures by using two-dimensional projections, which can preclude disclosure of useful features. A more precise visualization of the three-dimensional configuration of the atomic geometry in the models can be achieved through the implementation of immersive virtual reality (VR). In this work, we present a freely available software pipeline for visualising protein structures through VR. New customer hardware, such as the HTC Vive and the Oculus Rift utilized in this study, are available at reasonable prices. Moreover, we have combined VR visualization with fast algorithms for simulating intramolecular motions of protein flexibility, in an effort to further improve structure-lead drug design by exposing molecular interactions that might be hidden in the less informative static models."}, {"title": "SSEalign: accurate function prediction of bacterial unannotated protein, based on effective training dataset", "url": "https://www.biorxiv.org/content/early/2017/10/10/200915", "tag": "Bioinformatics", "abstract": "The functions of numerous bacterial proteins remain unknown because of the variety of their sequences. The performances of existing prediction methods are highly weak toward these proteins, leading to the annotation of \"hypothetical protein\" deposited in NCBI database. Elucidating the functions of these unannotated proteins is an urgent task in computational biology. We report a method about secondary structure element alignment called SSEalign based on an effective training dataset extracting from 20 well-studied bacterial genomes. The experimentally validated same genes in different species were selected as training positives, while different genes in different species were selected as training negatives. Moreover, SSEalign used a set of well-defined basic alignment elements with the backtracking line search algorithm to derive the best parameters for accurate prediction. Experimental results showed that SSEalign achieved 91.2% test accuracy, better than existing prediction methods. SSEalign was subsequently applied to identify the functions of those unannotated proteins in the latest published minimal bacteria genome JCVI-syn3.0. Results indicated that At least 99 proteins out of 149 unannotated proteins in the JCVI-syn3.0 genome could be annotated by SSEalign. In conclusion, our method is effective for the identification of protein homology and the annotation of uncharacterized proteins in the genome."}, {"title": "Unsupervised multiple kernel learning for heterogeneous data integration", "url": "https://www.biorxiv.org/content/early/2017/10/10/139287", "tag": "Bioinformatics", "abstract": "Recent high-throughput sequencing advances have expanded the breadth of available omics datasets and the integrated analysis of multiple datasets obtained on the same samples has allowed to gain important insights in a wide range of applications. However, the integration of various sources of information remains a challenge for systems biology since produced datasets are often of heterogeneous types, with the need of developing generic methods to take their different specificities into account. We propose a multiple kernel framework that allows to integrate multiple datasets of various types into a single exploratory analysis. Several solutions are provided to learn either a consensus meta-kernel or a meta-kernel that preserves the original topology of the datasets. We applied our framework to analyse two public multi-omics datasets. First, the multiple metagenomic datasets, collected during the TARA Oceans expedition, was explored to demonstrate that our method is able to retrieve previous findings in a single KPCA as well as to provide a new image of the sample structures when a larger number of datasets are included in the analysis. To perform this analysis, a generic procedure is also proposed to improve the interpretability of the kernel PCA in regards with the original data. Second, the multi-omics breast cancer datasets, provided by The Cancer Genome Atlas, is analysed using a kernel Self-Organizing Maps with both single and multi-omics strategies. The comparison of this two approaches demonstrates the benefit of our integration method to improve the representation of the studied biological system. Proposed methods are available in the RR package mixKernel, released on CRAN. It is fully compatible with the mixOmics package and a tutorial describing the approach can be found on mixOmics web site http://mixomics.org/mixkernel/."}, {"title": "Tribe: The collaborative platform for reproducible web-based analysis of gene sets", "url": "https://www.biorxiv.org/content/early/2017/10/09/055913", "tag": "Bioinformatics", "abstract": "Background: The adoption of new bioinformatics webservers provides biological researchers with new analytical opportunities but also raises workflow challenges. These challenges include sharing collections of genes with collaborators, translating gene identifiers to the most appropriate nomenclature for each server, tracking these collections across multiple analysis tools and webservers, and maintaining effective records of the genes used in each analysis. Description: In this paper, we present the Tribe webserver (available at https://tribe.greenelab.com), which addresses these challenges in order to make multi-server workflows seamless and reproducible. This allows users to create analysis pipelines that use their own sets of genes in combinations of specialized data mining webservers and tools while seamlessly maintaining gene set version control. Tribe's web interface facilitates collaborative editing: users can share with collaborators, who can then view, download, and edit these collections. Tribe's fully-featured API allows users to interact with Tribe programmatically if desired. Tribe implements the OAuth 2.0 standard as well as gene identifier mapping, which facilitates its integration into existing servers. Access to Tribe's resources is facilitated by an easy-to-install Python application called tribe-client. We provide Tribe and tribe-client under a permissive open-source license to encourage others to download the source code and set up a local instance or to extend its capabilities. Conclusions: The Tribe webserver addresses challenges that have made reproducible multi-webserver workflows difficult to implement until now. It is open source, has a user-friendly web interface, and provides a means for researchers to perform reproducible gene set based analyses seamlessly across webservers and command line tools."}, {"title": "DGIdb 3.0: a redesign and expansion of the drug-gene interaction database", "url": "https://www.biorxiv.org/content/early/2017/10/09/200527", "tag": "Bioinformatics", "abstract": "The Drug-Gene Interaction Database (DGIdb, www.dgidb.org) consolidates, organizes, and presents drug-gene interactions and gene druggability information from papers, databases, and web resources. DGIdb normalizes content from more than thirty disparate sources and allows for user-friendly advanced browsing, searching and filtering for ease of access through an intuitive web user interface, application programming interface (API), and public cloud-based server image. DGIdb v3.0 represents a major update of the database. Nine of the previously included twenty-eight sources were updated. Six new resources were added, bringing the total number of sources to thirty-three. These updates and additions of sources have cumulatively resulted in 56,309 interaction claims. This has also substantially expanded the comprehensive catalogue of druggable genes and antineoplastic drug-gene interactions included in the DGIdb. Along with these content updates, v3.0 has received a major overhaul of its codebase, including an updated user interface, preset interaction search filters, consolidation of interaction information into interaction groups, greatly improved search response times, and upgrading the underlying web application framework. In addition, the expanded API features new endpoints which allow users to extract more detailed information about queried drugs, genes, and drug-gene interactions, including listings of PubMed IDs (PMIDs), interaction type, and other interaction metadata."}, {"title": "A transcriptome and literature guided algorithm for reconstruction of pathways to assess activity of telomere maintenance mechanisms", "url": "https://www.biorxiv.org/content/early/2017/10/09/200535", "tag": "Bioinformatics", "abstract": "Activation of telomere maintenance mechanisms (TMMs) is a crucial factor for indefinite proliferation of cancer cells. The most common TMM is based on the action of telomerase, but in some cancers telomeres are elongated via homologous recombination based alternative mechanism (ALT). Despite their importance, little is known about TMM regulation and factors responsible for TMM phenotype choice in different cells. Currently, many studies address the involvement of few genes in TMMs, but a consensus unified picture of the full process is missing. We have developed a computational biology framework combining knowledge- and data-driven approaches to aid in understanding of TMMs. It is based on a greedy algorithm with three core modules: (1) knowledge-based construction/modification of molecular pathways for telomerase-dependent and alternative TMMs, (2) coupled with gene expression data-based validation with an in-house pathway signal flow (PSF) algorithm, and (3) iteration of these two coupled steps until converging at pathway topologies that best reflect state of the art knowledge and are in maximum accordance with the data. We have used gene expression data derived from cell lines and tumor tissues and have performed extensive literature search and multiple cycles of greedy iterations until reaching TMM assessment accuracy of 100% and 77%, respectively. Availability of TMM pathways that best reflect recent knowledge and data will facilitate better understanding of TMM processes. As novel experimental findings in TMM biology emerge, and new datasets are generated, our approach may be used to further expand/improve the pathways, possibly allowing for making distinctions not only between telomerase-dependent and ALT TMMs, but also among their different subtypes. Moreover, this method may be used for assessment of TMM phenotypes from gene expression data, which is crucial for studies where experimental detection of TMM states is missing. Furthermore, it can also be used to assess TMM activities in proliferating healthy cells."}, {"title": "Vital Signs Analysis Algorithm Detects Inflammatory Response in Premature Infants with Late Onset Sepsis and Necrotizing Enterocolitis", "url": "https://www.biorxiv.org/content/early/2017/10/09/200329", "tag": "Bioinformatics", "abstract": "Background: Nonspecific clinical signs and suboptimal diagnostic tests limit accurate identification of late onset sepsis (LOS) and necrotizing enterocolitis (NEC) in premature infants, resulting in significant morbidity and antibiotic overuse. An infant's systemic inflammatory response may be identified earlier than clinical suspicion through analysis of multiple vital signs by a computerized algorithm (RALIS). Aim: To evaluate the revised RALIS algorithm for detection of LOS and NEC in preterm infants. Methods: In this nested case-control study, VS data (heart rate, respiratory rate, temperature, desaturations, bradycardias) were extracted from medical records of infants 23-32 weeks gestation. RALIS generated an output, with score \u22655 triggering an alert. Patient episodes were classified based on culture, radiograph, and antibiotic data into categories: LOS, expanded LOS, NEC, and controls. Paired t-tests, linear regression and cross-validation analyses were used to evaluate the relationship between RALIS alert and LOS/NEC. Results: Among 155 infants with 161 episodes, there were 41 expanded LOS (+ blood, CSF, urine, respiratory culture), 31 LOS (+ blood, CSF, urine), 9 NEC, and 93 controls. RALIS alert was 43.1+/-79 hours before culture in LOS (p=0.012). There was a significant association between RALIS alert and LOS/NEC (\u03b2=0.72, p<0.0001). Sensitivity and specificity for LOS/NEC were 84% and 80%, (PPV=63%; NPV=93%). The regression model demonstrated an AUC of 89.9%. Conclusions: For infants \u226432 weeks, RALIS detects systemic inflammatory responses in LOS and NEC in the first month of life. The algorithm identifies infection earlier than clinical suspicion, even for NEC with negative cultures. RALIS has high NPV to rule-out LOS and NEC, and may, after prospective validation, aid in antibiotic treatment decisions."}, {"title": "Backbone brackets and arginine tweezers delineate class I and class II aminoacyl tRNA synthetases", "url": "https://www.biorxiv.org/content/early/2017/10/09/198846", "tag": "Bioinformatics", "abstract": "All living organisms share the machinery to translate RNA into amino acid sequences. One key component of this machinery are aminoacyl tRNA synthetases, which ligate tRNAs to amino acids. Sequence analyses revealed that these enzymes evolved to complementary classes, which can be characterized by several sequence motifs. However, there are no structural motifs, which capture the core function of the classes: high specificity ligand interaction. We identified backbone brackets and arginine tweezers and show that these two motifs optimize ligand recognition with complementary mechanisms. They are the most compact and simple characteristic to distinguish the aminoacyl tRNA synthetase class I from II. These findings support the hypothesis that the evolutionary convergence regarding function of aminoacyl tRNA synthetases was balanced by a divergence regarding ligand interaction."}, {"title": "Re-Identification of Individuals in Genomic Data-Sharing Beacons via Allele Inference", "url": "https://www.biorxiv.org/content/early/2017/10/09/200147", "tag": "Bioinformatics", "abstract": "Genomic datasets are often associated with sensitive phenotypes. Therefore, the leak of membership information is a major privacy risk. Genomic beacons aim to provide a secure, easy to implement, and standardized interface for data sharing by only allowing yes/no queries on the presence of specific alleles in the dataset. Previously deemed secure against re-identification attacks, beacons were shown to be vulnerable despite their stringent policy. Recent studies have demonstrated that it is possible to determine whether the victim is in the dataset, by repeatedly querying the beacon for his/her single nucleotide polymorphisms (SNPs). In this work, we propose a novel re-identification attack and show that the privacy risk is more serious than previously thought. Using the proposed attack, even if the victim systematically hides informative SNPs (i.e., SNPs with very low minor allele frequency -MAF-), it is possible to infer the alleles at positions of interest as well as the beacon query results with very high confidence. Our method is based on the fact that alleles at different loci are not necessarily independent. We use the linkage disequilibrium and a high-order Markov chain-based algorithm for the inference. We show that in a simulated beacon with 65 individuals from the CEU population, we can infer membership of individuals with 95% confidence with only 5 queries, even when SNPs with MAF less than 0.05 are hidden. This means, we need less than 0.5% of the number of queries that existing works require, to determine beacon membership under the same conditions. We further show that countermeasures such as hiding certain parts of the genome or setting a query budget for the user would fail to protect the privacy of the participants under our adversary model."}, {"title": "Detection of complex structural variation from paired-end sequencing data", "url": "https://www.biorxiv.org/content/early/2017/10/08/200170", "tag": "Bioinformatics", "abstract": "Detecting structural variants (SVs) from sequencing data is a key problem in genome analysis, but the full diversity of SVs is not captured by most methods. We introduce the Automated Reconstruction of Complex Structural Variants (ARC-SV) method, which detects a broad class of structural variants from paired-end whole genome sequencing (WGS) data. Analysis of samples from NA12878 and HuRef suggests that complex SVs are often misclassified by traditional methods. We validated our results both experimentally and by comparison to whole genome assembly and PacBio data; ARC-SV compares favorably to existing algorithms in general and gives state-of-the-art results on complex SV detection. By expanding the range of detectable SVs compared to commonly-used algorithms, ARC-SV allows additional information to be extracted from existing WGS data."}, {"title": "Delta integrates 3D physical structure with topology and genomic data of chromosomes", "url": "https://www.biorxiv.org/content/early/2017/10/08/199950", "tag": "Bioinformatics", "abstract": "Delta is an integrative visualization and analysis platform to facilitate visually annotating and exploring the 3D physical architecture of genomes. Delta takes Hi-C or ChIA-PET contact matrix as input and predicts the topology-associated domains and chromatin loops in the genome. It then generates a physical 3D model which represents the plausible consensus 3D structure of the genome. Delta features a highly interactive visualization tool which enhances the integration of genome topology/physical structure with extensive genome annotation by juxtaposing the 3D model with diverse genomc assay outputs. Finally, by visually comparing the 3D model of the \u03b2-globin gene locus and its annotation, we speculated a plausible transitory interaction pattern in the locus. Experimental evidence was found to support this speculation by literature survey. This served as an example of intuitive hypothesis testing with the help of Delta. Availability and implementation: Delta is freely accessible from http://delta.big.ac.cn, and the source code is available at https://github.com/zhangzhwlab/delta."}, {"title": "Mapping Patient Trajectories using Longitudinal Extraction and Deep Learning in the MIMIC-III Critical Care Database", "url": "https://www.biorxiv.org/content/early/2017/10/07/177428", "tag": "Bioinformatics", "abstract": "Electronic Health Records (EHRs) contain a wealth of patient data useful to biomedical researchers. At present, both the extraction of data and methods for analyses are frequently designed to work with a single snapshot of a patient's record. Health care providers often perform and record actions in small batches over time. By extracting these care events, a sequence can be formed providing a trajectory for a patient's interactions with the health care system. These care events also offer a basic heuristic for the level of attention a patient receives from health care providers. We show that is possible to learn meaningful embeddings from these care events using two deep learning techniques, unsupervised autoencoders and long short-term memory networks. We compare these methods to traditional machine learning methods which require a point in time snapshot to be extracted from an EHR."}, {"title": "LDJump: Estimating Variable Recombination Rates from Population Genetic Data", "url": "https://www.biorxiv.org/content/early/2017/10/06/190876", "tag": "Bioinformatics", "abstract": "Recombination results in the reciprocal exchange of genetic information occurring in meiosis which increases genetic variation by producing new haplotypes. Recombination rates are heterogeneous between species and also along different genomic regions. Large fractions of recombination events are often concentrated on short segments known as recombination hotspots. In this work we statistically inferred heterogeneous recombination rates by using relevant summary statistics as explanatory variables in a regression model. We used for this purpose a frequentist segmentation algorithm with type I error control to estimate the variation in local recombination rates. Under various simulation setups we have obtained very fast and accurate estimates. We also show an example of an inference on a 103kb region of the human genome and compare our inferred historical- and population-specific hotspots with results from experimental data (sperm-typing and double strand break maps). For the analyzed region, our method shows a good congruence between historical and experimental hotspots, except for one hotspot hypothesized to be population-specific. This method is implemented in the R-package LDJump, which is available from https://github.com/PhHermann/LDJump."}, {"title": "VASC: dimension reduction and visualization of single cell RNA sequencing data by deep variational autoencoder", "url": "https://www.biorxiv.org/content/early/2017/10/06/199315", "tag": "Bioinformatics", "abstract": "Single cell RNA sequencing (scRNA-seq) is a powerful technique to analyze the transcriptomic heterogeneities in single cell level. It is an important step for studying cell sub-populations and lineages based on scRNA-seq data by finding an effective low-dimensional representation and visualization of the original data. The scRNA-seq data are much noiser than traditional bulk RNA-Seq: in the single cell level, the transcriptional fluctuations are much larger than the average of a cell population and the low amount of RNA transcripts will increase the rate of technical dropout events. In this study, we proposed VASC (deep Variational Autoencoder for scRNA-seq data), a deep multi-layer generative model, for the unsupervised dimension reduction and visualization of scRNA-seq data. It can explicitly model the dropout events and find the nonlinear hierarchical feature representations of the original data. Tested on twenty datasets, VASC shows superior performances in most cases and broader dataset compatibility compared with four state-of-the-art dimension reduction methods. Then, for a case study of pre-implantation embryos, VASC successfully re-establishes the cell dynamics and identifies several candidate marker genes associated with the early embryo development."}, {"title": "CogStack - Experiences Of Deploying Integrated Information Retrieval And Extraction Services In A Large National Health Service Foundation Trust Hospital", "url": "https://www.biorxiv.org/content/early/2017/10/05/123299", "tag": "Bioinformatics", "abstract": "Traditional health information systems are generally devised to support clinical data collection at the point of care. However, as the significance of the modern information economy expands in scope and permeates the healthcare domain, there is an increasing urgency for healthcare organisations to offer information systems that address the expectations of clinicians, researchers and the business intelligence community alike. Amongst other emergent requirements, the principal unmet need might be defined as the 3R principle (right data, right place, right time) to address deficiencies in organisational data flow while retaining the strict information governance policies that apply within the UK National Health Service (NHS). Here, we describe our work on deploying structured and unstructured data search and information extraction technologies within King's College Hospital, the management of governance concerns and the associated use cases and cost saving opportunities that such components present."}, {"title": "Maximum Entropy Methods for Extracting the Learned Features of Deep Neural Networks", "url": "https://www.biorxiv.org/content/early/2017/10/05/105957", "tag": "Bioinformatics", "abstract": "New architectures of multilayer artificial neural networks and new methods for training them are rapidly revolutionizing the application of machine learning in diverse fields, including business, social science, physical sciences, and biology. Interpreting deep neural networks, however, currently remains elusive, and a critical challenge lies in understanding which meaningful features a network is actually learning. We present a general method for interpreting deep neural networks and extracting network-learned features from input data. We describe our algorithm in the context of biological sequence analysis. Our approach, based on ideas from statistical physics, samples from the maximum entropy distribution over possible sequences, anchored at an input sequence and subject to constraints implied by the empirical function learned by a network. Using our framework, we demonstrate that local transcription factor binding motifs can be identified from a network trained on ChIP-seq data and that nucleosome positioning signals are indeed learned by a network trained on chemical cleavage nucleosome maps. Imposing a further constraint on the maximum entropy distribution also allows us to probe whether a network is learning global sequence features, such as the high GC content in nucleosome-rich regions. This work thus provides valuable mathematical tools for interpreting and extracting learned features from feed- forward neural networks."}, {"title": "Haystack: systematic analysis of the variation of epigenetic states and cell-type specific regulatory elements", "url": "https://www.biorxiv.org/content/early/2017/10/05/199067", "tag": "Bioinformatics", "abstract": "Motivation: With the increasing amount of genomic and epigenomic data in the public domain, a pressing challenge is how to integrate these data to investigate the role of epigenetic mechanisms in regulating gene expression and maintenance of cell-identity. To this end, we have implemented a computational pipeline to systematically study epigenetic variability and uncover regulatory DNA sequences that play a role in gene regulation. Results: Haystack is a bioinformatics pipeline to characterize hotspots of epigenetic variability across different cell-types as well as cell-type specific cis-regulatory elements along with their corresponding transcription factors. Our approach is generally applicable to any epigenetic mark and provides an important tool to investigate cell-type identity and the mechanisms underlying epigenetic switches during development. Additionally, we make available a set of precomputed tracks for a number of epigenetic marks across several cell types. These precomputed results may be used as an independent resource for functional annotation of the human genome. Availability: The Haystack pipeline is implemented as an open-source, multiplatform, Python package called haystack_bio available at https://github.com/pinellolab/haystack_bio."}, {"title": "Allosteric Modulation of Conformational Dynamics in Human Hsp90\u03b1: A Computational Study", "url": "https://www.biorxiv.org/content/early/2017/10/05/198341", "tag": "Bioinformatics", "abstract": "Central to Hsp90's biological function is its ability to interconvert between various conformational states. Drug targeting of Hsp90's regulatory mechanisms, including its modulation by co-chaperone association, presents as an attractive therapeutic strategy for Hsp90 associated pathologies. Here, we utilize homology modeling techniques to calculate full-length structures of human Hsp90\u03b1 in closed and partially-open conformations. Atomistic simulations of these structures demonstrated that bound ATP stabilizes the dimer by 'tensing' each protomer, while ADP and apo configurations 'relax' the complex by increasing global flexibility. Dynamic residue network analysis revealed regions of the protein involved in intra-protein communication, and identified several overlapping key communication hubs that correlate with known functional sites. Perturbation response scanning analysis identified several potential residue sites capable of modulating conformational change in favour of interstate conversion. For the ATP-bound open conformation, these sites were found to overlap with known Aha1 and client binding sites, demonstrating how naturally occurring forces associated with co-factor binding could allosterically modulate conformational dynamics."}, {"title": "Shearing in flow environment promotes evolution of social behavior in microbial populations", "url": "https://www.biorxiv.org/content/early/2017/10/04/198507", "tag": "Bioinformatics", "abstract": "It is advantageous for microbes to form social aggregates when they commonly benefit from secreting a public good. However, cooperating microbial groups can be evolutionarily unstable, since a cheating strain that does not secrete the public good can reproduce quicker and take over. Here we study the effects of fluid advection patterns on group reproduction as a mechanism to enable or enhance social behavior in microbial populations. We use a realistic advection-diffusion-reaction model to describe microbial growth and mutation in a flow environment. Social groups arise naturally from our model as self-reproducing Turing patterns that can avoid mutant takeovers at steady state. Our central finding is that flow shear enables and promotes social behavior in microbes by limiting the spread of cheating strains. Regions of the flow domain with higher shear admits high cooperativity and large population density, whereas low shear regions are devoid of life due to opportunistic mutations."}, {"title": "The Elastic Network Contact Model applied to RNA: enhanced accuracy for conformational space prediction", "url": "https://www.biorxiv.org/content/early/2017/10/04/198531", "tag": "Bioinformatics", "abstract": "Motivation: The use of Normal Mode Analysis (NMA) methods to study both protein and nucleic acid dynamics is well established. However, the most widely used coarse-grained methods are based on backbone geometry alone and do not take into account the chemical nature of the residues. Elastic Network Contact Model (ENCoM) is a coarse-grained NMA method that includes a pairwise atom-type non-bonded interaction term, which makes it sensitive to the sequence of the studied molecule. We adapted ENCoM to simulate the dynamics of ribonucleic acid (RNA) molecules. Results: ENCoM outperforms the most commonly used coarse-grained model on RNA, Anisotropic Network Model (ANM), in the prediction of b-factors, in the prediction of conformational change as measured by overlap (a measure of effective prediction of structural transitions) and in the prediction of structural variance from NMR ensembles. These benchmarks were derived from the set of all RNA structures available from the Protein Data Bank (PDB) and contain more total cases than previous studies applying NMA to RNA. We thus established ENCoM as an attractive tool for fast and accurate exploration of the conformational space of RNA molecules."}, {"title": "Comparison of RNA-seq and Microarray Platforms for Splice Event Detection using a Cross-Platform Algorithm", "url": "https://www.biorxiv.org/content/early/2017/10/04/197798", "tag": "Bioinformatics", "abstract": "RNA-seq is a reference technology for determining alternative splicing at genome-wide level. Exon arrays remain widely used for the analysis of gene expression, but show poor validation rate with regard to splicing events. Commercial arrays that include probes within exon junctions have been developed in order to overcome this problem. We compare the performance of RNA-seq (Illumina HiSeq) and junction arrays (Affymetrix Human Transcriptome array) for the analysis of transcript splicing events. Three different breast cancer cell lines were treated with CX-4945, a drug that severely affects splicing. To enable a direct comparison of the two platforms, we adapted EventPointer, an algorithm that detects and labels alternative splicing events using junction arrays, to work also on RNA-seq data. Common results and discrepancies between the technologies were validated and/or resolved by over 200 PCR experiments. As might be expected, RNA-seq appears superior in cases where the technologies disagree, and is able to discover novel splicing events beyond the limitations of physical probe-sets. We observe a high degree of coherence between the two technologies, however, with correlation of EventPointer results over 0.90. Through decimation, the detection power of the junction arrays is equivalent to RNA-seq with up to 60 million reads. Our results suggest, therefore, that exon-junction arrays are a viable alternative to RNA-seq for detection of alternative splicing events when focusing on well-described transcriptional regions."}, {"title": "Abundance-based reconstitution of microbial pan-genomes from whole-metagenome shotgun sequencing data", "url": "https://www.biorxiv.org/content/early/2017/10/04/173203", "tag": "Bioinformatics", "abstract": "Analysis toolkits for whole-metagenome shotgun sequencing data achieved strain-level characterization of complex microbial communities by capturing intra-species gene content variation. Yet, these tools are hampered by the extent of reference genomes that are far from covering all microbial variability, as many species are still not sequenced or have only few strains available. Binning co-abundant genes obtained from de novo assembly is a powerful reference-free technique for discovering and reconstituting gene repertoire of microbial species. While current methods accurately identify species core genes, they miss many accessory genes or split them in small separated clusters. We introduce MSPminer, a computationally efficient software tool that reconstitutes Metagenomic Species Pan-genomes (MSPs) by binning co-abundant genes across large-scale metagenomic datasets. MSPminer relies on a new robust measure for grouping not only species core genes but accessory genes also. In MSPs, an empirical classifier distinguishes core from accessory and shared genes. We applied MSPminer to the largest publicly available gene abundance table which is composed of 9.9M genes quantified in 1 267 stool samples. We show that MSPminer successfully reconstitutes in a matter of several hours gene repertoire of > 1600 microbial species (some hitherto unknown) and detects many more accessory genes than existing tools. By compiling the information from thousands of samples, species gene content variability is better accounted for and their quantification is subsequently more precise."}, {"title": "iREAD: A Tool For Intron Retention Detection From RNA-seq Data", "url": "https://www.biorxiv.org/content/early/2017/10/04/135624", "tag": "Bioinformatics", "abstract": "Detecting intron retention (IR) events is emerging as a specialized need for RNA-seq data analysis. Here we present iREAD (intron REtention Analysis and Detector), a tool to detect IR events genome-wide from high-throughput RNA-seq data. The command line interface for iREAD is implemented in Python. iREAD takes as input an existing BAM file, representing the transcriptome, and a text file containing the intron coordinates of a genome. It then 1) counts all reads that overlap intron regions, 2) detects IR vents by analyzing features of reads such as depth and distribution patterns, and 3) outputs a list of retained introns into a tab-delimited text file. The output can be directly used for further exploratory analysis such as differential intron expression and functional enrichment. iREAD provides a new and generic tool to interrogate poly-A enriched transcriptomic data of intron regions. Availability: www.libpls.net/iread Contact: Nathan.Price@systemsbiology.org"}, {"title": "Predicting cancer outcomes from histology and genomics using convolutional networks", "url": "https://www.biorxiv.org/content/early/2017/10/03/198010", "tag": "Bioinformatics", "abstract": "Cancer histology reflects underlying molecular processes and disease progression, and contains rich phenotypic information that is predictive of patient outcomes. In this study, we demonstrate a computational approach for learning patient outcomes from digital pathology images using deep learning to combine the power of adaptive machine learning algorithms with traditional survival models. We illustrate how this approach can integrate information from both histology images and genomic biomarkers to predict time-to-event patient outcomes, and demonstrate performance surpassing the current clinical paradigm for predicting the survival of patients diagnosed with glioma. We also provide techniques to visualize the tissue patterns learned by these deep learning survival models, and establish a framework for addressing intratumoral heterogeneity and training data deficits."}, {"title": "Comparative Transcriptomics of Mango (Mangifera indica L.) Cultivars Provide Insights of Biochemical Pathways Involved in Flavor and Color", "url": "https://www.biorxiv.org/content/early/2017/10/03/196881", "tag": "Bioinformatics", "abstract": "Mango is an economically important fruit crop of many tropical and subtropical countries. Recently, leaf and fruit transcriptomes of mango cultivars grown in different geographical regions have characterized. Here, we presented comparative transcriptome analysis of four mango cultivars i.e. cv. Langra, cv. Zill, cv. Shelly and cv. Kent from Pakistan, China, Israel and Mexico respectively. De-novo sequence assembly generated 30,953-85,036 unigenes from RNA-Seq datasets of mango cultivars. KEGG pathway mapping of mango unigenes identified terpenoids, flavonoids and carotenoids biosynthetic pathways involved in flavor and color. The analysis revealed linalool as major monoterpenoid found in all cultivars studied whereas, monoterpene \u03b1-terpineol was specifically found in cv. Shelly. Ditepene gibberellin biosynthesis pathway was found in all cultivars whereas, homoterpene synthase involved in biosynthesis of 4,8,12-trimethyltrideca-1,3,7,11-tetraene (TMTT; an insect induced diterpene) was found in cv. Kent. Among sesquiterpenes and triterpenes, biosynthetic pathway of Germacrene-D, an anti-bacterial and anti-insecticidal metabolite was found in cv. Zill and cv. Shelly. Two bioactive triterpenes, lupeol and \u03b2-amyrin were found in cv. Langra and cv. Zill. Unigenes involved in biosynthesis of carotenoids, \u03b2-carotene and lycopene, were found in cultivars studied. Many unigenes involved in flavonoid biosynthesis were also found. Comparative transcriptomics revealed naringenin (an anti-inflammatory and antioxidant metabolite) as central flavanone responsible for biosynthesis of an array of flavonoids. The present study provided insights on genetic resources responsible for flavor and color of mango fruit."}, {"title": "Association Analysis and Meta-Analysis of Multi-allelic Variants for Large Scale Sequence Data", "url": "https://www.biorxiv.org/content/early/2017/10/03/197913", "tag": "Bioinformatics", "abstract": "Motivation: There is great interest to understand the impact of rare variants in human diseases using large sequence datasets. In deep sequences datasets of >10,000 samples, ~10% of the variant sites are observed to be multi-allelic. Many of the multi-allelic variants have been shown to be functional and disease relevant. Proper analysis of multi-allelic variants is critical to the success of a sequencing study, but existing methods do not properly handle multi-allelic variants and can produce highly misleading association results. Results: We propose novel methods to encode multi-allelic sites, conduct single variant and gene-level association analyses, and perform meta-analysis for multi-allelic variants. We evaluated these methods through extensive simulations and the study of a large meta-analysis of ~18,000 samples on the cigarettes-per-day phenotype. We showed that our joint modeling approach provided an unbiased estimate of genetic effects, greatly improved the power of single variant association tests, and enhanced gene-level tests over existing approaches. Availability: Software packages implementing these methods are available at (https://github.com/zhanxw/rvtests http://genome.sph.umich.edu/wiki/RareMETAL)."}, {"title": "FastViromeExplorer: A Pipeline for Virus and Phage Identification and Abundance Profiling in Metagenomics Data", "url": "https://www.biorxiv.org/content/early/2017/10/03/196998", "tag": "Bioinformatics", "abstract": "Identifying viruses and phages in a metagenomics sample has important implication in improving human health, preventing viral outbreaks, and developing personalized medicine. With the rapid increase in data files generated by next generation sequencing, existing tools for identifying and annotating viruses and phages in metagenomics samples suffer from expensive running time. In this paper, we developed a stand-alone pipeline, FastViromeExplorer, for rapid identification and abundance quantification of viruses and phages in big metagenomic data. Both real and simulated data validated FastViromeExplorer as a reliable tool to accurately identify viruses and their abundances in large data, as well as in a time efficient manner."}, {"title": "Exploring the phenotypic consequences of tissue specific gene expression variation inferred from GWAS summary statistics", "url": "https://www.biorxiv.org/content/early/2017/10/03/045260", "tag": "Bioinformatics", "abstract": "Scalable, integrative methods to understand mechanisms that link genetic variants with phenotypes are needed. Here we derive a mathematical expression to compute PrediXcan (a gene mapping approach) results using summary data (S-PrediXcan) and show its accuracy and general robustness to misspecified reference sets. We apply this framework to 44 GTEx tissues and 100+ phenotypes from GWAS and meta-analysis studies, creating a growing public catalog of associations that seeks to capture the effects of gene expression variation on human phenotypes. Replication in an independent cohort is shown. Most of the associations were tissue specific, suggesting context specificity of the trait etiology. Colocalized significant associations in unexpected tissues underscore the need for an agnostic scanning of multiple contexts to improve our ability to detect causal regulatory mechanisms. Monogenic disease genes are enriched among significant associations for related traits, suggesting that smaller alterations of these genes may cause a spectrum of milder phenotypes."}, {"title": "Reverse Association Of Single Cells To Rheumatoid Arthritis Accounting For Mixed Effects Identifies An Expanded CD27- HLA-DR+ Effector Memory CD4+ T Cell Population", "url": "https://www.biorxiv.org/content/early/2017/10/02/172403", "tag": "Bioinformatics", "abstract": "High dimensional single-cell analyses have dramatically improved the ability to resolve complex mixtures of cells from human disease samples; however, identifying disease-associated cell types or cell states in patient samples remains challenging due to technical and inter-individual variation. Here we present Mixed effects modeling of Associations of Single Cells (MASC), a novel reverse single cell association strategy for testing whether case-control status influences the membership of single cells in any of multiple cellular subsets while accounting for technical confounds and biological variation. Applying MASC to mass cytometry analyses of CD4+ T cells from blood of rheumatoid arthritis (RA) patients and controls revealed a significantly expanded population of CD4+ T cells, identified as CD27- HLA-DR+ effector memory cells, in RA patients (OR = 1.7; p = 1.1 x 10-3). The frequency of CD27- HLA-DR+ cells was similarly elevated in blood samples from a second RA patient cohort, and CD27- HLA-DR+ cell frequency decreased in RA patients who respond to immunosuppressive therapy. Compared to peripheral blood, synovial fluid and synovial tissue samples from RA patients contained ~5-fold higher frequencies of CD27- HLA-DR+ cells, which comprised ~10% of synovial CD4+ T cells. We find that CD27- HLA-DR+ cells are abundant producers of IFN-\u03b3; and also express perforin and granzyme A at elevated levels. Thus MASC identified the expansion of a unique Th1 skewed effector T cell population with cytotoxic capacity in RA. We propose that MASC is a broadly applicable method to identify disease-associated cell populations in high-dimensional single cell data."}, {"title": "C3D: A tool to predict 3D genomic interactions between cis-regulatory elements", "url": "https://www.biorxiv.org/content/early/2017/10/02/197301", "tag": "Bioinformatics", "abstract": "Motivation: The 3D genome architecture influences the regulation of genes by facilitating chromatin interactions between distal cis-regulatory elements and gene promoters. We implement Cross Cell-type Correlation based on DNA accessibility (C3D), a highly customizable computational tool that predicts chromatin interactions using an unsupervised algorithm that utilizes correlations in chromatin measurements, such as DNaseI hypersensitivity signals. Results: C3D accurately predicts 32.7%, 18.3% and 24.1% of interactions, validated by ChIA-PET assays, between promoters and distal regions that overlie DNaseI hypersensitive sites in K562, MCF-7 and GM12878 cells, respectively. Availability: Source code is open-source and freely available on GitHub (https://github.com/LupienLabOrganization/C3D) under the GNU GPLv3 license. C3D is implemented in Bash and R; it runs on any platform with Bash (\u22654.0), R (\u22653.1.1) and BEDTools (\u22652.19.0). It requires the following R packages: GenomicRanges, Sushi, data.table, preprocessCore and dynamicTreeCut."}, {"title": "Modeling transcription factor combinatorics in promoters and enhancers", "url": "https://www.biorxiv.org/content/early/2017/10/02/197418", "tag": "Bioinformatics", "abstract": "We propose a new approach (TFcoop) that takes into account cooperation between transcription factors (TFs) for predicting TF binding sites. For a given a TF, TFcoop bases its prediction upon the binding affinity of the target TF as well as any other TF identified as cooperating with this TF. The set of cooperating TFs and the model parameters are learned from ChIP-seq data of the target TF. We used TFcoop to investigate the TF combinations involved in the binding of 106 different TFs on 41 different cell types and in four different regulatory regions: promoters of mRNAs, lncRNAs and pri-miRNAs, and enhancers. Our experiments show that the approach is accurate and outperforms simple PWM methods. Moreover, analysis of the learned models sheds light on important properties of TF combinations. First, for a given TF and region, we show that TF combinations governing the binding of the target TF are similar for the different cell-types. Second, for a given TF, we observe that TF combinations are different between promoters and enhancers, but similar for promoters of distinct gene classes (mRNAs, lncRNAs and miRNAs). Analysis of the TFs cooperating with the different targets show over-representation of pioneer TFs and a clear preference for TFs with binding motif composition similar to that of the target. Lastly, our models accurately distinguish promoters into classes associated with specific biological processes."}, {"title": "A framework for generating interactive reports for cancer genome analysis", "url": "https://www.biorxiv.org/content/early/2017/10/02/194035", "tag": "Bioinformatics", "abstract": "We introduce paplot, the software for generating dynamic reports that are frequently necessary in the post analytical phases of cancer genome studies. The \"interactive\" nature of the paplot-generated reports enables users to extract much richer information than that obtained from static graphs via most conventional visualization tools. The python implementation for paplot (MIT license) is available at https://github.com/Genomon-Project/paplot. The documentation is at http://paplot-doc.readthedocs.io/en/latest/."}, {"title": "Deep Convolutional Neural Networks Enable Discrimination of Heterogeneous Digital Pathology Images", "url": "https://www.biorxiv.org/content/early/2017/10/02/197517", "tag": "Bioinformatics", "abstract": "Pathological evaluation of tumor tissue is pivotal for diagnosis in cancer patients and automated image analysis approaches have great potential to increase precision of diagnosis and help reduce human error. In this study, we utilize various computational methods based on convolutional neural networks (CNN) and build a stand-alone pipeline to effectively classify different histopathology images across different types of cancer. In particular, we demonstrate the utility of our pipeline to discriminate between two subtypes of lung cancer, four biomarkers of bladder cancer, and five biomarkers of breast cancer. In addition, we apply our pipeline to discriminate among four immunohistochemistry (IHC) staining scores of bladder and breast cancers. Our classification pipeline utilizes a basic architecture of CNN, Google's Inceptions within three training strategies, and an ensemble of two state-of-the-art algorithms, Inception and ResNet. These strategies include training the last layer of Google's Inceptions, training the network from scratch, and fine-tunning the parameters for our data using two pre-trained version of Google's Inception architectures, Inception-V1 and Inception-V3. We demonstrate the power of deep learning approaches for identifying cancer subtypes, and the robustness of Google's Inceptions even in presence of extensive tumor heterogeneity. Our pipeline on average achieved accuracies of 100% , 92%, 95%, and 69% for discrimination of various cancer types, subtypes, biomarkers, and scores, respectively. Our pipeline and related documentation is freely available at https://github.com/ih-lab/CNN_Smoothie"}, {"title": "Functional network community detection can disaggregate and filter multiple underlying pathways in enrichment analyses", "url": "https://www.biorxiv.org/content/early/2017/10/02/166207", "tag": "Bioinformatics", "abstract": "Differential expression experiments or other analyses often end in a list of genes. Pathway enrichment analysis is one method to discern important biological signals and patterns from noisy expression data. However, pathway enrichment analysis may perform suboptimally in situations where there are multiple implicated pathways - such as in the case of genes that define subtypes of complex diseases. Our simulation study shows that in this setting, standard overrepresentation analysis identifies many false positive pathways along with the true positives. These false positives hamper investigators' attempts to glean biological insights from enrichment analysis. We develop and evaluate an approach that combines community detection over functional networks with pathway enrichment to reduce false positives. Our simulation study demonstrates that a large reduction in false positives can be obtained with a small decrease in power. Though we hypothesized that multiple communities might underlie previously described subtypes of high-grade serous ovarian cancer and applied this approach, our results do not support this hypothesis. In summary, applying community detection before enrichment analysis may ease interpretation for complex gene sets that represent multiple distinct pathways."}, {"title": "Extracting a Biologically Relevant Latent Space from Cancer Transcriptomes with Variational Autoencoders", "url": "https://www.biorxiv.org/content/early/2017/10/02/174474", "tag": "Bioinformatics", "abstract": "The Cancer Genome Atlas (TCGA) has profiled over 10,000 tumors across 33 different cancer-types for many genomic features, including gene expression levels. Gene expression measurements capture substantial information about the state of each tumor. Certain classes of deep neural network models are capable of learning a meaningful latent space. Such a latent space could be used to explore and generate hypothetical gene expression profiles under various types of molecular and genetic perturbation. For example, one might wish to use such a model to predict a tumor's response to specific therapies or to characterize complex gene expression activations existing in differential proportions in different tumors. Variational autoencoders (VAEs) are a deep neural network approach capable of generating meaningful latent spaces for image and text data. In this work, we sought to determine the extent to which a VAE can be trained to model cancer gene expression, and whether or not such a VAE would capture biologically-relevant features. In the following report, we introduce a VAE trained on TCGA pan-cancer RNA-seq data, identify specific patterns in the VAE encoded features, and discuss potential merits of the approach. We name our method \"Tybalt\" after an instigative, cat-like character who sets a cascading chain of events in motion in Shakespeare's Romeo and Juliet. From a systems biology perspective, Tybalt could one day aid in cancer stratification or predict specific activated expression patterns that would result from genetic changes or treatment effects."}, {"title": "Data-driven Assessment of Structural Image Quality", "url": "https://www.biorxiv.org/content/early/2017/10/01/125161", "tag": "Bioinformatics", "abstract": "Data quality is increasingly recognized as one of the most important confounding factors in brain imaging research. It is particularly important for studies of brain development, where age is systematically related to in-scanner motion and data quality. Prior work has demonstrated that in-scanner head motion biases estimates of structural neuroimaging measures. However, objective measures of data quality are not available for most structural brain images. Here we sought to identify quantitative measures of data quality for T1-weighted volumes, describe how such measures of quality relate to cortical thickness, and delineate how this in turn may bias inference regarding associations with age in youth. Three highly-trained raters provided manual ratings of 1,840 raw T1-weighted volumes. These images included a training set of 1,065 images from Philadelphia Neurodevelopmental Cohort (PNC), a test set of 533 images from the PNC, as well as an external test set of 242 adults acquired on a different scanner. Manual ratings were compared to automated quality measures provided by the Preprocessed Connectomes Project's Quality Assurance Protocol (QAP), as well as FreeSurfer's Euler number, which summarizes the topological complexity of the reconstructed cortical surface. Results revealed that the Euler number was consistently correlated with manual ratings across samples. Furthermore, the Euler number could be used to identify images scored \"unusable\" by human raters with a high degree of accuracy (AUC: 0.98-0.99), and out-performed proxy measures from functional timeseries acquired in the same scanning session. The Euler number also was significantly related to cortical thickness in a regionally heterogeneous pattern that was consistent across datasets and replicated prior results. Finally, data quality both inflated and obscured associations with age during adolescence. Taken together, these results indicate that reliable measures of data quality can be automatically derived from T1-weighted volumes, and that failing to control for data quality can systematically bias the results of studies of brain maturation."}, {"title": "How is structural divergence related to evolutionary information?", "url": "https://www.biorxiv.org/content/early/2017/10/01/196782", "tag": "Bioinformatics", "abstract": "Conservation and covariation measures, as other evolutionary analysis, require a high number of distant homologous sequences, therefore a lot of structural divergence can be expected in such divergent alignments. However, most works linking evolutionary and structural information use a single structure ignoring the structural variability inside a protein family. That common practice seems unrealistic to the light of this work. In this work we studied how structural divergence affects conservation and covariation estimations. We uncover that, within a protein family, ~51% of multiple sequence alignment columns change their exposed/buried status between structures. Also, ~53% of residue pairs that are in contact in one structure are not in contact in another structure from the same family. We found out that residue conservation is not directly related to the relative solvent accessible surface area of a single protein structure. Using information from all the available structures rather than from a single representative structure gives more confidence in the structural interpretation of the evolutionary signals. That is particularly important for diverse multiple sequence alignments, where structures can drastically differ. High covariation scores tend to indicate residue contacts that are conserved in the family, therefore, are not suitable to find protein/conformer specific contacts. Our results suggest that structural divergence should be considered for a better understanding of protein function, to transfer annotation by homology and to model protein evolution."}, {"title": "The Impact of Random Models on Clustering Similarity", "url": "https://www.biorxiv.org/content/early/2017/10/01/196840", "tag": "Bioinformatics", "abstract": "Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, multiple runs of K-means clustering returns clusterings with a fixed number of clusters, while the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on the ranking of similar clustering pairs, and the evaluation of a clustering method with respect to a random baseline; thus, the choice of random clustering model should be carefully justified."}, {"title": "Fast and Accurate Genomic Analyses using Genome Graphs", "url": "https://www.biorxiv.org/content/early/2017/09/29/194530", "tag": "Bioinformatics", "abstract": "The human reference genome serves as the foundation for genomics by providing a scaffold for sequencing read alignment, but currently only reflects a single consensus haplotype, impairing read alignment and downstream analysis accuracy. Reference genome structures incorporating known genetic variation have been shown to improve the accuracy of genomic analyses, but have so far remained computationally prohibitive for routine large-scale use. Here we present a graph genome implementation that enables read alignment across 2,800 diploid genomes encompassing 12.6 million SNPs and 4.0 million indels. Our graph genome aligner and variant calling pipeline consume around 5.5 and 2 hours per high coverage whole-genome-sequenced sample, respectively, comparable to those of state-of-the-art linear reference genome-based methods. Using orthogonal benchmarks based on real and simulated data, we show that using a graph genome reference improves read mapping sensitivity and produces a 0.5 percentage point increase in variant calling recall, which extrapolates into 20,000 additional variants being detected per sample, while variant calling specificity is unaffected. Structural variations (SVs) incorporated into a graph genome can be directly genotyped from read alignments in a rapid and accurate fashion. Finally, we show that iterative augmentation of graph genomes yields incremental gains in variant calling accuracy. Our implementation is the first practical step towards fulfilling the promise of graph genomes to radically enhance the scalability and precision of genomic analysis by incorporating prior knowledge of population characteristics."}, {"title": "SAMSA2: A standalone metatranscriptome analysis pipeline", "url": "https://www.biorxiv.org/content/early/2017/09/29/195826", "tag": "Bioinformatics", "abstract": "Background: Complex microbial communities are an area of rapid growth in biology. Metatranscriptomics allows one to investigate the gene activity in an environmental sample via high-throughput sequencing. Metatranscriptomic experiments are computationally intensive because the experiments generate a large volume of sequence data and the sequences must be compared with many references. Results: Here we present SAMSA2, an upgrade to the original Simple Annotation of Metatranscriptomes by Sequence Analysis (SAMSA) pipeline that has been redesigned for use on a supercomputing cluster. SAMSA2 is faster due to the use of the DIAMOND aligner, and more flexible and reproducible because it uses local databases. SAMSA2 is available with detailed documentation, and example input and output files along with examples of master scripts for full pipeline execution. Conclusions: Using publicly available example data, we demonstrate that SAMSA2 is a rapid and efficient metatranscriptome pipeline for analyzing large paired-end RNA-seq datasets in a supercomputing cluster environment. SAMSA2 provides simplified output that can be examined directly or used for further analyses, and its reference databases may be upgraded, altered or customized to fit the specifics of any experiment."}, {"title": "OpenEHR modeling for genomics in clinical practice", "url": "https://www.biorxiv.org/content/early/2017/09/28/194720", "tag": "Bioinformatics", "abstract": "The increasing usage of high throughput sequencing in personalized medicine brings new challenges to the realm of healthcare informatics. Patient records need to accommodate data of unprecedented size and complexity as well as keep track of their production process. In this work we present a solution for integrating genomic data into electronic health records via openEHR archetypes. We introduce new genomics-specific archetypes based on the popular variant call format and show their applicability to a practical use case. Finally, we discuss their structure in comparison with the HL7\u00ae FHIR\u00ae standard."}, {"title": "ChromoTrace: Reconstruction of 3D Chromosome Configurations by Super-Resolution Microscopy", "url": "https://www.biorxiv.org/content/early/2017/09/28/115436", "tag": "Bioinformatics", "abstract": "Motivation: The 3D structure of chromatin plays a key role in genome function, in- cluding gene expression, DNA replication, chromosome segregation, and DNA repair. Furthermore the location of genomic loci within the nucleus, especially relative to each other and nuclear structures such as the nuclear envelope and nuclear bodies strongly correlates with aspects of function such as gene expression. Therefore, determining the 3D position of the 6 billion DNA base pairs in each of the 23 chromosomes inside the nucleus of a human cell is a central challenge of biology. Recent advances of super- resolution microscopy in principle enable the mapping of specific molecular features with nanometer precision inside cells. Combined with highly specific, sensitive and multiplexed fluorescence labeling of DNA sequences this opens up the possibility of mapping the 3D path of the genome sequence in situ. Results: Here we develop computational methodologies to reconstruct the sequence configuration of all human chromosomes in the nucleus from a super-resolution image of a set of fluorescent in situ probes hybridized to the genome in a cell. To test our approach we develop a method for the simulation of chromatin packing in an idealized human nucleus. Our reconstruction method, ChromoTrace, uses suffix trees to assign a known linear ordering of in situ probes on the genome to an unknown set of 3D in situ probe positions in the nucleus from super-resolved images using the known genomic probe spacing as a set of physical distance constraints between probes. We find that ChromoTrace can assign the 3D positions of the majority of loci with high accuracy and reasonable sensitivity to specific genome sequences. By simulating spatial resolution, label multiplexing and noise scenarios we assess algorithm performance under realistic experimental constraints. Our study shows that it is feasible to achieve genome-wide reconstruction of the 3D DNA path in chromatin based on super-resolution microscopy images."}, {"title": "PharmacoDB: an integrative database for mining in vitro anticancer drug screening studies", "url": "https://www.biorxiv.org/content/early/2017/09/27/195149", "tag": "Bioinformatics", "abstract": "Recent pharmacogenomic studies profiled large panels of cancer cell lines against hundreds of approved drugs and experimental chemical compounds. The overarching goal of these screens is to measure sensitivity of cell lines to chemical perturbation, correlate these measures to genomic features, and thereby develop novel predictors of drug response. However, leveraging this valuable data is challenging due to the lack of standards for annotating cell lines and chemical compounds, and quantifying drug response. Moreover, it has been recently shown that the complexity and complementarity of the experimental protocols used in the field result in high levels of technical and biological variation in the in vitro pharmacological profiles. There is therefore a need for new tools to facilitate rigorous comparison and integrative analysis of large-scale drug screening datasets. To address this issue, we have developed PharmacoDB (pharmacodb.pmgenomics.ca), a database integrating the largest pharmacogenomic studies published to date. Here, we describe how the curation of cell line and chemical compound identifiers maximizes the overlap between datasets and how users can leverage such data to compare and extract robust drug phenotypes. PharmacoDB provides a unique resource to mine a compendium of curated pharmacogenomic datasets that are otherwise disparate and difficult to integrate."}, {"title": "A coarse-graining, ultrametric approach to resolve the phylogeny of prokaryotic strains with frequent recombination", "url": "https://www.biorxiv.org/content/early/2017/09/27/094599", "tag": "Bioinformatics", "abstract": "Introduction: Homologous recombination happens when a foreign DNA stretch replaces a similar stretch on the genome of a prokaryotic cell. For a genome pair, recombination affects their phylogenetic reconstruction in multiple ways: (i) a genome can recombine with a DNA stretch that is similar to the other genome of the pair, thereby reducing their pairwise sequence divergence; (ii) a genome can also recombine with a stretch from an outgroup- genome and increase the pairwise divergence. Most phylogenetic algorithms cannot account for recombination; while some do, they cannot account for all effects of recombination. Results: We develop a fast algorithm that reconstructs ultrametric-trees while explicitly accounting for recombination. Instead of considering individual positions of genome sequences, we use a coarse-graining approach, which divides a genome sequence into short segments to account for local density of nucleotide-substitution. For each genome pair considered, our coarse-graining-phylogenetic (CGP) algorithm enumerates the pairwise single-site-polymorphisms (SSPs) on each segment to obtain the pairwise SSP-distribution; we fit each empirical SSP-distribution to a theoretical SSP-distribution. We test the accuracy of our algorithm against other state-of-the-art algorithms on simulated and real genomes. For genomes with a substantial level of recombination, such as E. coli, we show that the age prediction of internal nodes by CGP is more accurate than other algorithms, while the tree topology is at least as accurate. Conclusion: The CGP algorithm is more accurate and faster than alternative recombination-aware methods for ultrametric phylogenetic reconstructions."}, {"title": "A distributed algorithm to maintain and repair the trail networks of arboreal ants", "url": "https://www.biorxiv.org/content/early/2017/09/27/194480", "tag": "Bioinformatics", "abstract": "We study how the arboreal turtle ant (Cephalotes goniodontus) solves a fundamental computing problem: maintaining a trail network and finding alternative paths to route around broken links in the network. Turtle ants form a routing backbone of foraging trails linking several nests together. This species travels only in the trees, so their foraging trails are constrained to lie on a natural graph formed by overlapping branches and vines in the tangled canopy. Links between branches, however, can be ephemeral, easily destroyed by wind, rain, or animal movements. Here we report a biologically feasible distributed algorithm, parameterized using field data, to describe how turtle ants maintain the routing backbone and find alternative paths to circumvent broken links in the backbone. We validate the ability of this probabilistic algorithm to circumvent simulated breaks in synthetic and real-world networks, and we derive an analytic explanation for why the algorithm succeeds under certain conditions. The turtle ant algorithm uses fewer computational resources than common distributed graph search algorithms, and thus may be useful in other domains, such as for swarm computing or for coordinating molecular robots."}, {"title": "Reconstruction of developmental landscapes by optimal-transport analysis of single-cell gene expression sheds light on cellular reprogramming.", "url": "https://www.biorxiv.org/content/early/2017/09/27/191056", "tag": "Bioinformatics", "abstract": "Understanding the molecular programs that guide cellular differentiation during development is a major goal of modern biology. Here, we introduce an approach, WADDINGTON-OT, based on the mathematics of optimal transport, for inferring developmental landscapes, probabilistic cellular fates and dynamic trajectories from large-scale single-cell RNA-seq (scRNA-seq) data collected along a time course. We demonstrate the power of WADDINGTON-OT by applying the approach to study 65,781 scRNA-seq profiles collected at 10 time points over 16 days during reprogramming of fibroblasts to iPSCs. We construct a high-resolution map of reprogramming that rediscovers known features; uncovers new alternative cell fates including neural- and placental-like cells; predicts the origin and fate of any cell class; highlights senescent-like cells that may support reprogramming through paracrine signaling; and implicates regulatory models in particular trajectories. Of these findings, we highlight Obox6, which we experimentally show enhances reprogramming efficiency. Our approach provides a general framework for investigating cellular differentiation."}, {"title": "Profiling DNA Methylation Differences Between Inbred Mouse Strains on the Illumina Human Infinium MethylationEPIC Microarray", "url": "https://www.biorxiv.org/content/early/2017/09/27/194464", "tag": "Bioinformatics", "abstract": "The Illumina Infinium MethylationEPIC provides an efficient platform for profiling DNA methylation in humans at over 850,000 CpGs. Model organisms such as mice do not currently benefit from an equivalent array. Here we used this array to measure DNA methylation in mice. We defined probes targeting conserved regions and performed a comparison between the array-based assay and affinity-based DNA sequencing of methyl-CpGs (MBD-seq). Mouse samples consisted of 11 liver DNA from two strains, C57BL/6J (B6) and DBA/2J (D2), that varied widely in age. Linear regression was applied to detect differential methylation. In total, 13,665 probes (1.6% of total probes) aligned to conserved CpGs. Beta-values (\u03b2-value) for these probes showed a distribution similar to that in humans. Overall, there was high concordance in methylation signal between the EPIC array and MBD-seq (Pearson correlation r = 0.70, p-value < 0.0001). However, the EPIC probes had higher quantitative sensitivity at CpGs that are hypo- (\u03b2-value < 0.3) or hypermethylated (\u03b2-value > 0.7). In terms of differential methylation, no EPIC probe detected significant difference between age groups at a Benjamini-Hochberg threshold of 10%, and the MBD-seq performed better at detecting age-dependent change in methylation. However, the top most significant probe for age (cg13269407; uncorrected p-value = 1.8 x 10-5) is part of the clock CpGs used to estimate the human epigenetic age. For strain, 219 Infinium probes detected significant differential methylation (FDR cutoff 10%) with ~80% CpGs associated with higher methylation in D2. This higher methylation profile in D2 compared to B6 was also replicated by the MBD-seq data. To summarize, we found only a small subset of EPIC probes that target conserved sites. However, for this small subset the array provides a reliable assay of DNA methylation and can be effectively used to measure differential methylation in mice."}, {"title": "MeDEStrand: an improved method to infer genome-wide absolute methylation level from DNA enrichment experiment", "url": "https://www.biorxiv.org/content/early/2017/09/27/194431", "tag": "Bioinformatics", "abstract": "Background: DNA methylation of dinucleotide CpG is an essential epigenetic modification that plays a key role in transcription. Bisulfite conversion method is a gold standard for DNA methylation profiling that provides single nucleotide resolution. However, whole-genome bisulfite conversion is very expensive. Alternatively, DNA enrichment-based methods offer high coverage of methylated CpG dinucleotides with the lowest cost per CpG covered genome-wide and have been used widely. They measure the DNA enrichment of methyl-CpG binding, therefore do not directly provide absolute methylation levels. Further, the enrichment is influenced by confounding factors besides the methylation status, e.g., CpG density. Computational models that can accurately derive the absolute methylation levels from the enrichment data are necessary. Results: We present MeDEStrand, a method uses sigmoid function to estimate and correct the CpG bias from the numbers of reads that fell within bins that divide the genome. In addition, unlike the previous methods, which estimate CpG bias based on reads mapped at the same genomic loci, MeDEStrand processes the reads for the positive and negative DNA strands separately. We compare the performance of MeDEStrand with three other state-of-the-art methods MEDIPS, BayMeth and QSEA on four independent datasets generated using immortalized cell lines (GM12878 and K562) and human patient primary cells (foreskin fibroblast and mammary epithelial). Based on the comparison between the inferred absolute methylation levels from MeDIP-seq and the corresponding RRBS data, MeDEStrand shows the best performance at high resolution of 25, 50 and 100 base pairs. Conclusions: MeDEStrand benefits from the estimation of CpG bias with a sigmoid function and the procedure to process reads mapped to the positive and negative DNA strands separately. MeDEStrand is a tool to infer whole-genome absolute DNA methylation level at the cost of enrichment-based methods with adequate accuracy and resolution. R package MeDEStrand and its tutorial is freely available for download at https://github.com/jxu1234/MeDEStrand.git."}, {"title": "Gauss-power mixing distributions comprehensively describe stochastic variations in RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/09/26/194118", "tag": "Bioinformatics", "abstract": "Motivation: Gene expression levels exhibit stochastic variations among genetically identical organisms under the same environmental conditions. In many recent transcriptome analyses based on RNA sequencing (RNA-seq), variations in gene expression levels among replicates were assumed to follow a negative binomial distribution although the physiological basis of this assumption remain unclear. Results: In this study, RNA-seq data were obtained from Arabidopsis thaliana under eight conditions (21-27 replicates), and the characteristics of gene-dependent distribution profiles of gene expression levels were analyzed. For A. thaliana and Saccharomyces cerevisiae, the distribution profiles could be described by a Gauss-power mixing distribution derived from a simple model of a stochastic transcriptional network containing a feedback loop. The distribution profiles of gene expression levels were roughly classified as Gaussian, power law-like containing a long tail, and mixed. The fitting function predicted that gene expression levels with long-tailed distributions would be strongly influenced by feedback regulation. Thus, the features of gene expression levels are correlated with their functions, with the levels of essential genes tending to follow a Gaussian distribution and those of genes encoding nucleic acid-binding proteins and transcription factors exhibiting long-tailed distributions. Availability: Fastq files of RNA-seq experiments were deposited into the DNA Data Bank of Japan Sequence Read Archive as accession no. DRA005887. Quantified expression data are available in supplementary information."}, {"title": "A literature review at genome scale: improving clinical variant assessment", "url": "https://www.biorxiv.org/content/early/2017/09/26/193870", "tag": "Bioinformatics", "abstract": "Introduction: Over 150,000 variants have been reported to cause Mendelian disease in the medical literature. It is still difficult to leverage this knowledge base in clinical practice as many reports lack strong statistical evidence or may include false associations. Clinical laboratories assess whether these variants (along with newly observed variants that are adjacent to these published ones) underlie clinical disorders. Materials and Methods: We measured whether citation data -- including journal impact factor and the number of cited variants (NCV) in each gene with published disease associations -- can be used to improve variant assessment. Results: Surprisingly, we find that impact factor is not predictive of pathogenicity, but the NCV score for each gene can provide statistical support of pathogenicity. When combining this gene-level citation metric with variant-level evolutionary conservation and structural features, classification accuracy reaches 89.5%. Further, variants identified in clinical exome sequencing cases have higher NCV scores than simulated rare variants from ExAC in matched genes and consequences (p<2.22x10-16). Discussion: Aggregate citation data can complement existing variant-based predictive algorithms, and can boost their performance without accessing and reviewing large numbers of manuscripts. The NCV is a slow-growing metric of scientific knowledge about each gene's association with disease."}, {"title": "Nanopore-based single molecule sequencing of the D4Z4 array responsible for facioscapulohumeral muscular dystrophy", "url": "https://www.biorxiv.org/content/early/2017/09/26/157040", "tag": "Bioinformatics", "abstract": "Subtelomeric macrosatellite repeats are difficult to sequence using conventional sequencing methods owing to the high similarity among repeat units and high GC content. Sequencing these repetitive regions is challenging, even with recent improvements in sequencing technologies. Among these repeats, a haplotype carrying a particular sequence and shortening of the D4Z4 array on human chromosome 4q35 causes one of the most prevalent forms of muscular dystrophy with autosomal-dominant inheritance, facioscapulohumeral muscular dystrophy (FSHD). Here, we applied a nanopore-based ultra-long read sequencer to sequence a BAC clone containing 13 D4Z4 repeats and flanking regions. We successfully obtained the whole D4Z4 repeat sequence, including the pathogenic gene DUX4 in the last D4Z4 repeat. The estimated sequence accuracy of the total repeat region was 99.8% based on a comparison with the reference sequence. Errors were typically observed between purine or between pyrimidine bases. Further, we analyzed the D4Z4 sequence from publicly available ultra-long whole human genome sequencing data obtained by nanopore sequencing. This technology may be a new tool for studying D4Z4 repeats and pathomechanism of FSHD in the future and has the potential to widen our understanding of subtelomeric regions."}, {"title": "DAFi: A Directed Recursive Filtering and Clustering Approach to Data-Driven Identification of Cell Populations from Polychromatic Flow Cytometry Data", "url": "https://www.biorxiv.org/content/early/2017/09/26/193912", "tag": "Bioinformatics", "abstract": "Computational methods for identification of cell populations from high-dimensional flow cytometry data are changing the paradigm of cytometry bioinformatics. Data clustering is the most common computational approach to unsupervised identification of cell populations from multidimensional cytometry data. We found that combining recursive filtering and clustering with constraints converted from the user manual gating strategy can effectively identify overlapping and rare cell populations from smeared data that would have been difficult to resolve by either a single run of data clustering or manual segregation. We named this new method DAFi: Directed Automated Filtering and Identification of cell populations. Design of DAFi preserves the data-driven characteristics of unsupervised clustering for identifying novel cell-based biomarkers, but also makes the results interpretable to experimental scientists as in supervised classification through mapping and merging the high-dimensional data clusters into the user-defined 2D gating hierarchy. By recursive data filtering before clustering, DAFi can uncover small local clusters which are otherwise difficult to identify due to the statistical interference of the irrelevant major clusters. Quantitative assessment of cell type specific characteristics demonstrates that the population proportions calculated by DAFi, while being highly consistent with those by expert centralized manual gating, have smaller technical variance than those from individual manual gating analysis. Visual examination of the dot plots showed that the boundaries of the DAFi-identified cell populations followed the natural shapes of the data distributions. To further exemplify the utility of DAFi, we show that DAFi can incorporate the FLOCK clustering method to identify novel cell-based biomarkers. Implementation of DAFi supports options including clustering, bisecting, slope-based gating, and reversed filtering to meet various auto-gating needs from different scientific use cases."}, {"title": "Developing an in silico minimum inhibitory concentration panel test for Klebsiella pneumoniae", "url": "https://www.biorxiv.org/content/early/2017/09/25/193797", "tag": "Bioinformatics", "abstract": "Antimicrobial resistant infections are a serious public health threat worldwide. Whole genome sequencing approaches to rapidly identify pathogens and predict antibiotic resistance phenotypes are becoming more feasible and may offer a way to reduce clinical test turnaround times compared to conventional culture-based methods, and in turn, improve patient outcomes. In this study, we use whole genome sequence data from 1668 clinical isolates of Klebsiella pneumoniae to develop a XGBoost-based machine learning model that accurately predicts minimum inhibitory concentrations (MICs) for 20 antibiotics. The overall accuracy of the model, within \u00b11 two-fold dilution factor, is 92%. Individual accuracies are \u226590% for 15/20 antibiotics. We show that the MICs predicted by the model correlate with known antimicrobial resistance genes. Importantly, the genome-wide approach described in this study offers a way to predict MICs for isolates without knowledge of the underlying gene content. This study shows that machine learning can be used to build a complete in silico MIC prediction panel for K. pneumoniae and provides a framework for building MIC prediction models for other pathogenic bacteria."}, {"title": "PSGfinder: fast identification of genes under divergent positive selection using the dynamic windows method", "url": "https://www.biorxiv.org/content/early/2017/09/25/193722", "tag": "Bioinformatics", "abstract": "Orthologous genes evolving under divergent positive selection are those involved in divergent adaptive trajectories between related species. Current methods to identify such genes are complex and conservative or present some imperfections, limiting genome-wide searches. We present a simple method, Dynamic Windows, to detect regions of protein-coding genes evolving under divergent positive selection. This method is implemented in PSGfinder, a user-friendly and flexible software, allowing rapid genome-wide screenings of regions with a dN/dS >1. PSGfinder additionally includes an alignment cleaning procedure and an adapted multiple comparison correction to identify significant signals of positive selection."}, {"title": "Strelka2: Fast and accurate variant calling for clinical sequencing applications", "url": "https://www.biorxiv.org/content/early/2017/09/25/192872", "tag": "Bioinformatics", "abstract": "We describe Strelka2 (https://github.com/Illumina/strelka), an open-source small variant calling method for clinical germline and somatic sequencing applications. Strelka2 introduces a novel mixture-model based estimation of indel error parameters from each sample, an efficient tiered haplotype modeling strategy and a normal sample contamination model to improve liquid tumor analysis. For both germline and somatic calling, Strelka2 substantially outperforms current leading tools on both variant calling accuracy and compute cost."}, {"title": "ANTENNA, a Multi-Rank, Multi-Layered Recommender System for Inferring Reliable Drug-Gene-Disease Associations: Repurposing Diazoxide as a Targeted Anti-Cancer Therapy", "url": "https://www.biorxiv.org/content/early/2017/09/25/192385", "tag": "Bioinformatics", "abstract": "Existing drug discovery process follows a reductionist model of 'one-drug-one-gene-one-disease,' which is not adequate to tackle complex diseases that involve multiple malfunctioned genes. The availability of big omics data offers new opportunities to transform the drug discovery process into a new paradigm of systems pharmacology that focuses on designing drugs to target molecular interaction networks instead of a single gene. Here, we develop a reliable multi-rank, multi-layered recommender system ANTENNA to mine large-scale chemical genomics and disease association data for the prediction of novel drug-gene-disease associations. ANTENNA integrates a novel tri-factorization based dual-regularized weighted and imputed One Class Collaborative Filtering (OCCF) algorithm tREMAP with a statistical framework that is based on Random Walk with Restart and can assess the reliability of a specific prediction. In the benchmark study, tREMAP clearly outperforms the single rank OCCF. We apply ANTENNA to a real-world problem: repurposing old drugs for new clinical indications that have yet had an effective treatment. We discover that FDA-approved drug diazoxide can inhibit multiple kinase genes whose malfunction is responsible for many diseases including cancer, and kill triple negative breast cancer (TNBC) cells effectively at a low concentration (IC50 = 0.87 \u03bcM). The TNBC is a deadly disease that currently does not have effective targeted therapies. Our finding demonstrates the power of big data analytics in drug discovery, and has a great potential toward developing a targeted therapy for the effective treatment of TNBC."}, {"title": "Scikit-ribo: Accurate estimation and robust modeling of translation dynamics at codon resolution", "url": "https://www.biorxiv.org/content/early/2017/09/25/156588", "tag": "Bioinformatics", "abstract": "Ribosome profiling (Riboseq) is a powerful technique for measuring protein translation, however, sampling errors and biological biases are prevalent and poorly understand. Addressing these issues, we present Scikit-ribo (https://github.com/hanfang/scikit-ribo), the first open-source software for accurate genome-wide A-site prediction and translation efficiency (TE) estimation from Riboseq and RNAseq data. Scikit-ribo accurately identifies A-site locations and reproduces codon elongation rates using several digestion protocols (r=0.99). Next we show commonly used RPKM-derived TE estimation is prone to biases, especially for low-abundance genes. Scikit-ribo introduces a codon-level generalized linear model with ridge penalty that correctly estimates TE while accommodating variable codon elongation rates and mRNA secondary structure. This corrects the TE errors for over 2000 genes in S. cerevisiae, which we validate using mass spectrometry of protein abundances (r=0.81) and allows us to determine the Kozak-like sequence directly from Riboseq. We conclude with an analysis of coverage requirements needed for robust codon-level analysis, and quantify the artifacts that can occur from cycloheximide treatment."}, {"title": "GFF3sort: a novel tool to sort GFF3 files for tabix indexing", "url": "https://www.biorxiv.org/content/early/2017/09/25/145938", "tag": "Bioinformatics", "abstract": "Background: The traditional method of visualizing gene annotation data in JBrowse is converting GFF3 files to JSON format, which is time-consuming. The latest version of JBrowse supports rendering sorted GFF3 files indexed by tabix, a novel strategy that is more convenient than the original conversion process. However, current tools available for GFF3 file sorting have some limitations and their sorting results would lead to erroneous rendering in JBrowse. Results: We developed GFF3sort, a script to sort GFF3 files for tabix indexing. Specifically designed for JBrowse rendering, GFF3sort can properly deal with the order of features that have the same chromosome and start position, either by remembering their original orders or by conducting parent-child topology sorting. Based on our test datasets from seven species, GFF3sort produced accurate sorting results with acceptable efficiency compared with currently available tools Conclusions: GFF3sort is a novel tool to sort GFF3 files for tabix indexing. We anticipate that GFF3sort will be useful to help with genome annotation data processing and visualization."}, {"title": "QuiXoT: quantification and statistics of high-throughput proteomics by stable isotope labelling", "url": "https://www.biorxiv.org/content/early/2017/09/25/193607", "tag": "Bioinformatics", "abstract": "In most software tools for quantification of mass spectrometry-based proteomics by stable isotope labelling (SIL), there is a recurrent disconnection between the use of a statistical model and convenient data visualisation to check correct data modelling. Most of them lack a robust statistical framework, using models which do not account for the major difficulties in proteomics, such as the unbalanced peptide-protein distribution, undersampling, or the correct separation of sources of variance. This makes especially difficult the interpretation of quantitative proteomics experiments. Here we present QuiXoT, an extensively tested quantification and statistics open source software based on a robust and extensively validated statistical model, the WSPP (weighted spectrum, peptide, and protein). Its associated software package allows the user to visually represent and inspect results at all modelled levels (scan, peptide and protein) on routine bases. It is applicable to practically any SIL method (SILAC, iTRAQ, and 18O among others) or MS instrument."}, {"title": "Natural changes in light interact with circadian regulation at promoters to control gene expression in cyanobacteria", "url": "https://www.biorxiv.org/content/early/2017/09/25/193557", "tag": "Bioinformatics", "abstract": "The circadian clock interacts with other regulatory pathways to tune physiology to predictable daily changes and unexpected environmental fluctuations. However, the complexity of circadian clocks in higher organisms has prevented a clear understanding of how natural environmental conditions affect circadian clocks and their physiological outputs. Here, we dissect the interaction between circadian regulation and responses to fluctuating light in the cyanobacterium Synechococcus elongatus. We demonstrate that natural changes in light intensity substantially affect the expression of hundreds of circadian-clock-controlled genes, many of which are involved in key steps in metabolism. These changes in expression arise from control of RNA polymerase recruitment to promoters by circadian and light-responsive regulation of a network of transcription factors including RpaA and RpaB. Using phenomenological modeling constrained by our data, we reveal simple principles that underlie the small number of stereotyped responses of dusk circadian genes to changes in light."}, {"title": "Efficient Multi-task chemogenomics for drug specificity prediction", "url": "https://www.biorxiv.org/content/early/2017/09/24/193391", "tag": "Bioinformatics", "abstract": "Adverse drug reactions (also called side effects) often interrupt the long and costly process of drug development. Side effects occur when drugs bind to proteins other than their intended target. As experimentally testing drug specificity against the entire proteome is out of reach, we investigate in this paper the application of chemogenomics approaches. We formulate the study of drug specificity as a proteome-wide drug-protein interaction prediction problem, and build appropriate data sets on which we evaluate multi-task support vector machines. Our observations lead us to propose NNMT, an efficient Multi-Task SVM for chemogenomics that is trained on a limited number of data points. Finally, we demonstrate the suitability of NNMT to study the specificity of drug-like molecules in real-life situations by suggesting secondary targets for 36 recently withdrawn drugs. In 9 cases, we identified secondary targets responsible for the withdrawal of the drug."}, {"title": "Extracting Evidence Fragments for Distant Supervision of Molecular Interactions", "url": "https://www.biorxiv.org/content/early/2017/09/23/192856", "tag": "Bioinformatics", "abstract": "We describe a methodology for automatically extracting \u2018evidence fragments\u2019 from a set of biomedical experimental research articles. These fragments provide the primary description of evidence that is presented in the papers' figures. They elucidate the goals, methods, results and interpretations of experiments that support the original scientific contributions the study being reported. Within this paper, we describe our methodology and showcase an example data set based on the European Bioinformatics Institute's INTACT database (http:www.ebi.ac.uk/intact/). Using figure codes as anchors, we linked evidence fragments to INTACT data records as an example of distant supervision so that we could use INTACT's preexisting, manually-curated structured interaction data to act as a gold standard for machine reading experiments. We report preliminary baseline event extraction measures from this collection based on a publicly available, machine reading system (REACH). We use semantic web standards for our data and provide open access to all source code."}, {"title": "miRNAgFree: prediction and profiling of novel microRNAs without genome assembly", "url": "https://www.biorxiv.org/content/early/2017/09/23/193094", "tag": "Bioinformatics", "abstract": "The prediction of novel miRNA genes generally requires the availability of genome sequences in order to assess important properties such as the characteristic hairpin-shaped secondary structure. However, although the sequencing costs have decreased over the last years, still many important species lack an assembled genome of certain quality. We implemented an algorithm which for the first time exploits characteristic biogenesis features like the 5' homogeneity that can be assessed without genome sequences. We used a phylogenetically broad spectrum of well annotated animal genomes for benchmarking. We found that between 90-100% of the most expressed miRNA candidates (top quartile) corresponded to known miRNA sequences."}, {"title": "Sensitive and robust assessment of ChIP-seq read distribution using a strand-shift profile", "url": "https://www.biorxiv.org/content/early/2017/09/22/165050", "tag": "Bioinformatics", "abstract": "Chromatin immunoprecipitation followed by sequencing (ChIP-seq) can detect read-enriched DNA loci for point-source (e.g., transcription factor binding) and broad-source factors (e.g., various histone modifications). Although numerous quality metrics for ChIP-seq data have been developed, the 'peaks' thus obtained are still difficult to assess with respect to signal-to-noise ratio (S/N) and the percentage of false positives. We developed a quality-assessment tool for ChIP-seq data, SSP (strand-shift profile), that quantifies S/N and peak reliability without peak calling. We validated SSP in-depth using over 1,000 publicly available ChIP-seq datasets along with virtual data to demonstrate that SSP is quantifiable and sensitive to different S/Ns for both point- and broad-source factors. Moreover, SSP is consistent among cell types and with respect to variance of sequencing depth, and identifies low-quality samples that cannot be identified by quality metrics currently available. Finally, we show that \"hidden-duplicate reads\" cause aberrantly high S/Ns, and SSP provides an additional metric to avoid them, which can also contribute to estimation of peak mode (point- or broad-source) of samples. Availability: https://github.com/rnakato/SSP"}, {"title": "Optimizing scoring function of dynamic programming of pairwise profile alignment using derivative free neural network", "url": "https://www.biorxiv.org/content/early/2017/09/21/182493", "tag": "Bioinformatics", "abstract": "A profile comparison method with position-specific scoring matrix (PSSM) is one of the most accurate alignment methods. Currently, cosine similarity and correlation coefficient are used as scoring functions of dynamic programming to calculate similarity between PSSMs. However, it is unclear that these functions are optimal for profile alignment methods. At least, by definition, these functions cannot capture non-linear relationships between profiles. Therefore, in this study, we attempted to discover a novel scoring function, which was more suitable for the profile comparison method than the existing ones. Firstly we implemented a new derivative free neural network by combining the conventional neural network with evolutionary strategy optimization method. Next, using the framework, the scoring function was optimized for aligning remote sequence pairs. Nepal, the pairwise profile aligner with the novel scoring function significantly improved both alignment sensitivity and precision, compared to aligners with the existing functions. Nepal improved alignment quality because of adaptation to remote sequence alignment and increasing the expressive power of similarity score. The novel scoring function can be realized using a simple matrix operation and easily incorporated into other aligners. With our scoring function, the performance of homology detection and/or multiple sequence alignment for remote homologous sequences would be further improved."}, {"title": "hichipper: A preprocessing pipeline for assessing library quality and DNA loops from HiChIP data", "url": "https://www.biorxiv.org/content/early/2017/09/21/192302", "tag": "Bioinformatics", "abstract": "Mumbach et al. recently described HiChIP, a novel protein-mediated chromatin conformation assay that lowers cellular input requirements while simultaneously increasing the yield of informative reads compared to previous methods. To facilitate the dissemination and adoption of this assay, we introduce hichipper (http://aryeelab.org/hichipper), an open-source HiChIP data preprocessing tool, with features that include bias-corrected peak calling, library quality control, DNA loop calling, and output of processed data for downstream analysis and visualization."}, {"title": "A pan-cancer landscape of interactions between solid tumors and infiltrating immune cell populations", "url": "https://www.biorxiv.org/content/early/2017/09/21/192286", "tag": "Bioinformatics", "abstract": "Throughout their development, tumors are challenged by the immune system and acquire features to evade its surveillance. A systematic view of these traits is still lacking. Here, we identify genomic and transcriptomic traits associated to the immune-phenotype of 9,403 tumors of 29 solid cancers. In highly cytotoxic immune-phenotypes we found tumors with low clonal heterogeneity enriched by alterations of genes involved in epigenetic regulation, ubiquitin mediated proteolysis, antigen-presentation and cell-cell communication, which may drive resistance. Tumors with immune-phenotypes with mid cytotoxicity present an over-activation of processes involved in invasion and remodeling of neighboring tissues that may foster the recruitment of immune-suppressive cells. Tumors with poor cytotoxic immune-phenotype tend to be of more advanced stages and present frequent alterations in cell cycle, hedgehog, beta-catenin and TGF-beta pathways, which may drive the immune depletion. These results may be exploited to develop novel combinatorial targeting strategies involving immunotherapies."}, {"title": "gene2drug: a Computational Tool for Pathway-based Rational Drug Repositioning", "url": "https://www.biorxiv.org/content/early/2017/09/21/192005.1", "tag": "Bioinformatics", "abstract": "Drug repositioning has been proposed as an effective shortcut to drug discovery. The availability of large collections of transcriptional responses to drugs enables computational approaches to drug repositioning directly based on measured molecular effects. We introduce a novel computational methodology for rational drug repositioning, which exploits the transcriptional responses following treatment with small molecule. Specifically, given a therapeutic target gene, a prioritisation of potential effective drugs is obtained by assessing their impact on the transcription of genes in the pathway(s) including the target. We performed in silico validation and comparison with a state-of-art technique based on similar principles. We next performed experimental validation in two different real-case drug repositioning scenarios: (i) upregulation of the glutamate-pyruvate transaminase, which has been shown to induce reduction of oxalate levels in a mouse model of primary hyperoxaluria, and (ii) activation of the transcription factor TFEB, a master regulator of lysosomal biogenesis and autophagy, whose modulation may be beneficial in neurodegenerative disorders. An oline implementation of the tool is available at http://gene2drug.tigem.it."}, {"title": "An Interpretable Framework for Clustering Single-Cell RNA-Seq Datasets", "url": "https://www.biorxiv.org/content/early/2017/09/21/191254", "tag": "Bioinformatics", "abstract": "With the recent proliferation of single-cell RNA-Seq experiments, several methods have been developed for unsupervised analysis of the resulting datasets. These methods often rely on unintuitive hyperparameters and do not explicitly address the subjectivity associated with clustering. In this work, we present DendroSplit, an interpretable framework for analyzing single-cell RNA-Seq datasets that addresses both these issues. Under this framework, we cluster using feature selection to uncover multiple levels of biologically meaningful populations in the data. We analyze several landmark single-cell datasets, demonstrating both the method's efficacy and computational efficiency. We provide the full DendroSplit software package at \\texttt{https://github.com/jessemzhang/dendrosplit}."}, {"title": "vU-net: accurate cell edge segmentation in time-lapse fluorescence live cell images based on convolutional neural network", "url": "https://www.biorxiv.org/content/early/2017/09/21/191858", "tag": "Bioinformatics", "abstract": "Time-lapse fluorescence live cell imaging has been widely used to study various dynamical processes in cell biology. However, fluorescence live cell images often have low contrast, noises, and uneven illumination, preventing accurate cell segmentation. The convolutional neural network has been successfully applied in natural image classification and segmentation by extracting hierarchical features, which could be transferred into other fields for image segmentation. Moreover, the temporal coherence in time-lapse images can allow us to extract sufficient features from a limited number of image frames to segment the entire time-lapse movies. In this paper, we propose a novel framework called vU-net, which integrates the VGG-161 pretrained model as an encoder and a U-net2 derived simplified convolutional structure to reconstruct cell edge with a higher accuracy using limited training images. We evaluated our framework on the high-resolution images of paxillin, a canonical adhesion marker in migrating PtK1 cells acquired by a Total Internal Reflection Fluorescence (TIRF) microscope, and achieved higher accuracy of cell segmentation than conventional U-net. We also validated our framework on noisy confocal fluorescence live images of GFP-mDia1 in PtK1 cells. We demonstrated that vU-net could be practically applied to challenging live cell movies since it required limited training sets and achieved highly accurate segmentation."}, {"title": "A space and time-efficient index for the compacted colored de Bruijn graph", "url": "https://www.biorxiv.org/content/early/2017/09/21/191874", "tag": "Bioinformatics", "abstract": "We present a novel data structure for representing and indexing the compacted colored de Bruijn graph, which allows for efficient pattern matching and retrieval of the reference information associated with each k-mer. As the popularity of the de Bruijn graph as an index has increased over the past few years, so have the number of proposed representations of this structure. Existing structures typically fall into two categories; those that are hashing-based and provide very fast access to the underlying k-mer information, and those that are space-frugal and provide asymptotically efficient but practically slower pattern search. Our representation achieves a compromise between these two extremes. By building upon minimum perfect hashing, carefully organizing our data structure, and making use of succinct representations where applicable, our data structure provides practically fast k-mer lookup while greatly reducing the space compared to traditional hashing-based implementations. Further, we describe a sampling scheme built on the same underlying representation, which provides the ability to trade off k-mer query speed for a reduction in the de Bruijn graph index size. We believe this representation strikes a desirable balance between speed and space usage, and it will allow for fast search on large reference sequences. Pufferfish is developed in C++11, is open source (GPL v3), and is available at https://github.com/COMBINE-lab/pufferfish. The scripts used to generate the results in this manuscript are available at https://github.com/COMBINE-lab/pufferfish_experiments."}, {"title": "Prediction of residue-residue contacts in CASP12 targets from its predicted tertiary structures", "url": "https://www.biorxiv.org/content/early/2017/09/21/192120", "tag": "Bioinformatics", "abstract": "One of the challenges in the field of structural proteomics is to predict residue-residue contacts in a protein. It is an integral part of CASP competitions due to its importance in the field of structural biology. This manuscript describes RRCPred2.0, a method participated in CASP12 and predicted residue-residue contact in targets with high precision. In this approach, firstly 150 predicted protein structures were obtained from CASP12 Stage 2 tarball and ranked using clustering-based quality assessment software. Secondly, residue-residue contacts were assigned in top 10 protein structures based on distance between residues. Finally, residue-residue contacts were predicted in target protein based on consensus/average in top 10 predicted structures. This simple approach performs better than most of CASP12 methods in the categories of TBM and TBM/FM. It ranked 1st in following categories; i) TBM domain on list size L/5, ii) TBM/FM domain on list size L/5 and iii) TBM/FM domain on Top 10. These observations indicate that predicted tertiary structure of a protein can be used for predicting residue-residue contacts in protein with high accuracy."}, {"title": "Updating the 97% identity threshold for 16S\u200c ribosomal RNA OTUs", "url": "https://www.biorxiv.org/content/early/2017/09/21/192211", "tag": "Bioinformatics", "abstract": "The 16S ribosomal RNA (rRNA) gene is widely used to survey microbial communities. Sequences are often clustered into Operational Taxonomic Units (OTUs) as proxies for species. The canonical clustering threshold is 97% identity, which was proposed in 1994 when few 16S rRNA sequences were available, motivating a reassessment on current data. Using a large set of high-quality 16S rRNA sequences from finished genomes, I assessed the correspondence of OTUs to species for five representative clustering algorithms using four accuracy metrics. All algorithms had comparable accuracy when tuned to a given metric. Optimal identity thresholds that best approximated species were ~99% for full-length sequences and ~100% for the V4 hypervariable region."}, {"title": "Stochastic Resonance Mediates the State-Dependent Effect of Periodic Stimulation on Cortical Alpha Oscillations", "url": "https://www.biorxiv.org/content/early/2017/09/21/191577", "tag": "Bioinformatics", "abstract": "Brain stimulation can be used to engage and modulate rhythmic activity in cortical networks. However, the outcomes have been shown to be impacted by behavioral states and endogenous brain fluctuations. To better understand how this intrinsic oscillatory activity controls the brain's susceptibility to stimulation, we analyzed a computational model of the thalamocortical system in both the rest and task states, to identify the mechanisms by which endogenous alpha oscillations (8Hz-12Hz) are impacted by periodic stimulation. Our analysis shows that the differences between different brain states can be explained by a passage through a bifurcation combined to stochastic resonance - a mechanism whereby irregular fluctuations amplify the response of a nonlinear system to weak signals. Indeed, our findings suggest that modulating brain oscillations is best achieved in states of low endogenous rhythmic activity, and that irregular state-dependent fluctuations in thalamic inputs shape the susceptibility of cortical population to periodic stimulation."}, {"title": "BioSankey: Visualizing microbial communities and gene expression data over time", "url": "https://www.biorxiv.org/content/early/2017/09/20/191767", "tag": "Bioinformatics", "abstract": "Metagenomics, RNA-seq, WGS (Whole Genome Sequencing) and other types of next-generation sequencing techniques provide quantitative measurements for single strains and genes over time. To obtain a global overview of the experiment and to explore the full potential of a given dataset, intuitive and interactive visualization tools are needed. Therefore, we established BioSankey, which allows to visualize microbial species in microbiome studies and gene expression over time as a Sankey diagram. These diagrams are embedded into a project-specific HTML page, that contains all information as provided during the installation process. BioSankey can be easily applied to analyse bacterial communities in time-series datasets. Furthermore, it can be used to analyse the fluctuations of differentially expressed genes (DEG). The output of BioSankey is a project-specific HTML page, which depends only on JavaScript to enable searches of interesting species or genes of interest without requiring a web server or connection to a database to exchange results among collaboration partners. BioSankey is a tool to visualize different data elements from single and dual RNA-seq datasets as well as from metagenomes studies."}, {"title": "MuClone: Somatic mutation detection and classification through probabilistic integration of clonal population structure", "url": "https://www.biorxiv.org/content/early/2017/09/20/191759", "tag": "Bioinformatics", "abstract": "Accurate detection and classification of somatic single nucleotide variants (SNVs) is important in defining the clonal composition of human cancers. Existing tools are prone to miss low prevalence mutations and methods for classification of mutations into clonal groups across the whole genome are underdeveloped. Increasing interest in deciphering clonal population dynamics over multiple samples in time or anatomic space from the same patient is resulting in whole genome sequence (WGS) data from phylogenetically related samples. With the access to this data, we posited that injecting clonal structure information into the inference of mutations from multiple samples would improve mutation detection. We developed MuClone: a novel statistical framework for simultaneous detection and classification of mutations across multiple tumour samples of a patient from whole genome or exome sequencing data. The key advance lies in incorporating prior knowledge about the cellular prevalences of clones to improve the performance of detecting mutations, particularly low prevalence mutations. We evaluated MuClone through synthetic and real data from spatially sampled ovarian cancers. Results support the hypothesis that clonal information improves sensitivity in detecting somatic mutations without compromising specificity. In addition, MuClone classifies mutations across whole genomes of multiple samples into biologically meaningful groups, providing additional phylogenetic insights and enhancing the study of WGS-derived clonal dynamics."}, {"title": "LRSDAY: Long-read sequencing data analysis for yeasts", "url": "https://www.biorxiv.org/content/early/2017/09/20/184572", "tag": "Bioinformatics", "abstract": "Long-read sequencing technologies have become increasingly popular in genome projects due to their strengths in resolving complex genomic regions. As a leading model organism with small genome size and great biotechnological importance, the budding yeast, Saccharomyces cerevisiae, has many isolates currently being sequenced with long reads. However, analyzing long-read sequencing data to produce high-quality genome assembly and annotation remains challenging. Here we present LRSDAY, the first one-stop solution to streamline this process. LRSDAY can produce chromosome-level end-to-end genome assembly and comprehensive annotations for various genomic features (including centromeres, protein-coding genes, tRNAs, transposable elements and telomere-associated elements) that are ready for downstream analysis. Although tailored for S. cerevisiae, we designed LRSDAY to be highly modular and customizable, making it adaptable for virtually any eukaryotic organisms. Applying LRSDAY to a S. cerevisiae strain takes ~43 hrs to generate a complete and well-annotated genome from ~100X Pacific Biosciences (PacBio) reads using four threads."}, {"title": "MEBS, a software platform to evaluate large (meta)genomic collections according to their metabolic machinery: unraveling the sulfur cycle", "url": "https://www.biorxiv.org/content/early/2017/09/20/191288", "tag": "Bioinformatics", "abstract": "BACKGROUND: The increasing number of metagenomic and genomic sequences has dramatically improved our understanding of microbial diversity, yet our ability to infer metabolic capabilities in such datasets remains challenging. FINDINGS: We describe the Multigenomic Entropy Based Score pipeline (MEBS), a software platform designed to evaluate, compare and infer complex metabolic pathways in large omic datasets, including entire biogeochemical cycles. MEBS is open source and available through https://github.com/eead-csic-compbio/metagenome_Pfam_score. To demonstrate its use we modeled the sulfur cycle by exhaustively curating the molecular and ecological elements involved (compounds, genes, metabolic pathways and microbial taxa). This information was reduced to a collection of 112 characteristic Pfam protein domains and a list of complete-sequenced sulfur genomes. Using the mathematical framework of relative entropy (H), we quantitatively measured the enrichment of these domains among sulfur genomes. The entropy of each domain was used to both: build up a final score that indicates whether a (meta)genomic sample contains the metabolic machinery of interest and to propose marker domains in metagenomic sequences such as DsrC (PF04358). MEBS was benchmarked with a dataset of 2,107 non-redundant microbial genomes from RefSeq and 935 metagenomes from MG-RAST. Its performance, reproducibility, and robustness were evaluated using several approaches, including random sampling, linear regression models, Receiver Operator Characteristic plots and the Area Under the Curve metric (AUC). Our results support the broad applicability of this algorithm to accurately classify (AUC=0.985) hard to culture genomes (e.g., Candidatus Desulforudis audaxviator), previously characterized ones and metagenomic environments such as hydrothermal vents, or deep-sea sediment. CONCLUSIONS: Our benchmark indicates that an entropy-based score can capture the metabolic machinery of interest and be used to efficiently classify large genomic and metagenomic datasets, including uncultivated/unexplored taxa"}, {"title": "Skip-mers: increasing entropy and sensitivity to detect conserved genic regions with simple cyclic q-grams", "url": "https://www.biorxiv.org/content/early/2017/09/19/179960", "tag": "Bioinformatics", "abstract": "Bioinformatic analyses and tools make extensive use of k-mers (fixed contiguous strings of k nucleotides) as an informational unit. K-mer analyses are both useful and fast, but are strongly affected by single nucleotide polymorphisms or sequencing errors, effectively hindering direct-analyses of whole regions and decreasing their usability between evolutionary distant samples. Q-grams or spaced seeds, subsequences generated with a pattern of used-and-skipped nucleotides, overcome many of these limitations but introduce larger complexity which hinders their wider adoption. We introduce a concept of skip-mers, a cyclic pattern of used-and-skipped positions of k nucleotides spanning a region of size S \u2265 k, and show how analyses are improved by using this simple subset of q-grams as a replacement for k-mers. The entropy of skip-mers increases with the larger span, capturing information from more distant positions and increasing the specificity, and uniqueness, of larger span skip-mers within a genome. In addition, skip-mers constructed in cycles of 1 or 2 nucleotides in every 3 (or a multiple of 3) lead to increased sensitivity in the coding regions of genes, by grouping together the more conserved nucleotides of the protein-coding regions. We implemented a set of tools to count and intersect skip-mers between different datasets, a simple task given that the properties of skip-mers make them a direct substitute for k-mers. We used these tools to show how skip-mers have advantages over k-mers in terms of entropy and increased sensitivity to detect conserved coding sequence, allowing better identification of genic matches between evolutionarily distant species. We then show benefits for multi-genome analyses provided by increased and better correlated coverage of conserved skip-mers across multiple samples. Software availability: the skm-tools implementing the methods described in this manuscript are available under MIT license at http://github.com/bioinfologics/skm-tools/"}, {"title": "Testing the moderation of quantitative gene by environment interactions in unrelated individuals", "url": "https://www.biorxiv.org/content/early/2017/09/19/191080", "tag": "Bioinformatics", "abstract": "The environment can moderate the effect of genes - a phenomenon called gene-environment (GxE) interaction. There are two broad types of GxE modeled in human behavior - qualitative GxE, where the effects of individual genetic variants differ depending on some environmental moderator, and quantitative GxE, where the additive genetic variance changes as a function of an environmental moderator. Tests of both qualitative and quantitative GxE have traditionally relied on comparing the covariances between twins and close relatives, but recently there has been interest in testing such models on unrelated individuals measured on genomewide data. However, to date, there has been no ability to test quantitative GxE effects in unrelated individuals using genomewide data because standard software cannot solve nonlinear constraints. Here, we introduce a maximum likelihood approach with parallel constrained optimization to fit such models. We use simulation to estimate the accuracy, power, and type I error rates of our method and to gauge its computational performance, and then apply this method to IQ data measured on 40,172 individuals with whole-genome SNP data from the UK Biobank. We found that the additive genetic variation of IQ tagged by SNPs increases as socioeconomic status (SES) decreases, opposite the direction found by several twin studies conducted in the U.S. on adolescents, but consistent with several studies from Europe and Australia on adults."}, {"title": "Superior ab initio Identification, Annotation and Characterisation of TEs and Segmental Duplications from Genome Assemblies.", "url": "https://www.biorxiv.org/content/early/2017/09/19/190694", "tag": "Bioinformatics", "abstract": "Transposable Elements (TEs) are mobile DNA sequences that make up significant fractions of amniote genomes. However, they are difficult to detect and annotate ab initio because of their variable features, lengths and clade-specific variants. We have addressed this problem by refining and developing a Comprehensive ab initio Repeat Pipeline (CARP) to identify and cluster TEs and other repetitive sequences in genome assemblies. The pipeline begins with a pairwise alignment using krishna, a custom aligner. Single linkage clustering is then carried out to produce families of repetitive elements. Consensus sequences are then filtered for protein coding genes and then annotated using Repbase and a custom library of retrovirus and reverse transcriptase sequences. This process yields three types of family: fully annotated, partially annotated and unannotated. Fully annotated families reflect recently diverged/young known TEs present in Repbase. The remaining two types of families contain a mixture of novel TEs and segmental duplications. These can be resolved by aligning these consensus sequences back to the genome to assess copy number vs. length distribution. Our pipeline has three significant advantages compared to other methods for ab initio repeat identification: 1) we generate not only consensus sequences, but keep the genomic intervals for the original aligned sequences, allowing straightforward analysis of evolutionary dynamics, 2) consensus sequences represent low-divergence, recently/currently active TE families, 3) segmental duplications are annotated as a useful by-product. We have compared our ab initio repeat annotations for 7 genome assemblies (1 unpublished) to other methods and demonstrate that CARP compares favourably with RepeatModeler, the most widely used repeat annotation package."}, {"title": "Efficient management and analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr", "url": "https://www.biorxiv.org/content/early/2017/09/19/190926", "tag": "Bioinformatics", "abstract": "Genome-wide datasets produced for association studies have dramatically increased in size over the past few years, with modern datasets commonly including millions of variants measured in dozens of thousands of individuals. This increase in data size is a major challenge severely slowing down genomic analyses. Specialized software for every part of the analysis pipeline have been developed to handle large genomic data. However, combining all these software into a single data analysis pipeline might be technically difficult. Here we present two R packages, bigstatsr and bigsnpr, allowing for management and analysis of large scale genomic data to be performed within a single comprehensive framework. To address large data size, the packages use memory-mapping for accessing data matrices stored on disk instead of in RAM. To perform data pre-processing and data analysis, the packages integrate most of the tools that are commonly used, either through transparent system calls to existing software, or through updated or improved implementation of existing methods. In particular, the packages implement a fast derivation of Principal Component Analysis, functions to remove SNPs in Linkage Disequilibrium, and algorithms to learn Polygenic Risk Scores on millions of SNPs. We illustrate applications of the two R packages by analysing a case-control genomic dataset for the celiac disease, performing an association study and computing Polygenic Risk Scores. Finally, we demonstrate the scalability of the R packages by analyzing a simulated genome-wide dataset including 500,000 individuals and 1 million markers on a single desktop computer."}, {"title": "Deep Learning based multi-omics integration robustly predicts survival in liver cancer", "url": "https://www.biorxiv.org/content/early/2017/09/18/114892", "tag": "Bioinformatics", "abstract": "Identifying robust survival subgroups of hepatocellular carcinoma (HCC) will significantly improve patient care. Currently, endeavor of integrating multi-omics data to explicitly predict HCC survival from multiple patient cohorts is lacking. To fill in this gap, we present a deep learning (DL) based model on HCC that robustly differentiates survival subpopulations of patients in six cohorts. We build the DL based, survival-sensitive model on 360 HCC patients' data using RNA-seq, miRNA-seq and methylation data from TCGA, which predicts prognosis as good as an alternative model where genomics and clinical data are both considered. This DL based model provides two optimal subgroups of patients with significant survival differences (P=7.13e-6) and good model fitness (C-index=0.68). More aggressive subtype is associated with frequent TP53 inactivation mutations, higher expression of stemness markers (KRT19, EPCAM) and tumor marker BIRC5, and activated Wnt and Akt signaling pathways. We validated this multi-omics model on five external datasets of various omics types: LIRI-JP cohort (n=230, C-index=0.75), NCI cohort (n=221, C-index=0.67), Chinese cohort (n=166, C-index=0.69), E-TABM-36 cohort (n=40, C-index=0.77), and Hawaiian cohort (n=27, C-index=0.82). This is the first study to employ deep learning to identify multi-omics features linked to the differential survival of HCC patients. Given its robustness over multiple cohorts, we expect this workflow to be useful at predicting HCC prognosis prediction."}, {"title": "An extensive enhancer-promoter map generated by genome-scale analysis of enhancer and gene activity patterns", "url": "https://www.biorxiv.org/content/early/2017/09/18/190231.1", "tag": "Bioinformatics", "abstract": "Massive efforts have documented hundreds of thousands of putative enhancers in the human genome. A pressing genomic challenge is to identify which of these enhancers are functional and map them to the genes they regulate. We developed a novel method for inferring enhancer-promoter (E-P) links based on correlated activity patterns across many samples. Our method, called FOCS, uses rigorous statistical validation tailored for zero-inflated data, identifying the most important E-P links in each gene model. We applied FOCS to the wide epigenomic and transcriptomic datasets recorded by the ENCODE, Roadmap Epigenomics and FANTOM5 projects, together covering 2,630 samples of human primary cells, tissues and cell lines. In addition, building on expression of enhancer RNAs (eRNAs) as an exquisite mark of enhancer activity and on the robust detection of eRNAs by the GRO-seq technique, we compiled a compendium of eRNA and gene expression profiles based on public GRO-seq data from 245 samples and 23 human cell types. Applying FOCS to this compendium further expanded the coverage of our inferred E-P map. Benchmarking against gold standard E-P links from ChIA-PET and eQTL data, we demonstrate that FOCS prediction of E-P links outperforms extant methods. Collectively, we inferred >300,000 cross-validated E-P links spanning ~16K known genes. Our study presents an improved method for inferring regulatory links between enhancers and promoters, and provides an extensive resource of E-P maps that could greatly assist the functional interpretation of the noncoding regulatory genome. FOCS and our predicted E-P map are publicly available at http://acgt.cs.tau.ac.il/focs."}, {"title": "k-mer Distributions of Aminoacid Sequences are Optimised Across the Proteome", "url": "https://www.biorxiv.org/content/early/2017/09/18/190280", "tag": "Bioinformatics", "abstract": "k-mer based methods are widely utilized for the analysis of nucleotide sequences and were successfully applied to proteins in several works. However, the reasons for the species-specificity of aminoacid k-mer distributions are unknown. In this work I show that performance of these methods is not only due to orthology between k-mers in different proteomes, which implies the existence of some factors optimizing k-mer distributions of proteins in a species-specific manner. Whatever these factors could be, they are affecting most if not all proteins and are more pronounced in structurally organized regions."}, {"title": "METHimpute: Imputation-guided construction of complete methylomes from WGBS data", "url": "https://www.biorxiv.org/content/early/2017/09/18/190223", "tag": "Bioinformatics", "abstract": "Whole-genome Bisulfite sequencing (WGBS) has become the standard method for interrogating plant methylomes at base resolution. However, deep WGBS measurements remain cost prohibitive for large, complex genomes and for population-level studies. As a result, most published plant methylomes are sequenced far below saturation, with a large proportion of cytosines having either missing data or insufficient coverage. Here we present METHimpute, a Hidden Markov Model (HMM) based imputation algorithm for the analysis of WGBS data. Unlike existing methods, METHimpute enables the construction of complete methylomes by inferring the methylation status and level of all cytosines in the genome regardless of coverage. Application of METHimpute to maize, rice and Arabidopsis shows that the algorithm infers cytosine-resolution methylomes with high accuracy from data as low as 6X, compared to data with 60X, thus making it a cost-effective solution for large-scale studies. Although METHimpute has been extensively tested in plants, it should be broadly applicable to other species."}, {"title": "Gene-level differential analysis at transcript-level resolution", "url": "https://www.biorxiv.org/content/early/2017/09/18/190199", "tag": "Bioinformatics", "abstract": "Gene-level differential expression analysis based on RNA-Seq is more robust, powerful and biologically actionable than transcript-level differential analysis. However aggregation of transcript counts prior to analysis results can mask transcript-level dynamics. We demonstrate that aggregating the results of transcript-level analysis allow for gene-level analysis with transcript-level resolution. We also show that p-value aggregation methods, typically used for meta-analyses, greatly increase the sensitivity of gene-level differential analyses. Furthermore, such aggregation can be applied directly to transcript compatibility counts obtained during pseudoalignment, thereby allowing for rapid and accurate model-free differential testing. The methods are general, allowing for testing not only of genes but also of any groups of transcripts, and we showcase an example where we apply them to perturbation analysis of gene ontologies."}, {"title": "Inferring demographic parameters in bacterial genomic data using Bayesian and hybrid phylogenetic methods", "url": "https://www.biorxiv.org/content/early/2017/09/18/190041", "tag": "Bioinformatics", "abstract": "Background: Recent developments in sequencing technologies make it possible to obtain genome sequences from a large number of isolates in a very short time. Bayesian phylogenetic approaches can take advantage of these data by simultaneously inferring the phylogenetic tree, evolutionary timescale, and phylodynamic parameters (such as population growth rates), while naturally integrating uncertainty in all parameters. Despite their desirable properties, Bayesian approaches can be computationally intensive, hindering their use for outbreak investigations involving large numbers of pathogen isolates. An alternative to using full Bayesian inference is to use a hybrid approach, where the phylogenetic tree and evolutionary timescale are estimated first using maximum likelihood. Under this hybrid approach, demographic parameters are inferred from estimated trees instead of the sequence data, using maximum likelihood, Bayesian inference, or approximate Bayesian computation. This can vastly reduce the computational burden, but has the disadvantage of ignoring the uncertainty in the phylogenetic tree and evolutionary timescale. Results: We compared the performance of a fully Bayesian and a hybrid method by analysing six whole-genome SNP data sets from a range of bacteria. The estimates from the two methods were very similar, suggesting that the hybrid method is a valid alternative for very large data datasets. However, we also found that congruence between these methods is contingent on the presence of strong temporal structure in the data (i.e. clocklike behaviour), which is typically verified using a date-randomisation test in a Bayesian framework. To reduce the computational burden of this Bayesian test we implemented a date-randomisation test using a rapid likelihood method, which has similar performance to its Bayesian counterpart. Conclusions: Hybrid approaches can produce reliable inferences of evolutionary timescales and phylodynamic parameters in a fraction of the time required for fully Bayesian analyses. As such, they are a valuable alternative in outbreak studies involving a large number of isolates."}, {"title": "A closer look at cross-validation for assessing the accuracy of gene regulatory networks and models", "url": "https://www.biorxiv.org/content/early/2017/09/18/190157", "tag": "Bioinformatics", "abstract": "Cross-validation (CV) is a technique to assess the generalizability of a model to unseen data. This technique relies on assumptions that may not be satisfied when studying genomics datasets. For example, random CV (RCV) assumes that a randomly selected set of samples, the test set, well represents unseen data. This assumption does not hold true where samples are obtained from different experimental conditions, and the goal is to learn regulatory relationships among the genes that generalize beyond the observed conditions. In this study, we investigated how the CV procedure affects the assessment of methods used to learn gene regulatory networks. We compared the performance of a regression-based method for gene expression prediction, estimated using RCV with that estimated using a clustering-based CV (CCV) procedure. Our analysis illustrates that RCV can produce over-optimistic estimates of generalizability of the model compared to CCV. Next, we defined the distinctness of a test set from a training set and showed that this measure is predictive of the performance of the regression method. Finally, we introduced a simulated annealing method to construct partitions with gradually increasing distinctness and showed that performance of different gene expression prediction methods can be better evaluated using this method."}, {"title": "Flexible, Cluster-Based Analysis of the Electronic Medical Record of Sepsis with Composite Mixture Models", "url": "https://www.biorxiv.org/content/early/2017/09/18/160465", "tag": "Bioinformatics", "abstract": "The widespread adoption of electronic medical records (EMRs) in healthcare has provided vast new amounts of data for statistical machine learning researchers in their efforts to model and predict patient health status, potentially enabling novel advances in treatment. In the case of sepsis, a debilitating, dysregulated host response to infection, extracting subtle, uncataloged clinical phenotypes from the EMR with statistical machine learning methods has the potential to impact patient diagnosis and treatment early in the course of their hospitalization. However, there are significant barriers that must be overcome to extract these insights from EMR data. First, EMR datasets consist of both static and dynamic observations of discrete and continuous-valued variables, many of which may be missing, precluding the application of standard multivariate analysis techniques. Second, clinical populations observed via EMRs and relevant to the study and management of conditions like sepsis are often heterogeneous; properly accounting for this heterogeneity is critical. Here, we describe an unsupervised, probabilistic framework called a composite mixture model that can simultaneously accommodate the wide variety of observations frequently observed in EMR datasets, characterize heterogeneous clinical populations, and handle missing observations. We demonstrate the efficacy of our approach on a large-scale sepsis cohort, developing novel techniques built on our model-based clusters to track patient mortality risk over time and identify physiological trends and distinct subgroups of the dataset associated with elevated risk of mortality during hospitalization."}, {"title": "Canvas SPW: Calling De Novo Copy Number Variants In Pedigrees", "url": "https://www.biorxiv.org/content/early/2017/09/18/121939", "tag": "Bioinformatics", "abstract": "Motivation: Whole genome sequencing is becoming a diagnostics of choice for the identification of rare inherited and de novo copy number variants in families with various pediatric and late-onset genetic diseases. However, joint variant calling in pedigrees is hampered by the complexity of consensus breakpoint alignment across samples within an arbitrary pedigree structure. Results: We have developed a new tool, Canvas SPW, for the identification of inherited and de novo copy number variants from pedigree sequencing data. Canvas SPW supports a number of family structures and provides a wide range of scoring and filtering options to automate and streamline identification of de novo variants. Availability: Canvas SPW is available for download from https://github.com/Illumina/canvas."}, {"title": "DeepLung: 3D Deep Convolutional Nets for Automated Pulmonary Nodule Detection and Classification", "url": "https://www.biorxiv.org/content/early/2017/09/17/189928", "tag": "Bioinformatics", "abstract": "In this work, we present a fully automated lung CT cancer diagnosis system, DeepLung. DeepLung contains two parts, nodule detection and classification. Considering the 3D nature of lung CT data, two 3D networks are designed for the nodule detection and classification respectively. Specifically, a 3D Faster R-CNN is designed for nodule detection with a U-net-like encoder-decoder structure to effectively learn nodule features. For nodule classification, gradient boosting machine (GBM) with 3D dual path network (DPN) features is proposed. The nodule classification subnetwork is validated on a public dataset from LIDC-IDRI, on which it achieves better performance than state-of-the-art approaches, and surpasses the average performance of four experienced doctors. For the DeepLung system, candidate nodules are detected first by the nodule detection subnetwork, and nodule diagnosis is conducted by the classification subnetwork. Extensive experimental results demonstrate the DeepLung is comparable to the experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset."}, {"title": "Applying expression profile similarity for discovery of patient-specific functional mutations", "url": "https://www.biorxiv.org/content/early/2017/09/17/172015", "tag": "Bioinformatics", "abstract": "The progress of cancer genome sequencing projects yields unprecedented information of mutations for numerous patients. However, the complexity of mutation profiles of patients hinders the further understanding of mechanisms of oncogenesis. One basic question is how to uncover mutations with functional impacts. In this work, we introduce a computational method to predict functional somatic mutations for each of patient by integrating mutation recurrence with similarity of expression profiles of patients. With this method, the functional mutations are determined by checking the mutation enrichment among a group of patients with similar expression profiles. We applied this method to three cancer types and identified the functional mutations. Comparison of the predictions for three cancer types suggested that most of the functional mutations were cancer-type-specific with one exception to p53. By checking prediction results, we found that our method effectively filtered non-functional mutations resulting from large protein sizes. In addition, this methods can also perform functional annotation to each patient to describe their association with signalling pathways or biological processes. In breast cancer, we predicted \"cell adhesion\" and other mutated gene associated terms to be significantly enriched among patients."}, {"title": "Phylogeny-corrected identification of microbial gene families relevant to human gut colonization", "url": "https://www.biorxiv.org/content/early/2017/09/16/189795", "tag": "Bioinformatics", "abstract": "The mechanisms by which different microbes colonize the healthy human gut versus free-living communities, other body sites, or the gut in disease states remain largely unknown. Identifying microbial genes influencing fitness in the gut could lead to new ways to engineer probiotics or disrupt pathogenesis. We propose a statistical approach to this problem that measures the association between having a gene and the probability that a species is present in the gut microbiome. The challenge is that closely related species tend to be jointly present or absent in the microbiome and also share many genes, only a subset of which are involved in gut adaptation. We show that this phylogenetic correlation indeed leads to many false discoveries and propose phylogenetic linear regression as a powerful solution. To apply this method across the bacterial tree of life, where most species have not been experimentally phenotyped, we used metagenomes from hundreds of people to quantify each species' prevalence in and specificity for the gut microbiome. This analysis revealed thousands of genes potentially involved across species in adaptation to the gut, including many novel candidates as well as processes known to contribute to fitness of gut bacteria, such as acid tolerance in Bacteroidetes and sporulation in Firmicutes. We also found microbial genes associated with a preference for the gut over other body sites, which were significantly enriched for genes linked to fitness in an in vivo competition experiment. Finally, we identified gene families associated with higher prevalence in patients with Crohn's disease, including Proteobacterial genes involved in conjugation and fimbria regulation, processes previously linked to inflammation. These gene targets may represent new avenues for modulating host colonization and disease. Our strategy of combining metagenomics with phylogenetic modeling is general and can be used to identify genes associated with adaptation to any environment."}, {"title": "Large-Scale Analysis of Disease Pathways in the Human Interactome", "url": "https://www.biorxiv.org/content/early/2017/09/16/189787", "tag": "Bioinformatics", "abstract": "Discovering disease pathways, which can be defined as sets of proteins associated with a given disease, is an important problem that has the potential to provide clinically actionable insights for disease diagnosis, prognosis, and treatment. Computational methods aid the discovery by relying on protein-protein interaction (PPI) networks. They start with a few known disease-associated proteins and aim to find the rest of the pathway by exploring the PPI network around the known disease proteins. However, the success of such methods has been limited, and failure cases have not been well understood. Here we study the PPI network structure of 519 disease pathways. We find that 90% of pathways do not correspond to single well-connected components in the PPI network. Instead, proteins associated with a single disease tend to form many separate connected components/regions in the network. We then evaluate state-of-the-art disease pathway discovery methods and show that their performance is especially poor on diseases with disconnected pathways. Thus, we conclude that network connectivity structure alone may not be sufficient for disease pathway discovery. However, we show that higher-order network structures, such as small subgraphs of the pathway, provide a promising direction for the development of new methods."}, {"title": "canvasXpress: A versatile interactive high-resolution scientific multi-panel visualization toolkit", "url": "https://www.biorxiv.org/content/early/2017/09/15/186213", "tag": "Bioinformatics", "abstract": "CanvasXpress was developed as the core visualization component for bioinformatics and systems biology analysis at Bristol-Myers Squibb and further enhanced by scientists around the world and served as key visualization engine for many popular bioinformatics tools. It supports a large number of visualizations as shown at http://canvasxpress.org to display mainly scientific and genomics data that includes sophisticated plots such as oncoprint of cancer mutations, heatmap, 3D scatter, violin, spider, and profile plot as illustrated in Figure 1 that is arranged together by the accompanying html based tool, canvasDesigner (https://baohongz.github.io/canvasDesigner). Recently, reproducibility and usability of the package in real world bioinformatics and clinical use cases have been improved significantly witnessed by continuous add-on features and wide adoption of the toolkit in the scientific communities. Furthermore, It is the first noteworthy package harmonizing real time interactive exploration and analysis of big data, full-fledged customization of look-n-feel, and production of multi-panel publication-ready figures in PDF format simultaneously."}, {"title": "SnapperDB: A database solution for routine sequencing analysis of bacterial isolates", "url": "https://www.biorxiv.org/content/early/2017/09/15/189118", "tag": "Bioinformatics", "abstract": "Real-time surveillance of infectious disease using whole genome sequencing data poses challenges in both result generation and communication. SnapperDB represents a set of tools to store bacterial variant data and facilitate reproducible and scalable analysis of bacterial populations. We also introduce the SNP address nomenclature to describe the relationship between isolates in a population to the single nucleotide resolution."}, {"title": "Selenzyme: Enzyme selection tool for pathway design", "url": "https://www.biorxiv.org/content/early/2017/09/15/188979", "tag": "Bioinformatics", "abstract": "Synthetic biology applies the principles of engineering to biology in order to create biological functionalities not seen before in nature. One of the most exciting applications of synthetic biology is the design of new organisms with the ability to produce valuable chemicals including pharmaceuticals and biomaterials in a greener; sustainable fashion. Selecting the right enzymes to catalyze each reaction step in order to produce a desired target compound is, however, not trivial. Here, we present Selenzyme, a free online enzyme selection tool for metabolic pathway design. The user is guided through several decision steps in order to shortlist the best candidates for a given pathway step. The tool graphically presents key information about enzymes based on existing databases and tools such as: similarity of sequences and of catalyzed reactions; phylogenetic distance between source organism and intended host species; multiple alignment highlighting conserved regions, predicted catalytic site, and active regions; and relevant properties such as predicted solubility and transmembrane regions. Selenzyme provides bespoke sequence selection for automated workflows in biofoundries. The tool is integrated as part of the pathway design stage into the design/build/test/learn SYNBIOCHEM pipeline. The Selenzyme web server is available at http://selenzyme.synbiochem.co.uk."}, {"title": "PlasmidTron: assembling the cause of phenotypes from NGS data", "url": "https://www.biorxiv.org/content/early/2017/09/15/188920", "tag": "Bioinformatics", "abstract": "When defining bacterial populations through whole genome sequencing (WGS) the samples often have detailed associated metadata that relate to disease severity, antimicrobial resistance, or even rare biochemical traits. When comparing these bacterial populations, it is apparent that some of these phenotypes do not follow the phylogeny of the host i.e. they are genetically unlinked to the evolutionary history of the host bacterium. One possible explanation for this phenomenon is that the genes are moving independently between hosts and are likely associated with mobile genetic elements (MGE). However, identifying the element that is associated with these traits can be complex if the starting point is short read WGS data. With the increased use of next generation WGS in routine diagnostics, surveillance and epidemiology a vast amount of short read data is available and these types of associations are relatively unexplored. One way to address this would be to perform assembly de novo of the whole genome read data, including its MGEs. However, MGEs are often full of repeats and can lead to fragmented consensus sequences. Deciding which sequence is part of the chromosome, and which is part of a MGE can be ambiguous. We present PlasmidTron, which utilises the phenotypic data normally available in bacterial population studies, such as antibiograms, virulence factors, or geographic information, to identify sequences that are likely to represent MGEs linked to the phenotype. Given a set of reads, categorised into cases (showing the phenotype) and controls (phylogenetically related but phenotypically negative), PlasmidTron can be used to assemble de novo reads from each sample linked by a phenotype. A k-mer based analysis is performed to identify reads associated with a phylogenetically unlinked phenotype. These reads are then assembled de novo to produce contigs. By utilising k-mers and only assembling a fraction of the raw reads, the method is fast and scalable to large datasets. This approach has been tested on plasmids, because of their contribution to important pathogen associated traits, such as AMR, hence the name, but there is no reason why this approach cannot be utilized for any MGE that can move independently through a bacterial population. PlasmidTron is written in Python 3 and available under the open source licence GNU GPL3 from https://github.com/sanger-pathogens/plasmidtron ."}, {"title": "Massive Mining of Publicly Available RNA-seq Data from Human and Mouse", "url": "https://www.biorxiv.org/content/early/2017/09/15/189092", "tag": "Bioinformatics", "abstract": "RNA-sequencing (RNA-seq) is currently the leading technology for genome-wide transcript quantification. While the volume of RNA-seq data is rapidly increasing, the currently publicly available RNA-seq data is provided mostly in raw form, with small portions processed non-uniformly. This is mainly because the computational demand, particularly for the alignment step, is a significant barrier for global and integrative retrospective analyses. To address this challenge, we developed all RNA-seq and ChIP-seq sample and signature search (ARCHS4), a web resource that makes the majority of previously published RNA-seq data from human and mouse freely available at the gene count level. Such uniformly processed data enables easy integration for downstream analyses. For developing the ARCHS4 resource, all available FASTQ files from RNA-seq experiments were retrieved from the Gene Expression Omnibus (GEO) and aligned using a cloud-based infrastructure. In total 137,792 samples are accessible through ARCHS4 with 72,363 mouse and 65,429 human samples. Through efficient use of cloud resources and dockerized deployment of the sequencing pipeline, the alignment cost per sample is reduced to less than one cent. ARCHS4 is updated automatically by adding newly published samples to the database as they become available. Additionally, the ARCHS4 web interface provides intuitive exploration of the processed data through querying tools, interactive visualization, and gene landing pages that provide average expression across cell lines and tissues, top co-expressed genes, and predicted biological functions and protein-protein interactions for each gene based on prior knowledge combined with co-expression. Benchmarking the quality of these predictions, co-expression correlation data created from ARCHS4 outperforms co-expression data created from other major gene expression data repositories such as GTEx and CCLE. ARCHS4 is freely accessible from: http://amp.pharm.mssm.edu/archs4."}, {"title": "Redkmer: An assembly-free pipeline for the identification of abundant and specific X-chromosome target sequences for X-shredding by CRISPR endonucleases", "url": "https://www.biorxiv.org/content/early/2017/09/15/189431", "tag": "Bioinformatics", "abstract": "CRISPR-based synthetic sex ratio distorters, that operate by shredding the X-chromosome during male meiosis, are promising tools for the area-wide control of harmful insect pest or disease vector species. However, the selection of gRNA targets, in the form of high-copy sequence repeats on the X chromosome of a given species, is difficult since such repeats are not accurately resolved in genome assemblies and can't be assigned to chromosomes with confidence. We have therefore developed the redkmer computational pipeline, designed to identify short and highly-abundant sequence elements occurring uniquely on the X-chromosome. Redkmer was designed to use as input exclusively raw WGS data from males and females. We tested redkmer with suitable short and long read WGS data of An. gambiae , the major vector of human malaria, in which the X-shredding paradigm was originally developed. Redkmer establishes long reads as chromosomal proxies with excellent correlation to the genome assembly and uses them to rank X-candidate kmers for their level of X-specificity and abundance. Redkmer identified a high-confidence set of 25-mers, many of which belong to previously known X-chromosome specific repeats of An. gambiae , including the ribosomal gene array and the selfish genetics elements harbored within it. WGS data from a control strain in which these repeats are also present on the Y chromosome confirmed the elimination of these kmers in the filtering steps. Finally, we show that redkmer output can be linked directly to gRNA selection and can also inform gRNA off-target prediction. The redkmer pipeline is designed to enable the generation of synthetic sex ratio distorters for the control of harmful insect species of medical or agricultural importance. It proceeds from WGS input data to deliver candidate X-specific CRISPR gRNA candidate target sequences. In addition the output of redkmer, including the prediction of chromosomal origin of single-molecule long reads and chromosome specific kmers, could also be used for the characterization of other biologically relevant sex chromosome sequences, a task that is frequently hampered by the repetitiveness of sex chromosome sequence content."}, {"title": "Deep learning of the splicing (epi)genetic code reveals a novel candidate mechanism linking histone modifications to ESC fate decision", "url": "https://www.biorxiv.org/content/early/2017/09/15/189183", "tag": "Bioinformatics", "abstract": "Alternative splicing (AS) is a genetically and epigenetically regulated pre-mRNA processing to increase transcriptome and proteome diversity. Comprehensively decoding these regulatory mechanisms holds promise in getting deeper insights into a variety of biological contexts involving in AS, such as development and diseases. We assembled splicing (epi)genetic code, DeepCode, for human embryonic stem cell (hESC) differentiation by integrating heterogeneous features of genomic sequences, 16 histone modifications with a multi-label deep neural network. With the advantages of epigenetic features, DeepCode significantly improves the performance in predicting the splicing patterns and their changes during hESC differentiation. Meanwhile, DeepCode reveals the superiority of epigenomic features and their dominant roles in decoding AS patterns, highlighting the necessity of including the epigenetic properties when assembling a more comprehensive splicing code. Moreover, DeepCode allows the robust predictions across cell lineages and datasets. Especially, we identified a putative H3K36me3-regulated AS event leading to a nonsense-mediated mRNA decay of BARD1. Reduced BARD1 expression results in the attenuation of ATM/ATR signalling activities and further the hESC differentiation. These results suggest a novel candidate mechanism linking histone modifications to hESC fate decision. In addition, when trained in different contexts, DeepCode can be expanded to a variety of biological and biomedical fields."}, {"title": "seq-seq-pan: Building a computational pan-genome data structure on whole genome alignment", "url": "https://www.biorxiv.org/content/early/2017/09/15/188904", "tag": "Bioinformatics", "abstract": "The increasing application of next generation sequencing technologies has led to the availability of thousands of reference genomes, often providing multiple genomes for the same or closely related species. The current approach to represent a species or a population with a single reference sequence and a set of variations cannot represent their full diversity and introduces bias towards the chosen reference. There is a need for the representation of multiple sequences in a composite way that is compatible with existing data sources for annotation and suitable for established sequence analysis methods. At the same time, this representation needs to be easily accessible and extendable to account for the constant change of available genomes. We introduce seq-seq-pan, a framework that provides methods for adding or removing new genomes from a set of aligned genomes and uses these to construct a whole genome alignment. Throughout the sequential workflow the alignment is optimized for generating a representative linear presentation of the aligned set of genomes, that enables its usage for annotation and in downstream analyses. By providing dynamic updates and optimized processing, our approach enables the usage of whole genome alignment in the field of pan-genomics. In addition, the sequential workflow can be used as a fast alternative to existing whole genome aligners. seq-seq-pan is freely available at https://gitlab.com/groups/rki_bioinformatics"}, {"title": "SodaPop: A computational suite for simulating the dynamics of asexual populations", "url": "https://www.biorxiv.org/content/early/2017/09/15/189142", "tag": "Bioinformatics", "abstract": "Motivation: Simulating protein evolution with realistic constraints from population genetics is essential in addressing problems in molecular evolution, from understanding the forces shaping the evolutionary landscape to the clinical challenges of antibiotic resistance, viral evolution and cancer. Results: To address this need, we present SodaPop, a new forward-time simulator of large asexual populations aimed at studying their structure, dynamics and the distribution of fitness effects with flexible assumptions on the fitness landscape. SodaPop integrates biochemical and biophysical properties in a cell-based, object-oriented framework and provides an efficient, open-source toolkit for performing large-scale simulations of protein evolution. Availability and implementation: Source code and binaries are freely available at https://github.com/louisgt/SodaPop under the GNU GPLv3 license. The software is implemented in C++ and supported on Linux, Mac OS/X and Windows."}, {"title": "Pathways of DNA unlinking: A story of stepwise simplification", "url": "https://www.biorxiv.org/content/early/2017/09/14/188722.1", "tag": "Bioinformatics", "abstract": "In Escherichia coli DNA replication yields interlinked chromosomes. Controlling topological changes associated with replication and returning the newly replicated chromosomes to an unlinked monomeric state is essential to cell survival. In the absence of the topoisomerase topoIV, the site-specific recombination complex XerCD-dif-FtsK can remove replication links by local reconnection. We previously showed mathematically that there is a unique minimal pathway of unlinking replication links by reconnection while stepwise reducing the topological complexity. However, the possibility that reconnection preserves or increases topological complexity is biologically plausible. In this case, are there other unlinking pathways? Which is the most probable? We consider these questions in an analytical and numerical study of minimal unlinking pathways. We use a Markov Chain Monte Carlo algorithm with Multiple Markov Chain sampling to model local reconnection on 491 different substrate topologies, 166 knots and 325 links, and distinguish between pathways connecting a total of 881 different topologies. We conclude that the minimal pathway of unlinking replication links that was found under more stringent assumptions is the most probable. We also present exact results on unlinking a 6-crossing replication link. These results point to a general process of topology simplification by local reconnection, with applications going beyond DNA."}, {"title": "FlashFry: a fast and flexible tool for large-scale CRISPR target design", "url": "https://www.biorxiv.org/content/early/2017/09/14/189068", "tag": "Bioinformatics", "abstract": "FlashFry is a fast and flexible command-line tool for characterizing large numbers of CRISPR target sequences. While several CRISPR web application exist, genome-wide knockout studies, noncoding deletion scans, and other large-scale studies or methods development projects require a simple and lightweight framework that can quickly discover and score thousands of candidates guides targeting an arbitrary DNA sequence. With FlashFry, users can specify an unconstrained number of mismatches to putative off-targets, richly annotate discovered sites, and tag potential guides with commonly used on-target and off-target scoring metrics. FlashFry runs at speeds comparable to widely used genome-wide sequence aligners, and output is provided as an easy-to-manipulate text file."}, {"title": "A whole genome analysis of the red-crowned crane provides insight into avian longevity", "url": "https://www.biorxiv.org/content/early/2017/09/14/188656", "tag": "Bioinformatics", "abstract": "The red-crowned crane (Grus japonensis) is an endangered and large-bodied crane native to East Asia. It is a traditional symbol of longevity and its long lifespan has been confirmed both in captivity and in the wild. Lifespan in birds is positively correlated with body size and negatively correlated with metabolic rate; although the genetic mechanisms for the red-crowned crane\u2032s long lifespan have not previously been investigated. Using whole genome sequencing and comparative evolutionary analyses against the grey-crowned crane and other avian genomes, we identified candidate genes that are correlated with longevity. Included among these are positively selected genes with known associations with longevity in metabolism and immunity pathways (NDUFA5, NDUFA8, NUDT12, IL9R, SOD3, NUDT12, PNLIP, CTH, and RPA1). Our analyses provide genetic evidence for low metabolic rate and longevity, accompanied by possible convergent adaptation signatures among distantly related large and long-lived birds. Finally, we identified low genetic diversity in the red-crowned crane, consistent with its listing as an endangered species, and we hope this genome will provide a useful genetic resource for future conservation studies of this rare and iconic species."}, {"title": "Developing a 3- to 6-state EEG-based brain-computer interface for a robotic manipulator control", "url": "https://www.biorxiv.org/content/early/2017/09/14/171025", "tag": "Bioinformatics", "abstract": "Recent developments in BCI techniques have demonstrated high-performance control of robotic prosthetic systems primarily via invasive methods. In this work we develop an electroencephalography (EEG) based noninvasive BCI system that can be used for a similar, albeit lower-speed robotic control, and a signal processing system for detecting user's mental intent from EEG data based on up to 6-state motor-imagery BCI communication paradigm. We examine the performance of that system on experimental data collected from 12 healthy participants and analyzed offline. We show that our EEG BCI system can correctly identify different motor imageries in EEG data with high accuracy: 3 out of 12 participants achieved accuracy of 6-state communication in 80-90% range, while 2 participants could not achieve a satisfactory accuracy. We further implement an online BCI system for control of a virtual 3 degree-of-freedom prosthetic manipulator and test it with our 3 best participants. The participants' ability to control the BCI is quantified by using the percentage of successfully completed BCI tasks, the time required to complete a task, and the error rate. 2 participants were able to successfully complete 100% of the test tasks, demonstrating on average the error rate of 80% and requiring 5-10 seconds to execute a manipulator move. 1 participant failed to demonstrate a satisfactory performance in online trials. Our results lay a foundation for further development of EEG BCI-based robotic assistive systems and demonstrate that EEG-based BCI may be feasible for robotic control by paralyzed and immobilized individuals."}, {"title": "Organisation of feed-forward loop motifs reveals architectural principles in natural and engineered networks", "url": "https://www.biorxiv.org/content/early/2017/09/14/188821", "tag": "Bioinformatics", "abstract": "Network motifs are significantly expressed sub-graphs that have been proposed as building blocks for natural and engineered networks. Detailed functional analysis has been performed for many types of motif in isolation, but less is known about how motifs work together to perform complex tasks. To address this issue we measure the aggregation of network motifs via methods that extract precisely how these structures are connected. Applying this approach to a broad spectrum of networked systems and focusing on the widespread feed-forward loop motif, we uncover striking differences in motif organisation. The types of connection are often highly constrained, differ between domains, and clearly capture architectural principles. We show how this information can be used to effectively predict functionally important nodes in the metabolic network of Escherichia coli. Our findings have implications for understanding how networked systems are constructed from motif parts and elucidates constraints that guide their evolution."}, {"title": "xenoGI: reconstructing the history of genomic island insertions in clades of closely related bacteria", "url": "https://www.biorxiv.org/content/early/2017/09/14/188599", "tag": "Bioinformatics", "abstract": "Genomic islands play an important role in microbial genome evolution, providing a mechanism for strains to adapt to new ecological conditions. A variety of computational methods, both genome-composition based and comparative have been developed to identify them. Some of these methods are explicitly designed to work in single strains, while others make use of multiple strains. In general, existing methods do not identify islands in the context of the phylogeny in which they evolved. Even multiple strain approaches are best suited to identifying genomic islands that are present in one strain but absent in others. They do not automatically recognize islands which are shared between some strains in the clade or determine the branch on which these islands inserted within the phylogenetic tree. We have developed a software package, xenoGI, that identifies genomic islands and maps their origin within a clade of closely related bacteria, determining which branch they inserted on. It takes as input a set of sequenced genomes and a tree specifying their phylogenetic relationships. Making heavy use of synteny information, the package builds gene families in a species-tree-aware way, and then attempts to combine into islands those families whose members are adjacent and whose most recent common ancestor is shared. The package provides a variety of text-based analysis functions, as well as the ability to export genomic islands into formats suitable for viewing in a genome browser. We demonstrate the capabilities of the package with several examples from enteric bacteria, including an examination of the evolution of the acid fitness island in the genus Escherichia. In addition we use output from simulations and a set of known genomic islands from the literature to show that xenoGI can accurately identify genomic islands and place them on a phylogenetic tree. xenoGI is an effective tool for studying the history of genomic island insertions in a clade of microbes. It identifies genomic islands, and determines which branch they inserted on within the phylogenetic tree for the clade. Such information is valuable because it helps us understand the adaptive path that has produced living species. Given the large and growing number of sequenced microbial genomes, this sort of analysis will become increasingly useful in the future."}, {"title": "Edge Detection of Cryptic Lamellipodia Assisted by Deep Learning", "url": "https://www.biorxiv.org/content/early/2017/09/13/181263", "tag": "Bioinformatics", "abstract": "Cell protrusion plays important roles in cell migration by pushing plasma membrane forward. Cryptic lamellipodia induce the protrusion of submarginal cells in collective cell migration where cells are attached and move together. Although computational image analysis of cell protrusion has been done extensively, the study on protrusion activities of cryptic lamellipodia is limited due to difficulties in image segmentation. This study seeks to aid in the computational analysis of submarginal cell protrusion in collective cell migration by using deep learning to detect the cryptic lamellipodial edges from fluorescence time-lapse movies. Due to the noisy features within overlapping cells, the conventional image analysis algorithms such as Canny edge detector and intensity thresholding are limited. In this paper we combined Canny edge detector, Deep Neural Networks (DNNs), and local intensity thresholding. We were able to detect cryptic lamellipodial edges of submarginal cells with high accuracy from the fluorescence time-lapse movies of PtK1 cells using both simple convolutional neural networks and VGG-16 based neural networks. We used relatively small effort to prepare the training set to train the DNNs to detect the cryptical lamellipodial edges in fluorescence time-lapse movies. This work demonstrates that deep learning can be combined with the conventional image analysis algorithms to facilitate the computational analysis of highly complex time-lapse movies of collective cell migration."}, {"title": "Motif signatures of transcribed enhancers", "url": "https://www.biorxiv.org/content/early/2017/09/13/188557", "tag": "Bioinformatics", "abstract": "In mammalian cells, transcribed enhancers (TrEn) play important roles in the initiation of gene expression and maintenance of gene expression levels in spatiotemporal manner. One of the most challenging questions in biology today is how the genomic characteristics of enhancers relate to enhancer activities. This is particularly critical, as several recent studies have linked enhancer sequence motifs to specific functional roles. To date, only a limited number of enhancer sequence characteristics have been investigated, leaving space for exploring the enhancers genomic code in a more systematic way. To address this problem, we developed a novel computational method, TELS, aimed at identifying predictive cell type/tissue specific motif signatures. We used TELS to compile a comprehensive catalog of motif signatures for all known TrEn identified by the FANTOM5 consortium across 112 human primary cells and tissues. Our results confirm that distinct cell type/tissue specific motif signatures characterize TrEn. These signatures allow discriminating successfully a) TrEn from random controls, proxy of non-enhancer activity, and b) cell type/tissue specific TrEn from enhancers expressed and transcribed in different cell types/tissues. TELS codes and datasets are publicly available at http://www.cbrc.kaust.edu.sa/TELS."}, {"title": "clusterSeq: methods for identifying co-expression in high-throughput sequencing data", "url": "https://www.biorxiv.org/content/early/2017/09/13/188581", "tag": "Bioinformatics", "abstract": "Identifying gene co-expression is a significant step in understanding functional relationships between genes. Existing methods primarily depend on analyses of correlation between pairs of genes; however, this neglects structural elements between experimental conditions. We present a novel approach to identifying clusters of co-expressed genes that incorporates these structures."}, {"title": "CscoreTool: Fast Hi-C Compartment Analysis at High Resolution", "url": "https://www.biorxiv.org/content/early/2017/09/13/188490", "tag": "Bioinformatics", "abstract": "Summary: The chromosome conformation capture (Hi-C) has revealed that the eukaryotic genome can be partitioned into A and B compartments that have distinctive chromatin and transcription features. Current Principle Component Analyses (PCA)-based method for the prediction of A/B compartment prediction from Hi-C data requires substantial CPU time and memory. We report the development of a method, CscoreTool, that enables fast and memory-efficient determination of A/B compartments at high resolution even in dataset with low sequencing depth. Availability and Implementation: Source code is freely available at https://github.com/scoutzxb/CscoreTool, implemented in C++. Contact: xzheng@carnegiescience.edu"}, {"title": "Interactive Visual Analysis of Mass Cytometry Data by Hierarchical Stochastic Neighbor Embedding Reveals Rare Cell Types", "url": "https://www.biorxiv.org/content/early/2017/09/13/169888", "tag": "Bioinformatics", "abstract": "Mass cytometry allows high-resolution dissection of the cellular composition of the immune system. However, the high-dimensionality, large size, and non-linear structure of the data poses considerable challenges for data analysis. In particular, dimensionality reduction-based techniques like t-SNE offer single-cell resolution but are limited in the number of cells that can be analysed. Here we introduce Hierarchical Stochastic Neighbor Embedding (HSNE) for the analysis of mass cytometry datasets. HSNE constructs a hierarchy of non-linear similarities that can be interactively explored with a stepwise increase in detail up to the single-cell level. We applied HSNE to a study on gastrointestinal disorders and three other available mass cytometry datasets. We found that HSNE efficiently replicates previous observations and identifies rare cell populations that were previously missed due to downsampling. Thus, HSNE removes the scalability limit of conventional t-SNE analysis, a feature that makes it highly suitable for the analysis of massive high-dimensional datasets."}, {"title": "NetMHCpan 4.0: Improved peptide-MHC class I interaction predictions integrating eluted ligand and peptide binding affinity data", "url": "https://www.biorxiv.org/content/early/2017/09/13/149518", "tag": "Bioinformatics", "abstract": "Cytotoxic T cells are of central importance in the immune systems response to disease. They recognize defective cells by binding to peptides presented on the cell surface by MHC (major histocompatibility complex) class I molecules. Peptide binding to MHC molecules is the single most selective step in the antigen presentation pathway. On the quest for T cell epitopes, the prediction of peptide binding to MHC molecules has therefore attracted large attention. In the past, predictors of peptide-MHC interaction have in most cases been trained on binding affinity data. Recently an increasing amount of MHC presented peptides identified by mass spectrometry has been published containing information about peptide processing steps in the presentation pathway and the length distribution of naturally presented peptides. Here, we present NetMHCpan-4.0, a method trained on both binding affinity and eluted ligand data leveraging the information from both data types. Large-scale benchmarking of the method demonstrates an increased predictive performance compared to state-of-the-art when it comes to identification of naturally processed ligands, cancer neoantigens, and T cell epitopes."}, {"title": "Population networks from DNA sequences: methodological developments", "url": "https://www.biorxiv.org/content/early/2017/09/13/188268", "tag": "Bioinformatics", "abstract": "Several classes of methods have been proposed for inferring the history of populations from genetic polymorphism data. As connectivity is a key factor to explain the structure of populations, several graph-based methods have been developed to this aim, using popu- lation genetics data. Here we propose an original method based on graphical models that uses DNA sequences to provide relationships between populations. We tested our method on various simulated data sets, describing typical demographic scenarios, for different param- eters values. We found that our method behaved noticeably well for realistic demographic evolutionary processes and recovered suitably the migration processes. Our method provides thus a complementary tool for investigating population history based on genetic material."}, {"title": "hmmIBD: software to infer pairwise identity by descent between haploid genotypes", "url": "https://www.biorxiv.org/content/early/2017/09/12/188078", "tag": "Bioinformatics", "abstract": "We introduce hmmIBD, software to estimate pairwise identity by decent between haploid genomes, such as those of the malaria parasite, sampled from one or more populations. We verified hmmIBD using simulated data, benchmarked it against a previously published method for detecting IBD within populations, and demonstrated its utility using Plasmodium falciparum data from Cambodia and Ghana. Availability and Implemetation: Source code written in C99/C11-compliant C and requiring no external libraries, is freely available for download at https://github.com/glipsnort/hmmIBD/releases, alongside test datasets."}, {"title": "Chiron: Translating nanopore raw signal directly into nucleotide sequence using deep learning", "url": "https://www.biorxiv.org/content/early/2017/09/12/179531", "tag": "Bioinformatics", "abstract": "Sequencing by translocating DNA fragments through an array of nanopores is a rapidly maturing technology which offers faster and cheaper sequencing than other approaches. However, accurately deciphering the DNA sequence from the noisy and complex electrical signal is challenging. Here, we report the first deep learning model - Chiron - that can directly translate the raw signal to DNA sequence without the error-prone segmentation step. We show that our model provides state-of-the-art basecalling accuracy when trained with only a small set of 4000 reads. Chiron achieves basecalling speeds of over 2000 bases per second using desktop computer graphics processing units, making it competitive with other deep-learning basecalling algorithms."}, {"title": "SHAMAN: bin-free randomization, normalization and screening of Hi-C matrices", "url": "https://www.biorxiv.org/content/early/2017/09/12/187203", "tag": "Bioinformatics", "abstract": "Genome wide chromosome conformation capture (Hi-C) is used to interrogate contact frequencies among genomic elements at multiple scales and intensities, ranging from high frequency interactions among proximal regulatory elements, through specific long-range loops between insulator binding sites and up to rare and transient cis- and trans-chromosomal contacts. Visualization and statistical analysis of Hi-C data is made difficult by the extreme variation in the background frequencies of chromosomal contacts between elements at short and long genomic distances. Here we introduce SHAMAN for performing Hi-C analysis at dynamic scales, without predefined resolution, and while minimizing biases over very large datasets. Algorithmically, we devise a Markov Chain Monte Carlo-like procedure for randomizing contact matrices such that coverage and contact distance distributions are preserved. We combine this strategy with bin-free assessment of contact enrichment using a K-nearest neighbor approach. We show how to use the new method for visualizing contact hotspots and for quantifying differential contacts in matching Hi-C maps. We demonstrate how contact preferences among regulatory elements, including promoters, enhancers and insulators can be assessed with minimal bias by comparing pooled empirical and randomized matrices. Full support for our methods is available in a new software package that is freely available."}, {"title": "Sensitive and specific post-call filtering of genetic variants in xenograft and primary tumors", "url": "https://www.biorxiv.org/content/early/2017/09/12/187468", "tag": "Bioinformatics", "abstract": "Tumor genome sequencing offers great promise for guiding research and therapy, but spurious variant calls can arise from multiple sources. Mouse contamination can generate many spurious calls when sequencing patient-derived xenografts (PDXs). Paralogous genome sequences can also generate spurious calls when sequencing any tumor. We developed a BLAST-based algorithm, MAPEX, to identify and filter out spurious calls from both these sources. When calling variants from xenografts, MAPEX has similar sensitivity and specificity to more complex algorithms. When applied to any tumor, MAPEX also automatically flags calls that potentially arise from paralogous sequences. Our implementation, mapexr, runs quickly and easily on a desktop computer. MAPEX is thus a useful addition to almost any pipeline for calling genetic variants in tumors."}, {"title": "Methylation-To-Expression Feature Models of Breast Cancer Accurately Predict Overall Survival, Distant-Recurrence Free Survival, And Pathologic Complete Response in Multiple Cohorts", "url": "https://www.biorxiv.org/content/early/2017/09/12/187526", "tag": "Bioinformatics", "abstract": "Background: Approaches that capitalize on the benefits of multi-omic data integration in invasive breast carcinoma to define prognostic biomarkers for precision medicine have been slow to emerge. In this work, we examined the efficacy of our methylation-to-expression feature model (M2EFM) approach to combining molecular and clinical predictors as part of a single analysis to create prognostic risk scores for overall survival, distant metastasis, and chemosensitivity. Methods: Gene expression and DNA methylation values as well as clinical variables were integrated via M2EFM to build prognostic models of overall survival using 1028 breast tumor samples and further applied to external validation cohorts of 61 and 327 samples. Data-integrated prognostic models of distant recurrence-free survival and pathologic complete response were built using 306 samples and validated on 182 samples of external validation data. Additionally, we compared the discrimination and calibration of M2EFM models to other approaches. Results: Despite different populations and assays, M2EFM models validated with good accuracy (C-index or AUC \u2265 .7) for all outcomes in all validation data. M2EFM models had the most consistent performance overall and superior calibration, suggesting a greater likelihood of clinical utility. Finally, we demonstrated that M2EFM identifies functionally relevant genes, which could be useful in translating an M2EFM biomarker to the clinic. Conclusion: M2EFM uses multiple levels of genomic data to infer disrupted regulatory patterns, thus providing a gene signature that connects loss of regulatory control with cancer prognosis."}, {"title": "dBBQs : dataBase of Bacterial Quality scores", "url": "https://www.biorxiv.org/content/early/2017/09/12/187641", "tag": "Bioinformatics", "abstract": "Background: It is well-known that genome sequencing technologies are becoming significantly cheaper and faster. As a result of this, the exponential growth in sequencing data in public databases allows us to explore ever growing large collections of genome sequences. However, it is less known that the majority of available sequenced genome sequences in public databases are not complete, drafts of varying qualities. We have calculated quality scores for around 100,000 bacterial genomes from all major genome repositories and put them in a fast and easy-to-use database. Results: Prokaryotic genomic data from all sources were collected and combined to make a non-redundant set of bacterial genomes. The genome quality score for each was calculated by four different measurements: assembly quality, number of rRNA and tRNA genes, and the occurrence of conserved functional domains. The dataBase of Bacterial Quality scores (dBBQs) was designed to store and retrieve quality scores. It offers fast searching and download features which the result can be used for further analysis. In addition, the search results are shown in interactive JavaScript chart framework using DC.js. The analysis of quality scores across major public genome databases find that around 68% of the genomes are of acceptable quality for many uses. Conclusions: dBBQs (available at http://arc-gem.uams.edu/dbbqs) provides genome quality scores for all available prokaryotic genome sequences with a user-friendly Web-interface. These scores can be used as cut-offs to get a high-quality set of genomes for testing bioinformatics tools or improving the analysis. Moreover, all data of the four measurements that were combined to make the quality score for each genome, which can potentially be used for further analysis. dBBQs will be updated regularly and is freely use for non-commercial purpose."}, {"title": "Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,609 UK Biobank participants", "url": "https://www.biorxiv.org/content/early/2017/09/12/187625", "tag": "Bioinformatics", "abstract": "Current public health guidelines on physical activity and sleep duration are limited by a reliance on subjective self-reported evidence. Using data from simple wrist-worn activity monitors, we developed a tailored machine learning model, using balanced random forests with Markov confusion matrices, to reliably detect a number of activity modes. We show that physical activity and sleep behaviours can be classified with 87% accuracy in 84,616 minutes of recorded free-living behaviours from 57 adults. These trained models can be used to infer fine resolution activity patterns at the population scale in 96,609 participants. For example, we find that men spend more time in both low- and high-intensity behaviours, while women spend more time in mixed behaviours. Walking time is highest in spring and sleep time lowest during the summer. This work opens the possibility of future public health guidelines informed by the health consequences associated with specific, objectively measured, physical activity and sleep behaviours."}, {"title": "Ensemble gene function prediction database reveals genes important for complex I formation in Arabidopsis thaliana", "url": "https://www.biorxiv.org/content/early/2017/09/11/181396", "tag": "Bioinformatics", "abstract": "Despite increasing availability of sequenced genomes, accurate characterization of gene functions is needed to close the genotype-phenotype gap. Recent advances in gene function prediction rely on ensemble approaches that integrate the results from multiple inference methods to produce superior predictions. Yet, these developments remain largely unexplored in plants. We present Neighbor Counting Ensemble, a gene function prediction method which integrates eleven gene co-function networks for Arabidopsis thaliana, and produces more accurate gene function predictions for a larger fraction of genes with unknown function. We used these predictions to identify genes involved in mitochondrial complex I formation, and for five of them we confirmed the predictions experimentally. The ensemble predictions are provided as a user-friendly online database, EnsembleNet, available at http://www.gene2function.de/ensemblenet.html."}, {"title": "GARFIELD-NGS: Genomic vARiants FIltering by dEep Learning moDels in NGS", "url": "https://www.biorxiv.org/content/early/2017/09/11/149146", "tag": "Bioinformatics", "abstract": "Exome sequencing approach is extensively used in research and diagnostic laboratories to discover pathological variants and study genetic architecture of human diseases. Even if present platforms produce high quality sequencing data, false positives variants remain an issue and can confound subsequent analysis and result interpretation. Here, we propose a new tool named GARFIELD-NGS (Genomic vARiants FIltering by dEep Learning moDels in NGS), which uses deep learning algorithm to dissect false and true variants in exome sequencing experiments performed with Illumina or ION platforms. GARFIELD-NGS consists of 4 distinct models tested on NA12878 gold-standard exome variants dataset (NIST v.3.3.2): Illumina INDELs, Illumina SNPs, ION INDELs, and ION SNPs. AUC values for each variant category are 0.9267, 0.7998, 0.9464, and 0.9757, respectively. GARFIELD-NGS is robust on low coverage data down to 30X and on Illumina two-colour data, as well. Our tool outperformed previous hard-filters, and calculates for each variant a score from 0.0 to 1.0, allowing application of different thresholds based on desired level of sensitivity and specificity. GARFIELD-NGS processes standard VCF file input using Perl and Java scripts and produces a regular VCF output. Thus, it can be easily integrated in existing analysis pipeline. GARFIELD-NGS is freely available at https://github.com/gedoardo83/GARFIELD-NGS."}, {"title": "Capturing the Difference in Humoral Immunity between Normal and Tumor Environments from RNA Sequences of B-Cell Receptors Using Supervised Machine Learning", "url": "https://www.biorxiv.org/content/early/2017/09/11/187120", "tag": "Bioinformatics", "abstract": "Motivation: The success of immunotherapy in treating tumors has been attracting increasing interest in the adaptive immune system in the tumor microenvironment in recent years. Progress in next-generation sequencing technology now enables sequencing of whole T-cell receptors (TCRs) and B-cell receptors (BCRs)/immunoglobulin (Igs) in the tumor microenvironment. Since BCRs/Igs in tumor tissue have high affinity for tumor-specific antigens, the patterns of amino acid sequences of BCRs/Igs might differ in the normal and tumor environment. However, given the high diversity of BCRs/Igs and the rare recurrent sequences among individuals, it is far more difficult to capture such differences in BCRs/Igs sequences compared to obtaining similar information for TCRs. The aim of this study was to capture this difference by applying supervised machine learning methods to RNA sequences of BCRs/Igs obtained from matched normal and tumor specimens of 90 patients with gastric cancer. Results: We adopted two approaches to evaluate the possibility of accurately differentiating BCRs/Igs amino acid sequences of the tumor environment. First, we used two kinds of discriminative models for sequence classification between normal and tumor tissues. One takes into account only V and J frame usages and the other deals with the physicochemical properties of amino acid sequences. The results demonstrated clear differences in the physicochemical properties of amino acid sequences between BCRs/Igs derived from normal and tumor tissues, which could not be captured by applying V and J frame usage alone. In the second approach, we assumed a diagnostic situation in which a set of BCR/Ig-sequences is available for each patient, and we classified normal and tumor tissues based on the BCR sequences. Combining BCR/Ig-level classifiers and the Shannon entropy of the BCRs/Igs repertoire, we achieved an area under the curve value of 0.834, suggesting that BCRs/Igs repertoires have sufficient environment-specific features, which can be used to determine the tissue type. Conclusion: To the best of our knowledge, this is the first study to show that BCRs/Igs sequences derived from tumor and normal tissues have globally distinct patterns, indicating that these tissues can be effectively differentiated using BCRs/Igs repertoires. Our research therefore provides comprehensive benefits in advancing the field of cancer immunity."}, {"title": "A minimalist approach to protein identification", "url": "https://www.biorxiv.org/content/early/2017/09/10/187013", "tag": "Bioinformatics", "abstract": "Computations on proteome sequence databases show that most proteins can be identified from a protein's isoelectric point (IEP) and digitized linear sequence volume (equal to the total volume of its residues). This is illustrated with four proteomes: H. pylori (1553 proteins), E. coli (4306 proteins), S. cerevisiae (6721 proteins), and H. sapiens (20207 proteins); the identification rate exceeds 90% in all four cases for appropriate parameter values. IEP can be obtained with 1-d gel electrophoresis (GE), whose accuracy is better than 0.01. Linear protein sequence volumes of unbroken proteins can be obtained with a sub-nanometer diameter nanopore that can measure residue volume with a resolution of 0.07-0.1 nm^3 (Kennedy et al., Nature Nanotech., 2016, 11, 968-976; Dong et al., ACS Nano, 2017, doi: 10.1021/acsnano.6b08452); the blockade current due to a translocating protein is roughly proportional to the volume it excludes in the pore. There is no need to identify any of the residues. More than 90% of all the proteins have estimated translocation times higher than 1 \u03bcs, which is within the time resolution of available detectors. This is a minimalist proteolysis-free GE- and nanopore-based single-molecule approach requires very small samples, is non-destructive (the sample can be recovered for reuse), and can be translated with currently available technology into a portable device for possible use in the field, an academic lab, or a pre-screening step preceding conventional mass spectrometry."}, {"title": "Allosteric signalling paths in hemoglobin: a protein dynamics network analysis", "url": "https://www.biorxiv.org/content/early/2017/09/10/134288", "tag": "Bioinformatics", "abstract": "Hemoglobin is the paradigm of cooperative protein-ligand binding. Cooperativity is the consequence of inter-subunit allosteric communication: binding at one site increases the affinity of the others. Despite half a century of studies, the mechanism behind oxygen binding in hemoglobin is not fully understood yet. In particular, it is not clear if cooperativity arises from preferential inter-subunit channels and which residues propagate the allosteric signal from one heme to the others. In this work, the heme-heme dynamical interactions have been mapped through a network-based analysis of residue conformational fluctuations, as described by molecular dynamics simulations. In particular, it was possible to suggest which inter-subunit interactions are mostly responsible of allosteric signalling and, within each pair of subunits, which protein fragments convey such signalling process."}, {"title": "PDZ Domains in Microorganisms: Link Between Stress Response and Protein Synthesis", "url": "https://www.biorxiv.org/content/early/2017/09/10/186510", "tag": "Bioinformatics", "abstract": "The PSD-95/Dlg-A/ZO-1 (PDZ) domain is highly expanded and diversified in metazoa where it is known to assemble diverse signalling components by virtue of interactions with other proteins in sequence-specific manner. In contrast, in bacteria it monitors protein quality control during stress response. The distribution, functions and origin of PDZ domain-containing proteins in prokaryotes are largely unknown. We analyzed 7,852 PDZ domain-containing proteins in 1,474 prokaryotes and fungi. PDZ domains are abundant in eubacteria; and, this study confirms their occurrence also in archaea and fungi. Of all eubacterial PDZ domain-containing proteins, 89% are predicted to be membrane and periplasmic, explaining the depletion of bacterial domain forms in metazoa. Planctomycetes, myxobacteria and other eubacteria occupying terrestrial and aquatic niches encode more domain copies, which may have contributed towards multi-cellularity and prokaryotic-eukaryotic transition. Over 93% of the 7,852 PDZ-containing proteins classified into 12 families including 6 novel families. Out of these 88% harbor eight different protease domains, suggesting their substrate-specificity is guided by PDZ domains. The genomic context provides tantalizing insight towards the functions associated with PDZ domains and reinforces their involvement in protein synthesis. We propose that the highly variable PDZ domain of the uncharacterized Fe-S oxidoreductase superfamily, exclusively found in gladobacteria and several anaerobes and acetogens, may have preceded all existing PDZ domains."}, {"title": "IPCAPS: an R package for iterative pruning to capture population structure", "url": "https://www.biorxiv.org/content/early/2017/09/10/186874", "tag": "Bioinformatics", "abstract": "Background: Resolving population genetic structure is challenging, especially when dealing with closely related or geographically confined populations. Although Principal Component Analysis (PCA)-based methods and genomic variation with single nucleotide polymorphisms (SNPs) are widely used to describe shared genetic ancestry, improvements can be made especially when fine-scale population structure is the target. Results: This work presents an R package called IPCAPS, which uses SNP information for resolving possibly fine-scale population structure. The IPCAPS routines are built on the iterative pruning Principal Component Analysis (ipPCA) framework that systematically assigns individuals to genetically similar subgroups. In each iteration, our tool is able to detect and eliminate outliers, hereby avoiding severe misclassification errors. Conclusions: IPCAPS supports different measurement scales for variables used to identify substructure. Hence, panels of gene expression and methylation data can be accommodated as well. The tool can also be applied in patient sub-phenotyping contexts. IPCAPS is developed in R and is freely available from http://bio3.giga.ulg.ac.be/ipcaps"}, {"title": "Understanding and mitigating some limitations of Illumina \u00a9 MiSeq for environmental sequencing of fungi.", "url": "https://www.biorxiv.org/content/early/2017/09/10/184960", "tag": "Bioinformatics", "abstract": "ITS-amplicon metabarcode studies using the illumina MiSeq sequencing platform are the current standard tool for fungal ecology studies. Here we report on some of the particular challenges experienced while creating and using a ribosomal RNA gene (rDNA) amplicon library for an ecological study. Two significant complications were encountered. First, artificial differences in read abundances among OTUs were observed, apparently resulting from bias at two stages: PCR amplification of genomic DNA with ITS-region Illumina-sequence-adapted-primers, and during Illumina sequencing. These differential read abundances were only partially corrected by a common variance-stabilization method. Second, tag-switching (or the shifting of amplicons to incorrect sample indices) occurred at high levels in positive mock-community controls. An example of a bioinformatic method to estimate the rate of tag switching is shown, some recommendations on the use of positive controls and primer choice are given, and one approach to reducing potential false positives resulting from these technological biases is presented."}, {"title": "GSimp: A Gibbs sampler based left-censored missing value imputation approach for metabolomics studies", "url": "https://www.biorxiv.org/content/early/2017/09/09/177410", "tag": "Bioinformatics", "abstract": "Left-censored missing values commonly exist in targeted metabolomics datasets and can be considered as missing not at random (MNAR). Improper data processing procedures for missing values will cause adverse impacts on subsequent statistical analyses. However, few imputation methods have been developed and applied to the situation of MNAR in the field of metabolomics. Thus, a practical left-censored missing value imputation method is urgently needed. We have developed an iterative Gibbs sampler based left-censored missing value imputation approach (GSimp). We compared GSimp with other three imputation methods on two real-world targeted metabolomics datasets and one simulation dataset using our imputation evaluation pipeline. The results show that GSimp outperforms other imputation methods in terms of imputation accuracy, observation distribution, univariate and multivariate analyses, and statistical sensitivity. The R code for GSimp, evaluation pipeline, vignette, real-world and simulated targeted metabolomics datasets are available at: https://github.com/WandeRum/GSimp."}, {"title": "Specter: linear deconvolution as a new paradigm for targeted analysis of data-independent acquisition mass spectrometry proteomics", "url": "https://www.biorxiv.org/content/early/2017/09/08/152744", "tag": "Bioinformatics", "abstract": "Mass spectrometry with data-independent acquisition (DIA) has emerged as a promising method to greatly improve the comprehensiveness and reproducibility of targeted and discovery proteomics, in theory systematically measuring all peptide precursors within a biological sample. Despite the technical maturity of DIA, the analytical challenges involved in discriminating between peptides with similar sequences in convoluted spectra have limited its applicability in important cases, such as the detection of single-nucleotide polymorphisms and alternative site localizations in phosphoproteomics data. We have developed Specter, an open-source software tool that uses linear algebra to deconvolute DIA mixture spectra directly in terms of a spectral library, circumventing the problems associated with typical fragment correlation-based approaches. We validate the sensitivity of Specter and its performance relative to other methods by means of several complex datasets, and show that Specter is able to successfully analyze cases involving highly similar peptides that are typically challenging for DIA analysis methods."}, {"title": "Paratope Prediction using Convolutional and Recurrent Neural Networks", "url": "https://www.biorxiv.org/content/early/2017/09/08/185488", "tag": "Bioinformatics", "abstract": "Antibodies play an essential role in the immune system of vertebrates and are vital tools in research and diagnostics. While hypervariable regions of antibodies, which are responsible for binding, can be readily identified from their amino acid sequence, it remains challenging to accurately pinpoint which amino acids will be in contact with the antigen (the paratope). In this work, we present a sequence-based probabilistic machine learning algorithm for paratope prediction, named Parapred. Parapred uses a deep-learning architecture to leverage features from both local residue neighbourhoods and across the entire sequence. The method outperforms the current state-of-the-art methodology, and only requires a stretch of amino acid sequence corresponding to a hypervariable region as an input, without any information about the antigen. We further show that our predictions can be used to improve both speed and accuracy of a rigid docking algorithm. The Parapred method is freely available at https://github.com/eliberis/parapred for download."}, {"title": "Interaction of N-3-oxododecanoyl homoserine lactone with transcriptional regulator LasR of Pseudomonas aeruginosa: Insights from molecular docking and dynamics simulations", "url": "https://www.biorxiv.org/content/early/2017/09/08/121681", "tag": "Bioinformatics", "abstract": "In 2017 World Health Organization announced the list of the most dangerous superbugs and among them is Pseudomonas aeruginosa, which is an antibiotic resistant opportunistic human pathogen as well as one of the \"ESKAPE\" pathogens. The central problem is that it affects patients suffering from AIDS, cystic fibrosis, cancer, burn victims etc. P. aeruginosa creates and inhabits surface-associated biofilms. Biofilms increase resistance to antibiotics and host immune responses, because of those current treatments are not effective. It is imperative to find new antibacterial treatment strategies against P. aeruginosa, but detailed molecular properties of the LasR protein are not clearly known to date. In the present study, we tried to analyse the molecular properties of the LasR protein as well as the mode of its interactions with autoinducer (AI) the N-3-oxododecanoyl homoserine lactone (3-O-C12-HSL). We performed docking and molecular dynamics (MD) simulations of the LasR protein of P. aeruginosa with the 3-O-C12-HSL ligand. We assessed the conformational changes of the interaction and analysed the molecular details of the binding of the 3-O-C12-HSL with LasR. A new interaction site of the 3-O-C12-HSL with LasR protein was found, which involves interaction with conservative residues from ligand binding domain (LBD), beta turns in the short linker region (SLR) and DNA binding domain (DBD). It will be referenced as the LBD-SLR-DBD bridge interaction or \"the bridge\". We have also performed LasR monomer protein docking and found a new form of dimerization. This study may offer new insights for future experimental studies to detect the interaction of the autoinducer with \"the bridge\" of LasR protein and a new interaction site for drug design."}, {"title": "Trade-off shapes diversity in eco-evolutionary dynamics", "url": "https://www.biorxiv.org/content/early/2017/09/08/184432", "tag": "Bioinformatics", "abstract": "We introduce an Interaction and Trade-off based Eco-Evolutionary Model (ITEEM), in which species are competing for resources in a well-mixed system, and their evolution in interaction trait space is subject to a life-history trade-off between replication rate and competitive ability. We demonstrate that the strength of the trade-off has a fundamental impact on eco-evolutionary dynamics, as it imposes four phases of diversity, including a sharp phase transition. Despite its minimalism, ITEEM produces without further ad hoc features a remarkable range of observed patterns of eco-evolutionary dynamics. Most notably we find self-organization towards structured communities with high and sustainable diversity, in which competing species form interaction cycles similar to rock-paper-scissors games."}, {"title": "Enabling rapid cloud-based analysis of thousands of human genomes via Butler", "url": "https://www.biorxiv.org/content/early/2017/09/08/185736", "tag": "Bioinformatics", "abstract": "We present Butler, a computational framework developed in the context of the international Pan-cancer Analysis of Whole Genomes (PCAWG) project to overcome the challenges of orchestrating analyses of thousands of human genomes on the cloud. Butler operates equally well on public and academic clouds. This highly flexible framework facilitates management of virtual cloud infrastructure, software configuration, genomics workflow development, and provides unique capabilities in workflow execution management. By comprehensively collecting and analysing metrics and logs, performing anomaly detection as well as notification and cluster self-healing, Butler enables large-scale analytical processing of human genomes with 43% increased throughput compared to prior setups. Butler was key for delivering the germline genetic variant call-sets in 2,834 cancer genomes analysed by PCAWG."}, {"title": "DEsingle: A new method for single-cell differentially expressed genes detection and classification", "url": "https://www.biorxiv.org/content/early/2017/09/08/173997", "tag": "Bioinformatics", "abstract": "There are excessive zero values in single-cell RNA-seq (scRNA-seq) data. Some of them are real zeros of non-expressed genes, while the others are the so-called \"dropout\" zeros caused by the low mRNA capture efficiency of tiny amounts of mRNAs in single cells. These two types of zeros should be distinguished in differential expression (DE) analysis and other types of analyses of scRNA-seq data. We proposed a new method DEsingle for DE analysis in scRNA-seq data by employing the Zero-Inflated Negative Binomial (ZINB) model. We proved that DEsingle could estimate the percentage of real zeros and dropout zeros by modelling the mRNA capture procedure. According to this model, DEsingle can distinguish three types of differential expression between two groups of single cells, with regard to differences in expression status, in expression abundances, and in both. We validated the performance of the method on simulation data and applied it on real scRNA-seq data of human preimplantation embryonic cells of different days of embryo development. Results showed that DEsingle outperforms existing methods for scRNA-seq DE analysis, and can reveal different types of DE genes that are enriched in different functions."}, {"title": "Statistical power of gene-set enrichment analysis is a function of gene set correlation structure", "url": "https://www.biorxiv.org/content/early/2017/09/08/186288", "tag": "Bioinformatics", "abstract": "We develop an analytic statistical framework for examining a variety of gene-set enrichment analysis tests. Within this framework, we describe why statistical power for both self-contained and competitive gene set tests is a function of the correlation structure of co-expressed genes, and why this characteristic is undesireable for gene-set analyses. We additionally describe why past gene-set tests have suffered from inflated type 1 error, and how permutation-based methods have sought to address the issue with some success in the case of self-contained tests and with less success in the case of competitive tests. While the context of this investigation is microarray analysis, with particular focus on leading tests CAMERA, ROAST, SAFE, and GAGE, the observations are also relevant to recently proposed RNAseq gene-set tests, including MAST. The variable statistical power we describe as a function of gene correlation structure has not been studied. While type 1 error inflation has been well-studied and described previously for both self-contained and competitive tests, it has less often been done in an analytical framework and so it is useful to make assumptions explicit and examine parametric distributions. We propose three alternative tests, one of which replicates the properties of permutation-based self-contained tests but obviates the need for even recently proposed, rotation-based approximations to burdensome permutations in favor of closed-form densities. The two other tests we propose have the unique property that their statistical power is not a function of co-expression correlation in the gene-set and therefore may be the preferred methodology. We provide simulation support for these proposed methods, compare their results to leading gene-set tests, and apply them to an already-published study of smoking exposure on pregnant women. We call the suite of three proposed test \"JAGST\" -- Just Another Gene-Set Test -- and make the methods accessible via an R package of the same name."}, {"title": "A comprehensive profile of circulating RNAs in human serum", "url": "https://www.biorxiv.org/content/early/2017/09/08/186320", "tag": "Bioinformatics", "abstract": "Non-coding RNA (ncRNA) molecules have fundamental roles in cells and many are also stable in body fluids as extracellular RNAs. In this study, we used RNA sequencing (RNA-seq) to investigate the profile of small non-coding RNA (sncRNA) in human serum. We analyzed 10 billion Illumina reads from 477 serum samples, included in the Norwegian population-based Janus Serum Bank (JSB). We found that the core serum RNA repertoire includes 258 micro RNAs (miRNA), 441 piwi-interacting RNAs (piRNA), 411 transfer RNAs (tRNA), 24 small nucleolar RNAs (snoRNA), 125 small nuclear RNAs (snRNA) and 123 miscellaneous RNAs (misc-RNA). We also investigated biological and technical variation in expression, and the results suggest that many RNA molecules identified in serum contain signs of biological variation. They are therefore unlikely to be random degradation by-products. In addition, the presence of specific fragments of tRNA, snoRNA, Vault RNA and Y_RNA indicates protection from degradation. Our results suggest that many circulating RNAs in serum can be potential biomarkers."}, {"title": "Acceleration Of Nucleotide Semi-Global Alignment With Adaptive Banded Dynamic Programming", "url": "https://www.biorxiv.org/content/early/2017/09/07/130633", "tag": "Bioinformatics", "abstract": "Pairwise alignment of nucleotide sequences has previously been carried out using the seed-and-extend strategy, where we enumerate seeds (shared patterns) between sequences and then extend the seeds by Smith-Waterman-like semi-global dynamic programming to obtain full pairwise alignments. With the advent of massively parallel short read sequencers, algorithms and data structures for efficiently finding seeds have been extensively explored. However, recent advances in single-molecule sequencing technologies have enabled us to obtain millions of reads, each of which is orders of magnitude longer than those output by the short-read sequencers, demanding a faster algorithm for the extension step that accounts for most of the computation time required for pairwise local alignment. Our goal is to design a faster extension algorithm suitable for single-molecule sequencers with high sequencing error rates (e.g., 10-15%) and with more frequent insertions and deletions than substitutions. We propose an adaptive banded dynamic programming algorithm for calculating pairwise semi-global alignment of nucleotide sequences that allows a relatively high insertion or deletion rate while keeping band width relatively low (e.g., 32 or 64 cells) regardless of sequence lengths. Our new algorithm eliminated mutual dependences between elements in a vector, allowing an efficient Single-Instruction-Multiple-Data parallelization. We experimentally demonstrate that our algorithm runs approximately 5x faster than the extension alignment algorithm in NCBI BLAST+ while retaining similar sensitivity (recall). We also show that our extension algorithm is more sensitive than the extension alignment routine in DALIGNER, while the computation time is comparable."}, {"title": "The number of orphans in yeast and fly is drastically reduced by using combining searches in both proteomes and genomes", "url": "https://www.biorxiv.org/content/early/2017/09/07/185983", "tag": "Bioinformatics", "abstract": "The detection of orphans, i.e. genes without homologs in other species, is important as it provides a glimpse on the evolutionary processes that create novel genes. However, for an unbiased view of such de novo gene creation the detection of orphans needs to be accurate. The estimation of orphanicity, and in general the age determination of any gene, is dependent on two factors: (i) a method to detect homologs in a genome and (ii) a set of related genomes. Here, we set out to investigate how the detection of orphans is influenced be these factors. We show that when using multiple genomes and six-frame translations of complete genomes the number of orphans is significantly reduced, when compared with earlier studies. Given these premises we obtain a strict set of 34 most likely de novo created yeast genes, and show that the number of orphans in Drosophila melanogaster and Drosophila pseudoobscur can be reduced to only 30 and 17, respectively."}, {"title": "mosdepth: quick coverage calculation for genomes and exomes", "url": "https://www.biorxiv.org/content/early/2017/09/07/185843", "tag": "Bioinformatics", "abstract": "Mosdepth is a new command-line tool for rapidly calculating genome-wide sequencing coverage. It measures depth from BAM (Li et al. 2009) or CRAM files at either each nucleotide position in a genome or for sets of genomic regions. Genomic regions may be specified as either a BED file to evaluate coverage across capture regions, or as a fixed-size window as required for copy-number calling. Mosdepth uses a simple algorithm that is computationally efficient and enables it to quickly produce optional coverage summaries. We demonstrate that mosdepth is faster than existing tools and provides flexibility in the types of coverage profiles produced."}, {"title": "A sequence-based, deep learning model accurately predicts RNA splicing branchpoints", "url": "https://www.biorxiv.org/content/early/2017/09/07/185868", "tag": "Bioinformatics", "abstract": "Experimental detection of RNA splicing branchpoints, the nucleotide serving as the nucleophile in the first catalytic step of splicing, is difficult. To date, annotations exist for only 16-21% of 3' splice sites in the human genome and even these limited annotations have been shown to be plagued by noise. We develop a sequence-only, deep learning based branchpoint predictor, LaBranchoR, which we conclude predicts a correct branchpoint for over 90% of 3' splice sites genome-wide. Our predicted branchpoints show large agreement with trends observed in the raw data, but analysis of conservation signatures and overlap with pathogenic variants reveal that our predicted branchpoints are generally more reliable than the raw data itself. We use our predicted branchpoints to identify a sequence element upstream of branchpoints consistent with extended U2 snRNA base pairing, show an association between weak branchpoints and alternative splicing, and explore the effects of variants on branchpoints."}, {"title": "A Methodology for Evaluating the Performance of Alerting and Detection Algorithms Running on Continuous Patient Data", "url": "https://www.biorxiv.org/content/early/2017/09/07/182154", "tag": "Bioinformatics", "abstract": "Objectives: Clinicians in the intensive care unit (ICU) are presented with a large number of physiological data consisting of periodic and frequently sampled measurements, such as heart rate and blood pressure, as well as aperiodic measurements, such as noninvasive blood pressure and laboratory studies. Because this data can be overwhelming, there is considerable interest in designing algorithms that help integrate and interpret this data and assist ICU clinicians in detecting patients who may be deteriorating. In order to decide whether to deploy such algorithms in a clinical trial, it is important to evaluate these algorithms using retrospective data. However, the fact that these algorithms will be running continuously, i.e., repeatedly sampling incoming patient data, presents some novel challenges for algorithm evaluation. Commonly used measures of performance such as sensitivity and positive predictive value (PPV) are easily applied to static \"snapshots\" of patient data, but can be very misleading when applied to indicators or alerting algorithms that are running on continuous data. Our objective is to create a method for evaluating algorithm performance on retrospective data with the algorithm running continuously throughout the patient's stay as it would in a real ICU. Methods: We introduce our evaluation methodology in the context of evaluating an algorithm, Hemodynamic Instability Indicator (HII), for assisting bedside ICU clinicians with the early detection of hemodynamic instability before the onset of acute hypotension. Each patient's ICU stay is divided into segments that are labelled as hemodynamically stable or unstable based on clinician interventions typically aimed at treating r hemodynamic instability. These segments can be of varying length with varying degrees of exposure to potential alerts, whether true positive or false positive. Furthermore, to simulate how clinicians might interact with the alerting algorithm, we use a dynamic alert supervision mechanism which suppresses subsequent alerts unless the indicator has significantly deteriorated since the prior alert. Under these conditions it is no longer straightforward what counts as a positive or negative instance, and calculations of sensitivity, specificity, and positive predictive value can be problematic. We introduce a methodology for consistently counting positive and negative instances. The methodology distinguishes between counts based on alerting events and counts based on subsegments and how they can be applied in calculating measures of alert performance such as sensitivity, specificity and positive predictive value. Results: The introduced methodology is applied to retrospective evaluation of two algorithms, HII and an alerting algorithm based on systolic blood pressure. We use a database, consisting of data from 41,707 patients from 25 US hospitals, to evaluate the algorithms. Both algorithms are evaluated running continuously throughout each patient's stay as they would in a real ICU setting. We show how the introduced performance measures differ for different algorithms and for different assumptions. Discussion: The standard measures of diagnostic tests in terms of true positives, false positives, etc. are based on certain assumptions which may not apply when used in the context of measuring the performance on an algorithm running continuously, and thus repeatedly sampling from the same patient. When such measures are being reported it is important that the underlying assumptions be made explicit; otherwise, the results can be very misleading. Conclusion: We introduce a methodology for evaluating how an alerting algorithm or indicator will perform running continuously throughout every patient's ICU stay, not just for a subset of patients for selected episodes."}, {"title": "BraCeR: Reconstruction of B-cell receptor sequences and clonality inference from single-cell RNA-sequencing", "url": "https://www.biorxiv.org/content/early/2017/09/07/185504", "tag": "Bioinformatics", "abstract": "Reconstruction of antigen receptor sequences from single-cell RNA-sequencing (scRNA-seq) data allows the linking of antigen receptor usage to the full transcriptomic identity of individual B lymphocytes, without having to perform additional targeted repertoire sequencing (Rep-seq). Here we report BraCeR (freely available at https://github.com/teichlab/bracer/), an extension of TraCeR, for reconstruction of paired full-length B-cell receptor sequences and inference of clonality from scRNA-seq data. With an easy-to-use command-line interface, BraCeR provides a complete pipeline for clonal inference and lineage tracing of B cells. Raw scRNA-seq reads can be processed all the way to clonal networks and lineage trees, facilitating linkage of transcriptomic phenotype to the evolution of immunoglobulin sequences."}, {"title": "Channel crosstalk correction in suspension and imaging mass cytometry", "url": "https://www.biorxiv.org/content/early/2017/09/07/185744", "tag": "Bioinformatics", "abstract": "Mass cytometry enables simultaneous analysis of over 40 proteins and their modifications in single cells through use of metal-tagged antibodies. Compared to fluorescent dyes, the use of pure metal isotopes strongly reduces spectral overlap among measurement channels. Crosstalk still exists, however, caused by isotopic impurity, oxide formation, and mass cytometer properties. Spillover effects can be minimized, but not avoided, by following a set of constraining rules when designing an antibody panel. Generation of such low crosstalk panels requires considerable expert knowledge, knowledge of the abundance of each marker and substantial experimental effort. Here we describe a novel bead-based compensation workflow that includes R-based software and a web tool, which enables correction for interference between channels. We demonstrate utility in suspension mass cytometry and show how this approach can be applied to imaging mass cytometry. Our approach greatly simplifies the development of new antibody panels, increases flexibility for antibody-metal pairing, improves overall data quality, thereby reducing the risk of reporting cell phenotype and function artifacts, and greatly facilitates analysis of complex samples for which antigen abundances are unknown."}, {"title": "Integrated transcriptome and epigenome analyses identify alternative splicing as a novel candidate linking histone modifications to embryonic stem cell fate decision", "url": "https://www.biorxiv.org/content/early/2017/09/07/181875", "tag": "Bioinformatics", "abstract": "Understanding embryonic stem cell (ESC) fate decision between self-renewal and proper differentiation is important for developmental biology and regenerative medicine. Attentions have focused on underlying mechanisms involving histone modifications (HMs), alternative pre-mRNA splicing (AS), and cell-cycle progression. However, their intricate interrelations and joint contributions to ESC fate decision remain unclear. We performed integrative analyses on transcriptomic and epigenomic data derived from human ESC (hESC) and five types of differentiated cells. We identified thousands of AS exons and revealed their development- and lineage-dependent characterizations. Following the observation that dynamic HM changes predominantly occur in AS exons upon hESCs differentiation, we identified 3 of 16 investigated HMs (H3K36me3, H3K27ac, and H4K8ac) that are strongly associated with 52.8% of hESC differentiation-related AS events. Further analyses showed that the HM-associated AS genes predominantly function in G2/M phases and ATM/ATR-mediated DNA damage response pathway for cell differentiation, whereas HM-unassociated AS genes enrich in G1 phase and pathways for self-renewal. These results imply a potential epigenetic mechanism by which some HMs contribute to ESC fate decision through the AS regulation of specific pathways and cell-cycle genes. We exemplified the potential mechanism by a cell cycle-related transcription factor, PBX1, which regulates the pluripotency regulatory network by binding to NANOG. We found that the isoform switch between PBX1a and PBX1b is strongly associated with H3K36me3 alteration and implicated in hESC fate determination. Supported by extended dataset from Roadmap/ENCODE projects, we identified the alternative splicing of PBX1 as a novel candidate linking H3K36me3 to embryonic stem cell fate decision."}, {"title": "iSeg: an efficient algorithm for segmentation of genomic and epigenomic data", "url": "https://www.biorxiv.org/content/early/2017/09/07/184515", "tag": "Bioinformatics", "abstract": "Background: Identification of functional elements of a genome often requires dividing a sequence of measurements along a genome into segments where adjacent segments have different properties, such as different mean values. This problem is often called the segmentation problem in the field of genomics, and the change-point problem in other scientific disciplines. Despite dozens of algorithms developed to address this problem in genomics research, methods with improved accuracy and speed are still needed to effectively tackle both existing and emerging genomic and epigenomic segmentation problems. Results: We designed an efficient algorithm, called iSeg, for segmentation of genomic and epigenomic profiles. iSeg first utilizes dynamic programming to identify candidate segments and test for significance. It then uses a novel data structure based on two coupled balanced binary trees to detect overlapping significant segments and update them simultaneously during searching and refinement stages. Refinement and merging of significant segments are performed at the end to generate the final set of segments. By using an objective function based on the p-values of the segments, the algorithm can serve as a general computational framework to be combined with different assumptions on the distributions of the data. As a general segmentation method, it can segment different types of genomic and epigenomic data, such as DNA copy number variation, nucleosome occupancy, nuclease sensitivity, and differential nuclease sensitivity data. Using simple t-tests to compute p-values across multiple datasets of different types, we evaluate iSeg using both simulated and experimental datasets and show that it performs satisfactorily when compared with some state-of-art procedures, which often employ more sophisticated statistical models. Implemented in C++, iSeg is also computationally efficient, and well suited for large numbers of input profiles and data with very long sequences. Conclusions: We have developed an effective and efficient general-purpose segmentation tool for sequential data and illustrated its use in segmentation of genomic and epigenomic profiles."}, {"title": "SQUID: Transcriptomic Structural Variation Detection from RNA-seq", "url": "https://www.biorxiv.org/content/early/2017/09/06/162776", "tag": "Bioinformatics", "abstract": "Transcripts are frequently modified by structural variations, which leads to a fused transcript of either multiple genes (known as a fusion gene) or a gene and a previously non-transcribing sequence. Detecting these modifications (called transcriptomic structural variations, or TSVs), especially in cancer tumor sequencing, is an important and challenging computational problem. We introduce SQUID, a novel algorithm to accurately predict both fusion-gene and non-fusion-gene TSVs from RNA-seq alignments. SQUID unifies both concordant and discordant read alignments into one model, and doubles the accuracy on simulation data compared to other approaches. With SQUID, we identified novel non-fusion-gene TSVs on TCGA samples."}, {"title": "Virtual Genome Walking: Generating gene models for the salamander Ambystoma mexicanum", "url": "https://www.biorxiv.org/content/early/2017/09/06/185157", "tag": "Bioinformatics", "abstract": "Large repeat rich genomes present challenges for assembly and identification of gene models with short read technologies. Here we present a method we call Virtual Genome Walking which uses an iterative assembly approach to first identify exons from de novo assembled transcripts and assemble whole genome reads against each exon. This process is iterated allowing the extension of exons. These linked assemblies are refined to generate gene models including upstream and downstream genomic sequence as well as intronic sequence. We test this method using a 20X genomic read set for the axolotl, the genome of which is estimated to be 30 Gb in size. These reads were previously reported to be effectively impossible to assemble. Here we provide almost 1 Gb of assembled sequence describing over 19,000 gene models for the axolotl. Gene models stop assembling either due to localised low coverage in the genomic reads, or the presence of repeats. We validate our observations by comparison with previously published axolotl bacterial artificial chromosome (BAC) sequences. In addition we analysed axolotl intron length, intron-exon structure, repeat content and synteny. These gene-models, sequences and annotations are freely available for download from https://tinyurl.com/y8gydc6n. The software pipeline including a docker image is available from https://github.com/LooseLab/iterassemble. These methods will increase the value of low coverage sequencing of understudied model systems."}, {"title": "Taxon sampling unequally affects individual nodes in a phylogenetic tree: consequences for model gene tree construction in SwissTree", "url": "https://www.biorxiv.org/content/early/2017/09/05/181966", "tag": "Bioinformatics", "abstract": "Medium to large phylogenetic gene trees constructed from datasets of different species density and taxonomic range are rarely topologically consistent because of missing phylogenetic signal, non-phylogenetic signal and error. In this study, we first use simulations to show that taxon sampling unequally affects nodes in a gene tree, which likely contributes to controversial conclusions from taxon sampling experiments and contradicting species phylogenies such as for the boreoeutherians. Hence, because it is unlikely that a large gene tree can be reconstructed correctly based on a single optimized dataset, we take a two-step approach for the construction of model gene trees. First, stable and unstable clades are identified by comparing phylogenetic trees inferred from multiple datasets and data types (nucleotide, amino acid, codon) from the same gene family. Subsequently, data subsets are optimized for the analysis of individual uncertain clades. Results are summarized in form of a model tree that illustrates the evolutionary relationship of gene loci. A case study shows how a seemingly complex gene phylogeny becomes increasingly consistent with the reference species tree by attentive taxon sampling and subtree analysis. The procedure is progressively introduced to SwissTree (http://swisstree.vital-it.ch), a resource of high confidence model gene (locus) trees. Finally we demonstrate the usefulness of SwissTree for orthology benchmarking."}, {"title": "IntelliEppi: Intelligent reaction monitoring and holistic data management system for the molecular biology lab", "url": "https://www.biorxiv.org/content/early/2017/09/05/184721", "tag": "Bioinformatics", "abstract": "Daily alterations of routines and protocols create high, yet so far unmet demands for intelligent reaction monitoring, quality control and data management in molecular biology laboratories. To meet such needs, the internet of things is implemented here. We propose an approach which combines direct tracking of lab tubes, reactions and racks with a comprehensive data management system. Reagent tubes in this system are tagged with 2D data matrices or imprinted RFID-chips using a unique identification number. For each tube, individual content and all relevant information based on conducted experimental procedures are stored in an experimental data management system. This information is managed automatically but allow scientists to engage and interfere via user-friendly graphical interface. Tagged tubes are used in connection with a detectable RFID-tagged rack. We show that reaction protocols, HTS storage and complex reactions are easily planned and controlled."}, {"title": "Methods For Estimation Of Model Accuracy In CASP12", "url": "https://www.biorxiv.org/content/early/2017/09/05/143925", "tag": "Bioinformatics", "abstract": "Methods for reliably estimating the quality of 3D models of proteins are essential drivers for the wide adoption and serious acceptance of protein structure predictions by life scientists. In this paper, the most successful groups in CASP12 describe their latest methods for Estimates of Model Accuracy (EMA). We show that pure single model accuracy estimation methods has shown clear progress since CASP11; the three top methods (MESHI, ProQ3, SVMQA) all perform better than the top method of CASP11 (ProQ2). The pure single model accuracy estimation methods outperform quasi-single (ModFOLD6 variations) and consensus methods (Pcons, ModFOLDclust2, Pcomb-domain and Wallner) in model selection, but are still not as good as those methods in absolute model quality estimation and predictions of local quality. Finally, we show that when using contact based model quality measures (CAD, lDDT) the single model quality methods perform relatively better."}, {"title": "Unsupervised learning of DNA sequence features using a convolutional restricted Boltzmann machine", "url": "https://www.biorxiv.org/content/early/2017/09/05/183095", "tag": "Bioinformatics", "abstract": "Transcription factors (TFs) are important contributors to gene regulation. They specifically bind to short DNA stretches known as transcription factor binding sites (TFBSs), which are contained in regulatory regions (e.g. promoters), and thereby influence a target gene's expression level. Computational biology has contributed substantially to understanding regulatory regions by developing numerous tools, including for discovering de novo motif. While those tools primarily focus on determining and studying TFBSs, the surrounding sequence context is often given less attention. In this paper, we attempt to fill this gap by adopting a so-called convolutional restricted Boltzmann machine (cRBM) that captures redundant features from the DNA sequences. The model uses an unsupervised learning approach to derive a rich, yet interpretable, description of the entire sequence context. We evaluated the cRBM on a range of publicly available ChIP-seq peak regions and investigated its capability to summarize heterogeneous sets of regulatory sequences in comparison with MEME-Chip, a popular motif discovery tool. In summary, our method yields a considerably more accurate description of the sequence composition than MEME-Chip, providing both a summary of strong TF motifs as well as subtle low-complexity features."}, {"title": "SeroBA: rapid high-throughput serotyping of Streptococcus pneumoniae from whole genome sequence data", "url": "https://www.biorxiv.org/content/early/2017/09/05/179465", "tag": "Bioinformatics", "abstract": "Streptococcus pneumoniae is responsible for 240,000-460,000 deaths in children under 5 years of age each year. Accurate identification of pneumococcal serotypes is important for tracking the distribution and evolution of serotypes following the introduction of effective vaccines. Recent efforts have been made to infer serotypes directly from genomic data but current software approaches are limited and do not scale well. Here, we introduce a novel method, SeroBA, which uses a hybrid assembly and mapping approach. We compared SeroBA against real and simulated data and present results on the concordance and computational performance against a validation dataset, the robustness and scalability when analysing a large dataset, and the impact of varying the depth of coverage in the cps locus region on sequence-based serotyping. SeroBA can predict serotypes, by identifying the cps locus, directly from raw whole genome sequencing read data with 98% concordance using a k-mer based method, can process 10,000 samples in just over 1 day using a standard server and can call serotypes at a coverage as low as 10x. SeroBA is implemented in Python3 and is freely available under an open source GPLv3 license from: https://github.com/sanger-pathogens/seroba."}, {"title": "Substrate binding and specificity appear as major forces in the functional diversification of eqolisins.", "url": "https://www.biorxiv.org/content/early/2017/09/04/167544", "tag": "Bioinformatics", "abstract": "Background: Eqolisins are rare acid proteases found in archaea, bacteria and fungi. Certain fungi secrete acids as part of their lifestyle and interestingly these also have many eqolisin paralogs, up to nine paralogs have been recorded. This suggests functional redundancy and diversification, which was the subject of the research we performed and describe here. Results: We identified eqolisin homologs by means of iterative HMMER analysis of the NR database. The identified sequences were scrutinized for which we defined novel hallmarks, identified by molecular dynamics simulations of mutants of highly conserved positions, using the structure of an eqolisin that was crystallized in the presence of a transition state inhibitor. Four conserved glycines were shown to be required for functionality. A substitution of W67F is shown to be accompanied by the L105W substitution. Molecular dynamics shows that the W67 binds to the substrate via a \u03c0-\u03c0 stacking and a salt bridge, the latter being stronger in a virtual W67F/L105W double mutant of the resolved structure of Scytalido-carboxyl peptidase-B (PDB ID: 2IFW)). Additional likely fatal mutants are discussed. Upon sequence scrutiny we obtained a set of 233 sequences that in all likelihood lack false positives. This was used to reconstruct a Bayesian phylogenetic tree. We identified 14 putative specificity determining positions (SDPs) of which four are explained by mere structural explanations and nine seem to correspond to functional diversification related wit substrate binding ans specificity. A first sub-network of SDPs is related to substrate specificity whereas the second sub-network seems to affect the dynamics of three loops that are involved in substrate binding."}, {"title": "Learning mutational graphs of individual tumor evolution from multi-sample sequencing data", "url": "https://www.biorxiv.org/content/early/2017/09/04/132183", "tag": "Bioinformatics", "abstract": "Phylogenetic techniques quantify intra-tumor heterogeneity by deconvolving either clonal or mutational trees from multi-sample sequencing data of individual tumors. Most of these methods rely on the well-known infinite sites assumption, and are limited to process either multi-region or single-cell sequencing data. Here, we improve over those methods with TRaIT, a unified statistical framework for the inference of the accumulation order of multiple types of genomic alterations driving tumor development. TRaIT supports both multi-region and single-cell sequencing data, and output mutational graphs accounting for violations of the infinite sites assumption due to convergent evolution, and other complex phenomena that cannot be detected with phylogenetic tools. Our method displays better accuracy, performance and robustness to noise and small sample size than state-of-the-art phylogenetic methods. We show with single-cell data from breast cancer and multi-region data from colorectal cancer that TRaIT can quantify the extent of intra-tumor heterogeneity and generate new testable experimental hypotheses."}, {"title": "On statistical modeling of sequencing noise in high depth data to assess tumor evolution", "url": "https://www.biorxiv.org/content/early/2017/09/04/128587", "tag": "Bioinformatics", "abstract": "One cause of cancer mortality is tumor evolution to therapy-resistant disease. First line therapy often targets the dominant clone, and drug resistance can emerges from preexisting clones that gain fitness through therapy-induced natural selection. Such mutations may be identified using targeted sequencing assays by analysis of noise in high-depth data. Here, we develop a comprehensive, unbiased model for sequencing error background. We find that noise in sufficiently deep DNA sequencing data can be approximated by aggregating negative binomial distributions. Mutations with frequencies above noise may have prognostic value. We evaluate our model with simulated exponentially expanded populations as well as data from cell line and patient sample dilution experiments, demonstrating its utility in prognosticating tumor progression. Our results may have the potential to identify significant mutations that can cause recurrence. These results are relevant in the pretreatment clinical setting to determine appropriate therapy and prepare for potential recurrence pretreatment."}, {"title": "MERIT: a Mutation Error Rate Identification Toolkit for Ultra-deep Sequencing Applications", "url": "https://www.biorxiv.org/content/early/2017/09/04/184291", "tag": "Bioinformatics", "abstract": "Rapid progress in high-throughput sequencing (HTS) has enabled the molecular characterization of mutational landscapes in heterogeneous populations and has improved our understanding of clonal evolution processes. Analyzing the sensitivity of detecting genomic mutations in HTS requires comprehensive profiling of sequencing artifacts. To this end, we introduce MERIT, designed for in-depth quantification of erroneous substitutions and small insertions and deletions, specifically for ultra-deep applications. MERIT incorporates an all-inclusive variant caller and considers genomic context, including the nucleotides immediately at 5' and 3', thereby establishing error rates for 96 possible substitutions as well as four single-base and 16 double-base indels. We apply MERIT to ultra-deep sequencing data (1,300,000x) and show a significant relationship between error rates and genomic contexts. We devise an in silico approach to determine the optimal sequencing depth, where errors occur at rates similar to those of true mutations. Finally, we assess nucleotide-incorporation fidelity of four high-fidelity DNA polymerases in clinically relevant loci, and demonstrate how fixed detection thresholds may result in substantial false positive as well as false negative calls."}, {"title": "Improving Min Hash via the Containment Index with applications to Metagenomic Analysis", "url": "https://www.biorxiv.org/content/early/2017/09/04/184150", "tag": "Bioinformatics", "abstract": "Min hash is a probabilistic method for estimating the similarity of two sets in terms of their Jaccard index, defined as the ration of the size of their intersection to their union. We demonstrate that this method performs best when the sets under consideration are of similar size and the performance degrades considerably when the sets are of very different size. We introduce a new and efficient approach, called the containment min hash approach, that is more suitable for estimating the Jaccard index of sets of very different size. We accomplish this by leveraging another probabilistic method (in particular, Bloom filters) for fast membership queries. We derive bounds on the probability of estimate errors for the containment min hash approach and show it significantly improves upon the classical min hash approach. We also show significant improvements in terms of time and space complexity. As an application, we use this method to detect the presence/absence of organisms in a metagenomic data set, showing that it can detect the presence of very small, low abundance microorganisms."}, {"title": "ASaiM: a Galaxy-based framework to analyze raw shotgun data from microbiota", "url": "https://www.biorxiv.org/content/early/2017/09/04/183970", "tag": "Bioinformatics", "abstract": "New generation of sequencing platforms coupled to numerous bioinformatics tools has led to rapid technological progress in metagenomics and metatranscriptomics to investigate complex microorganism communities. Nevertheless, a combination of different bioinformatic tools remains necessary to draw conclusions out of microbiota studies. Modular and user-friendly tools would greatly improve such studies. We therefore developed ASaiM, an Open-Source Galaxy-based framework dedicated to microbiota data analyses. ASaiM provides a curated collection of tools to explore and visualize taxonomic and functional information from raw amplicon, metagenomic or metatranscriptomic sequences. To guide different analyses, several customizable workflows are included. All workflows are supported by tutorials and Galaxy interactive tours to guide the users through the analyses step by step. ASaiM is implemented as Galaxy Docker flavour. It is scalable to many thousand datasets, but also can be used a normal PC. The associated source code is available under Apache 2 license at https://github.com/ASaiM/framework and documentation can be found online (http://asaim.readthedocs.io/). Based on the Galaxy framework, ASaiM offers sophisticated analyses to scientists without command-line knowledge. ASaiM provides a powerful framework to easily and quickly explore microbiota data in a reproducible and transparent environment."}, {"title": "A predictive model of rats' calorie intake as a function of diet energy density", "url": "https://www.biorxiv.org/content/early/2017/09/04/184085", "tag": "Bioinformatics", "abstract": "Easy access to high-energy food has been linked to high rates of obesity in the world. Understanding the way that access to palatable (high fat or high calorie) food can lead to overconsumption is essential for both preventing and treating obesity. Although the body of studies focused on the effects of high energy diets is growing, our understanding of how different factors contribute to food choices is not complete. In this study, we present a mathematical model that is able to predict rats' calorie intake to a high-energy diet based on their ingestive behavior to a standard chow diet. Specifically, we propose an equation that describes the relation between the body weight (W), energy density (E), time elapsed from the start of diet (T), and daily calorie intake (C). We tested our model on two independent data sets. Our results show that the suggested model is able to predict the calorie intake patterns with high accuracy. Additionally, the only free parameter of our proposed equation (\u03c1), which is unique to each animal, has a strong correlation with their calorie intake and weight gain. Additionally, we discuss the relevance of our derived parameter in the context of measuring reward sensitivity in reinforcement learning based studies."}, {"title": "Assessment of the impact of shared data on the scientific literature", "url": "https://www.biorxiv.org/content/early/2017/09/04/183814", "tag": "Bioinformatics", "abstract": "Data sharing is increasingly recommended as a means of accelerating science by facilitating collaboration, transparency, and reproducibility. While few oppose data sharing philosophically, a range of barriers deter most researchers from implementing it in practice (e.g., workforce and infrastructural demands, sociocultural and privacy concerns, lack of standardization). To justify the significant effort required for sharing data (e.g., organization, curation, distribution), funding agencies, institutions, and investigators need clear evidence of benefit. Here, using the International Neuroimaging Data-sharing Initiative, we present a brain imaging case study that provides direct evidence of the impact of open sharing on data use and resulting publications over a seven-year period (2010-2017). We dispel the myth that scientific findings using shared data cannot be published in high-impact journals and demonstrate rapid growth in the publication of such journal articles, scholarly theses, and conference proceedings. In contrast to commonly used 'pay to play' models, we demonstrate that openly shared data can increase the scale (i.e., sample size) of scientific studies conducted by data contributors, and can recruit scientists from a broader range of disciplines. These findings suggest the transformative power of data sharing for accelerating science and underscore the need for the scientific ecosystem to embrace the challenge of implementing data sharing universally."}, {"title": "Learning sequence patterns of AGO-sRNA affinity from high-throughput sequencing libraries to improve in silico functional small RNA detection and classification in plants", "url": "https://www.biorxiv.org/content/early/2017/09/02/173575", "tag": "Bioinformatics", "abstract": "The loading of small RNA (sRNA) into Argonaute (AGO) complexes is a crucial step in all regulatory pathways identified so far in plants that depend on such non-coding sequences. Important transcriptional and post-transcriptional silencing mechanisms can be activated depending on the specific AGO protein to which sRNA bind. It is known that sRNA-AGO associations are at least partly encoded in the sRNA primary structure, but the sequence features that drive this association have not been fully explored. Here we train support vector machines (SVM) on sRNA sequencing data obtained from AGO-immunoprecipitation experiments to identify features that determine sRNA affinity to specific AGOs. Our SVM reveal that AGO affinity is strongly determined by complex k-mers in the 5\u2032 and 3\u2032 ends of sRNA, in addition to well-known features such as sRNA length and the base composition of the first nucleotide. Moreover, we find that these k-mers tend to overlap known transcription factor (TF) binding motifs, thus highlighting a close interplay between TF and sRNA-mediated transcriptional regulation. We embedded the learned SVM in a computational pipeline that can be used for de novo functional classification of sRNA sequences. This tool, called SAILS, is provided as a web portal accessible at http://sails.eu.nu."}, {"title": "MOSAIC: a chemical-genetic interaction data repository and web resource for exploring chemical modes of action", "url": "https://www.biorxiv.org/content/early/2017/09/02/112854", "tag": "Bioinformatics", "abstract": "Summary: Chemical-genomic approaches that map interactions between small molecules and genetic perturbations offer a promising strategy for functional annotation of uncharacterized bioactive compounds. We recently developed a new high-throughput platform for mapping chemical-genetic (CG) interactions in yeast that can be scaled to screen large compound collections, and we applied this system to generate CG interaction profiles for more than 13,000 compounds. When integrated with the existing global yeast genetic interaction network, CG interaction profiles can enable mode-of-action prediction for previously uncharacterized compounds as well as discover unexpected secondary effects for known drugs. To facilitate future analysis of these valuable data, we developed a public database and web interface named MOSAIC. The website provides a convenient interface for querying compounds, bioprocesses (GO terms), and genes for CG information including direct CG interactions, bioprocesses, and gene-level target predictions. MOSAIC also provides access to chemical structure information of screened molecules, chemical-genomic profiles, and the ability to search for compounds sharing structural and functional similarity. This resource will be of interest to chemical biologists for discovering new small molecule probes with specific modes-of-action as well as computational biologists interested in analyzing CG interaction networks."}, {"title": "Estimating the scale of biomedical data generation using text mining", "url": "https://www.biorxiv.org/content/early/2017/09/01/182857", "tag": "Bioinformatics", "abstract": "While the impact of biomedical research has traditionally been measured using bibliographic metrics such as citation or journal impact factor, the data itself is an output which can be directly measured to provide additional context about a publication's impact. Data are a resource that can be repurposed and reused providing dividends on the original investment used to support the primary work. Moreover, it is the cornerstone upon which a tested hypothesis is rejected or accepted and specific scientific conclusions are reached. Understanding how and where it is being produced enhances the transparency and reproducibility of the biomedical research enterprise. Most biomedical data are not directly deposited in data repositories and are instead found in the publication within figures or attachments making it hard to measure. We attempted to address this challenge by using recent advances in word embedding to identify the technical and methodological features of terms used in the free text of articles' methods sections. We created term usage signatures for five types of biomedical research data, which were used in univariate clustering to correctly identify a large fraction of positive control articles and a set of manually annotated articles where generation of data types could be validated. The approach was then used to estimate the fraction of PLOS articles generating each biomedical data type over time. Out of all PLOS articles analyzed (n = 129,918), ~7%, 19%, 12%, 18%, and 6% generated flow cytometry, immunoassay, genomic microarray, microscopy, and high-throughput sequencing data. The estimate portends a vast amount of biomedical data being produced: in 2016, if other publishers generated a similar amount of data then roughly 40,000 NIH-funded research articles would produce ~56,000 datasets consisting of the five data types we analyzed."}, {"title": "Assessing actimeters for inclusion in the Healthy Brain Network", "url": "https://www.biorxiv.org/content/early/2017/09/01/183772", "tag": "Bioinformatics", "abstract": "Background: The Healthy Brain Network is an openly shared pediatric psychiatric biobank with a target of 10,000 participants between the ages of 5 and 21, inclusively. In adding ecological actimetry to the Healthy Brain Network, we intend to use appropriate, accurate, reliable tools. Currently a wide range of personal activity trackers are commercially available, providing a wide variety of sensor configurations. For many of these devices, accelerometry provides the basis of measuring both physical activity and sleep with comparable derivative measures. Results: In order to include an ecological biotracker in the Healthy Brain Network protocol, we first evaluated the specifications of a variety of actimeters available for purchase. We then acquired physical instances of 5 of these devices (ActiGraph wGT3X-BT, Empatica Embrace, Empatica E4, GENEActiv Original, and Wavelet Wristband) and wore each of them in our daily lives, annotating our activities and evaluating the reasonableness of the data from each device and the logistical affordances of each device. Conclusions: We decided that the ActiGraph wGT3X-BT is the most appropriate device for inclusion in the Healthy Brain Network. However, none of the devices we evaluated was clearly superior or inferior to the rest; rather, each device seems to have use cases in which that device excels beyond the others."}, {"title": "Interpretable dimensionality reduction of single cell transcriptome data with deep generative models", "url": "https://www.biorxiv.org/content/early/2017/09/01/178624", "tag": "Bioinformatics", "abstract": "Single-cell RNA-sequencing has great potential to discover cell types, identify cell states, trace development lineages, and reconstruct the spatial organization of cells. However, dimension reduction to interpret structure in single-cell sequencing data re- mains a challenge. Existing algorithms are either not able to uncover the clustering structures in the data, or lose global information such as groups of clusters the are close to each other. We present a robust statistical model, scvis to capture and visualize the low-dimensional structures in single-cell gene expression data. Simulation results demonstrate that low-dimensional representations learned by scvis preserve both the local and global neighbour structures in the data. In addition, scvis is robust to the number of data points and learns a probabilistic parametric mapping function to add new data points to an existing embedding. We then use scvis to analyze four single-cell RNA-sequencing datasets, exemplifying interpretable two-dimensional representations of the high-dimensional single-cell RNA-sequencing data."}, {"title": "Semi-Parametric Covariate-Modulated Local False Discovery Rate For Genome-Wide Association Studies", "url": "https://www.biorxiv.org/content/early/2017/08/31/183384", "tag": "Bioinformatics", "abstract": "While genome-wide association studies (GWAS) have discovered thousands of risk loci for heritable disorders, so far even very large meta-analyses have recovered only a fraction of the heritability of most complex traits. Recent work utilizing variance components models has demonstrated that a larger fraction of the heritability of complex phenotypes is captured by the additive effects of SNPs than is evident only in loci surpassing genome-wide significance thresholds, typically set at a Bonferroni-inspired p \u2264 5 * 10^(-8). Procedures that control false discovery rate can be more powerful, yet these are still under-powered to detect the majority of non-null effects from GWAS. The current work proposes a novel Bayesian semi-parametric two-group mixture model and develops a Markov Chain Monte Carlo (MCMC) algorithm for a covariate-modulated local false discovery rate (cmfdr). The probability of being non-null depends on a set of covariates via a logistic function, and the non-null distribution is approximated as a linear combination of B-spline densities, where the weight of each B-spline density depends on a multinomial function of the covariates. The proposed methods were motivated by work on a large meta-analysis of schizophrenia GWAS performed by the Psychiatric Genetics Consortium (PGC). We show that the new cmfdr model fits the PGC schizophrenia GWAS test statistics well, performing better than our previously proposed parametric gamma model for estimating the non-null density and substantially improving power over usual fdr. Using loci declared significant at cmfdr \u2264 0.20, we perform follow-up pathway analyses using the Kyoto Encyclopedia of Genes and Genomes (KEGG) Homo sapiens pathways database. We demonstrate that the increased yield from the cmfdr model results in an improved ability to test for pathways associated with schizophrenia compared to using those SNPs selected according to usual fdr."}, {"title": "Recovering genomic clusters of secondary metabolites from lakes: a Metagenomics 2.0 approach", "url": "https://www.biorxiv.org/content/early/2017/08/31/183061", "tag": "Bioinformatics", "abstract": "Background: Metagenomic approaches became increasingly popular in the past decades due to decreasing costs of DNA sequencing and bioinformatics development. So far, however, the recovery of long genes coding for secondary metabolism still represents a big challenge. Often, the quality of metagenome assemblies is poor, especially in environments with a high microbial diversity where sequence coverage is low and complexity of natural communities high. Recently, new and improved algorithms for binning environmental reads and contigs have been developed to overcome such limitations. Some of these algorithms use a similarity detection approach to classify the obtained reads into taxonomical units and to assemble draft genomes. This approach, however, is quite limited since it can classify exclusively sequences similar to those available (and well classified) in the databases. In this work, we used draft genomes from Lake Stechlin, north-eastern Germany, recovered by MetaBat, an efficient binning tool that integrates empirical probabilistic distances of genome abundance, and tetranucleotide frequency for accurate metagenome binning. These genomes were screened for secondary metabolism genes, such as polyketide synthases (PKS) and non-ribosomal peptide synthases (NRPS), using the Anti-SMASH and NAPDOS workflows. Results: With this approach we were able to identify 243 secondary metabolite clusters from 121 genomes recovered from the lake samples. A total of 18 NRPS, 19 PKS and 3 hybrid PKS/NRPS clusters were found. In addition, it was possible to predict the partial structure of several secondary metabolite clusters allowing for taxonomical classifications and phylogenetic inferences. Conclusions: Our approach revealed a great potential to recover and study secondary metabolites genes from any aquatic ecosystem."}, {"title": "CancerDiscover: A configurable pipeline for cancer prediction and biomarker identification using machine learning framework", "url": "https://www.biorxiv.org/content/early/2017/08/31/182998", "tag": "Bioinformatics", "abstract": "Motivation: Use of various high-throughput screening techniques has resulted in an abundance of data, whose complete utility is limited by the tools available for processing and analysis. Machine learning holds great potential for deciphering these data in the context of cancer classification and biomarker identification. However, current machine learning tools require manual processing of raw data from various sequencing platforms, which is both tedious and time-consuming. The current classification tools lack flexibility in choosing the best feature selection algorithms from a range of algorithms and most importantly inability to compare various learning algorithms. Results: We developed CancerDiscover, an open-source software pipeline that allows users to efficiently and automatically integrate large high-throughput datasets, preprocess, normalize, and selects best performing features from multiple feature selection algorithms. The pipeline lets users apply various learning algorithms and generates multiple classification models and evaluation reports that distinguish cancer from normal samples, as well as different types and subtypes of cancer. Availability and Implementation: The open source pipeline is freely available for download at https://github.com/HelikarLab/CancerDiscover. Supplementary Information: Please refer to the CancerDiscover README (Supplementary File 1) for detailed instructions on installation and operation of the pipeline. For a list of available feature selection methods, see Supplementary File 2."}, {"title": "Functional Diversification of Tripeptidyl Peptidase and Endopeptidase Sedolisins in Fungi", "url": "https://www.biorxiv.org/content/early/2017/08/31/167379", "tag": "Bioinformatics", "abstract": "Sedolisins are acid proteases that are related to the basic subtilisins. They have been identified in all three superkingdoms but are not ubiquitous, although fungi that secrete acids as part of their lifestyle can have up to six paralogs. Both tripeptidyl peptidase (TPP) and endopeptidase activity have been identified and it has been suggested that these correspond to separate subfamilies. We studied eukaryotic sedolisins by computational analysis. A maximum likelihood tree shows three major clades of which two contain only fungal sequences. One fungal clade contains all known TPPs whereas the other contains the endosedolisins. We identified four cluster specific inserts (CSIs) in endosedolisins, of which CSIs 1, 3 and 4 appear as solvent exposed according to structure modeling. Part of CSI2 is exposed but a short stretch forms a novel and partially buried \u03b1-helix that induces a conformational change near the binding pocket. We also identified a total of 12 specificity determining positions (SDPs) divided over three SDP sub-networks. The major SDP network contains eight directly connected SDPs and modeling of virtual mutants suggests a key role for the W307A or F307A substitution. This substitution is accompanied by a group of four SDPs that physically interact at the interface of the catalytic domain and the enzyme's prosegment. Modeling of virtual mutants suggests these SDPs are indeed required to compensate the conformational change induced by CSI2 and the A307. The additional major network SDPs as well as the two small SDP networks appear to be linked to this major substitution, all together explaining the hypothesized functional diversification of fungal sedolisins."}, {"title": "Systematic integration of biomedical knowledge prioritizes drugs for repurposing", "url": "https://www.biorxiv.org/content/early/2017/08/31/087619", "tag": "Bioinformatics", "abstract": "The ability to computationally predict whether a compound treats a disease would improve the economy and success rate of drug approval. This study describes Project Rephetio to systematically model drug efficacy based on 755 existing treatments. First, we constructed Hetionet (https://neo4j.het.io), an integrative network encoding knowledge from millions of biomedical studies. Hetionet v1.0 consists of 47,031 nodes of 11 types and 2,250,197 relationships of 24 types. Data was integrated from 29 public resources to connect compounds, diseases, genes, anatomies, pathways, biological processes, molecular functions, cellular components, pharmacologic classes, side effects, and symptoms. Next, we identified network patterns that distinguish treatments from non-treatments. Then we predicted the probability of treatment for 209,168 compound\u2013disease pairs (http://het.io/repurpose). Our predictions validated on two external sets of treatment and provided pharmacological insights on epilepsy, suggesting they will help prioritize drug repurposing candidates. This study was entirely open and received realtime feedback from 40 community members."}, {"title": "lme4qtl: linear mixed models with flexible covariance structure for genetic studies of related individuals", "url": "https://www.biorxiv.org/content/early/2017/08/31/139816", "tag": "Bioinformatics", "abstract": "Quantitative trait locus (QTL) mapping in genetic data often involves analysis of correlated observations, which need to be accounted for to avoid false association signals. This is commonly performed by modeling such correlations as random effects in linear mixed models (LMMs). The R package lme4 is a well-established tool that implements major LMM features using sparse matrix methods; however, it is not fully adapted for QTL mapping association and linkage studies. In particular, two LMM features are lacking in the base version of lme4: the definition of random effects by custom covariance matrices; and parameter constraints, which are essential in advanced QTL models. Apart from applications in linkage studies of related individuals, such functionalities are of high interest for association studies in situations where multiple covariance matrices need to be modeled, a scenario not covered by many genome-wide association study (GWAS) software. To address the aforementioned limitations, we developed a new R package lme4qtl as an extension of lme4. First, lme4qtl contributes new models for genetic studies within a single tool integrated with lme4 and its companion packages. Second, lme4qtl offers a flexible framework for scenarios with multiple levels of relatedness and becomes efficient when covariance matrices are sparse. We showed the value of our package using real family-based data in the Genetic Analysis of Idiopathic Thrombophilia 2 (GAIT2) project. lme4qtl is available at https://github.com/variani/lme4qtl."}, {"title": "SpliceVec: distributed feature representations for splice junction prediction", "url": "https://www.biorxiv.org/content/early/2017/08/31/183087", "tag": "Bioinformatics", "abstract": "Background: Identification of intron boundaries, called splice junctions, is an important part of delineating gene structure and functions. This also provides valuable insights into the role of alternative splicing (AS) in increasing functional diversity of genes. Identification of splice junctions through RNA-seq is by mapping short reads to the reference genome which is prone to errors due to random sequence matches. This encourages identification of splicing junctions through computational methods based on machine learning. Existing models are dependent on feature extraction and selection for capturing splicing signals lying in vicinity of splice junctions. But such manually extracted features are not exhaustive. Methods: We introduce distributed feature representation, SpliceVec, to avoid explicit and biased feature extraction generally adopted for such tasks. SpliceVec is based on two widely used distributed representation models in natural language processing. Learned feature representation in form of SpliceVec is fed to multilayer perceptron for splice junction classification task. Results: An intrinsic evaluation of SpliceVec indicates that it is able to group true and false sites distinctly. Our study on optimal context to be considered for feature extraction indicates inclusion of entire intronic sequence is better than flanking upstream and downstream region around splice junctions. Further, SpliceVec is invariant to canonical and non-canonical splice junction detection. The proposed model is consistent in its performance even with reduced dataset and class-imbalanced dataset. Conclusions: SpliceVec is computationally efficient and can be trained with user defined data as well."}, {"title": "Lessons Learned: Recommendations for Establishing Critical Periodic Scientific Benchmarking", "url": "https://www.biorxiv.org/content/early/2017/08/31/181677", "tag": "Bioinformatics", "abstract": "The dependence of life scientists on software has steadily grown in recent years. For many tasks, researchers have to decide which of the available bioinformatics software are more suitable for their specific needs. Additionally researchers should be able to objectively select the software that provides the highest accuracy, the best efficiency and the highest level of reproducibility when integrated in their research projects. Critical benchmarking of bioinformatics methods, tools and web services is therefore an essential community service, as well as a critical component of reproducibility efforts. Unbiased and objective evaluations are challenging to set up and can only be effective when built and implemented around community driven efforts, as demonstrated by the many ongoing community challenges in bioinformatics that followed the success of CASP. Community challenges bring the combined benefits of intense collaboration, transparency and standard harmonization. Only open systems for the continuous evaluation of methods offer a perfect complement to community challenges, offering to larger communities of users, that could extend far beyond the community of developers, a window to the developments status that they can use for their specific projects. We understand by continuous evaluation systems as those services which are always available and periodically update their data and/or metrics according to a predefined schedule keeping in mind that the performance has to be always seen in terms of each research domain. We argue here that technology is now mature to bring community driven benchmarking efforts to a higher level that should allow effective interoperability of benchmarks across related methods. New technological developments allow to overcome the limitations of the first experiences on online benchmarking e.g. EVA. We therefore describe OpenEBench, a novel infra-structure designed to establish a continuous automated benchmarking system for bioinformatics methods, tools and web services. OpenEBench is being developed so as to cater for the needs of the bioinformatics community, especially software developers who need an objective and quantitative way to inform their decisions as well as the larger community of end-users, in their search for unbiased and up-to-date evaluation of bioinformatics methods. As such OpenEBench should soon become a central place for bioinformatics software developers, community-driven benchmarking initiatives, researchers using bioinformatics methods, and funders interested in the result of methods evaluation."}, {"title": "R2DGC: Threshold-free peak alignment and identification for 2D gas chromatography mass spectrometry in R", "url": "https://www.biorxiv.org/content/early/2017/08/31/179168", "tag": "Bioinformatics", "abstract": "Comprehensive two dimensional gas chromatography-mass spectrometry is a powerful method for analyzing complex mixtures of volatile compounds. This method produces a large amount of raw data that requires downstream processing to align signals of interest (peaks) across multiple samples and match peak characteristics to reference standard libraries prior to downstream statistical analysis. To address the paucity of applications addressing this need, we have developed an R package that implements retention time and mass spectra similarity threshold-free alignments, seamlessly integrates retention time standards for universally reproducible alignments, performs common ion filtering, and provides compatibility with multiple peak quantification methods. We demonstrate the packages utility on a controlled mix of metabolite standards separated under variable chromatography conditions and data generated from cell lines."}, {"title": "Genome-wide profiling of transcribed enhancers during macrophage activation", "url": "https://www.biorxiv.org/content/early/2017/08/30/163519", "tag": "Bioinformatics", "abstract": "Macrophages are sentinel cells essential for tissue homeostasis and host defence. Owing to their plasticity, macrophages acquire a range of functional phenotypes in response to microenvironmental stimuli, of which M(IFN-\u03b3) and M(IL-4/IL-13) are well-known for their opposing pro- and anti-inflammatory roles. Enhancers have emerged as regulatory DNA elements crucial for transcriptional activation of gene expression. Using cap analysis of gene expression and epigenetic data, we identify on large-scale transcribed enhancers in mouse macrophages, their time kinetics and target protein-coding genes. We observe an increase in target gene expression, concomitant with increasing numbers of associated enhancers and find that genes associated to many enhancers show a shift towards stronger enrichment for macrophage-specific biological processes. We infer enhancers that drive transcriptional responses of genes upon M(IFN-\u03b3) and M(IL-4/IL-13) macrophage activation and demonstrate stimuli-specificity of regulatory associations. Finally, we show that enhancer regions are enriched for binding sites of inflammation-related transcription factors, suggesting a link between stimuli response and enhancer transcriptional control. Our study provides new insights into genome-wide enhancer-mediated transcriptional control of macrophage genes, including those implicated in macrophage activation, and offers a detailed genome-wide catalogue to further elucidate enhancer regulation in macrophages."}, {"title": "TiSAn: Estimating Tissue Specific Effects of Coding and Non-coding Variants", "url": "https://www.biorxiv.org/content/early/2017/08/30/141408", "tag": "Bioinformatics", "abstract": "Measures of general deleteriousness, like CADD or PolyPhen, have become indispensable tools in the interpretation of genetic variants. However, these estimates say little about where in the organism these deleterious effects will be most apparent. An additional, complementary measure is needed to link deleterious variants (as determined by e.g., CADD) to tissues in which their effect will be most meaningful. Here, we introduce TiSAn (Tissue Specific Annotation), a tool that predicts how related a genomic position is to a given tissue (http://github.com/kevinVervier/TiSAn). TiSAn uses machine learning on genome-scale, tissue-specific data to discriminate variants relevant to a tissue from those having no bearing on the development or function of that tissue. Predictions are made genome-wide, and these scores can then be used to contextualize and filter variants of interest in whole genome sequencing or genome wide association studies (GWAS). We demonstrate the performance and versatility of TiSAn training predictive models for human heart and brain, and detecting tissue-relevant variations in large cohorts for autism spectrum disorder (using TiSAn-brain) and coronary artery disease (using TiSAn-heart). We find that TiSAn is better able to prioritize genetic variants according to their tissue-specific action than the current state of the art method, GenoSkyLine."}, {"title": "A bestiary of localized sequence rearrangements in human DNA", "url": "https://www.biorxiv.org/content/early/2017/08/30/175943", "tag": "Bioinformatics", "abstract": "Genomes mutate and evolve in ways simple (substitution or deletion of bases) and complex (e.g. chromosome shattering). We do not fully understand what types of complex mutation occur, and we cannot routinely characterize arbitrarily-complex mutations in a high-throughput, genome-wide manner. Long-read DNA sequencing methods (e.g. PacBio, nanopore) are promising for this task, because one read may encompass a whole complex mutation. We describe an analysis pipeline to characterize arbitrarily-complex \"local\" mutations, i.e. intrachromosomal mutations encompassed by one DNA read. We apply it to nanopore and PacBio reads from one human cell line (NA12878), and survey sequence rearrangements, both real and artifactual. Almost all the real rearrangements belong to recurring patterns or motifs: the most common is tandem multiplication (e.g. heptuplication), but there are also complex patterns such as localized shattering, which resembles DNA damage by radiation. Gene conversions are identified, including one between hemoglobin gamma genes. This study demonstrates a way to find intricate rearrangements with any number of duplications, deletions, and repositionings. It demonstrates a probability-based method to resolve ambiguous rearrangements involving highly similar sequences, as occurs in gene conversion. We present a catalog of local rearrangements in one human cell line, and show which rearrangement patterns occur."}, {"title": "DNAlogo: a smart mini application for generating DNA sequence logos", "url": "https://www.biorxiv.org/content/early/2017/08/30/096933", "tag": "Bioinformatics", "abstract": "Sequence logos are frequently used for presenting consensus sequences and motifs of nucleic acids and proteins. WebLogo of UC Berkeley is the most popular tool for creating sequence logos, whereas, it is often restricted by the internet speed in some developing countries and no graphic interface for its stand-alone version. Here, the author generated an application, DNAlogo, using VB.net, which runs in Windows system. DNAlogo is small and convenient. It creates both bitmap and vector map. Beside the classic sequence logo function, DNAlogo introduced compensated logo to generate consensus sequences considering different GC contents of different genomes. DNAlogo provides a simple way for researchers without programming knowledge to create DNA sequence logos."}, {"title": "Phylogenetic approaches to identifying fragments of the same gene, with application to the wheat genome", "url": "https://www.biorxiv.org/content/early/2017/08/30/182550", "tag": "Bioinformatics", "abstract": "As the time and cost of sequencing decrease, the number of available genomes and transcriptomes rapidly increases. Yet the quality of the assemblies and the gene annotations varies considerably and often remains poor, affecting downstream analyses. This is particularly true when fragments of the same gene are annotated as distinct genes and consequently wrongly appear as paralogs. In this study, we introduce two novel phylogenetic tests to infer non-overlapping or partially overlapping genes that are in fact parts of the same gene. One approach collapses branches with low bootstrap support and the other computes a likelihood ratio test. We extensively validated these methods by 1) introducing and recovering fragmentation on the bread wheat, Triticum aestivum cv. Chinese Spring, chromosome 3B; 2) by applying the methods to the low-quality 3B assembly and validating predictions against the high-quality 3B assembly; and 3) by comparing the performance of the proposed methods to the performance of existing methods, namely Ensembl Compara and ESPRIT. Application of this combination to a draft shotgun assembly of the entire bread wheat genome revealed 1221 pairs of genes which are highly likely to be fragments of the same gene. Our approach demonstrates the power of fine-grained evolutionary inferences across multiple species to improving genome assemblies and annotations. An open source software tool is available at https://github.com/DessimozLab/esprit2."}, {"title": "ApoplastP: prediction of effectors and plant proteins in the apoplast using machine learning", "url": "https://www.biorxiv.org/content/early/2017/08/30/182428", "tag": "Bioinformatics", "abstract": "The plant apoplast is integral to intercellular signalling, transport and plant-pathogen interactions. Plant pathogens deliver effectors both into the apoplast and inside host cells, but no computational method currently exists to discriminate between these localizations. We present ApoplastP, the first method for predicting if an effector or plant protein localizes to the apoplast. ApoplastP uncovers features for apoplastic localization common to both effectors and plant proteins, namely an enrichment in small amino acids and cysteines as well as depletion in glutamic acid. ApoplastP predicts apoplastic localization in effectors with sensitivity of 75% and false positive rate of 5%, improving accuracy of cysteine-rich classifiers by over 13%. ApoplastP does not depend on the presence of a signal peptide and correctly predicts the localization of unconventionally secreted plant and effector proteins. The secretomes of fungal saprophytes, necrotrophic pathogens and extracellular pathogens are enriched for predicted apoplastic proteins. Rust pathogen secretomes have the lowest percentage of apoplastic proteins, but these are highly enriched for predicted effectors. ApoplastP pioneers apoplastic localization prediction using machine learning. It will facilitate functional studies and will be valuable for predicting if an effector localizes to the apoplast or if it enters plant cells. ApoplastP is available at http://apoplastp.csiro.au."}, {"title": "PrimerServer: a high-throughput primer design and specificity-checking platform", "url": "https://www.biorxiv.org/content/early/2017/08/30/181941", "tag": "Bioinformatics", "abstract": "Summary: Designing specific primers for multiple sites across the whole genome is still challenging, especially in species with complex genomes. Here we present PrimerServer, a high-throughput primer design and specificity-checking platform with both web and command-line interfaces. This platform efficiently integrates site selection, primer design, specificity checking and data presentation. In our case study, PrimerServer achieved high accuracy and a fast running speed for a large number of sites, suggesting its potential for molecular biology applications such as molecular breeding or medical testing. Availability and Implementation: Source code for PrimerServer is available at https://github.com/billzt/PrimerServer. A demo server is freely accessible at https://primerserver.org, with all major browsers supported."}, {"title": "GenomeDISCO: A concordance score for chromosome conformation capture experiments using random walks on contact map graphs", "url": "https://www.biorxiv.org/content/early/2017/08/29/181842", "tag": "Bioinformatics", "abstract": "The three-dimensional organization of chromatin plays a critical role in gene regulation and disease. High-throughput chromosome conformation capture experiments such as Hi-C are used to obtain genome-wide maps of 3D chromatin contacts. However, robust estimation of data quality and systematic comparison of these contact maps is challenging due to the multi-scale, hierarchical structure of the data and the resulting idiosyncratic properties of experimental noise. We introduce a multi-scale concordance measure called GenomeDISCO (DIfferences between Smoothed COntact maps) for assessing the similarity of a pair of contact maps obtained from chromosome capture experiments. We denoise the contact maps using random walks on the contact map graph, and integrate concordance at multiple scales of smoothing. We use simulated datasets to benchmark GenomeDISCO's sensitivity to different types of noise typically affecting chromatin contact maps. When applied to a large collection of Hi-C datasets, GenomeDISCO accurately distinguishes biological replicates from samples obtained from different cell types. Software implementing GenomeDISCO is available at http://github.com/kundajelab/genomedisco."}, {"title": "Identifying weak signals in inhomogeneous neuronal images for large-scale tracing of neurites", "url": "https://www.biorxiv.org/content/early/2017/08/29/181867", "tag": "Bioinformatics", "abstract": "Reconstructing neuronal morphology across different regions or even the whole brain is important in many areas of neuroscience research. Large-scale tracing of neurites constitutes the core of this type of reconstruction and has many challenges. One key challenge is how to identify a weak signal from an inhomogeneous background. Here, we addressed this problem by constructing an identification model. In this model, empirical observations made from neuronal images are summarized into rules, which are used to design feature vectors that display the differences between the foreground and background, and a support vector machine is used to learn these feature vectors. We embedded this identification model into a tool that we previously developed, SparseTracer, and termed this integration SparseTracer-Learned Feature Vector (ST-LFV). ST-LFV can trace neurites with extremely weak signals (signal-to-background-noise ratio < 1.1) against an inhomogeneous background. By testing 12 sub-blocks extracted from a whole imaging dataset, ST-LFV can achieve an average recall rate of 0.99 and precision rate of 0.97, which is superior to that of SparseTracer (which has an average recall rate of 0.93 and average precision rate of 0.86), indicating that this method is well suited to weak signal identification. We applied ST-LFV to trace neurites from large-scale images (approximately 105 GB). During the tracing process, obtaining results equivalent to the ground truth required only one round of manual editing for ST-LFV compared to 20 rounds of manual editing for SparseTracer. This improvement in the level of automatic reconstruction indicates that ST-LFV has the potential to rapidly reconstruct sparsely distributed neurons at the scale of an entire brain."}, {"title": "Peptide Pattern Recognition for high-throughput protein sequence analysis and clustering", "url": "https://www.biorxiv.org/content/early/2017/08/29/181917", "tag": "Bioinformatics", "abstract": "Large collections of protein sequences with divergent sequences are tedious to analyze for understanding their phylogenetic or structure-function relation. Peptide Pattern Recognition is an algorithm that was developed to facilitate this task but the previous version does only allow a limited number of sequences as input. I implemented Peptide Pattern Recognition as a multithread software designed to handle large numbers of sequences and perform analysis in a reasonable time frame. Benchmarking showed that the new implementation of Peptide Pattern Recognition is twenty times faster than the previous implementation on a small protein collection with 673 MAP kinase sequences. In addition, the new implementation could analyze a large protein collection with 48,570 Glycosyl Transferase family 20 sequences without reaching its upper limit on a desktop computer. Peptide Pattern Recognition is a useful software for providing comprehensive groups of related sequences from large protein sequence collections."}, {"title": "Kafka interfaces for composable streaming genomics pipelines", "url": "https://www.biorxiv.org/content/early/2017/08/29/182030", "tag": "Bioinformatics", "abstract": "Modern sequencing machines produce order of a terabyte of data per day, which need subsequently to go through a complex processing pipeline. The standard workflow begins with a few independent, shared-memory tools, which communicate by means of intermediate files. Given the constant increase of the amount of data produced, this approach is proving more and more unmanageable, due to its lack of robustness and scalability. In this work we propose the adoption of stream computing to simplify the genomic pipeline, boost its performance and improve its fault-tolerance. We decompose the first steps of the genomic processing in two distinct and specialized modules (preprocessing and alignment) and we loosely compose them via communication through Kafka streams, in order to allow for easy composability and integration in the already existing Hadoop-based pipelines. The proposed solution is then experimentally validated on real data and shown to scale almost linearly."}, {"title": "GVC: A superfast and universal genomic variant caller", "url": "https://www.biorxiv.org/content/early/2017/08/29/182089", "tag": "Bioinformatics", "abstract": "Germline and somatic variant detection from human and cancer whole-genome sequencing data is a challenge task for genome-wide association study and cancer genomics in precision medicine. Many confounding factors contribute the difficulties including complexity of variant, sequencing and alignment error, tumor clonality and sample purity etc. Current genomic variant callers are too time-consuming to meet the requirement of clinical application in precision medicine. We developed superfast and universal Genomic Variant Caller (GVC), which can simultaneously detect various genomic variants including SNV, sINDEL and SV from personal and normal-cancer paired whole-genome/exome sequencing data within fifteen minutes. What's more, it achieved higher sensitivity and precision than popular variant callers including GATK4, Mutect, NovoBreak in germline and somatic variant detection from NA12878 and ICGC-TCGA Dream Challenge Datasets respectively. It is worth mentioning that GVC achieved comparable performance in variant detection from NA12878 sequenced by three different high-throughput sequencing platforms including Illumina HiSeq2000, NovaSeq and BGISEQ-500."}, {"title": "BITE: an R package for biodiversity analyses", "url": "https://www.biorxiv.org/content/early/2017/08/29/181610", "tag": "Bioinformatics", "abstract": "Nowadays, molecular data analyses for biodiversity studies often require advanced bioinformatics skills, preventing many life scientists from analyzing their own data autonomously. BITE R package provides complete and user-friendly functions to handle SNP data and third-party software results (i.e. Admixture, TreeMix), facilitating their visualization, interpretation and use. Furthermore, BITE implements additional useful procedures, such as representative sampling and bootstrap for TreeMix, filling the gap in existing biodiversity data analysis tools. Availability: https://github.com/marcomilanesi/BITE."}, {"title": "Parallel sequencing lives, or what makes large sequencing projects successful", "url": "https://www.biorxiv.org/content/early/2017/08/29/136358", "tag": "Bioinformatics", "abstract": "T47D_rep2 and b1913e6c1_51720e9cf were two Hi-C samples. They were born and processed at the same time, yet their fates were very different. The life of b1913e6c1_51720e9cf was simple and fruitful, while that of T47D_rep2 was full of accidents and sorrow. At the heart of these differences lies the fact that b1913e6c1_51720e9cf was born under a lab culture of Documentation, Automation, Traceability, Autonomy and compliance with the FAIR Principles. Their lives are a lesson for those who wish to embark on the journey of managing high throughput sequencing data."}, {"title": "GTC: a novel attempt to maintenance of huge genome collections compressed", "url": "https://www.biorxiv.org/content/early/2017/08/29/131649", "tag": "Bioinformatics", "abstract": "We present GTC, a novel compressed data structure for representation of huge collections of genetic variation data. GTC significantly outperforms existing solutions in terms of compression ratio and time of answering various types of queries. We show that the largest of publicly available database of about 60 thousand haplotypes at about 40 million SNPs can be stored in less than 4Gbytes, while the queries related to variants are answered in a fraction of a second. GTC can be downloaded from https://github.com/refresh-bio/GTC or http://sun.aei.polsl.pl/REFRESH/gtc."}, {"title": "HiCcompare: a method for joint normalization of Hi-C datasets and differential chromatin interaction detection", "url": "https://www.biorxiv.org/content/early/2017/08/28/147850", "tag": "Bioinformatics", "abstract": "Changes in spatial chromatin interactions are now emerging as a unifying mechanism orchestrating regulation of gene expression. Evolution of chromatin conformation capture methods into Hi-C sequencing technology now allows an insight into chromatin interactions on a genome-wide scale. However, Hi-C data contains many DNA sequence- and technology-driven biases. These biases prevent effective comparison of chromatin interactions aimed at identifying genomic regions differentially interacting between, disease-normal states or different cell types. Several methods have been developed for normalizing individual Hi-C datasets. However, they fail to account for biases between two or more Hi-C datasets, hindering comparative analysis of chromatin interactions. We developed a simple and effective method HiCcompare for the joint normalization and differential analysis of multiple Hi-C datasets. The method avoids constraining Hi-C data within a rigid statistical model, allowing a data-driven normalization of biases using locally weighted linear regression (loess). The method identifies region-specific chromatin interaction changes complementary to changes due to large-scale genomic rearrangements, such as copy number variants (CNVs). HiCcompare outperforms methods for normalizing individual Hi-C datasets in detecting a priori known chromatin interaction differences in simulated and real-life settings while detecting biologically relevant changes. HiCcompare is freely available as a Bioconductor R package https://bioconductor.org/packages/HiCcompare."}, {"title": "Folding membrane proteins by deep transfer learning", "url": "https://www.biorxiv.org/content/early/2017/08/28/181628", "tag": "Bioinformatics", "abstract": "Computational elucidation of membrane protein (MP) structures is challenging partially due to lack of sufficient solved structures for homology modeling. Here we describe a high-throughput deep transfer learning method that first predicts MP contacts by learning from non-membrane proteins (nonMPs) and then predicting three dimensional structure models using the predicted contacts as distance restraints. Tested on 510 non-redundant MPs, our method has contact prediction accuracy at least 0.18 better than existing methods, predicts correct folds for 218 MPs (TMscore>0.6), and generates three-dimensional models with RMSD less than 4 Angstrom and 5 Angstrom for 57 and 108 MPs, respectively. A rigorous blind test in the continuous automated model evaluation (CAMEO) project shows that our method predicted high-resolution three-dimensional models for two recent test MPs of 210 residues with RMSD ~2 Angstrom. We estimated that our method could predict correct folds for between 1,345 and 1,871 reviewed human multi-pass MPs including a few hundred new folds, which shall facilitate the discovery of drugs targeting at membrane proteins."}, {"title": "Analysis of deep learning methods for blind protein contact prediction in CASP12", "url": "https://www.biorxiv.org/content/early/2017/08/28/181586", "tag": "Bioinformatics", "abstract": "Here we present the results of protein contact prediction achieved in CASP12 by our RaptorX-Contact server, which is an early implementation of our deep learning method for contact prediction. On a set of 38 free-modeling target domains with a median family size of around 58 effective sequences, our server obtained an average top L/5 long- and medium-range contact accuracy of 47% and 44%, respectively (L=length). A more advanced implementation has an average accuracy of 59% and 57%, respectively. Our deep learning method formulates contact prediction as an image pixel-level labeling problem and simultaneously predicts all residue pairs of a protein using a combination of two deep residual neural networks, taking as input the residue conservation information, predicted secondary structure and solvent accessibility, contact potential, and co-evolution information. Our approach differs from existing methods mainly in (1) formulating contact prediction as a pixel-level image labeling problem instead of an image-level classification problem; (2) simultaneously predicting all contacts of an individual protein to make effective use of contact occurrence patterns; and (3) integrating both 1D and 2D deep convolutional neural networks to effectively learn complex sequence-structure relationship including high-order residue correlation. This paper discusses the RaptorX-Contact pipeline, both contact prediction and contact-based folding results, and finally the strength and weakness of our method."}, {"title": "DrImpute: Imputing dropout events in single cell RNA sequencing data", "url": "https://www.biorxiv.org/content/early/2017/08/28/181479", "tag": "Bioinformatics", "abstract": "The single cell RNA sequencing (scRNA-seq) technique began a new era by allowing the observation of gene expression at the single cell level. However, there is also a large amount of technical and biological noise. Because of the low number of RNA transcriptomes and the stochastic nature of the gene expression pattern, there is a high chance of missing nonzero entries as zero, which are called dropout events. However, many statistical methods used for analyzing scRNA-seq data in cell type identification, visualization, and lineage reconstruction do not model for dropout events. We have developed DrImpute to impute dropout events, and it improves many of the statistical tools used for scRNA-seq analysis that do not account for dropout events. Our numerical studies with real data demonstrate the promising performance of the proposed method, which has been implemented in R."}, {"title": "Intra-voxel incoherent motion magnetic resonance imaging of the living human fetus: the technique and within-subject reproducibility", "url": "https://www.biorxiv.org/content/early/2017/08/28/180844", "tag": "Bioinformatics", "abstract": "Our purpose was to test the within-subject repeatability of the perfusion fraction, diffusion coefficient and pseudo diffusion coefficient measurements in various fetal organs and in the placenta based on the intravoxel incoherent motion imaging principle. In utero diffusion-weighted magnetic resonance imaging was performed on 1.5T and 3.0T clinical scanners with b-factors ranging from 0 to 900 s/mm2 in 16 steps and a tetrahedral diffusion-weighting encoding scheme. Data from 16 pregnant women (maternal age: 34 \u00b1 4.9 years, range: 24.6-40.8) were included in this pilot study. For 15 cases, IVIM was repeated (maternal age: 33.7 \u00b1 5.2 years, range: 24.6-40.8). A bi-exponential model was fitted on the volume-averaged diffusion values and the perfusion fraction (f), diffusion coefficient (d) and pseudo diffusion coefficient (D*) were calculated. Within-subject repeatability was given as the test-retest variability of the IVIM parameters in the fetal frontal cortex, frontal white matter, cerebellum, lungs, kidneys, liver and in the placenta. An in-house developed image processing script was utilized to remove the image frames with excessive motion, and to perform motion correction by using non-linear freeform deformations. For the fetal lungs, liver and the placenta, within-subject variability ranged from 14.4% to 20.4% for f, 12.2% to 14.1% for d and 16.8% to 25.3% for D*. The diffusion coefficients of the investigated brain regions were moderately to highly reproducible (4.8%-15.2%), however, f and D* showed inferior reproducibility compared to corresponding measures derived for the lungs, liver and placenta. The IVIM-based parameters of the fetal kidney were revealed to be highly variable across scans. Our results indicate that in utero intravoxel incoherent motion magnetic resonance imaging potentially provides a novel method for examining microvascular perfusion and diffusion in the developing human fetus. The reproducible quantification of the perfusion and diffusion parameters depend greatly upon data quality, fetal and maternal movements, and image post processing to detect and remove corrupted data before calculating the IVIM model."}, {"title": "Unearthing new genomic markers of drug response by improved measurement of discriminative power", "url": "https://www.biorxiv.org/content/early/2017/08/27/033092", "tag": "Bioinformatics", "abstract": "Oncology drugs are only effective in a small proportion of cancer patients. To make things worse, our current ability to identify these responsive patients before treatment is still very limited. Thus, there is a pressing need to discover response markers for marketed and research oncology drugs in order to improve patient survival, reduce healthcare costs and enhance success rates in clinical trials. Screening these drugs against a large panel of cancer cell lines has been recently employed to discover new genomic markers of in vitro drug response, which can now be further evaluated on more accurate tumour models. However, while the identification of discriminative markers among thousands of candidate drug-gene associations in the data is error-prone, an appraisal of the effectiveness of such detection task is currently lacking. Here we present a new non-parametric method to measuring the discriminative power of a drug-gene association. This is enabled by the identification of an auxiliary threshold posing this task as a binary classification problem. Unlike parametric statistical tests, the adopted non-parametric test has the advantage of not making strong assumptions about the data distorting the identification of genomic markers. Furthermore, we introduce a new benchmark to further validate these markers in vitro using more recent data not used to identify the markers. Thus, the application of this new methodology has led to the identification of 128 new genomic markers distributed across 61% of the analysed drugs, including 5 drugs without previously known markers, which were missed by the MANOVA test initially applied to analyse data from the Genomics of Drug Sensitivity in Cancer consortium."}, {"title": "Prediction of early breast cancer patient survival using ensembles of hypoxia signatures", "url": "https://www.biorxiv.org/content/early/2017/08/27/181289", "tag": "Bioinformatics", "abstract": "Background: Biomarkers are a key component of precision medicine. However, full clinical integration of biomarkers has been met with challenges, partly attributed to analytical difficulties. It has been shown that biomarker reproducibility is susceptible to data preprocessing approaches. Here, we systematically evaluated machine-learning ensembles of preprocessing methods as a general strategy to improve biomarker performance for prediction of survival from early breast cancer. Results: We risk stratified breast cancer patients into either low-risk or high-risk groups based on four published hypoxia signatures (Buffa, Winter, Hu, and Sorensen), using 24 different preprocessing approaches for microarray normalization. The 24 binary risk profiles determined for each hypoxia signature were combined using a random forest to evaluate the efficacy of a preprocessing ensemble classifier. We demonstrate that the best way of merging preprocessing methods varies from signature to signature, and that there is likely no \"best\" preprocessing pipeline that is universal across datasets, highlighting the need to evaluate ensembles of preprocessing algorithms. Further, we developed novel signatures for each preprocessing method and the risk classifications from each were incorporated in a meta-random forest model. Interestingly, the classification of these biomarkers and its ensemble show striking consistency, demonstrating that similar intrinsic biological information are being faithfully represented. As such, these classification patterns further confirm that there is a subset of patients whose prognosis is consistently challenging to predict. Conclusions: Performance of different prognostic signatures varies with pre-processing method. A simple classifier by unanimous voting of classifications is a reliable way of improving on single preprocessing methods. Future signatures will likely require integration of intrinsic and extrinsic clinico-pathological variables to better predict disease-related outcomes."}, {"title": "A probabilistic model-based bi-clustering method for single-cell transcriptomic data analysis", "url": "https://www.biorxiv.org/content/early/2017/08/27/181362", "tag": "Bioinformatics", "abstract": "We present here novel computational techniques for tackling four problems related to analyses of single-cell RNA-Seq data: (1) a mixture model for coping with multiple cell types in a cell population; (2) a truncated model for handling the unquantifiable errors caused by large numbers of zeros or low-expression values; (3) a bi-clustering technique for detection of sub-populations of cells sharing common expression patterns among subsets of genes; and (4) detection of small cell sub-populations with distinct expression patterns. Through case studies, we demonstrated that these techniques can derive high-resolution information from single-cell data that are not feasible using existing techniques."}, {"title": "B-cell receptor reconstruction from single-cell RNA-seq with VDJPuzzle", "url": "https://www.biorxiv.org/content/early/2017/08/26/181156", "tag": "Bioinformatics", "abstract": "The B-cell receptor (BCR) performs essential functions for the adaptive immune system including recognition of pathogen-derived antigens. Cell-to-cell variability of BCR sequences due to V(D)J recombination and somatic hypermutation (SHM) necessitates single-cell characterization of BCR sequences. Single-cell RNA sequencing (scRNA-seq) presents the opportunity for simultaneous capture of the BCR sequence and transcriptomic signature for a detailed understanding of the dynamics of an immune response. We developed VDJPuzzle 2.0, a bioinformatic tool that reconstructs productive, full-length B-cell receptor sequences of both heavy and light chains. VDJPuzzle successfully reconstructs BCRs from 98.3% (n=117) of human and 96.5% (n=200) from murine B cells. 92.0% of clonotypes and 90.3% of mutations were concordant with single-cell Sanger sequencing of the immunoglobulin chains. VDJPuzzle is available at https://bitbucket.org/kirbyvisp/vdjpuzzle2"}, {"title": "Multi-reference spectral library yields almost complete coverage of heterogeneous LC-MS/MS data sets", "url": "https://www.biorxiv.org/content/early/2017/08/26/180448", "tag": "Bioinformatics", "abstract": "Spectral libraries play a central role in the analysis of data independent acquisition (DIA) proteomics experiments. DIA experiments require spectral libraries, as most current methods cannot apply traditional peptide identification via database searching on DIA data. A central assumption in current spectral library tools is that a single characteristic intensity pattern (CIP) suffices to describe the fragmentation of an unmodified peptide in a particular parent ion charge state (peptide charge pair). However, we find that this is often not the case. We analyze a heterogeneous dataset of 440.000 MaxQuant - preprocessed peptide spectra from a QToF mass spectrometer, stemming from over 100 different LC-MS/MS runs. The dataset corresponds to 10.580 peptide charge pairs, which have each been measured and identified at least 20 times. We demonstrate that the same charged and unmodified peptide can fragment in multiple reproducible ways, even within the same LC-MS/MS run. We integrate multiple reference CIPs (MCIPs) in our model library and observe a >99% coverage of replicate fragmentation spectra for 95% of peptide charge pairs (using up to four CIPs). Using a single CIP (as in current spectral library approaches), we find >99% coverage for only 50% of the peptide charge pairs. Our approach achieves substantially greater sensitivity in comparison to the popular SpectraST library generation tool. Using randomized decoy spectra, we demonstrate that identification accuracy of the MCIP approach is improved by up to 12% compared to a single CIP approach. We test the MCIP approach on a SWATH data set and observe a ~30% increase in peptide recognition. We conclude that including MCIPs in spectral library approaches would yield increased sensitivity without compromising the false discovery rate."}, {"title": "Identifying the genetic determinants of particular phenotypes in microbial genomes with very small training sets", "url": "https://www.biorxiv.org/content/early/2017/08/26/181222", "tag": "Bioinformatics", "abstract": "Background: Machine learning (ML) encompasses a large set of algorithms that aim at discovering complex patterns between elements within large data sets without any prior assumptions or modeling. However, some scientific disciplines still produce small data sets: in particular, empirical studies that try to link complex phenotypes such as virulence or drug resistance to individual sets of protein-coding genes (proteomes) typically have very small sample sizes. To date, it is unknown how ML performs in such cases. Results: To address this question, we evaluated the performance of adaptive boosting, a general ML classifier, on two data sets containing both the phenotype and the complete proteome of a small number of individuals. To assess the impact of proteome size, we contrasted a small genome (a virus: influenza) with a larger one (a bacterium: Pseudomonas). In order to analyze large proteomes, we developed a chunking algorithm. With the influenza data, we were able to rediscover amino acid sites experimentally implicated in three different complex phenotypes (infectivity, transmissibility, and pathogenicity). However, results for the much larger pseudomonas proteome, pertaining to three types of drug resistance (Ciprofloxacin, Ceftazidime, and Gentamicin), proved unstable, depended on a number of assumptions, and were not always biologically sensible. Conclusions: Our results show that ML algorithms such as adaptive boosting can be used to successfully identify the genetic determinants of microbes with small proteomes (viruses). Our chunking algorithm improved runtimes by an order of magnitude without sacrificing accuracy. Yet we found that the size of bacterial proteomes pushed ML to its limits in the face of small number of individuals. The use of these algorithms should probably be limited to preliminary or exploratory analysis, as long as both phenotyping and sequencing are too costly to perform on more individuals."}, {"title": "Unique genomic features and deeply-conserved functions of long non-coding RNAs in the Cancer LncRNA Census (CLC)", "url": "https://www.biorxiv.org/content/early/2017/08/25/152769", "tag": "Bioinformatics", "abstract": "Long non-coding RNAs (lncRNAs) that drive tumorigenesis are a growing focus of cancer genomics studies. To facilitate further discovery, we have created the \"Cancer LncRNA Census\" (CLC), a manually-curated and strictly-defined compilation of lncRNAs with causative roles in cancer. CLC has two principle applications: first, as a resource for training and benchmarking de novo identification methods; and second, as a dataset for studying the fundamental properties of these genes. CLC Version 1 comprises 122 lncRNAs implicated in 29 distinct cancers. LncRNAs are included based on functional or genetic evidence for causative roles in cancer progression. All belong to the GENCODE reference annotation, to enable integration across projects and datasets. For each entry, the evidence type, biological activity (oncogene or tumour suppressor), source reference and cancer type are recorded. Supporting its usefulness, CLC genes are significantly enriched amongst de novo predicted driver genes from PCAWG. CLC genes are distinguished from other lncRNAs by a series of features consistent with biological function, including gene length, high expression and sequence conservation of both exons and promoters. We identify a trend for CLC genes to be co-localised with known protein-coding cancer genes along the human genome. Finally, by integrating data from transposon-mutagenesis functional screens, we show that mouse orthologues of CLC genes tend also to be cancer genes. Thus CLC represents a valuable resource for research into long non-coding RNAs in cancer. Their evolutionary and genomic properties have implications for understanding disease mechanisms and point to conserved functions across ~80 million years of evolution."}, {"title": "Determining the optimal number of independent components for reproducible transcriptomic data analysis", "url": "https://www.biorxiv.org/content/early/2017/08/25/180687", "tag": "Bioinformatics", "abstract": "Background: Independent Component Analysis (ICA) is a method that models gene expression data as an action of a set of statistically independent hidden factors. The output of ICA depends on a fundamental parameter: the number of components (factors) to compute. The optimal choice of this parameter, related to determining the effective data dimension, remains an open question in the application of blind source separation techniques to transcriptomic data. Results: Here we address the question of optimizing the number of statistically independent components in the analysis of transcriptomic data for reproducibility of the components in multiple runs of ICA (within the same or within varying effective dimensions) and in multiple independent datasets. To this end, we introduce ranking of independent components based on their stability in multiple ICA computation runs and define a distinguished number of components (Most Stable Transcriptome Dimension, MSTD) corresponding to the point of the qualitative change of the stability profile. Based on a large body of data, we demonstrate that a sufficient number of dimensions is required for biological interpretability of the ICA decomposition and that the most stable components with ranks below MSTD have more chances to be reproduced in independent studies compared to the less stable ones. At the same time, we show that a transcriptomics dataset can be reduced to a relatively high number of dimensions without losing the interpretability of ICA, even though higher dimensions give rise to components driven by small gene sets. Conclusions: We suggest a protocol of ICA application to transcriptomics data with a possibility of prioritizing components with respect to their reproducibility that strengthens the biological interpretation. Computing too few components (much less than MSTD) is not optimal for interpretability of the results. The components ranked within MSTD range have more chances to be reproduced in independent studies."}, {"title": "Notos - a Galaxy tool to analyze CpN observed expected ratios for inferring DNA methylation types", "url": "https://www.biorxiv.org/content/early/2017/08/25/180463", "tag": "Bioinformatics", "abstract": "Background: DNA methylation patterns store epigenetic information in the vast majority of eukaryotic species. The relatively high costs and technical challenges associated with the detection of DNA methylation however have created a bias in the number of methylation studies towards model organisms. Consequently, it remains challenging to infer kingdom-wide general rules about the functions and evolutionary conservation of DNA methylation. Methylated cytosine is often found in specific CpN dinucleotides, and the frequency distributions of, for instance, CpG observed/expected (CpG o/e) ratios have been used to infer DNA methylation types based on higher mutability of methylated CpG. Results: Predominantly model-based approaches essentially founded on mixtures of Gaussian distributions are currently used to investigate questions related to the number and position of modes of CpG o/e ratios. These approaches require the selection of an appropriate criterion for determining the best model and will fail if empirical distributions are complex or even merely moderately skewed. We use a kernel density estimation (KDE) based technique for robust and precise characterization of complex CpN o/e distributions without a priori assumptions about the underlying distributions. Conclusions: We show that KDE delivers robust descriptions of CpN o/e distributions. For straightforward processing, we have developed a Galaxy tool, called Notos and available at the ToolShed, that calculates these ratios of input FASTA files and fits a density to their empirical distribution. Based on the estimated density the number and shape of modes of the distribution is determined, providing a rational for the prediction of the number and the types of different methylation classes. Notos is written in R and Perl."}, {"title": "An ensemble learning approach to auto-annotation for whole-brain C. elegans imaging", "url": "https://www.biorxiv.org/content/early/2017/08/25/180430", "tag": "Bioinformatics", "abstract": "Shifting from individual neuron analysis to whole-brain neural network analysis opens up new research opportunities for Caenorhabditis elegans (C. elegans). An automated data processing pipeline, including neuron detection, segmentation, tracking and annotation, will significantly improve the efficiency of analyzing whole-brain C. elegans imaging. The resulting large data sets may motivate new scientific discovery by exploiting many promising analysis tools for big data. In this study, we focus on the development of an automated annotation procedure. With only around 180 neurons in the central nervous system of a C. elegans, the annotation of each individual neuron still remains a major challenge because of the high density in space, similarity in neuron shape, unpredictable distortion of the worm's head during motion, intrinsic variations during worm development, etc. We use an ensemble learning approach to achieve around 25% error for a test based on real experimental data. Also, we demonstrate the importance of exploring extra source of information for annotation other than the neuron positions."}, {"title": "GPU-accelerated alignment of bisulfite-treated short-read sequences", "url": "https://www.biorxiv.org/content/early/2017/08/25/175729", "tag": "Bioinformatics", "abstract": "The alignment of bisulfite-treated DNA sequences (BS-seq reads) to a large genome involves a significant computational burden beyond that required to align non-bisulfite-treated reads. In the analysis of BS-seq data, this can present an important performance bottleneck that can potentially be addressed by appropriate software-engineering and algorithmic improvements. One strategy is to integrate this additional programming logic into the read-alignment implementation in a way that the software becomes amenable to optimizations that lead to both higher speed and greater sensitivity than can be achieved without this integration. We have evaluated this strategy using Arioc, a short-read aligner that uses GPU (general-purpose graphics processing unit) hardware to accelerate computationally-expensive programming logic. We integrated the BS-seq computational logic into both GPU and CPU code throughout the Arioc implementation. We then carried out a read-by-read comparison of Arioc's reported alignments with the alignments reported by the most widely used BS-seq read aligners. With simulated reads, Arioc's accuracy is equal to or better than the other read aligners we evaluated. With human sequencing reads, Arioc's throughput is at least 10 times faster than existing BS-seq aligners across a wide range of sensitivity settings. The Arioc software is available at https://github.com/RWilton/Arioc. It is released under a BSD open-source license."}, {"title": "Towards selective-alignment: Bridging the accuracy gap between alignment-based and alignment-free transcript quantification", "url": "https://www.biorxiv.org/content/early/2017/08/24/138800", "tag": "Bioinformatics", "abstract": "Motivation: We introduce an algorithm for selectively aligning high-throughput sequencing reads to a transcriptome, with the goal of improving transcript-level quantification. This algorithm attempts to bridge the gap between fast \"mapping\" algorithms and more traditional alignment procedures. Results: We adopt a hybrid approach that is able to increase mapping accuracy while still retaining much of the efficiency of fast mapping algorithms. To achieve this, we introduce a new approach that explores the candidate search space with high sensitivity as well as a collection of carefully-engineered heuristics to efficiently filter these candidates. Additionally, unlike the strategies adopted in most aligners which first align the ends of paired-end reads independently, we introduce a notion of co-mapping. This procedure exploits relevant information between the \"hits\" from the left and right ends of paired-end reads before full alignments or mappings for each are generated, which improves the efficiency of filtering likely-spurious alignments. Finally, we demonstrate the utility of selective alignment in improving the accuracy of efficient transcript-level quantification from RNA-seq reads. Specifically, we show that selective-alignment is able to resolve certain complex mapping scenarios that can confound existing fast mapping procedures, while simultaneously eliminating spurious alignments that fast mapping approaches can produce. Availability: Selective-alignment is implemented in C++11 as a part of Salmon, and is available as open source software, under GPL v3, at: https://github.com/COMBINE-lab/salmon/tree/selective-alignment."}, {"title": "SOMSC: Self-Organization-Map for High-Dimensional Single-Cell Data of Cellular States and Their Transitions", "url": "https://www.biorxiv.org/content/early/2017/08/24/124693", "tag": "Bioinformatics", "abstract": "Measurement of gene expression levels for multiple genes in single cells provides a powerful approach to study heterogeneity of cell populations and cellular plasticity. While the expression levels of multiple genes in each cell are available in such data, the potential connections among the cells (e.g. the cellular state transition relationship) are not directly evident from the measurement. Classifying the cellular states, identifying their transitions among those states, and extracting the pseudotime ordering of cells are challenging due to the noise in the data and the high-dimensionality in the number of genes in the data. In this paper we adapt the classical self-organizing-map (SOM) approach for single-cell gene expression data (SOMSC), such as those based on single cell qPCR and single cell RNA-seq. In SOMSC, a cellular state map (CSM) is derived and employed to identify cellular states inherited in the population of the measured single cells. Cells located in the same basin of the CSM are considered as in one cellular state while barriers among the basins in CSM provide information on transitions among the cellular states. A cellular state transitions path (e.g. differentiation) and a temporal ordering of the measured single cells are consequently obtained. In addition, SOMSC could estimate the cellular state replication probability and transition probabilities. Applied to a set of synthetic data, one single-cell qPCR data set on mouse early embryonic development and two single-cell RNA-seq data sets, SOMSC shows effectiveness in capturing cellular states and their transitions presented in the high-dimensional single-cell data. This approach will have broader applications to analyzing cellular fate specification and cell lineages using single cell gene expression data."}, {"title": "npInv: accurate detection and genotyping of inversions mediated by non-allelic homologous recombination using long read sub-alignment", "url": "https://www.biorxiv.org/content/early/2017/08/23/178103", "tag": "Bioinformatics", "abstract": "Detection of genomic inversions remains challenging. Many existing methods primarily target inversions with a non repetitive breakpoint, leaving inverted repeat (IR) mediated non-allelic homologous recombination (NAHR) inversions largely unexplored. We present npInv, a novel tool specifically for detecting and genotyping NAHR inversion using long read sub-alignment of long read sequencing data. We use npInv to generate a whole-genome inversion map for NA12878 consisting of 30 NAHR inversions (of which 15 are novel), including all previously known NAHR mediated inversions in NA12878 with flanking IR less than 7kb. Our genotyping accuracy on this dataset was 94%. We used PCR to confirm presence of two of these novel NAHR inversions. We show that there is a near linear relationship between the length of flanking IR and the size of the NAHR inversion."}, {"title": "A risk stratification model for lung cancer based on gene coexpression network", "url": "https://www.biorxiv.org/content/early/2017/08/23/179770", "tag": "Bioinformatics", "abstract": "Risk stratification model for lung cancer with gene expression profile is of great interest. Instead of the previously reported models based on individual prognostic genes, we aimed to develop a novel system-level risk stratification model for lung adenocarcinoma based on gene coexpression network. Using multiple microarray datasets obtained from lung adenocarcinoma, gene coexpression network analysis was performed to identify survival-related network modules. Representative genes of these network modules were selected and then, risk stratification model was constructed exploiting deep learning algorithm. The model was validated in two independent test cohorts. Survival analysis using univariate and multivariate Cox regression was performed using the output of the model to evaluate whether the model could predict patients\u2032 overall survival independent of clinicopathological variables. Five network modules were significantly associated with patients' survival. Considering prognostic significance and representativeness, genes of the two survival-related modules were selected for input data of the risk stratification model. The output of the model was significantly associated with patients' overall survival in the two independent test sets as well as training set (p < 0.00001, p < 0.0001 and p = 0.02 for training set, test set 1 and 2, respectively). In multivariate analyses, the model was associated with patients' prognosis independent of other clinical and pathological features. Our study presents a new perspective on incorporating gene coexpression networks into the gene expression signature, and the clinical application of deep learning in genomic data science for prognosis prediction."}, {"title": "PathCORE: identifying and visualizing globally co-occurring pathways in large transcriptomic compendia", "url": "https://www.biorxiv.org/content/early/2017/08/22/147645", "tag": "Bioinformatics", "abstract": "Background: Investigators often interpret genome-wide data by analyzing the expression levels of genes within pathways. While this within-pathway analysis is routine, the products of any one pathway can affect the activity of other pathways. Past efforts to identify relationships between biological processes have evaluated overlap in knowledge bases or evaluated changes that occur after specific treatments. Individual experiments can highlight condition-specific pathway-pathway relationships; however, constructing a complete network of such relationships across many conditions requires analyzing results from many studies. Results: We developed the PathCORE software to identify global pathway-pathway relationships, i.e. those evident across a broad data compendium. PathCORE starts with the results of robust feature construction algorithms, which are now being developed and applied to transcriptomic data. PathCORE identifies pathways grouped together in features more than expected by chance as functionally co-occurring. We performed example analyses using PathCORE for a microbial compendium for which eADAGE features were already available and a TCGA dataset of 33 cancer types that we analyzed via NMF. PathCORE recapitulated previously described pathway-pathway relationships and suggested additional relationships with biological plausibility that remain to be explored. The software also identifies genes associated with each relationship and includes demonstration source code for a web interface that users can modify to (1) visualize the network produced by applying the PathCORE software to their specific inputs and (2) review the expression levels of associated genes in the original data. The web interface can help biologists design experiments to test the relationships that were identified using PathCORE. Conclusions: PathCORE is a hypothesis generation tool that identifies co-occurring pathways from the results of unsupervised analysis of the growing body of gene expression data and visualizes the relationships between pathways as a network. Software that steps beyond within-pathway relationships to between-pathway relationships can reveal levels of organization that have been less frequently considered."}, {"title": "Low rate of index hopping on the Illumina HiSeq X platform", "url": "https://www.biorxiv.org/content/early/2017/08/22/179028", "tag": "Bioinformatics", "abstract": "The high throughput capacities of the Illumina sequencing platforms and the possibility to label samples with unique identifiers has encouraged a wide use of sample multiplexing. However, this practice results in low rates of read misassignment (<1%) across samples sequenced on the same lane on all Illumina sequencing platforms that rely on the traditional bridge amplification. Alarmingly high rates of read misassignment of up to 10% were recently reported for the newest Illumina machines (HiSeq X and HiSeq 4000). This potentially calls into question previously generated and published results and may make future use of these platforms prohibitive for many applications in biology and medicine. In this study we rely on inline barcodes that are ligated to both ends of the DNA insert, to directly quantify the amount of index hopping in historical museum-preserved samples. As the barcodes become part of the sequencing read, they allow us to reliably infer the read origin even in the presence of index hopping. After sequencing the same pooled library of seven samples on three independent HiSeq X lanes and accounting for multiple possible sources of error, including barcode and index cross-contamination, we identified on average only 0.470% hopped reads. We conclude that index hopping happens on the newest generation of Illumina sequencing platforms, but results in a similar rate of read missagnment as reported for older Illumina machines. We nonetheless recommend using inline barcodes in multiplexing studies that rely on low-coverage data, require absolute certainty and/or aim to characterize rare variants."}, {"title": "MRIQC: Advancing the Automatic Prediction of Image Quality in MRI from Unseen Sites", "url": "https://www.biorxiv.org/content/early/2017/08/22/111294", "tag": "Bioinformatics", "abstract": "Quality control of MRI is essential for excluding problematic acquisitions and avoiding bias in subsequent image processing and analysis. Visual inspection is subjective and impractical for large scale datasets. Although automated quality assessments have been demonstrated on single-site datasets, it is unclear that solutions can generalize to unseen data acquired at new sites. Here, we introduce the MRI Quality Control tool (MRIQC), a tool for extracting quality measures and fitting a binary (accept/exclude) classifier. Our tool can be run both locally and as a free online service via the OpenNeuro.org portal. The classifier is trained on a publicly available, multi-site dataset (17 sites, N=1102). We perform model selection evaluating different normalization and feature exclusion approaches aimed at maximizing across-site generalization and estimate an accuracy of 76%\u00b113% on new sites, using leave-one-site-out cross-validation. We confirm that result on a held-out dataset (2 sites, N=265) also obtaining a 76% accuracy. Even though the performance of the trained classifier is statistically above chance, we show that it is susceptible to site effects and unable to account for artifacts specific to new sites. MRIQC performs with high accuracy in intra-site prediction, but performance on unseen sites leaves space for improvement which might require more labeled data and new approaches to the between-site variability. Overcoming these limitations is crucial for a more objective quality assessment of neuroimaging data, and to enable the analysis of extremely large and multi-site samples."}, {"title": "Best practices for genome-wide RNA structure analysis: combination of mutational profiles and drop-off information", "url": "https://www.biorxiv.org/content/early/2017/08/21/176883", "tag": "Bioinformatics", "abstract": "Genome-wide RNA structure maps have recently become available through the coupling of in vivo chemical probing reagents with next-generation sequencing. Initial analyses relied on the identification of truncated reverse transcription reads to identify the chemically modified nucleotides, but recent studies have shown that mutational signatures can also be used. While these two methods have been employed interchangeably, here we show that they actually provide complementary information. Consequently, analyses using exclusively one of the two methodologies may disregard a significant portion of the structural information. We find that the identity and sequence environment of the modified nucleotide greatly affects the odds of introducing a mismatch or causing reverse transcriptase drop-off. Finally, we identify specific mismatch signatures generated by dimethyl sulfate probing that can be used to remove false positives typically produced in RNA structurome analyses, and how these signatures vary depending on the reverse transcription enzyme used."}, {"title": "Genetic and real-world clinical data, combined with empirical validation, nominate JAK-STAT signalling as a target for Alzheimer's Disease therapeutic development", "url": "https://www.biorxiv.org/content/early/2017/08/21/179267", "tag": "Bioinformatics", "abstract": "As Genome Wide Association Studies (GWAS) have grown in size, the number of genetic variants that have been nominated for an increasing number of diseases has correspondingly increased. Despite this increase in the number of associated SNPs per disease, their biological interpretation has in many cases remained elusive. To address this, we have combined GWAS results with an orthogonal source of evidence, namely real-world, routinely collected clinical data from more than 6 million patients in order to drive target nomination. First we show that when examined at a pathway level, analysis of all GWAS studies groups Alzheimer's disease (AD) in a cluster with disorders of immunity and inflammation. Using clinical data we show that the degree of comorbidity of these diseases with AD correlates with the strength of their genetic association with molecular participants in the JAK-STAT pathway. Using four independent open-science datasets we then find evidence for altered regulation of JAK-STAT pathway genes in AD. Finally, we use both in vitro and in vivo rodent models to demonstrate that A-beta induces gene expression of key drivers of this pathway, providing experimental evidence validating these data-driven observations. These results therefore nominate JAK-STAT anomalies as a prominent aetiopathological event in AD and hence potential target for therapeutic development, and moreover demonstrate a de-novo multi-modal approach to derive information from rapidly increasing genomic datasets."}, {"title": "Identification of copy number variations and translocations in cancer cells from Hi-C data", "url": "https://www.biorxiv.org/content/early/2017/08/21/179275", "tag": "Bioinformatics", "abstract": "Motivation: Eukaryotic chromosomes adapt a complex and highly dynamic three-dimensional (3D) structure, which profoundly affects different cellular functions and outcomes including changes in epigenetic landscape and in gene expression. Making the scenario even more complex, cancer cells harbor chromosomal abnormalities (e.g., copy number variations (CNVs) and translocations) altering their genomes both at the sequence level and at the level of 3D organization. High-throughput chromosome conformation capture techniques (e.g., Hi-C), which are originally developed for decoding the 3D structure of the chromatin, provide a great opportunity to simultaneously identify the locations of genomic rearrangements and to investigate the 3D genome organization in cancer cells. Even though Hi-C data has been used for validating known rearrangements, computational methods that can distinguish rearrangement signals from the inherent biases of Hi-C data and from the actual 3D conformation of chromatin, and can precisely detect rearrangement locations de novo have been missing. Results: In this work, we characterize how intra and inter-chromosomal Hi-C contacts are distributed for normal and rearranged chromosomes to devise a new set of algorithms (i) to identify genomic segments that correspond to CNV regions such as amplifications and deletions (HiCnv), (ii) to call inter-chromosomal translocations and their boundaries (HiCtrans) from Hi-C experiments, and (iii) to simulate Hi-C data from genomes with desired rearrangements and abnormalities (AveSim) in order to select optimal parameters for and to benchmark the accuracy of our methods. Our results on 10 different cancer cell lines with Hi-C data show that we identify a total number of 105 amplifications and 45 deletions together with 90 translocations, whereas we identify virtually no such events for two karyotypically normal cell lines. Our CNV predictions correlate very well with whole genome sequencing (WGS) data among chromosomes with CNV events for a breast cancer cell line (r=0.89) and capture most of the CNVs we simulate using Avesim. For HiCtrans predictions, we report evidence from the literature for 30 out of 90 translocations for eight of our cancer cell lines. Furthermore, we show that our tools identify and correctly classify relatively understudied rearrangements such as double minutes (DMs) and homogeneously staining regions (HSRs). Conclusions: Considering the inherent limitations of existing techniques for karyotyping (i.e., missing balanced rearrangements and those near repetitive regions), the accurate identification of CNVs and translocations in a cost-effective and high-throughput setting is still a challenge. Our results show that the set of tools we develop effectively utilize moderately sequenced Hi-C libraries (100-300 million reads) to identify known and de novo chromosomal rearrangements/abnormalities in well-established cancer cell lines. With the decrease in required number of cells and the increase in attainable resolution, we believe that our framework will pave the way towards comprehensive mapping of genomic rearrangements in primary cells from cancer patients using Hi-C."}, {"title": "Building a genome browser with GIVE", "url": "https://www.biorxiv.org/content/early/2017/08/21/177832", "tag": "Bioinformatics", "abstract": "Genome browsers that manage, communicate and visualize data have greatly facilitated genomics research. Here, we present an open-source programming library, called GIVE library, that allows anyone with HTML programing experience to build custom genome browser websites or apps. With a few lines of codes, one can add to a personal webpage an interactive genome browser that host custom data. This portable library encapsulates novel data communication and data visualization technologies, including new data structures and new memory management methods, that enable efficient data transfer between the data-hosting website and internet browsers."}, {"title": "LOGAN: A framework for LOssless Graph-based ANalysis of high throughput sequence data", "url": "https://www.biorxiv.org/content/early/2017/08/21/175976", "tag": "Bioinformatics", "abstract": "Recent massive growth in the production of sequencing data necessitates matching improvements in bioinformatics tools to effectively utilize it. Existing tools suffer from limitations in both scalability and applicability which are inherent to their underlying algorithms and data structures. We identify the key requirements for the ideal data structure for sequence analyses: it should be informationally lossless, locally updatable, and memory efficient; requirements which are not met by data structures underlying the major assembly strategies Overlap Layout Consensus and De Bruijn Graphs. We therefore propose a new data structure, the LOGAN graph, which is based on a memory efficient Sparse De Bruijn Graph with routing information. Innovations in storing routing information and careful implementation allow sequence datasets for Escherichia coli (4.6Mbp, 117x coverage), Arabidopsis thaliana (135Mbp, 17.5x coverage) and Solanum pennellii (1.2Gbp, 47x coverage) to be loaded into memory on a desktop computer in seconds, minutes, and hours respectively. Memory consumption is competitive with state of the art alternatives, while losslessly representing the reads in an indexed and updatable form. Both Second and Third Generation Sequencing reads are supported. Thus, the LOGAN graph is positioned to be the backbone for major breakthroughs in sequence analysis such as integrated hybrid assembly, assembly of exceptionally large and repetitive genomes, as well as assembly and representation of pan-genomes."}, {"title": "SQANTI: extensive characterization of long read transcript sequences for quality control in full-length transcriptome identification and quantification", "url": "https://www.biorxiv.org/content/early/2017/08/21/118083", "tag": "Bioinformatics", "abstract": "High-throughput sequencing of full-length transcripts using long reads has paved the way for the discovery of thousands of novel transcripts, even in very well annotated organisms as mice and humans. Nonetheless, there is a need for studies and tools that characterize these novel isoforms. Here we present SQANTI, an automated pipeline for the classification of long-read transcripts that computes 47 descriptors that can be used to assess the quality of the data and of the preprocessing pipelines. We applied SQANTI to a neuronal mouse transcriptome using PacBio long reads and illustrate how the tool is effective in readily describing the composition of and characterizing the full-length transcriptome. We perform extensive evaluation of ToFU PacBio transcripts by PCR to reveal that an important number of the novel transcripts are technical artifacts of the sequencing approach, and that SQANTI quality descriptors can be used to engineer a filtering strategy to remove them. Most novel transcripts in this curated transcriptome are novel combinations of existing splice sites, result more frequently in novel ORFs than novel UTRs and are enriched in both general metabolic and neural specific functions. We show that these new transcripts have a major impact in the correct quantification of transcript levels by state-of-the-art short-read based quantification algorithms. By comparing our iso-transcriptome with public proteomics databases we find that alternative isoforms are elusive to proteogenomics detection and are variable in protein changes with respect to the principal isoform of their genes. SQANTI allows the user to maximize the analytical outcome of long read technologies by providing the tools to deliver quality-evaluated and curated full-length transcriptomes. SQANTI is available at https://bitbucket.org/ConesaLab/sqanti."}, {"title": "Accurate typing of class I human leukocyte antigen by Oxford nanopore sequencing", "url": "https://www.biorxiv.org/content/early/2017/08/20/178590", "tag": "Bioinformatics", "abstract": "Oxford Nanopore Technologies' MinION has expanded the current DNA sequencing toolkit by delivering long read lengths and extreme portability. The MinION has the potential to enable expedited point-of-care human leukocyte antigen (HLA) typing, an assay routinely used to assess the immunological compatibility between organ donors and recipients, but the platform's high error rate makes it challenging to type alleles with clinical-grade accuracy. Here, we developed and validated Athlon, an algorithm that iteratively scores nanopore reads mapped to a hierarchical database of HLA alleles to arrive at a consensus diploid genotype; Athlon achieved a 100% accuracy in class I HLA typing at high resolution."}, {"title": "DrugPattern: a web-based tool for drug set enrichment analysis", "url": "https://www.biorxiv.org/content/early/2017/08/20/178632", "tag": "Bioinformatics", "abstract": "Set enrichment analysis based methods (e.g. gene set enrichment analysis) have provided great helps in mining patterns in biomedical datasets, however, tools for inferring regular patterns in drug-related datasets are still limited. For the above purpose, here we developed a web-based tool, DrugPattern. DrugPattern first collected and curated 7019 drug sets, including indications, adverse reaction, targets, pathways etc. For a list of interested drugs, DrugPattern then evaluates the significance of the enrichment of these drugs in each of the 7019 drug sets. To validate DrugPattern, we applied it to predict the potential protective roles of oxidized low-density lipoprotein (oxLDL), a widely accepted deleterious factor for the body. We predicted that oxLDL has beneficial effects on some diseases, most of which were supported by literature except type 2 diabetes (T2D), in which oxLDL was previously believed to be a risk factor. Animal experiments further validated that oxLDL indeed has beneficial effects on T2D. These data confirmed the prediction accuracy of our approach and revealed unexpected protective roles for oxLDL in various diseases including T2D. This study provides a tool to infer regular patterns in biomedical datasets based on drug set enrichment analysis."}, {"title": "ActiveDriverDB: human disease mutations and genome variation in post-translational modification sites of proteins", "url": "https://www.biorxiv.org/content/early/2017/08/20/178392", "tag": "Bioinformatics", "abstract": "Interpretation of genetic variation is required for understanding genotype-phenotype associations, mechanisms of inherited disease, and drivers of cancer. Millions of single nucleotide variants (SNVs) in human genomes are known and thousands are associated with disease. An estimated 20% of disease-associated missense SNVs are located in protein sites of post-translational modifications (PTMs), chemical modifications of amino acids that extend protein function. ActiveDriverDB is a comprehensive human proteo-genomics database that annotates disease mutations and population variants using PTMs. We integrated >385,000 published PTM sites with ~3.8 million missense SNVs from The Cancer Genome Atlas (TCGA), the ClinVar database of disease genes, and inter-individual variation from human genome sequencing projects. The database includes interaction networks of proteins, upstream enzymes such as kinases, and drugs targeting these enzymes. We also predicted network-rewiring impact of mutations by analyzing gains and losses of kinase-bound sequence motifs. ActiveDriverDB provides detailed visualization, filtering, browsing and searching options for studying PTM-associated SNVs. Users can upload mutation datasets interactively and use our application programming interface for pipelines. Integrative analysis of SNVs and PTMs helps decipher molecular mechanisms of phenotypes and disease, as exemplified by case studies of disease genes TP53, BRCA2 and VHL. The open-source database is available at https://www.ActiveDriverDB.org."}, {"title": "Facilitated analysis of large data sets by interactive visualisation", "url": "https://www.biorxiv.org/content/early/2017/08/20/178616", "tag": "Bioinformatics", "abstract": "In biological research analysis of large data sets, such as RNA-seq gene expression, often involves visualisation of thousands of data points and associated database query. Static charts produced by traditional tools lack the ability to reveal underlying information, and separated database query is laborious and involves a lot of manual effort. Interactive charting is able to make the data transparent but the use of visualisation tools often requires certain programming skills, which hinders most academic users. We present here an open-source chart editor for interactive visualisation, which is designed for academic users with no programming experience. It can not only visualise the data in an interactive way, but also link the data points to external databases, by which the user can save a lot of manual effort. We believe that interactive visualisation using such tools will facilitate analysis of large data sets as well as presenting and interpreting the data."}, {"title": "Pathogen-Host Analysis Tool (PHAT): an Integrative Platform to Analyze Pathogen-Host Relationships in Next-Generation Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/08/18/178327", "tag": "Bioinformatics", "abstract": "Summary: The Pathogen-Host Analysis Tool (PHAT) is an application for processing and analyzing next-generation sequencing (NGS) data as it relates to relationships between pathogen and host organisms. Unlike custom scripts and tedious pipeline programming, PHAT provides an integrative platform encompassing sequence and reference file input, quality control (QC) reporting, alignment and variant calling, linear and circular alignment viewing, and graphical and tabular output. This novel tool aims to be user-friendly for biologists studying diverse pathogen-host relationships Availability and Implementation: The project is publicly available on GitHub (https://github.com/chgibb/PHAT) and includes convenient installers, as well as portable and source versions, for both Windows and Linux (Debian and RedHat). Up-to-date documentation for PHAT, including user guides and development notes, can be found at https://chgibb.github.io/PHATDocs/. We encourage users and developers to provide feedback (error reporting, suggestions, and comments) using GitHub Issues."}, {"title": "Resources For Interpreting Variants In Precision Genomic Oncology Applications", "url": "https://www.biorxiv.org/content/early/2017/08/18/144766", "tag": "Bioinformatics", "abstract": "Precision genomic oncology -- applying high throughput sequencing (HTS) at the point-of-care to inform clinical decisions -- is a developing precision medicine paradigm that is seeing increasing adoption. Simultaneously, new developments in targeted agents and immunotherapy, when informed by rich genomic characterization, offer potential benefit to a growing subset of patients. Multiple previous studies have commented on methods for identifying both germline and somatic variants. However, interpreting individual variants remains a significant challenge, relying in large part on the integration of observed variants with biological knowledge. A number of data and software resources have been developed to assist in interpreting observed variants, determining their potential clinical actionability, and augmenting them with ancillary information that can inform clinical decisions and even generate new hypotheses for exploration in the laboratory. Here, we review available variant catalogs, variant and functional annotation software and tools, and databases of clinically actionable variants that can be used in an ad hoc approach with research samples or incorporated into a data platform for interpreting and formally reporting clinical results."}, {"title": "Comparative analyses of toxin-associated gene homologs from an Old World viper, Daboia russelii.", "url": "https://www.biorxiv.org/content/early/2017/08/18/152082", "tag": "Bioinformatics", "abstract": "Availability of snake genome sequences has opened up exciting areas of research on comparative genomics and gene diversity. One of the challenges in studying snake genomes is the acquisition of biological material from live animals, especially from the venomous ones. Additionally, in certain countries, Government permission is required to handle live snakes, making the process cumbersome and time-consuming. Here, we report comparative sequence analyses of toxin gene homologs from Russell's viper (Daboia russelii) using whole-genome sequencing data obtained from the shed skin. When compared with the major venom proteins in Russell's viper studied previously, we found 45-100% sequence similarity between the venom proteins and their skin homologs. Additionally, comparative analyses of 20 toxin gene family homologs provided evidence of unique sequence motifs in nerve growth factor (NGF), platelet derived growth factor (PDGF), Kunitz/Bovine pancreatic trypsin inhibitor (Kunitz BPTI), cysteine-rich secretory proteins, antigen 5, and pathogenesis-related 1 proteins (CAP) and cysteine-rich secretory protein (CRISP). We identified V11 and T35 in the NGF domain; F23 and A29 in the PDGF domain; N69, K2 and A5 in the CAP domain; and Q17 in the CRISP domain to be responsible for differences in the largest pockets across the protein domain structures in crotalines, viperines and elapids from the in silico structure-based analysis. Similarly, residues F10, Y11 and E20 appear to play an important role in the protein structures across the kunitz protein domain of viperids and elapids. Our study sheds light on the usefulness of studying venom protein homologs from skin, their unique features and evolution in vipers. Data deposition: Russell's viper sequence data is deposited in the NCBI SRA database under the accession number SRR5506741 and sequences for the individual venom-associated gene homologs to GenBank (accession numbers in Table S1)."}, {"title": "Uniform Resolution of Compact Identifiers for Biomedical Data", "url": "https://www.biorxiv.org/content/early/2017/08/18/101279", "tag": "Bioinformatics", "abstract": "Compact identifiers have been widely used in biomedical informatics both formally and informally. They consist of two parts: 1) a unique prefix or namespace indicating the assigning authority and 2) a locally assigned database identifier sometimes called an accession number. The former is used to avoid global identifier collisions when integrating separately managed datasets that are run by different communities and consortia under a variety of autonomous data management systems and practices. This bi-partite identifier approach predates the invention of the Web, but can be leveraged to work more harmoniously with it. Identifiers.org and N2T.net are two meta-resolvers that take any given identifier from over 500 source databases and reliably redirect it to its original source on the Web. Identifiers.org is based at the European Molecular Biology Laboratory-European Bioinformatics Institute (EMBL-EBI) and serves the biomedical domain; whereas N2T.net (Name-to-Thing) is based at the California Digital Library (CDL), University of California Office of the President, and is domain-agnostic. Both resolvers, while derived from independently developed code bases, with different features and objectives, can now uniformly resolve compact identifiers using a set of common procedures and redirection rules. Here we report on significant further work by our teams toward a more unified approach to making compact identifiers available for long-term use in an ecosystem supporting formal citation of primary scholarly research data. This approach is intended to be robust beyond the operational and funding scope of any one organization, enabling long-term resolution of persistent archived data, whether it is cited in the literature, or is referenced in the web at large. We demonstrate that multiple resolvers with fundamentally different underlying code bases, organizational settings and international alignments, can readily support this approach. As part of this project we have deployed public, production-quality resolvers using a common registry of prefix-based redirection rules. We believe these products and our approach will be of significant help to publishers, authors and others implementing persistent, machine-resolvable citation of research data in compliance with emerging science policy recommendations and funder requirements."}, {"title": "CarrierSeq: a sequence analysis workflow for low-input nanopore sequencing", "url": "https://www.biorxiv.org/content/early/2017/08/18/175281", "tag": "Bioinformatics", "abstract": "Motivation: Long-read nanopore sequencing technology is of particular significance for taxonomic identification at or below the species level. For many environmental samples, the total extractable DNA is far below the current input requirements of nanopore sequencing, preventing \"sample to sequence\" metagenomics from low-biomass or recalcitrant samples. Results: Here we address this problem by employing carrier sequencing, a method to sequence low-input DNA by preparing the target DNA with a genomic carrier to achieve ideal library preparation and sequencing stoichiometry without amplification. We then use CarrierSeq, a sequence analysis workflow to identify the low-input target reads from the genomic carrier. We tested CarrierSeq experimentally by sequencing from a combination of 0.2 ng Bacillus subtilis ATCC 6633 DNA in a background of 1 \u03bcg Enterobacteria phage DNA. After filtering of carrier, low quality, and low complexity reads, we detected target reads (B. subtilis), contamination reads, and \"high quality noise reads\" (HQNRs) not mapping to the carrier, target or known lab contaminants. These reads appear to be artifacts of the nanopore sequencing process as they are associated with specific channels (pores). By treating reads as a Poisson arrival process, we implement a statistical test to reject data from channels dominated by HQNRs while retaining target reads. Availability: CarrierSeq is an open-source bash script with supporting python scripts which leverage a variety of bioinformatics software packages on macOS and Ubuntu. Supplemental documentation is available from Github - https://github.com/amojarro/carrierseq. In addition, we have complied all required dependencies in a Docker image available from - https://hub.docker.com/r/mojarro/carrierseq."}, {"title": "CHIC: a short read aligner for pan-genomic references", "url": "https://www.biorxiv.org/content/early/2017/08/18/178129", "tag": "Bioinformatics", "abstract": "Recently the topic of computational pan-genomics has gained increasing attention, and particularly the problem of moving from a single-reference paradigm to a pan-genomic one. Perhaps the simplest way to represent a pan-genome is to represent it as a set of sequences. While indexing highly repetitive collections has been intensively studied in the computer science community, the research has focused on efficient indexing and exact pattern patching, making most solutions not yet suitable to be used in bioinformatic analysis pipelines. Results: We present CHIC, a short-read aligner that indexes very large and repetitive references using a hybrid technique that combines Lempel-Ziv compression with Burrows-Wheeler read aligners. Availability: Our tool is open source and available online at https://gitlab.com/dvalenzu/CHIC."}, {"title": "SFMetaDB: A Comprehensive Annotation of Mouse RNA Splicing Factor RNA-Seq Datasets", "url": "https://www.biorxiv.org/content/early/2017/08/18/177931", "tag": "Bioinformatics", "abstract": "Although the number of RNA-Seq datasets deposited publicly has increased over the past few years, incomplete annotation of the associated metadata limits their potential use. Because of the importance of RNA splicing in diseases and biological processes, we constructed a database called SFMetaDB by curating datasets related with RNA splicing factors. Our effort focused on the RNA-Seq datasets in which splicing factors were knocked-down, knocked-out or over-expressed, leading to 75 datasets corresponding to 56 splicing factors. These datasets can be used in differential alternative splicing analysis for the identification of the potential targets of these splicing factors and other functional studies. Surprisingly, only ~15% of all the splicing factors have been studied by loss- or gain-of-function experiments using RNA-Seq. In particular, splicing factors with domains from a few dominant Pfam domain families have not been studied. This suggests a significant gap that needs to be addressed to fully elucidate the splicing regulatory landscape. Indeed, there are already mouse models available for ~20 of the unstudied splicing factors, and it can be a fruitful research direction to study these splicing factors in vitro and in vivo using RNA-Seq."}, {"title": "Integrative analysis of large scale transcriptome data draws a comprehensive functional landscape of Phaeodactylum tricornutum genome and evolutionary origin of diatoms", "url": "https://www.biorxiv.org/content/early/2017/08/18/176024", "tag": "Bioinformatics", "abstract": "Diatoms are one of the most successful and ecologically important groups of eukaryotic phytoplankton in the modern ocean. Deciphering their genomes is a key step towards better understanding of their biological innovations, evolutionary origins, and ecological underpinnings. Here, we have used 90 RNA-Seq datasets from different growth conditions combined with published expressed sequence tags and protein sequences from multiple taxa to explore the genome of the model diatom Phaeodactylum tricornutum, and introduce 1,489 novel genes. The new annotation additionally permitted the discovery for the first time of extensive alternative splicing (AS) in diatoms, including intron retention and exon skipping which increases the diversity of transcripts to regulate gene expression in response to nutrient limitations. In addition, we have used up-to-date reference sequence libraries to dissect the taxonomic origins of diatom genomes. We show that the P. tricornutum genome is replete in lineage-specific genes, with up to 47% of the gene models present only possessing orthologues in other stramenopile groups. Finally, we have performed a comprehensive de novo annotation of repetitive elements showing novel classes of TEs such as SINE, MITE, LINE and TRIM/LARD. This work provides a solid foundation for future studies of diatom gene function, evolution and ecology."}, {"title": "GIC: A computational method for predicting the essentiality of long noncoding lncRNAs", "url": "https://www.biorxiv.org/content/early/2017/08/18/177923", "tag": "Bioinformatics", "abstract": "Measuring the essentiality of genes is critically important in biology and medicine. Some bioinformatic methods have been developed for this issue but none of them can be applied to long noncoding RNAs (lncRNAs), one big class of biological molecules. Here we developed a computational method, GIC (Gene Importance Calculator), which can predict the essentiality of both protein-coding genes and lncRNAs based on RNA sequence information. For identifying the essentiality of protein-coding genes, GIC is competitive with well-established computational scores. More important, GIC showed a high performance for predicting the essentiality of lncRNAs. In an independent mouse lncRNA dataset, GIC achieved an exciting performance (AUC=0.918). In contrast, the traditional computational methods are not applicable to lncRNAs. As a public web server, GIC is freely available at http://www.cuilab.cn/gic/."}, {"title": "TCGAbiolinksGUI: A graphical user interface to analyze GDC cancer molecular and clinical data", "url": "https://www.biorxiv.org/content/early/2017/08/17/147496", "tag": "Bioinformatics", "abstract": "Background: The GDC (Genomic Data Commons) data portal provides users with data from cancer genomics studies. Recently, we developed the R/Bioconductor TCGAbiolinks package, which allows users to search, download and prepare cancer genomics data for integrative data analysis. The use of this package requires users to have advanced knowledge of R thus limiting the number of users. Results: To overcome this obstacle and improve the accessibility of the package by a wider range of users, we developed TCGAbiolinksGUI that uses shiny graphical user interface (GUI) available through the R/Bioconductor package. Conclusion: The TCGAbiolinksGUI package is freely available within the Bioconductor project at http://bioconductor.org/packages/TCGAbiolinksGUI/. Links to the GitHub repository, a demo version of the tool, a docker image and PDF/video tutorials are available at http://bit.do/TCGAbiolinksDocs."}, {"title": "PBxplore: A Tool To Analyze Local Protein Structure And Deformability With Protein Blocks", "url": "https://www.biorxiv.org/content/early/2017/08/17/136408", "tag": "Bioinformatics", "abstract": "Proteins are highly dynamic macromolecules. A classical way to analyze their inner flexibility is to perform molecular dynamics simulations. In this context, we present the advantage to use small structural prototypes, namely the Protein Blocks (PBs). PBs give a good approximation of the local structure of the protein backbone. More importantly, by reducing the conformational complexity of protein structures, they allow analyzes of local protein deformability which cannot be done with other methods and had been used efficiently in different applications. PBxplore is a suite of tools to analyze the dynamics and deformability of protein structures using PBs. It is able to process large amount of data such as those produced by molecular dynamics simulations. It produces various outputs with text and graphics, such as frequencies, entropy and information logo. PBxplore is available at https://github.com/pierrepo/PBxplore and is released under the open-source MIT license"}, {"title": "Missing Value Imputation Approach for Mass Spectrometry-based Metabolomics Data", "url": "https://www.biorxiv.org/content/early/2017/08/17/171967", "tag": "Bioinformatics", "abstract": "Introduction: Missing values exist widely in mass-spectrometry (MS) based metabolomics data. Various methods have been applied for handling missing values, but the selection of methods can significantly affect following data analyses and interpretations. According to the definition, there are three types of missing values, missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). Objectives: The aim of this study was to comprehensively compare common imputation methods for different types of missing values using two separate metabolomics data sets (977 and 198 serum samples respectively) to propose a strategy to deal with missing values in metabolomics studies. Methods: Imputation methods included zero, half minimum (HM), mean, median, random forest (RF), singular value decomposition (SVD), k-nearest neighbors (kNN), and quantile regression imputation of left-censored data (QRILC). Normalized root mean squared error (NRMSE) and NRMSE-based sum of ranks (SOR) were applied to evaluate the imputation accuracy for MCAR/MAR and MNAR correspondingly. Principal component analysis (PCA)/partial least squares (PLS)-Procrustes sum of squared error were used to evaluate the overall sample distribution. Student's t-test followed by Pearson correlation analysis was conducted to evaluate the effect of imputation on univariate statistical analysis. Results: Our findings demonstrated that RF imputation performed the best for MCAR/MAR and QRILC was the favored one for MNAR. Conclusion Combining with \"modified 80% rule\", we proposed a comprehensive strategy and developed a public-accessible web-tool for missing value imputation in metabolomics data."}, {"title": "A Method for Quantifying Molecular Interactions Using Stochastic Modelling and Super-Resolution Microscopy", "url": "https://www.biorxiv.org/content/early/2017/08/17/177063", "tag": "Bioinformatics", "abstract": "We introduce the Interaction Factor (IF), a measure for quantifying the interaction of molecular clusters in super-resolution microscopy images. The IF is robust in the sense that it is independent of cluster density, and it only depends on the extent of the pair-wise interaction between different types of molecular clusters in the image. The IF for a single or a collection of images is estimated by first using stochastic modelling where the locations of clusters in the images are repeatedly randomized to estimate the distribution of the overlaps between the clusters in the absence of interaction (IF=0). Second, an analytical form of the relationship between IF and the overlap (which has the random overlap as its only parameter) is used to estimate the IF for the experimentally observed overlap. The advantage of IF compared to conventional methods to quantify interaction in microscopy images is that it is insensitive to changing cluster density and is an absolute measure of interaction, making the interpretation of experiments easier. We validate the IF method by using both simulated and experimental data and provide an ImageJ plugin for determining the IF of an image."}, {"title": "BUSCO applications from quality assessments to gene prediction and phylogenomics", "url": "https://www.biorxiv.org/content/early/2017/08/17/177485", "tag": "Bioinformatics", "abstract": "Genomics promises comprehensive surveying of genomes and metagenomes, but rapidly changing technologies and expanding data volumes make evaluation of completeness a challenging task. Technical sequencing quality metrics can be complemented by quantifying completeness in terms of the expected gene content of Benchmarking Universal Single-Copy Orthologs (BUSCO, http://busco.ezlab.org). Now in its third release, BUSCO utilities extend beyond quality control to applications in comparative genomics, gene predictor training, metagenomics, and phylogenomics."}, {"title": "A computational framework for detecting signatures of accelerated somatic evolution in cancer genomes", "url": "https://www.biorxiv.org/content/early/2017/08/16/177261", "tag": "Bioinformatics", "abstract": "By accumulation of somatic mutations, cancer genomes evolve, diverging away from the genome of the host. It remains unclear to what extent somatic evolutionary divergence is comparable across different regions of the cancer genome versus concentrated in specific genomic elements. We present a novel computational framework, SASE-mapper, to identify genomic regions that show signatures of accelerated somatic evolution (SASE) in a subset of samples in a cohort, marked by accumulation of an excess of somatic mutations compared to that expected based on local, context-aware background mutation rates in the cancer genomes. Analyzing tumor whole genome sequencing data for 365 samples from 5 cohorts we detect recurrent SASE at a genome-wide scale. The SASEs were enriched for genomic elements associated with active chromatin, and regulatory regions of several known cancer genes had SASE in multiple cohorts. Regions with SASE carried specific mutagenic signatures and often co-localized within the 3D nuclear space suggesting their common basis. A subset of SASEs was frequently associated with regulatory changes in key cancer pathways and also poor clinical outcome. While the SASE-associated mutations were not necessarily recurrent at base-pair resolution, the SASEs recurrently targeted same functional regions, with similar consequences. It is likely that regulatory redundancy and plasticity promote prevalence of SASE-like patterns in the cancer genomes."}, {"title": "ProphET, Prophage Estimation Tool: a standalone prophage sequence prediction tool with self-updating reference database", "url": "https://www.biorxiv.org/content/early/2017/08/16/176750", "tag": "Bioinformatics", "abstract": "Prophages are a significant force in prokaryote evolution. The remaining sequences of a bacteriophage integration event are known for altering gene expression, enabling creative destruction of the bacterial genome and to induce pathogenicity by harboring and transposing virulence and antibiotic resistance factors. In the light of the dreadful expansion of antibiotic resistance bacteriophages have gathered renewed interest from the scientific community and public health decision makers as a promising long forgotten alternative to control bacterial infections. Cataloging the repertoire of prophages and their integration sites is an important initial step in the understanding of bacteriophages either as tool or as a threat. In this work, we present ProphET (Prophage Estimation Tool), a standalone application without the limitations of their web based counterparts and which identifies prophages in bacterial genomes with higher precision than similar applications."}, {"title": "MentaLiST - A fast MLST caller for large MLST schemes", "url": "https://www.biorxiv.org/content/early/2017/08/16/172858", "tag": "Bioinformatics", "abstract": "MLST (multi-locus sequence typing) is a classic technique for genotyping bacteria, widely applied for pathogen outbreak surveillance. Traditionally, MLST is based on identifying sequence types from a set of a small number of housekeeping genes. With the increased availability of whole-genome sequencing (WGS) data, MLST methods have evolved toward larger typing schemes, based on a few hundred genes (core genome MLST, cgMLST) to a few thousand genes (whole genome MLST, wgMLST). Such large-scale MLST schemes have been shown to provide a finer resolution and are increasingly used in various contexts such as hospital outbreaks or foodborne pathogen outbreaks. This methodological shift raises new computational challenges, especially given the large size of the schemes involved. Very few available MLST callers are currently capable of dealing with large cgMLST and wgMLST schemes. We introduce MentaLiST, a new MLST caller, based on a k-mer counting algorithm and written in the Julia language, specifically designed and implemented to handle large typing schemes. We test it on real and simulated data to show that MentaLiST is faster than any other available MLST caller while providing the same or better accuracy, and is capable of dealing with MLST scheme with up to thousands of genes while requiring limited computational resources. MentaLiST source code and easy installation instructions using a Conda package are available at https://github.com/WGS-TB/MentaLiST."}, {"title": "Concept and in-silico assessment of an algorithm for monitoring cytosolic fluorescent aggregates in cells.", "url": "https://www.biorxiv.org/content/early/2017/08/16/177139", "tag": "Bioinformatics", "abstract": "Autophagy is an evolutionary conserved pathway, by which eukaryotic cells degrade long-living cellular proteins and intracellular organelles, to maintain a pool of available nutrients. Impaired autophagy has been associated to important pathophysiological conditions, and this is the reason why several techniques have been developed for its correct assessment and monitoring. Fluorescence microscopy is one of these tools, which relies on the detection of specific fluorescence changes of targeted GFP-based reporters in dot-like organelles in which autophagy is executed. Currently, several procedures exist to count and segment this punctate structures in the resulting fluorescence images, however, they are either based on subjective criteria, or no information is available related to them. Here we present the concept of an algorithm for a semi-automatic detection and segmentation in 2D fluorescence images of spot-like structures similar to those observed under induction of autophagy. By evaluating the algorithm on more than 20000 simulated images of cells containing a variable number of punctate structures of different sizes and different levels of applied noise, we demonstrate its high robustness of puncta detection, even on a high noise background. We further demonstrate this feature of our algorithm by testing it in experimental conditions of a high non-specific background signal. We conclude that our algorithm is a suitable tool to be tested in biologically-relevant contexts."}, {"title": "OligoMiner: A rapid, flexible environment for the design of genome-scale oligonucleotide in situ hybridization probes", "url": "https://www.biorxiv.org/content/early/2017/08/16/171504", "tag": "Bioinformatics", "abstract": "Oligonucleotide (oligo)-based fluorescence in situ hybridization (FISH) has emerged as an important tool for the study of chromosome organization and gene expression and has been empowered by the commercial availability of highly complex pools of oligos. However, a dedicated bioinformatic design utility has yet to be created specifically for the purpose of identifying optimal oligo FISH probe sequences on the genome-wide scale. Here, we introduce OligoMiner, a rapid and robust computational pipeline for the genome-scale design of oligo FISH probes that affords the scientist exact control over the parameters of each probe. Our streamlined method uses standard bioinformatic file formats, allowing users to seamlessly integrate existing and new utilities into the pipeline as desired, and introduces a novel method for evaluating the specificity of each probe molecule that connects simulated hybridization energetics to rapidly generated sequence alignments using supervised learning. We demonstrate the scalability of our approach by performing genome-scale probe discovery in numerous model organism genomes and showcase the performance of the resulting probes with both diffraction-limited and single-molecule super-resolution imaging of chromosomal and RNA targets. We anticipate this pipeline will make the FISH probe design process much more accessible and will more broadly facilitate the design of pools of hybridization probes for a variety of applications."}, {"title": "SiFit: A Method for Inferring Tumor Trees from Single-Cell Sequencing Data under Finite-site Models", "url": "https://www.biorxiv.org/content/early/2017/08/16/091595", "tag": "Bioinformatics", "abstract": "Tumor phylogenies provide insightful information on intra-tumor heterogeneity and evolutionary trajectories. Single-cell sequencing (SCS) enables the inference of tumor phylogenies and methods were recently introduced for this task under the infinite-sites assumption. Violations of this assumption, due to chromosomal deletions and loss of heterozygosity, necessitate the development of statistical inference methods that utilize finite-site models. We propose a statistical inference method for tumor phylogenies from noisy SCS data under a finite-sites model. We demonstrate the performance of our method on synthetic and biological data sets. Our results suggest that employing a finite-sites model leads to improved inference of tumor"}, {"title": "The antibody repertoire of colorectal cancer", "url": "https://www.biorxiv.org/content/early/2017/08/15/176768", "tag": "Bioinformatics", "abstract": "Immunotherapy is becoming increasingly important in the fight against cancers, utilizing and manipulating the body's immune response to treat tumors. Understanding the immune repertoire - the collection of immunological proteins - of treated and untreated cells is possible at the genomic, but technically difficult at the protein level. Standard protein databases do not include the highly divergent sequences of somatic rearranged immunoglobulin genes, and may lead to missed identifications in a mass spectrometry search. We introduce a novel proteogenomic approach, AbScan, to identify these highly variable antibody peptides, by developing a customized antibody database construction method using RNA-seq reads aligned to immunoglobulin (Ig) genes. AbScan starts by filtering transcript (RNA-seq) reads that match the template for Ig genes. The retained reads are used to construct a repertoire graph using the 'split' de Bruijn graph: a graph structure that improves upon the standard de Bruijn graph to capture the high diversity of Ig genes in a compact manner. AbScan corrects for sequencing errors, and converts the graph to a format suitable for searching with MS/MS search tools. We used AbScan to create an antibody database from 90 RNA-seq colorectal tumor samples. Next, we used proteogenomics analysis to search MS/MS spectra of matched colorectal samples from the Clinical Proteomic Tumor Analysis Consortium (CPTAC) against the AbScan generated database. AbScan identified 1,940 distinct antibody peptides. Correlating with previously identified Single Amino-Acid Variants (SAAVs) in the tumor samples, we identified 163 pairs (antibody peptide, SAAV) with significant co-occurrence pattern in the 90 samples. The presence of co-expressed antibody and mutated peptides was correlated with survival time of the individuals. Our results suggest that AbScan (https://github.com/csw407/AbScan.git) is an effective tool for a proteomic exploration of the immune response in cancers."}, {"title": "Large-scale determination and characterization of cell type-specific regulatory elements in the human genome", "url": "https://www.biorxiv.org/content/early/2017/08/15/176602", "tag": "Bioinformatics", "abstract": "Histone modifications have been widely elucidated to play vital roles in gene regulation and cell identity. The Roadmap Epigenomics Consortium generated a reference catalogue of several key histone modifications across >100s of human cell types and tissues. Decoding these epigenomes into functional regulatory elements is a challenging task in computational biology. To this end, we adopted a differential chromatin modification analysis framework to comprehensively determine and characterize cell type-specific regulatory elements (CSREs) and their histone modification codes in the human epigenomes of five histone modifications across 127 tissues or cell types. The CSREs show significant relevance with cell type-specific biological functions and diseases and cell identity. Clustering of CSREs with their specificity signals reveals diverse histone codes, demonstrating the diversity of functional roles of CSREs within the same cell or tissue. Last but not least, dynamics of CSREs from close cell types or tissues can give a detailed view of developmental processes such as normal tissue development and cancer occurrence."}, {"title": "Evaluating a topic model approach for parsing microbiome data structure", "url": "https://www.biorxiv.org/content/early/2017/08/15/176412", "tag": "Bioinformatics", "abstract": "The increasing availability of microbiome survey data has led to the use of complex machine learning and statistical approaches to measure taxonomic diversity and extract relationships between taxa and their host or environment. However, many approaches inadequately account for the difficulties inherent to microbiome data. These difficulties include (1) insufficient sequencing depth resulting in sparse count data, (2) a large feature space relative to sample space, resulting in data prone to overfitting, (3) library size imbalance, requiring normalization strategies that lead to compositional artifacts, and (4) zero-inflation. Recent work has used probabilistic topics models to more appropriately model microbiome data, but a thorough inspection of just how well topic models capture underlying microbiome signal is lacking. Also, no work has determined whether library size or variance normalization improves model fitting. Here, we assessed a topic model approach on 16S rRNA gene survey data. Through simulation, we show, for small sample sizes, library-size or variance normalization is unnecessary prior to fitting the topic model. In addition, by exploiting topic-to-topic correlations, the topic model successfully captured dynamic time-series behavior of simulated taxonomic subcommunities. Lastly, when the topic model was applied to the David et al. time-series dataset, three distinct gut configurations emerged. However, unlike the David et al. approach, we characterized the events in terms of topics, which captured taxonomic co-occurrence, and posterior uncertainty, which facilitated the interpretation of how the taxonomic configurations evolved over time."}, {"title": "mixOmics: an R package for 'omics feature selection and multiple data integration", "url": "https://www.biorxiv.org/content/early/2017/08/15/108597", "tag": "Bioinformatics", "abstract": "The advent of high throughput technologies has led to a wealth of publicly available 'omics data coming from different sources, such as transcriptomics, proteomics, metabolomics. Combining such large-scale biological data sets can lead to the discovery of important biological insights, provided that relevant information can be extracted in a holistic manner. Current statistical approaches have been focusing on identifying small subsets of molecules (a 'molecular signature') to explain or predict biological conditions, but mainly for a single type of 'omics. In addition, commonly used methods are univariate and consider each biological feature independently. We introduce mixOmics, an R package dedicated to the multivariate analysis of biological data sets with a specific focus on data exploration, dimension reduction and visualisation. By adopting a system biology approach, the toolkit provides a wide range of methods that statistically integrate several data sets at once to probe relationships between heterogeneous 'omics data sets. Our recent methods extend Projection to Latent Structure (PLS) models for discriminant analysis, for data integration across multiple 'omics data or across independent studies, and for the identification of molecular signatures. We illustrate our latest mixOmics integrative frameworks for the multivariate analyses of 'omics data available from the package."}, {"title": "Curatr: a web application for creating, curating, and sharing a mass spectral library", "url": "https://www.biorxiv.org/content/early/2017/08/15/170571", "tag": "Bioinformatics", "abstract": "Motivation: Identification from metabolomics mass spectrometry experiments requires the direct comparison of fragmentation spectra from experimental samples to spectra from analytical standards. As the quality of identification depends directly on the quality of the reference spectra, manual curation is routine during the selection of reference spectra to include in a spectral library. Whilst building our own in-house spectral library we realised that there is currently no vendor neutral open access tool for for facilitating manual curation of spectra from raw LC-MS data into a custom spectral library. Results: We developed a web application curatr for the rapid generation of high quality mass spectral fragmentation libraries for liquid-chromatography mass spectrometry analysis. Curatr handles datasets from single or multiplexed standards, automatically extracting chromatographic profiles and potential fragmentation spectra for multiple adducts. These are presented through an intuitive interface for manual curation before being documented in a custom spectral library. Searchable molecular information and the providence of each standard is stored along with metadata on the experimental protocol. As it is written using the django framework, curatr can be rapidly deployed for in-house use. Curatr support the export of spectral libraries in several standard formats for easy use with third party software or submission to community databases, maximising the return on investment for these costly measurements. We demonstrate the use of curatr to generate the EMBL Metabolomics Core Facility spectral library which is publicly available at http://curatr.mcf.embl.de. Availability: The source code is freely available at http://github.com/alexandrovteam/curatr/ along with example data."}, {"title": "PANOPLY: A computational method for identification of promising drugs for a patient based on multidimensional data", "url": "https://www.biorxiv.org/content/early/2017/08/15/176396", "tag": "Bioinformatics", "abstract": "The majority of cancer patients receive treatments that are minimally informed by omics data. Our goal was to develop a precision medicine computational framework (PANOPLY: Precision cancer genomic report: single sample inventory) to identify and prioritize drug targets and cancer therapy regimens. The PANOPLY approach integrates clinical data with germline and somatic features obtained from multi-omics platforms, and apply machine learning, and network analysis approaches in the context of the individual patient and matched controls. The PANOPLY employs multiple steps including i) selection of matched controls, ii) preprocessing of data and identification of driver mutations and altered genes in the patient, iii) identification of suitable drugs using the driver-gene network and random forest analysis, and iv) a multi-omics case report of the patient with prioritization of anti-cancer drugs. The PANOPLY framework can be executed on a stand-alone virtual machine and is also available for download as an R package. We applied PANOPLY to multiple publicly accessible multi-omics tumor datasets with survival data available. We also applied the method to an institutional breast cancer neoadjuvant chemotherapy study which collected clinical and genomic data as well as patient-derived xenografts (PDXs) to investigate the prioritization offered by PANOPLY. We found that that the prioritized drug, olaparib, was more effective than placebo at treating the tumor in the chemotherapy resistant PDX model (P < 0.05). Further studies are ongoing to determine the efficacy of PANOPLY using additional PDXs. In summary, PANOPLY prioritizes drugs based on both clinical and multi-omics data, and it can aid oncologists in their decision-making for therapy in the individual patient."}, {"title": "Revealing unidentified heterogeneity in different epithelial cancers using heterocellular subtype classification", "url": "https://www.biorxiv.org/content/early/2017/08/14/175505", "tag": "Bioinformatics", "abstract": "Cancers are currently diagnosed, categorised, and treated based on their tissue of origin. However, how different cellular compartments of tissues (e.g., epithelial, immune and stem cells) are similar across cancer types is unknown. Here we used colorectal cancer subtypes and their signatures representing different colonic crypt cell types as surrogates to classify different epithelial cancers into five heterotypic cellular (heterocellular) subtypes. The stem-like and inflammatory heterocellular subtypes are ubiquitous across epithelial cancers so capture intrinsic, tissue-independent properties. Conversely, well-differentiated/specialized goblet-like/enterocyte heterocellular subtypes differ across cancer types due to their colorectum-specific genes. The transit-amplifying heterocellular subtype shows a dynamic range of cellular differentiation with shared common pathways (Wnt, FGFR) in certain cancer types. Importantly, this approach revealed previously unrecognised heterogeneity in pancreatic, breast, microsatellite-instability enriched and KRAS mutation-dependent cancers. Immune cell-type differences are common and useful for patient stratification for immunotherapy. This unique approach identifies cell type-dependent but tissue-independent heterogeneity in different cancers for precision medicine."}, {"title": "scPipe: a flexible data preprocessing pipeline for single-cell RNA-sequencing data", "url": "https://www.biorxiv.org/content/early/2017/08/14/175927", "tag": "Bioinformatics", "abstract": "Single-cell RNA sequencing (scRNA-seq) technology allows researchers to profile the transcriptomes of thousands of cells simultaneously. Protocols that incorporate both designed and random barcodes to label individual cells and molecules have greatly increased the throughput of scRNA-seq, but give rise to a more complex data structure. There is a need for new tools that can handle the various barcodes used by different protocols and exploit this information for quality assessment at the sample-level and provide effective visualization of these results in preparation for higher-level analyses. We developed scPipe, an R package that allows barcode demultiplexing, read alignment and gene-level quantification and quality control of raw sequencing data generated by multiple 3 prime end sequencing protocols that include CEL-seq, MARS-seq and Drop-seq. scPipe produces a count matrix that is essential for downstream analysis along with an HTML report that summarises data quality. These results can be used as input for downstream analyses including normalization, visualization and statistical testing. scPipe performs this processing in a few simple R commands, promoting reproducible analysis of single-cell data that is compatible with the emerging suite of scRNA-seq analysis tools available in R/Bioconductor. The scPipe R package is available from https://github.com/LuyiTian/scPipe."}, {"title": "Genetic correlations among neuro-behavioral and immune-related phenotypes based on genome-wide association data", "url": "https://www.biorxiv.org/content/early/2017/08/13/070730", "tag": "Bioinformatics", "abstract": "Evidence of elevated autoimmune comorbidity and altered immune signaling has been observed in samples of individuals affected by neuropsychiatric disorders. It remains unclear whether these altered immunological states have a genetic basis. The present study sought to use existing summary-level data generated from previous genome-wide association studies (GWASs) in order to explore whether common variant genetic risk factors (i.e., SNPs) are shared between a set of neuro-behavioral phenotypes and a set of immune-inflammatory phenotypes. For each phenotype, we calculated the estimated heritability and examined pair-wise genetic correlations using the LD score regression method. We observed significant positive genetic correlations between bipolar disorder and each of: celiac disease (rg = 0.31 + 0.09, uncorrected p = 4.0x10-4), Crohns disease (rg = 0.21 + 0.05, uncorrected p = 3.7x10-5), and ulcerative colitis (rg = 0.23 + 0.06, uncorrected p = 2.0x10-4). We observed significant positive correlations between schizophrenia and each of: Crohns disease (rg = 0.12 + 0.03, uncorrected p = 3.0x10-4), ulcerative colitis (rg = 0.13 + 0.04, uncorrected p = 4.0x10-4), primary biliary cirrhosis (rg = 0.15 + 0.05, uncorrected p = 0.0012), and systemic lupus erythematous (rg = 0.14 + 0.04, uncorrected p = 0.0013). We also found significant positive correlations between major depression and hypothyroidism (rg = 0.33 + 0.09, uncorrected p = 5x10-4) and between Tourette syndrome and allergy (rg = 0.24 + 0.06, uncorrected p =2.7x10-5). We highlight genes mapping near the top single nucleotide polymorphisms (SNPs) that may contribute to these behavior-immune correlations. We also observed many significant genetic correlations amongst the immune-inflammatory phenotypes that survived testing for multiple correction. We discuss these results in the context of clinical epidemiologic literature and other genomic approaches and discuss important limitations and caveats of the LD score regression approach we utilized. These results shed new light on the immunogenetics of psychiatric disorders and suggest that similarities in a polygenic diathesis may contribute to increased comorbidity between psychiatric and immune-related disorders."}, {"title": "Evaluation of in silico algorithms for use with ACMG/AMP clinical variant interpretation guidelines", "url": "https://www.biorxiv.org/content/early/2017/08/12/146100", "tag": "Bioinformatics", "abstract": "The ACMG/AMP variant classification guidelines for clinical reporting recommend complete concordance of predictions among all in silico algorithms used without specifying the number or types of algorithms. The subjective nature of this recommendation contributes to discordance of variant classification among clinical laboratories. Using 14,819 benign or pathogenic missense variants from the ClinVar database, we compared performance of 25 algorithms across datasets differing in distinct biological and technical variables. There was wide variability in concordance among different combinations of algorithms with particularly low concordance for benign variants. We identified recently developed algorithms with high predictive power and robust to variables like disease mechanism, gene constraint and mode of inheritance, although poorer performing algorithms are more frequently used based on review of the clinical genetics literature (2011-2017). We describe high performing algorithm combinations with increased concordance in variant assertion, which should lead to more informed in silico algorithm usage by diagnostic laboratories."}, {"title": "Computational pipeline for the PGV-001 neoantigen vaccine trial", "url": "https://www.biorxiv.org/content/early/2017/08/11/174516", "tag": "Bioinformatics", "abstract": "This paper describes the sequencing protocol and computational pipeline for the PGV-001 personalized vaccine trial. PGV-001 is a therapeutic peptide vaccine targeting neoantigens identified from patient tumor samples. Peptides are selected by a computational pipeline which identifies mutations from tumor/normal exome sequencing and ranks mutant sequences by a combination of predicted Class I MHC affinity and abundance estimated from tumor RNA. The PGV pipeline is modular and consists of many independently usable tools and software libraries. We draw attention to three particular tools which may be useful to other groups working on neoantigen vaccination. (1) Epidisco is a workflow which orchestrates the parallel execution of the PGV pipeline, including both common steps such as alignment as well as tools which have been developed specifically for the PGV-001 trial. (2) Vaxrank uses somatic variants and tumor RNA reads to select peptides for inclusion in a patient's vaccine. (3) Isovar is a library used by Vaxrank to determine the mutated protein sequence associated with a genomic variant from supporting tumor RNA reads. We hope that the functionality of these tools may extend beyond the specifics of the PGV-001 trial and enable other research groups in their own neoantigen investigations."}, {"title": "Simulation of cancer cell line pharmacogenomics data to optimise experimental design and analysis strategy", "url": "https://www.biorxiv.org/content/early/2017/08/11/174862", "tag": "Bioinformatics", "abstract": "Background: Explaining the variability in drug sensitivity across a panel of cell lines using genomic information is a key aspect of cancer drug discovery. The results of such analyses may ultimately determine which patients are likely to benefit from a new treatment. There are numerous experimental factors that can influence the outcomes of cell line screening panels such as the number of replicates, number of doses explored etc. Simulation studies can aid in understanding how variability in these experimental factors can affect the statistical power of a given analysis method. In this study dose response data was simulated for a variety of experimental designs and the ability of different methods to retrieve the original simulation parameters was compared. The analysis methods under consideration were a combination of non-linear least squares and ANOVA, conventional approach, versus non-linear mixed effects. Results: Across the simulation studies explored the mixed-effects approach gave similar and in some situations greater statistical power than the conventional approach. In particular the mixed-effects approach gave significantly greater power when there was less information per dose response curve, and when more cell lines screened. More generally the best way to improve statistical power was to screen more cell lines. Conclusions: This study demonstrates the value of simulating data to understand design and analysis choices in the context of cancer drug sensitivity screening. By illustrating the performance of different methods in different situations these results will help researchers in the field generate and analyse data on future preclinical compounds. Ultimately this will benefit patients by ensuring that biomarkers of drug sensitivity have an increased chance of being identified at the preclinical stage."}, {"title": "Fast P(RMNE): Fast Forensic DNA Probability of Random Man Not Excluded Calculation", "url": "https://www.biorxiv.org/content/early/2017/08/11/173708", "tag": "Bioinformatics", "abstract": "High throughput sequencing (HTS) of DNA forensic samples is expanding from the sizing of short tandem repeats (STRs) to massively parallel sequencing (MPS). HTS panels are expanding from the FBI 20 core Combined DNA Index System (CODIS) loci to include single nucleotide polymorphisms (SNPs). The calculation of random man not excluded, P(RMNE), is used in DNA mixture analysis to estimate the probability that a random person matches this DNA mixture. Previous implementation of this calculation encounters calculation artifacts with expansion to larger panel sizes. Increasing the floating-point precision of the calculations allows for increased panel sizes but with a corresponding increase in computation time. The Taylor series higher precision libraries used had precision artifacts and also failed to complete on some input data sets leading to algorithm unreliability. Similar computational artifacts were observed with a BigDecimal implementation. Herein, a new formula is introduced for calculating P(RMNE) that scales to larger SNP panel sizes while being computationally efficient (patent pending)[1]."}, {"title": "FastID: Extremely Fast Forensic DNA Comparisons", "url": "https://www.biorxiv.org/content/early/2017/08/11/173666", "tag": "Bioinformatics", "abstract": "Rapid analysis of DNA forensic samples can have a critical impact on time sensitive investigations. Analysis of forensic DNA samples by massively parallel sequencing is creating the next gold standard for DNA forensic analysis. This technology enables the expansion of forensic profiles from the current 20 short tandem repeat (STR) loci to tens of thousands of single nucleotide polymorphism (SNP) loci. A forensic search scales by the product of the number of loci and the number of profile comparisons. This paper introduces a method (FastID) to address the need for rapid scalable analysis of DNA forensic samples. FastID can search a profile of 2,500 SNP loci against 20 million profiles in 5.08 seconds using a single computational thread on a laptop (Intel i7 4.0 GHz)."}, {"title": "Personal Cancer Genome Reporter: Variant Interpretation Report For Precision Oncology", "url": "https://www.biorxiv.org/content/early/2017/08/11/122366", "tag": "Bioinformatics", "abstract": "Summary: Individual tumor genomes pose a major challenge for clinical interpretation due to their unique sets of acquired mutations. There is a general scarcity of tools that can i) systematically interrogate cancer genomes in the context of diagnostic, prognostic, and therapeutic biomarkers, ii) prioritize and highlight the most important findings, and iii) present the results in a format accessible to clinical experts. We have developed a stand-alone, open-source software package for somatic variant annotation that integrates a comprehensive set of knowledge resources related to tumor biology and therapeutic biomarkers, both at the gene and variant level. Our application generates a tiered report that will aid the interpretation of individual cancer genomes in a clinical setting. Availability and Implementation: The software is implemented in Python/R, and is freely available through Docker technology. Documentation, example reports, and installation instructions are accessible via the project GitHub page: https://github.com/sigven/pcgr."}, {"title": "Population Genotype Calling from Low-coverage Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/08/11/085936", "tag": "Bioinformatics", "abstract": "In recent years, several large-scale whole-genome projects sequencing tens of thousands of individuals were completed, with larger studies are underway. These projects aim to provide high-quality genotypes for a large number of whole genomes in a cost-efficient manner, by sequencing each genome at low coverage and subsequently identifying alleles jointly in the entire cohort. Here we present Ref-Reveel, a novel method for large-scale population genotyping. We show that Ref-Reveel provides genotyping at a higher accuracy and higher efficiency in comparison to existing methods by applying our method to one of the largest whole-genome sequencing datasets presently available to the public. We further show that utilizing the resulting genotype panel as references, through the Ref-Reveel framework, greatly improves the ability to call genotypes accurately on newly sequenced genomes. In addition, we present a Ref-Reveel pipeline that is applicable for genotyping of very small datasets. In summary, Ref-Reveel is an accurate, scalable and applicable method for a wide range of genotyping scenarios, and will greatly improves the quality of calling genomic alterations in current and future large-scale sequencing projects."}, {"title": "Meta-analysis of liver and heart transcriptomic data for functional annotation transfer in mammalian orthologs", "url": "https://www.biorxiv.org/content/early/2017/08/11/123414", "tag": "Bioinformatics", "abstract": "Functional annotation transfer across multi-gene family orthologs can lead to functional misannotations. We hypothesised that co-expression network will help predict functional orthologs amongst complex homologous gene families. To explore the use of transcriptomic data available in public domain to identify functionally equivalent ones from all predicted orthologs, we collected genome wide expression data in mouse and rat liver from over 1500 experiments with varied treatments. We used a hyper-graph clustering method to identify clusters of orthologous genes co-expressed in both mouse and rat. We validated these clusters by analysing expression profiles in each species separately, and demonstrating a high overlap. We then focused on genes in 18 homology groups with one-to-many or many-to-many relationships between two species, to discriminate between functionally equivalent and non-equivalent orthologs. Finally, we further applied our method by collecting heart transcriptomic data (over 1400 experiments) in rat and mouse to validate the method in an independent tissue."}, {"title": "COGEM: A Toolbox for Computational Genomics in Matlab", "url": "https://www.biorxiv.org/content/early/2017/08/10/174896", "tag": "Bioinformatics", "abstract": "Motivation: The Matlab programming language is widely used for both teaching and research in engineering, computer science, and mathematics. Despite its many strengths, it has never been a dominant language in computational genomics or bioinformatics more generally. Results: Here, we introduce COGEM, a long-term project to develop computational genomics functionality in Matlab. The initial release provides functions for manipulating genomic intervals, stranded or unstranded, with or without numerical data associated. It includes features for both text and binary file input and output, conversion between BAM, BED and BEDGRAPH formats, and numerous functions for manipulating intervals, including shifting, expanding, overlapping, intersecting, unioning, finding nearest intervals, piling up intervals, and performing unary and binary numerical and logical operations on sets of intervals. The toolbox is well-suited to the analysis of high-throughput sequencing data. We demonstrate its functionality by creating a ChIP-seq peak-calling algorithm by chaining together a series of commands, and find it capable of analyzing genome-scale data in reasonable time. Availability: The current toolbox and reference manual is available as supplementary material, and updated versions will be maintained at www.perkinslab.ca online."}, {"title": "Does genetic risk help to predict amyloid burden in a non-demented population? A Bayesian approach.", "url": "https://www.biorxiv.org/content/early/2017/08/10/174995", "tag": "Bioinformatics", "abstract": "INTRODUCTION: In this study we investigate the association between A\u03b2 levels in cerebrospinal fluid (CSF) and genetic risk in a non-demented population. This paper presents the first analysis to use a Bayesian methodology in this area. METHODS: Data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the EDAR* and DESCRIPA** studies was used in a Bayesian logistic regression analysis. We modeled CSF A\u03b2 burden using age, diagnosis (healthy control or mild cognitive impairment), APOE and a polygenic risk score (PGRS) associated with Alzheimer's Disease (AD). We compared models built using informative priors on age, diagnosis and APOE with non-informative priors on all variables. RESULTS: The use of informative priors did not improve model performance in the majority of cases. Models using only age, diagnosis and APOE genotype showed the best predictive ability. DISCUSSION: A previous study indicated that a PGRS of AD case/control status was associated with CSF A\u03b2 burden in healthy controls. The current study suggests that this association does not lead to models that are more predictive of amyloid positivity than already known factors such as age and APOE. * 'Beta amyloid oligomers in the early diagnosis of AD and as marker for treatment response' ** 'Development of screening guidelines and criteria for pre-dementia Alzheimers disease'"}, {"title": "Text mining of disease-lifestyle associations to explain comorbidities in electronic health registries", "url": "https://www.biorxiv.org/content/early/2017/08/10/168211", "tag": "Bioinformatics", "abstract": "Mining of electronic health registries can reveal vast numbers of disease correlations (from hereon referred to as comorbidities for simplicity). However, the underlying causes can be hard to identify, in part because health registries usually do not record important lifestyle factors such as diet, substance consumption, and physical activity. To address this challenge, I developed a text-mining approach that uses dictionaries of diseases and lifestyle factors for named entity recognition and subsequently for co-occurrence extraction of disease-lifestyle associations from Medline. I show that this approach is able to extract many correct associations and provide proof-of-concept that these can provide plausible explanations for comorbidities observed in Swedish and Danish health registry data."}, {"title": "Theory of reinforcement learning and motivation in the basal ganglia", "url": "https://www.biorxiv.org/content/early/2017/08/10/174524", "tag": "Bioinformatics", "abstract": "This paper proposes how the neural circuits in vertebrates select actions on the basis of past experience and the current motivational state. According to the presented theory, the basal ganglia evaluate the utility of considered actions by combining the positive consequences (e.g. nutrition) scaled by the motivational state (e.g. hunger) with the negative consequences (e.g. effort). The theory suggests how the basal ganglia compute utility by combining the positive and negative consequences encoded in the synaptic weights of striatal Go and No-Go neurons, and the motivational state carried by neuromodulators including dopamine. Furthermore, the theory suggests how the striatal neurons to learn separately about consequences of actions, and how the dopaminergic neurons themselves learn what level of activity they need to produce to optimize behaviour. The theory accounts for the effects of dopaminergic modulation on behaviour, patterns of synaptic plasticity in striatum, and responses of dopaminergic neurons in diverse situations."}, {"title": "High resolution annotation of Zebrafish transcriptome using long-read sequencing", "url": "https://www.biorxiv.org/content/early/2017/08/10/174821", "tag": "Bioinformatics", "abstract": "With the emergence of zebrafish as an important model organism, a concerted effort has been made to study its transcriptome. This effort is limited, however, by gaps in zebrafish annotation, which are especially pronounced concerning transcripts dynamically expressed during zygotic genome activation (ZGA). To date, short read sequencing has been the principal technology for zebrafish transcriptome annotation. In part because these sequence reads are too short for assembly methods to resolve the full complexity of the transcriptome, the current annotation is rudimentary. By providing direct observation of full-length transcripts, recently refined long-read sequencing platforms can dramatically improve annotation coverage and accuracy. Here, we leveraged the SMRT platform to study transcriptome of zebrafish embryos before and after ZGA. Our analysis revealed additional novelty and complexity in the zebrafish transcriptome, identifying 2748 high confidence novel transcripts that originated from previously unannotated loci and 1835 high confidence new isoforms in previously annotated genes. We validated these findings using a suite of computational approaches including structural prediction, sequence homology and functional conservation analyses, as well as by confirmatory transcript quantification with short-read sequencing data. Our analyses provided insight into new homologs and paralogs of functionally important proteins and non-coding RNAs, isoform switching occurrences and different classes of novel splicing events. Several novel isoforms representing distinct splicing events were validated through PCR experiments, including the discovery and validation of a novel 8 kb transcript spanning multiple miR-430 elements, an important driver of early development. Our study provides a significantly improved zebrafish transcriptome annotation resource."}, {"title": "A Linear Algebra Approach to Fast DNA Mixture Analysis Using GPUs", "url": "https://www.biorxiv.org/content/early/2017/08/10/174813", "tag": "Bioinformatics", "abstract": "Analysis of DNA samples is an important step in forensics, and the speed of analysis can impact investigations. Comparison of DNA sequences is based on the analysis of short tandem repeats (STRs), which are short DNA sequences of 2-5 base pairs. Current forensics approaches use 20 STR loci for analysis. The use of single nucleotide polymorphisms (SNPs) has utility for analysis of complex DNA mixtures. The use of tens of thousands of SNPs loci for analysis poses significant computational challenges because the forensic analysis scales by the product of the loci count and number of DNA samples to be analyzed. In this paper, we discuss the implementation of a DNA sequence comparison algorithm by re-casting the algorithm in terms of linear algebra primitives. By developing an overloaded matrix multiplication approach to DNA comparisons, we can leverage advances in GPU hardware and algoithms for Dense Generalized Matrix-Multiply (DGEMM) to speed up DNA sample comparisons. We show that it is possible to compare 2048 unknown DNA samples with 20 million known samples in under 6 seconds using a NVIDIA K80 GPU."}, {"title": "High-resolution global peptide-protein docking using fragments-based PIPER-FlexPepDock", "url": "https://www.biorxiv.org/content/early/2017/08/10/174714", "tag": "Bioinformatics", "abstract": "Peptide-protein interactions contribute a significant fraction of the protein-protein interactome. Accurate modeling of these interactions is challenging due to the vast conformational space associated with interactions of highly flexible peptides with large receptor surfaces. To address this challenge we developed a fragment based high-resolution peptide-protein docking protocol. By streamlining the Rosetta fragment picker for accurate peptide fragment ensemble generation, the PIPER docking algorithm for exhaustive fragment-receptor rigid-body docking and Rosetta FlexPepDock for flexible full-atom refinement of PIPER docked models, we successfully addressed the challenge of accurate and efficient global peptide-protein docking at high-resolution with remarkable accuracy. Validation on a representative set of solved peptide-protein complex structures demonstrates the accuracy and robustness of our approach, and opens up the way to high-resolution modeling of many more peptide-protein interactions and to the detailed study of peptide-protein association in general. PIPER-FlexPepDock is freely available to the academic community as a server at http://piperfpd.furmanlab.cs.huji.ac.il."}, {"title": "Strategies for analyzing bisulfite sequencing data", "url": "https://www.biorxiv.org/content/early/2017/08/09/109512", "tag": "Bioinformatics", "abstract": "DNA methylation is one of the main epigenetic modifications in the eukaryotic genome; it has been shown to play a role in cell-type specific regulation of gene expression, and therefore cell-type identity. Bisulfite sequencing is the gold-standard for measuring methylation over the genomes of interest. Here, we review several techniques used for the analysis of high-throughput bisulfite sequencing. We introduce specialized short-read alignment techniques as well as pre/post-alignment quality check methods to ensure data quality. Furthermore, we discuss subsequent analysis steps after alignment. We introduce various differential methylation methods and compare their performance using simulated and real bisulfite sequencing datasets. We also discuss the methods used to segment methylomes in order to pinpoint regulatory regions. We introduce annotation methods that can be used for further classification of regions returned by segmentation and differential methylation methods. Finally, we review software packages that implement strategies to efficiently deal with large bisulfite sequencing datasets locally and we discuss online analysis workflows that do not require any prior programming skills. The analysis strategies described in this review will guide researchers at any level to the best practices of bisulfite sequencing analysis."}, {"title": "TaxMapper: An Analysis Tool, Reference Database and Workflow for Metatranscriptome Analysis of Eukaryotic Microorganisms", "url": "https://www.biorxiv.org/content/early/2017/08/09/174227", "tag": "Bioinformatics", "abstract": "Background: High-throughput sequencing (HTS) technologies are increasingly applied to analyse complex microbial ecosystems by mRNA sequencing of whole communities, also known as metatranscriptome sequencing. This approach is at the moment largely limited to prokaryotic communities and communities of few eukaryotic species with sequenced genomes. For eukaryotes the analysis is hindered mainly by a low and fragmented coverage of the reference databases to infer the community composition, but also by lack of automated workflows for the task. Results: From the databases of the National Center for Biotechnology Information and Marine Microbial Eukaryote Transcriptome Sequencing Project, 142 references were selected in such a way that the taxa represent the main lineages within each of the seven supergroups of eukaryotes and possess predominantly complete transcriptomes or genomes. From these references, we created an annotated microeukaryotic reference database. We developed a tool called TaxMapper for a reliably mapping of sequencing reads against this database and filtering of unreliable assignments. For filtering, a classifier was trained and tested on sequences in the database, sequences of related taxa to those in the database and randomly generated sequences. Additionally, TaxMapper is part of a metatranscriptomic Snakemake workflow developed to perform quality assessment, functional and taxonomic annotation and (multivariate) statistical analysis including environmental data. The workflow is provided and described in detail to empower researchers to easily apply it for metatranscriptome analysis of any environmental sample. Conclusions: TaxMapper shows superior performance compared to standard approaches, resulting in a higher number of true positive taxonomic assignments. Both the TaxMapper tool and the workflow are available as open-source code at Bitbucket under the MIT license: https://bitbucket.org/dbeisser/taxmapper and as a Bioconda package: https://bioconda.github.io/recipes/taxmapper/README.html."}, {"title": "Scanpy for analysis of large-scale single-cell gene expression data", "url": "https://www.biorxiv.org/content/early/2017/08/09/174029", "tag": "Bioinformatics", "abstract": "We present Scanpy, a scalable toolkit for analyzing single-cell gene expression data. It includes preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing and simulation of gene regulatory networks. The Python-based implementation efficiently deals with datasets of more than one million cells and enables easy interfacing of advanced machine learning packages. Code is available from https://github.com/theislab/scanpy."}, {"title": "Granatum: a graphical single-cell RNA-Seq analysis pipeline for genomics scientists", "url": "https://www.biorxiv.org/content/early/2017/08/08/110759", "tag": "Bioinformatics", "abstract": "Background: Single-cell RNA sequencing (scRNA-Seq) is an increasingly popular platform to study heterogeneity at the single-cell level. Computational methods to process scRNA-Seq have limited accessibility to bench scientists as they require significant amounts of bioinformatics skills. Results: We have developed Granatum, a web-based scRNA-Seq analysis pipeline to make analysis more broadly accessible to researchers. Without a single line of programming code, users can click through the pipeline, setting parameters and visualizing results via the interactive graphical interface. Granatum conveniently walks users through various steps of scRNA-Seq analysis. It has a comprehensive list of modules, including plate merging and batch-effect removal, outlier-sample removal, gene filtering, gene-expression normalization, cell clustering, differential gene expression analysis, pathway/ontology enrichment analysis, protein-network interaction visualization, and pseudo-time cell series construction. Conclusions: Granatum enables broad adoption of scRNA-Seq technology by empowering the bench scientists with an easy-to-use graphical interface for scRNA-Seq data analysis. The package is freely available for research use at http://garmiregroup.org/granatum/app"}, {"title": "SureMap: Versatile, Error Tolerant, and High Sensitive Read Mapper", "url": "https://www.biorxiv.org/content/early/2017/08/08/173740", "tag": "Bioinformatics", "abstract": "SureMap is a versatile, error tolerant and high sensitive read mapper which is able to map \"difficult\" reads, those requiring many edit operations to be mapped to the reference genome, with acceptable time complexity. Mapping real datasets reveal that many variants unidentifiable by other mappers can be called using Suremap. Moreover, SureMap has a very good running time and accuracy in aligning very long and noisy reads like PacBio and Nanopore against a reference genome."}, {"title": "neoantigenR: An annotation based pipeline for tumor neoantigen identification from sequencing data", "url": "https://www.biorxiv.org/content/early/2017/08/08/171843", "tag": "Bioinformatics", "abstract": "Studies indicate that more than 90% of human genes are alternatively spliced, suggesting the complexity of the transcriptome assembly and analysis. The splicing process is often disrupted, resulting in both functional and non-functional end-products (Sveen et al. 2016) in many cancers. Harnessing the immune system to fight against malignant cancers carrying aberrantly mutated or spliced products is becoming a promising approach to cancer therapy. Advances in immune checkpoint blockade have elicited adaptive immune responses with promising clinical responses to treatments against human malignancies (Tumor Neoantigens in Personalized Cancer Immunotherapy 2017). Emerging data suggest that recognition of patient-specific mutation-associated cancer antigens (i.e. from alternative splicing isoforms) may allow scientists to dissect the immune response in the activity of clinical immunotherapies (Schumacher and Schreiber 2015). The advent of high-throughput sequencing technology has provided a comprehensive view of both splicing aberrations and somatic mutations across a range of human malignancies, allowing for a deeper understanding of the interplay of various disease mechanisms. Meanwhile, studies show that the number of transcript isoforms reported to date may be limited by the short-read sequencing due to the inherit limitation of transcriptome reconstruction algorithms, whereas long-read sequencing is able to significantly improve the detection of alternative splicing variants since there is no need to assemble full-length transcripts from short reads. The analysis of these high-throughput long-read sequencing data may permit a systematic view of tumor specific peptide epitopes (also known as neoantigens) that could serve as targets for immunotherapy (Tumor Neoantigens in Personalized Cancer Immunotherapy 2017). Currently, there is no software pipeline available that can efficiently produce mutation-associated cancer antigens from raw high-throughput sequencing data on patient tumor DNA (The Problem with Neoantigen Prediction 2017). In addressing this issue, we introduce a R package that allows the discoveries of peptide epitope candidates, which are the tumor-specific peptide fragments containing potential functional neoantigens. These peptide epitopes consist of structure variants including insertion, deletions, alternative sequences, and peptides from nonsynonymous mutations. Analysis of these precursor candidates with widely used tools such as netMHC allows for the accurate in-silico prediction of neoantigens. The pipeline named neoantigeR is currently hosted in https://github.com/ICBI/neoantigeR."}, {"title": "NeatSeq-Flow: A Lightweight Software for Efficient Execution of High Throughput Sequencing Workflows", "url": "https://www.biorxiv.org/content/early/2017/08/08/173005", "tag": "Bioinformatics", "abstract": "Bioinformatics workflows (WFs) in general, and those involving High Throughput Sequencing data in particular, typically involve executing a sequence of programs on raw sequence files from as many as thousands of samples. Management of these WFs is laborious and error-prone. We have developed NeatSeq-Flow, a python package that manages WF creation for execution on computer clusters. NeatSeq-Flow creates shell scripts as well as a directory structure for storing analysis results, error messages, and execution logs. The user maintains full control over the execution of the WF, while the computer cluster enforces sequential execution and parallelization. NeatSeq-Flow also supplies tools for version tracking, documentation and execution logging."}, {"title": "TraPS-VarI: a python module for the identification of STAT3 modulating germline receptor variants", "url": "https://www.biorxiv.org/content/early/2017/08/07/173047.1", "tag": "Bioinformatics", "abstract": "Motivation: Human individuals differ because of variations in the DNA sequences of all the 46 chromosomes. Information on genetic variations altering the membrane-proximal binding sites for signal transducer of transcription 3 (STAT3) is valuable for understanding the genetic basis of cancer prognosis and disease progression (Ulaganathan et al, 2015). In this regard, non-synonymous coding region mutations resulting in the alteration of protein sequence in the juxtamembrane region of the type I membrane proteins are biologically and clinically relevant. The knowledge of such rare cell line- and individual-specific germline receptor variants is crucial for the investigation of cell-line specific biological mechanisms and genotype-centric therapeutic approaches. Results: Here we present TraPS-VarI (Transmembrane Protein Sequence Variant Identifier), a Python module to rapidly identify human germline receptor variants modulating STAT3 binding sites by using the genetic variation datasets in the variant call format 4.0. For the found protein variants the module also checks for the availability of associated therapeutic agents in the therapeutic target database and the drugbank records."}, {"title": "DeepATAC: A deep-learning method to predict regulatory factor binding activity from ATAC-seq signals", "url": "https://www.biorxiv.org/content/early/2017/08/06/172767", "tag": "Bioinformatics", "abstract": "Determining the binding locations of regulatory factors, such as transcription factors and histone modifications, is essential to both basic biology research and many clinical applications. Obtaining such genome-wide location maps directly is often invasive and resource-intensive, so it is common to impute binding locations from DNA sequence or measures of chromatin accessibility. We introduce DeepATAC, a deep-learning approach for imputing binding locations that uses both DNA sequence and chromatin accessibility as measured by ATAC-seq. DeepATAC significantly outperforms current approaches such as FIMO motif predictions overlapped with ATAC-seq peaks, and models based only on DNA sequence, such as DeepSEA. Visualizing the input importance for the DeepATAC model reveals DNA sequence motifs and ATAC-seq signal patterns that are important for predicting binding events. The Keras implementation and analysis pipelines of DeepATAC is available at https://github.com/hiranumn/deepatac ."}, {"title": "geneXtendeR: optimized functional annotation of ChIP-seq data", "url": "https://www.biorxiv.org/content/early/2017/08/06/082347", "tag": "Bioinformatics", "abstract": "Different ChIP-seq peak callers often produce different output results from the same input. Since different peak callers are known to produce differentially enriched peaks with a large variance in peak length distribution and total peak count, accurately annotating peak lists with their nearest genes can be an arduous process. Functional genomic annotation of histone modification ChIP-seq data can be a particularly challenging task, as chromatin marks that have inherently broad peaks with a diffuse range of signal enrichment (e.g., H3K9me1, H3K27me3) differ significantly from narrow peaks that exhibit a compact and localized enrichment pattern (e.g., H3K4me3, H3K9ac). In addition, varying degrees of tissue-dependent broadness of an epigenetic mark can make it difficult to accurately and reliably link sequencing data to biological function. Thus, it would be useful to develop a software program that can precisely tailor the computational analysis of a ChIP-seq dataset to the specific peak coordinates of the data. geneXtendeR is an R/Bioconductor package that optimizes the functional annotation of ChIP-seq peaks using fast iterative peak-coordinate/GTF alignment algorithms focused on cis-regulatory regions and proximal-promoter regions of nearest genes. The goal of geneXtendeR is to robustly link differentially enriched peaks with their respective genes, thereby aiding experimental follow-up and validation in designing primers for a set of prospective gene candidates during qPCR. We have tested geneXtendeR on 547 human transcription factor ChIP-seq ENCODE datasets and 214 human histone modification ChIP-seq ENCODE datasets, providing the analysis results as case studies."}, {"title": "HiCRep: assessing the reproducibility of Hi-C data using a stratum-adjusted correlation coefficient", "url": "https://www.biorxiv.org/content/early/2017/08/04/101386", "tag": "Bioinformatics", "abstract": "Hi-C is a powerful technology for studying genome-wide chromatin interactions. However, current methods for assessing Hi-C data reproducibility can produce misleading results because they ignore spatial features in Hi-C data, such as domain structure and distance dependence. We present HiCRep, a framework for assessing the reproducibility of Hi-C data that systematically accounts for these features. In particular, we introduce a novel similarity measure, the stratum adjusted correlation coefficient (SCC), for quantifying the similarity between Hi-C interaction matrices. Not only does it provide a statistically sound and reliable evaluation of reproducibility, SCC can also be used to quantify differences between Hi-C contact matrices and to determine the optimal sequencing depth for a desired resolution. The measure consistently shows higher accuracy than existing approaches in distinguishing subtle differences in reproducibility and depicting interrelationships of cell lineages. The proposed measure is straightforward to interpret and easy to compute, making it well-suited for providing standardized, interpretable, automatable, and scalable quality control. The freely available R package HiCRep implements our approach."}, {"title": "Uncovering Medical Insights from Vast Amounts of Biomedical Data in Clinical Case Reports", "url": "https://www.biorxiv.org/content/early/2017/08/04/172460.1", "tag": "Bioinformatics", "abstract": "Clinical case reports (CCRs) have a time-honored tradition in serving as an important means of sharing clinical experiences on patients presenting with atypical disease phenotypes or receiving new therapies. However, the huge amount of accumulated case reports are isolated, unstructured, and heterogeneous clinical data, posing a great challenge to clinicians and researchers in mining relevant information through existing indexing tools. In this investigation, in order to render CCRs more findable, accessible, interoperable, and reusable (FAIR) by the biomedical community, we created a resource platform, including the construction of a test dataset consisting of 1000 CCRs spanning 14 disease phenotypes, a standardized metadata template and metrics, and a set of computational tools to automatically retrieve relevant medical information and to analyze all published PubMed clinical case reports with respect to trends in publication journals, citations impact, MeSH Terms, drug use, distributions of patient demographics, and relationships with other case reports and databases. Our standardized metadata template and CCR test dataset may be valuable resources to advance medical science and improve patient care for researchers who are using machine learning approaches with a high-quality dataset to train and validate their algorithms. In the future, our analytical tools may be applied towards other large clinical data sources as well."}, {"title": "SeedVicious: analysis of microRNA target and near-target sites", "url": "https://www.biorxiv.org/content/early/2017/08/04/124529", "tag": "Bioinformatics", "abstract": "Here I describe seedVicious, a versatile microRNA target site prediction software that can be easily fitted into annotation pipelines and run over custom datasets. SeedVicious finds microRNA canonical sites plus other, less efficient, target sites. The program also detects near-target sites, which have one nucleotide different from a canonical site. Near-target sites are important to study population variation in microRNA regulation. Here I show that near-target sites can also be functional sites. Among other features, seedVicious can also compute evolutionary gains/losses of target sites using maximum parsimony. SeedVicious does not aim to outperform but to complement existing microRNA prediction tools. For instance, the precision of TargetScan is doubled (from 11% to ~22%) when we filter predictions by the distance between target sites using our program. The software is written in Perl and runs on 64-bit Unix computers (Linux and MacOS X). Users can also try the program in a dedicated web-server by uploading custom data, or browsing pre-computed predictions. SeedVicious and its associated web-server and database (SeedBank) are distributed under the GPL/GNU license."}, {"title": "SEA: The Small RNA Expression Atlas", "url": "https://www.biorxiv.org/content/early/2017/08/04/133199", "tag": "Bioinformatics", "abstract": "Motivation: Small RNAs (sRNAs) are important biomolecules that exert vital functions in organismal health and disease, from viruses to plants, animals, and humans. Given the ever-increasing amounts of sRNA deep sequencing data in online repositories and their potential roles in disease therapy and diagnosis, it is important to enable federated sRNA expression querying across samples, organisms, tissues, cell types, and diseases. Results: Here we present the sRNA Expression Atlas (SEA), a web application that allows for the search of known and novel small RNAs across ten organisms using standardized search terms and ontologies. SEA contains re-analyzed sRNA expression information for over 2000 published samples, including over 700 disease datasets and over 600 novel, high-quality predicted miRNAs. We believe that SEAs simple interface and fast search in combination with its detailed interactive reports will enable researchers to better understand the potential function and diagnostic value of sRNAs across tissues, diseases, and organisms."}, {"title": "Enabling cross-study analysis of RNA-Sequencing data", "url": "https://www.biorxiv.org/content/early/2017/08/04/110734", "tag": "Bioinformatics", "abstract": "Driven by the recent advances of next generation sequencing (NGS) technologies and an urgent need to decode complex human diseases, a multitude of large-scale studies were conducted recently that have resulted in an unprecedented volume of whole transcriptome sequencing (RNA-seq) data. While these data offer new opportunities to identify the mechanisms underlying disease, the comparison of data from different sources poses a great challenge, due to differences in sample and data processing. Here, we present a pipeline that processes and unifies RNA-seq data from different studies, which includes uniform realignment and gene expression quantification as well as batch effect removal. We find that uniform alignment and quantification is not sufficient when combining RNA-seq data from different sources and that the removal of other batch effects is essential to facilitate data comparison. We have processed data from the Genotype Tissue Expression project (GTEx) and The Cancer Genome Atlas (TCGA) and have successfully corrected for study-specific biases, enabling comparative analysis across studies. The normalized data are available for download via GitHub (at https://github.com/mskcc/RNAseqDB)."}, {"title": "Bots for Software-Assisted Analysis of Image-Based Transcriptomics", "url": "https://www.biorxiv.org/content/early/2017/08/03/172296", "tag": "Bioinformatics", "abstract": "We introduce software assistants -- bots -- for the task of analyzing image-based transcriptomic data. The key steps in this process are detecting nuclei, and counting associated puncta corresponding to labeled RNA. Our main release offers two algorithms for nuclei segmentation, and two for spot detection, to handle data of different complexities. For challenging nuclei segmentation cases, we enable the user to train a stacked Random Forest, which includes novel circularity features that leverage prior knowledge regarding nuclei shape for better instance segmentation. This machine learning model can be trained on a modern CPU-only computer, yet performs comparably with respect to a more hardware-demanding state-of-the-art deep learning approach, as demonstrated through experiments. While the primary motivation for the bots was image-based transcriptomics, we also demonstrate their applicability to the more general problem of scoring \"spots\" in nuclei."}, {"title": "Oasis2.0: improved online analysis of small RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/08/03/170738", "tag": "Bioinformatics", "abstract": "Oasis 2 is a new main release of the Oasis web application for the detection, differential expression, and classification of small RNAs in deep sequencing data. Compared to its predecessor Oasis, Oasis 2 features a novel and speed-optimized sRNA detection module that supports the identification of small RNAs in any organism with higher accuracy. Next to the improved detection of small RNAs in a target organism, the software now also recognizes potential cross-species miRNAs and viral and bacterial sRNAs in infected samples. In addition, novel miRNAs can now be queried and visualized interactively, providing essential information for over 700 high-quality miRNA predictions across 14 organisms. Robust biomarker signatures can now be obtained using the novel enhanced classification module. Oasis 2 enables biologists and medical researchers to rapidly analyze and query small RNA deep sequencing data with improved precision, recall, and speed, in an interactive and user-friendly environment. Availability and Implementation: Oasis 2 is implemented in Java, J2EE, mysql, Python, R, PHP and JavaScript. It is freely available at http://oasis.dzne.de."}, {"title": "Collection and Storage of HLA NGS Genotyping Data for the 17th International HLA and Immunogenetics Workshop", "url": "https://www.biorxiv.org/content/early/2017/08/03/116871", "tag": "Bioinformatics", "abstract": "For over 50 years, the International HLA and Immunogenetics Workshops (IHIW) have advanced the fields of histocompatibility and immunogenetics (H&I) via community sharing of technology, experience and reagents, and the establishment of ongoing collaborative projects. In the fall of 2017, the 17th IHIW will focus on the application of next generation sequencing (NGS) technologies for clinical and research goals in the H&I fields. NGS technologies have the potential to allow dramatic insights and advances in these fields, but the scope and sheer quantity of data associated with NGS raise challenges for their analysis, collection, exchange and storage. The 17th IHIW has adopted a centralized approach to these issues, and we have been developing the tools, services and systems to create an effective system for capturing and managing these NGS data. We have worked with NGS platform and software developers to define a set of distinct but equivalent NGS typing reports that record NGS data in a uniform fashion. The 17th IHIW database applies our standards, tools and services to collect, validate and store those structured, multi-platform data in an automated fashion. We are creating community resources to enable exploration of the vast store of curated sequence and allele-name data in the IPD-IMGT/HLA Database, with the goal of creating a long-term community resource that integrates these curated data with new NGS sequence and polymorphism data, for advanced analyses and applications."}, {"title": "Pathway centrality in protein interaction networks identifies functional mediators of pulmonary disease", "url": "https://www.biorxiv.org/content/early/2017/08/03/171942", "tag": "Bioinformatics", "abstract": "Identification of functional pathways mediating molecular responses may lead to better understanding of disease processes and suggest new therapeutic approaches. We introduce a method to detect such mediating functions using topological properties of protein-protein interaction networks. We introduce the concept of pathway centrality, a measure of communication between disease genes and differentially expressed genes. We find mediating pathways for three pulmonary diseases (asthma; bronchopulmonary dysplasia (BPD); and chronic obstructive pulmonary disease (COPD)) using pathway centrality. Mediating pathways shared by all three pulmonary disorders heavily favor inflammatory or immune responses and include specific pathways such as cytokine production, NF Kappa B, and JAK/STAT signaling. Disease-specific mediators, such as insulin signaling in BPD or homeostasis in COPD, are also highlighted. We support our findings, some of which suggest new treatment approaches, both with anecdotal evidence from the literature and via systematic evaluation using genetic interactions."}, {"title": "Bioinformatics Core Competencies for Undergraduate Life Sciences Education", "url": "https://www.biorxiv.org/content/early/2017/08/03/170993", "tag": "Bioinformatics", "abstract": "Bioinformatics is becoming increasingly central to research in the life sciences. However, despite its importance, bioinformatics skills and knowledge are not well integrated in undergraduate biology education. This curricular gap prevents biology students from harnessing the full potential of their education, limiting their career opportunities and slowing genomic research innovation. To advance the integration of bioinformatics into life sciences education, a framework of core bioinformatics competencies is needed. To that end, we here report the results of a survey of life sciences faculty in the United States about teaching bioinformatics to undergraduate life scientists. Responses were received from 1,260 faculty representing institutions in all fifty states with a combined capacity to educate hundreds of thousands of students every year. Results indicate strong, widespread agreement that bioinformatics knowledge and skills are critical for undergraduate life scientists, as well as considerable agreement about which skills are necessary. Perceptions of the importance of some skills varied with the respondent's degree of training, time since degree earned, and/or the Carnegie classification of the respondent's institution. To assess which skills are currently being taught, we analyzed syllabi of courses with bioinformatics content submitted by survey respondents. Finally, we used the survey results, the analysis of syllabi, and our collective research and teaching expertise to develop a set of bioinformatics core competencies for undergraduate life sciences students. These core competencies are intended to serve as a guide for institutions as they work to integrate bioinformatics into their life sciences curricula."}, {"title": "The harmonic mean p-value and model averaging by mean maximum likelihood", "url": "https://www.biorxiv.org/content/early/2017/08/02/171751", "tag": "Bioinformatics", "abstract": "Analysis of 'big data' frequently involves statistical comparison of millions of competing hypotheses to discover hidden processes underlying observed patterns of data, for example in the search for genetic determinants of disease in genome-wide association studies (GWAS). Model averaging is a valuable technique for evaluating the combined evidence of groups of hypotheses, simultaneously testing multiple levels of groupings, and determining post hoc the optimal trade-off between group composition versus significance. Here I introduce the harmonic mean p-value (HMP) for assessing model-averaged fit, which arises from a new method for model averaging by mean maximum likelihood (MAMML), underpinned by generalized central limit theorem. Through a human GWAS for neuroticism and a joint human-pathogen GWAS for hepatitis C viral load, I show how HMP easily combines information to detect statistically significant signals among groups of individually nonsignificant hypotheses, enhancing the potential for scientific discovery. HMP and MAMML have broad implications for the analysis of large datasets by enabling model averaging for classical statistics."}, {"title": "Enhanced Pipeline 'MetaGaAP-Py' for the Analysis of Quasispecies and Non-Model Microbial Populations using Ultra-Deep 'Meta-barcode' Sequencing", "url": "https://www.biorxiv.org/content/early/2017/08/02/171520", "tag": "Bioinformatics", "abstract": "A pipeline developed to establish sequence identity and estimate abundance of non-model organisms (such as viral quasispecies) using customized ultra-deep sequence meta-barcodes has been modified to improve performance by re-development in the Python programming language. Redundant packages were removed and new features added. RAM and storage usage have been optimized to facilitate the computational speeds though coding optimizations and improved cross-platform compatibility. However, computational limits restrict the approach to barcodes spanning a maximum of 30 polymorphisms. The modified pipeline, MetaGaAP-Py, is available for download here: https://github.com/CNoune/IMG_pipelines."}, {"title": "FUN-LDA: A latent Dirichlet allocation model for predicting tissue-specific functional effects of noncoding variation", "url": "https://www.biorxiv.org/content/early/2017/08/02/069229", "tag": "Bioinformatics", "abstract": "We describe here a new method based on a latent Dirichlet allocation model for predicting functional effects of noncoding genetic variants in a cell type and tissue specific way (FUN-LDA) by integrating diverse epigenetic annotations for specific cell types and tissues from large scale epigenomics projects such as ENCODE and Roadmap Epigenomics. Using this unsupervised approach we predict tissue-specific functional effects for every position in the human genome. We demonstrate the usefulness of our predictions using several validation experiments. Using eQTL data from several sources, including the Genotype-Tissue Expression project, the Geuvadis project and TwinsUK cohort, we show that eQTLs in specific tissues tend to be most enriched among the predicted functional variants in relevant tissues in Roadmap. We further show how these integrated functional scores can be used to derive the most likely cell/tissue type causally implicated for a complex trait using summary statistics from genome-wide association studies, and estimate a tissue-based correlation matrix of various complex traits. We find large enrichment of heritability in functional components of relevant tissues for various complex traits, with FUN-LDA yielding the highest enrichment estimates relative to existing methods. Finally, using experimentally validated functional variants from the literature and variants possibly implicated in disease by previous studies, we rigorously compare FUN-LDA to state-of-the-art functional annotation methods such as GenoSkyline, ChromHMM, Segway, and IDEAS, and show that FUN-LDA has better prediction accuracy and higher resolution compared to these methods. In summary, we describe a new approach and perform rigorous comparisons with the most commonly used functional annotation methods, providing a valuable resource for the community interested in the functional annotation of noncoding variants. Scores for each position in the human genome and for each ENCODE/Roadmap tissue are available from http://www.columbia.edu/~ii2135/funlda.html."}, {"title": "New barcoded primers for efficient retrieval of cercozoan sequences in high-throughput environmental diversity surveys, with emphasis on worldwide biological soil crusts", "url": "https://www.biorxiv.org/content/early/2017/08/02/171611", "tag": "Bioinformatics", "abstract": "We describe the performance of a new metabarcoding approach to investigate the environmental diversity of a prominent group of widespread unicellular organisms, the Cercozoa. Cercozoa is an immensely large group of protists and although it may dominate in soil and aquatic ecosystems, its environmental diversity remains undersampled. We designed PCR primers targeting the hyper-variable region V4 of the small subunit ribosomal RNA (SSU or 18S) gene, which is the recommended barcode marker for Cercozoa. The length of the amplified fragment (ca. 350 bp) is suitable for Illumina MiSeq, the most cost-effective platform for molecular environmental surveys. We provide barcoded primers, an economical alternative to multiple libraries for multiplex sequencing of over a hundred samples. In silico, our primers matched 68% of the cercozoan sequences of the reference database and performed better than previously proposed new generation sequencing primers. In mountain grasslands soils and in biological soil crusts from a variety of climatic regions, we were able to detect cercozoan sequences encompassing nearly the whole range of the phylum. We obtained 901 OTUs at 97% similarity threshold from 26 samples, with ca. 50,000 sequences per site, and only 8% of non-cercozoan sequences. We could contribute to a further increase of the diversity of Cercozoa, since only 43% of the OTUs were 97-100% similar to any known sequence. Our study thus provides an advanced tool for cercozoan metabarcoding and to investigate their diversity and distribution in the environment."}, {"title": "Kernel multitask regression for toxicogenetics", "url": "https://www.biorxiv.org/content/early/2017/08/01/171298", "tag": "Bioinformatics", "abstract": "The development of high-throughput in vitro assays to study quantitatively the toxicity of chemical compounds on genetically characterized human-derived cell lines paves the way to predictive toxicogenetics, where one would be able to predict the toxicity of any particular compound on any particular individual. In this paper we present a machine learning-based approach for that purpose, kernel multitask regression (KMR), which combines chemical characterizations of molecular compounds with genetic and transcriptomic characterizations of cell lines to predict the toxicity of a given compound on a given cell line. We demonstrate the relevance of the method on the recent DREAM8 Toxicogenetics challenge, where it ranked among the best state-of-the-art models, and discuss the importance of choosing good descriptors for cell lines and chemicals."}, {"title": "QuimP - Analyzing transmembrane signalling in highly deformable cells", "url": "https://www.biorxiv.org/content/early/2017/08/01/171199", "tag": "Bioinformatics", "abstract": "Summary: Transmembrane signalling plays important physiological roles, with G protein-coupled cell surface receptors being particularly important therapeutic targets. Fluorescent proteins are widely used to study signalling, but the analysis of image time series can be challenging, in particular when changes in cell shape are involved. To this end we have developed QuimP software. QuimP semi-automatically tracks cell outlines, quantifies spatio-temporal patterns of fluorescence at the cell membrane, and tracks local shape deformations. QuimP is particularly useful for studying cell motility, for example in immune or cancer cells. Availability and Implementation: QuimP (http://warwick.ac.uk/quimp) consists of a set of Java plugins for Fiji/ImageJ (http://fiji.sc/) and can be easily installed through the Fiji Updater (http://warwick.ac.uk/quimp/wiki-pages/installation). It is compatible with Mac, Windows and Unix-based operating systems, requiring version >1.45 of Fiji/ImageJ and Java 8. QuimP is released as open source (https://github.com/CellDynamics/QuimP/) under an academic licence."}, {"title": "BGP: Branched Gaussian processes for identifying gene-specific branching dynamics in single cell data", "url": "https://www.biorxiv.org/content/early/2017/08/01/166868", "tag": "Bioinformatics", "abstract": "High-throughput single-cell gene expression experiments can be used to uncover branching dynamics in cell populations undergoing differentiation through use of pseudotime methods. We develop the branching Gaussian process (BGP), a non-parametric model that is able to identify branching dynamics for individual genes and provides an estimate of branching times for each gene with an associated credible region. We demonstrate the effectiveness of our method on both synthetic data and a published single-cell gene expression hematopoiesis study. The method requires prior information about pseudotime and global cellular branching for each cell but the probabilistic nature of the method means that it is robust to errors in these global branch labels and can be used to discover early branching genes which diverge before the inferred global cell branching. The code is open-source and available at https://github.com/ManchesterBioinference/BranchedGP ."}, {"title": "An algorithm for template-based prediction of secondary structures of individual RNA sequences", "url": "https://www.biorxiv.org/content/early/2017/08/01/171108", "tag": "Bioinformatics", "abstract": "While understanding the structure of RNA molecules is vital for deciphering their functions, determining RNA structures experimentally is exceptionally hard. At the same time, extant approaches to computational RNA structure prediction have limited applicability and reliability. In this paper we provide a method to solve a simpler yet still biologically relevant problem: prediction of secondary RNA structure using structure of different molecules as a template. Our method identifies conserved and unconserved subsequences within an RNA molecule. For conserved subsequences, the template structure is directly transferred into the generated structure and combined with de-novo predicted structure for the unconserved subsequences with low evolutionary conservation. The method also determines, when the generated structure is unreliable. The method is validated using experimentally identified structures. The accuracy of the method exceeds that of classical prediction algorithms and constrained prediction methods. This is demonstrated by comparison using large number of heterogeneous RNAs. The presented method is fast and robust, and useful for various applications requiring knowledge of secondary structures of individual RNA sequences."}, {"title": "gkm-DNN: efficient prediction using gapped k-mer features and deep neural networks", "url": "https://www.biorxiv.org/content/early/2017/07/31/170761", "tag": "Bioinformatics", "abstract": "How to extract informative features from genome sequence is a challenging issue. Gapped k-mers frequency vectors (gkm-fv) has been presented as a new type of features in the last few years. Coupled with support vector machine (gkm-SVM), gkm-fvs have been used to achieve an effective sequence-based prediction (e.g., transcription factor binding site prediction). However, the huge computation of a large kernel matrix prevents it from using large amount of data. To this end, we proposed a flexible and scalable framework gkm-DNN to achieve feature representation and prediction from high-dimensional gkm-fvs using deep neural networks (DNN). We first implemented an efficient method to calculate the gkm-fv of a given sequence. We then adopted a DNN model with gkm-fvs as input to achieve a prediction task. Here, we took the transcription factor binding site prediction as an illustrative application. We applied gkm-DNN onto 467 small and 69 big human ENCODE ChIP-seq datasets to demonstrate its performance and compared it with the state-of-the-art method gkm-SVM. We demonstrated that gkm-DNN can not only overcome the drawbacks of high dimensionality, colinearity and sparsity of gkm-fvs, but also make comparable overall performance and distinct better accuracy compared with gkm-SVM in much shorter time. Moreover, gkm-DNN can be easily adapted to other applications and combine different types of data using computational graphs."}, {"title": "PyBoost: A parallelized Python implementation of 2D boosting with hierarchies", "url": "https://www.biorxiv.org/content/early/2017/07/31/170803", "tag": "Bioinformatics", "abstract": "Gene expression is controlled by networks of transcription factors that bind specific sequence motifs in regulatory DNA elements such as promoters and enhancers. GeneClass is a boosting-based algorithm that learns gene regulatory networks from complementary paired feature sets such as transcription factor expression levels and binding motifs across conditions. This algorithm can be used to predict functional genomics measures of cell state, such as gene expression and chromatin accessibility, in different cellular conditions. We present a parallelized, Python-based implementation of GeneClass, called PyBoost, along with a novel hierarchical implementation of the algorithm, called HiBoost. HiBoost allows regulatory logic to be constrained to a hierarchical group of conditions or cell types. The software can be used to dissect differentiation cascades, time courses or other perturbation data that naturally form a hierarchy or trajectory. We demonstrate the application of PyBoost and HiBoost to learn regulators of tadpole tail regeneration and hematopoeitic stem cell differentiation and validate learned regulators through an inducible CRISPR system."}, {"title": "Protein classification using modified n-gram and skip-gram models", "url": "https://www.biorxiv.org/content/early/2017/07/31/170407", "tag": "Bioinformatics", "abstract": "Motivation: Classification by supervised machine learning greatly facilitates the annotation of protein characteristics from their primary sequence. However, the feature generation step in this process requires detailed knowledge of attributes used to classify the proteins. Lack of this knowledge risks the selection of irrelevant features, resulting in a faulty model. In this study, we introduce a means of automating the work-intensive feature generation step via a Natural Language Processing (NLP) dependent model, using a modified combination of N-Gram and Skip-Gram models (m-NGSG). Results: A meta-comparison of cross validation accuracy with twelve training datasets from nine different published studies demonstrate a consistent increase in accuracy of m-NGSG when compared to contemporary classification and feature generation models. We expect this model to accelerate the classification of proteins from primary sequence data and increase the accessibility of protein prediction to a broader range of scientists. Availability: m-NGSG is freely available at Bitbucket: https://bitbucket.org/sm islam/mngsg/src."}, {"title": "Inferring proteome dynamics during yeast cell cycle using gene expression data", "url": "https://www.biorxiv.org/content/early/2017/07/31/170332", "tag": "Bioinformatics", "abstract": "Protein levels are most relevant physiologically, but measuring them genome-wide remains a challenge. In contrast, mRNA levels are much easier and less expensive to measure globally. Therefore, RNA levels are typically used to infer the corresponding protein levels. The steady-state condition (assumption that protein levels remain constant) is typically used to calculate protein abundances, as it is mathematically very convenient, even though it is often clear that it is not satisfied for proteins of interest. Here, we propose a simple, yet very effective, method to estimate genome wide protein abundances, which does not require the assumption that protein levels remain constant, and thus allows us to also predict proteome dynamics. Instead, we assume that the system returns to the baseline at the end of experiments; such an assumption is satisfied in many time-course experiments and in all periodic conditions (e.g. cell cycle). The approach requires availability of time-course gene expression data and only three protein abundance measurements, for organisms for which half-life data is not available. For organisms for which genome-wide half-life data is available, no protein abundance measurements are required. As proof-of-concept, we calculated the predicted proteome dynamics for the budding yeast proteome during the cell cycle, which can be conveniently browsed online. The approach was validated experimentally by verifying that the predicted protein concentration changes were consistent with measurements for all proteins tested. Additionally, if proteomic data are also available, our approach can be used to predict how half-lives change in response to posttranslational regulation. We illustrated this application of our method with de novo prediction of changes in the degradation rate of Clb2 in response to post-translational modifications. The predicted changes were consistent with earlier observations in the literature."}, {"title": "Automatic OpenAPI to Bio.tools Conversion", "url": "https://www.biorxiv.org/content/early/2017/07/30/170274", "tag": "Bioinformatics", "abstract": "Computation has become a central component of life sciences research. Making computational services FAIR has had a strong interest from the life sciences community in the past 15 years. Admittedly, uptake of any of the developed solutions has been limited, and the existence of multiple approaches will not have helped. Interoperability of solution may be essential. This work introduces an interoperability layer between two approaches for FAIR annotation of web services: OpenAPI and bio.tools."}, {"title": "miRNAtools: advanced training using the miRNA web of knowledge", "url": "https://www.biorxiv.org/content/early/2017/07/30/170126", "tag": "Bioinformatics", "abstract": "miRNAs are small non-coding RNAs, that act as negative regulators of the genomic output. Their intrinsic importance within cell biology and human disease is well known. Their mechanism of action based on the base pairing binding to their cognate targets, have helped the development of many computer applications for the prediction of miRNA target recognition. More recently, many other computer applications have appeared with the objective of studying miRNA roles in many contexts, trying to dissect and predict their functions in a specific biological process. Learning about miRNA function needs a practical training in the use of specific computer and web-based applications that are complementary to the wet-lab studies. In the last seven years we have been involved in the organization of advanced training courses about the in silico functional analysis of miRNAs and non-coding RNAs, for students ranging from the post-graduate to the post-doctoral level. In order to guide the learning process about miRNAs, we have created miRNAtools (http://mirnatools.eu), a web repository of miRNA tools and tutorials. This page is a compilation of tools to analyze miRNAs and their regulatory action that intends to collect and organize the information that is dispersed in the web. The miRNAtools webpage is completed by a collection of tutorials that can be used by students and tutors engaged in advanced training courses. The tutorials follow the rationale of the analysis of the function of selected miRNAs, starting from their nomenclature and genomic localization and finishing by assessing their involvement in specific cellular functions."}, {"title": "EMHP: An accurate automated hole masking algorithm for single-particle cryo-EM image processing", "url": "https://www.biorxiv.org/content/early/2017/07/28/154211", "tag": "Bioinformatics", "abstract": "The Electron Microscopy Hole Punch (EMHP) is a streamlined suite of tools for quick assessment, sorting, and hole masking of electron micrographs. With recent advances in single-particle electron cryo-microscopy (cryo-EM) data processing allowing for the rapid determination of protein structures using a smaller computational footprint, we saw the need for a fast and simple tool for data pre-processing that could run independent of existing high-performance computing (HPC) infrastructures. EMHP provides a data preprocessing platform in a small package that requires minimal python dependencies to function. Availability: https://www.bitbucket.org/chazbot/emhp Apache 2.0 License"}, {"title": "riboWaltz: optimization of ribosome P-site positioning in ribosome profiling data", "url": "https://www.biorxiv.org/content/early/2017/07/28/169862", "tag": "Bioinformatics", "abstract": "Ribosome profiling is a powerful technique used to study translation at the genome-wide level, generating unique information concerning ribosome positions along RNAs. Optimal localization of ribosomes requires the proper identification of the ribosome P-site in each ribosome protected fragment, a crucial step to determine trinucleotide periodicity of translating ribosomes, and draw correct conclusions concerning where ribosomes are located. To determine the P-site within ribosome footprints at nucleotide resolution, the precise estimation of its offset with respect to the protected fragment is necessary. Here we present riboWaltz, an R package for calculation of optimal P-site offsets, diagnostic analysis and visual inspection of data. Compared to existing tools, riboWaltz shows improved accuracies for P-site estimation and neat ribosome positioning in multiple case studies. Availability and Implementation: riboWaltz was implemented in R and is available at https://github.com/LabTranslationalArchitectomics/RiboWaltz."}, {"title": "PhaMers identifies novel bacteriophage sequences from thermophilic hot springs", "url": "https://www.biorxiv.org/content/early/2017/07/28/169672", "tag": "Bioinformatics", "abstract": "Metagenomic sequencing approaches have become popular for the purpose of dissecting environmental microbial diversity, leading to the characterization of novel microbial lineages. In addition of bacterial and fungal genomes, metagenomic analysis can also reveal genomes of viruses that infect microbial cells. Because of their small genome size and limited knowledge of phage diversity, discovering novel phage sequences from metagenomic data is often challenging. Here we describe PhaMers (Phage k-Mers), a phage identification tool that uses supervised learning to classify metagenomic contigs as phage or non-phage on the basis of tetranucleotide frequencies, a technique that does not depend on existing gene annotations. PhaMers compares the tetranucleotide frequencies of metagenomic contigs to phage and bacteria references from online databases, resulting in assignments of lower level phage taxonomy based on sequence similarity. Using PhaMers, we identified 103 novel phage sequences from hot spring samples of Yellowstone National Park based on data generated from a microfluidic-based mini-metagenomic approach. We analyzed assembled contigs over 5 kbp in length using PhaMers and compared the results with those generated by VirSorter, a publicly available phage identification and annotation package. We analyzed the performance of phage genome prediction and taxonomic classification using PhaMers, and presented putative hosts and taxa for some of the novel phage sequences. Finally, mini-metagenomic occurrence profiles of phage and prokaryotic genomes were used to verify putative hosts."}, {"title": "HPCDb: an integrated database of pancreatic cancer", "url": "https://www.biorxiv.org/content/early/2017/07/28/169771", "tag": "Bioinformatics", "abstract": "We have established a database of Human Pancreatic Cancer (HPCDb) through effectively mining, extracting, analyzing, and integrating PC-related genes, single-nucleotide polymorphisms (SNPs), and microRNAs (miRNAs), now available online at http://www.pancancer.org/. Data were extracted from established databases, \u22655 published literature (PubMed), and microarray chips (screening of differentially expressed genes using limma package in R, |log2 fold change (FC)| > 1). Further, protein-protein interactions (PPIs) were investigated through the Human Protein Reference Database. miRNA-target relationships were also identified using the online software TargetScan. Currently, HPCDb contains 3284 genes, 120 miRNAs, 589 SNPs, 10,139 PPIs, and 3904 miRNA-target pairs. The detailed information on PC-related genes (e.g., gene identifier (ID), symbol, synonyms, full name, chip sets, expression alteration, PubMed ID, and PPIs), miRNAs (e.g., accession number, chromosome location, related disease, PubMed ID, and miRNA-target interactions), and SNPs (e.g., SNP ID, allele, gene, PubMed ID, chromosome location, and disease) is presented through user-friendly query interfaces or convenient links to NCBI GEO, NCBI PubMed, NCBI Gene, NCBI dbSNP, and miRBase. Overall, HPCDb provides biologists with relevant information on human PC-related molecules at multiple levels, helping to generate new hypotheses or identify candidate markers."}, {"title": "Accurate detection of complex structural variations using single molecule sequencing", "url": "https://www.biorxiv.org/content/early/2017/07/28/169557", "tag": "Bioinformatics", "abstract": "Structural variations (SVs) are the largest source of genetic variation, but remain poorly understood because of limited genomics technology. Single molecule long read sequencing from Pacific Biosciences and Oxford Nanopore has the potential to dramatically advance the field, although their high error rates challenge existing methods. Addressing this need, we introduce open-source methods for long read alignment (NGMLR, https://github.com/philres/ngmlr) and SV identification (Sniffles, https://github.com/fritzsedlazeck/Sniffles) that enable unprecedented SV sensitivity and precision, including within repeat-rich regions and of complex nested events that can have significant impact on human disorders. Examining several datasets, including healthy and cancerous human genomes, we discover thousands of novel variants using long reads and categorize systematic errors in short-read approaches. NGMLR and Sniffles are further able to automatically filter false events and operate on low amounts of coverage to address the cost factor that has hindered the application of long reads in clinical and research settings."}, {"title": "Beyond the Hype: Deep Neural Networks Outperform Established Methods Using A ChEMBL Bioactivity Benchmark Set", "url": "https://www.biorxiv.org/content/early/2017/07/28/168914", "tag": "Bioinformatics", "abstract": "The increase of publicly available bioactivity data in recent years has fueled and catalyzed research in chemogenomics, data mining, and modeling approaches. As a direct result, over the past few years a multitude of different methods have been reported and evaluated, such as target fishing, nearest neighbor similarity-based methods, and Quantitative Structure Activity Relationship (QSAR)-based protocols. However, such studies are typically conducted on different datasets, using different validation strategies, and different metrics. In this study, different methods were compared using one single standardized dataset obtained from ChEMBL, which is made available to the public, using standardized metrics (BEDROC and Matthews Correlation Coefficient). Specifically, the performance of Naive Bayes, Random Forests, Support Vector Machines, Logistic Regression, and Deep Neural Networks was assessed using QSAR and proteochemometric (PCM) methods. All methods were validated using both a random split validation and a temporal validation, with the latter being a more realistic benchmark of expected prospective execution. Deep Neural Networks are the top performing classifiers, highlighting the added value of Deep Neural Networks over other more conventional methods. Moreover, the best method (\"DNN_PCM\") performed significantly better at almost one standard deviation higher than the mean performance. Furthermore, Multi task and PCM implementations were shown to improve performance over single task Deep Neural Networks. Conversely, target prediction performed almost two standard deviations under the mean performance. Random Forests, Support Vector Machines, and Logistic Regression performed around mean performance. Finally, using an ensemble of DNNs, alongside additional tuning, enhanced the relative performance by another 27% (compared with unoptimized DNN_PCM). Here, a standardized set to test and evaluate different machine learning algorithms in the context of multitask learning is offered by providing the data and the protocols."}, {"title": "PinAPL-Py: A comprehensive web-application for the analysis of CRISPR/Cas9 screens", "url": "https://www.biorxiv.org/content/early/2017/07/27/147462", "tag": "Bioinformatics", "abstract": "Large-scale genetic screens using CRISPR/Cas9 technology have emerged as a major tool for functional genomics. With its increased popularity, experimental biologists frequently acquire large sequencing datasets for which they often do not have an easy analysis option. While a few bioinformatic tools have been developed for this purpose, their utility is still hindered either due to limited functionality or the requirement of bioinformatic expertise. To make sequencing data analysis of CRISPR/Cas9 screens more accessible to a wide range of scientists, we developed a Platform-independent Analysis of Pooled Screens using Python (PinAPL-Py), which is operated as an intuitive web-service. PinAPL-Py implements state-of-the-art tools and statistical models, assembled in a comprehensive workflow covering sequence quality control, automated sgRNA sequence extraction, alignment, sgRNA enrichment/depletion analysis and gene ranking. The workflow is set up to use a variety of popular sgRNA libraries as well as custom libraries that can be easily uploaded. Various analysis options are offered, suitable to analyze a large variety of CRISPR/Cas9 screening experiments. Analysis output includes ranked lists of sgRNAs and genes, and publication-ready plots. PinAPL-Py helps to advance genome-wide screening efforts by combining comprehensive functionality with user-friendly implementation. PinAPL-Py is freely accessible at http://pinapl-py.ucsd.edu with instructions, documentation and test datasets. The source code is available at https://github.com/LewisLabUCSD/PinAPL-Py"}, {"title": "Evaluation of machine learning methods to predict peptide binding to MHC Class I proteins", "url": "https://www.biorxiv.org/content/early/2017/07/27/154757", "tag": "Bioinformatics", "abstract": "Binding of peptides to Major Histocompatibility Complex (MHC) proteins is a critical step in immune response. Peptides bound to MHCs are recognized by CD8+ (MHC Class I) and CD4+ (MHC Class II) T-cells. Successful prediction of which peptides will bind to specific MHC alleles would benefit many cancer immunotherapy applications. Currently, supervised machine learning is the leading computational approach to predict peptide-MHC binding, and a number of methods, trained using results of binding assays, have been published. Many clinical researchers are dissatisfied with the sensitivity and specificity of currently available methods and the limited number of alleles for which they can be applied. We evaluated several recent methods to predict peptide-MHC Class I binding affinities and a new method of our own design (MHCnuggets). We used a high-quality benchmark set of 51 alleles, which has been applied previously. The neural network methods NetMHC, NetMHCpan, MHCflurry, and MHCnuggets achieved similar best-in-class prediction performance in our testing, and of these methods MHCnuggets was significantly faster. MHCnuggets is a gated recurrent neural network, and the only method to our knowledge which can handle peptides of any length, without artificial lengthening and shortening. Seventeen alleles were problematic for all tested methods. Prediction difficulties could be explained by deficiencies in the training and testing examples in the benchmark, suggesting that biological differences in allele-specific binding properties are not as important as previously claimed. Advances in accuracy and speed of computational methods to predict peptide-MHC affinity are urgently needed. These methods will be at the core of pipelines to identify patients who will benefit from immunotherapy, based on tumor-derived somatic mutations. Machine learning methods, such as MHCnuggets, which efficiently handle peptides of any length will be increasingly important for the challenges of predicting immunogenic response for MHC Class II alleles."}, {"title": "Drug Target Ontology to Classify and Integrate Drug Discovery Data", "url": "https://www.biorxiv.org/content/early/2017/07/27/117564", "tag": "Bioinformatics", "abstract": "Background: One of the most successful approaches to develop new small molecule therapeutics has been to start from a validated druggable protein target. However, only a small subset of potentially druggable targets has attracted significant research and development resources. The Illuminating the Druggable Genome (IDG) project develops resources to catalyze the development of likely targetable, yet currently understudied prospective drug targets. A central component of the IDG program is a comprehensive knowledge resource of the druggable genome. Results: As part of that effort, we have been developing a framework to integrate, navigate, and analyze drug discovery data based on formalized and standardized classifications and annotations of druggable protein targets, the Drug Target Ontology (DTO). DTO was constructed by extensive curation and consolidation of various resources. DTO classifies the four major drug target protein families, GPCRs, kinases, ion channels and nuclear receptors, based on phylogenecity, function, target development level, disease association, tissue expression, chemical ligand and substrate characteristics, and target-family specific characteristics. The formal ontology was built using a new software tool to auto-generate most axioms from a database while also supporting manual knowledge acquisition. A modular, hierarchical implementation facilitates development and maintenance and makes use of various external ontologies, thus integrating the DTO into the ecosystem of biomedical ontologies. As a formal OWL-DL ontology, DTO contains asserted and inferred axioms. Modeling data from the Library of Integrated Network-based Cellular Signatures (LINCS) program illustrates the potential of DTO for contextual data integration and nuanced definition of important drug target characteristics. DTO has been implemented in the IDG user interface Portal, Pharos and the TIN-X explorer of protein target disease relationships. Conclusions: DTO was built based on the need for a formal semantic model for druggable targets including various related information such as protein, gene, protein domain, protein structure, binding site, small molecule drug, mechanism of action, protein tissue localization, disease association, and many other types of information. DTO will further facilitate the otherwise challenging integration and formal linking to biological assays, phenotypes, disease models, drug poly-pharmacology, binding kinetics and many other processes, functions and qualities that are at the core of drug discovery. The first version of DTO is publically available via the website http://drugtargetontology.org/, Github (https://github.com/DrugTargetOntology/DTO), and the NCBO Bioportal (https://bioportal.bioontology.org/ontologies/DTO). The long-term goal of DTO is to provide such an integrative framework and to populate the ontology with this information as a community resource."}, {"title": "Designing an intuitive web application for drug discovery scientists", "url": "https://www.biorxiv.org/content/early/2017/07/27/169193", "tag": "Bioinformatics", "abstract": "Although a scientific web application that is intuitive can help scientists utilize data more easily and advance their research, there is little guidance on how to design such an application in the academic literature. We discuss how we designed an intuitive application for bench scientists working in drug discovery following an approach that can be applied to the design and development of scientific resources in a broad range of disciplines."}, {"title": "Beyond Homology Transfer: Deep Learning for Automated Annotation of Proteins", "url": "https://www.biorxiv.org/content/early/2017/07/27/168120", "tag": "Bioinformatics", "abstract": "Accurate annotation of protein functions is important for a profound understanding of molecular biology. A large number of proteins remain uncharacterized because of the sparsity of available supporting information. For a large set of uncharacterized proteins, the only type of information available is their amino acid sequence. In this paper, we propose DeepSeq -- a deep learning architecture -- that utilizes only the protein sequence information to predict its associated functions. The prediction process does not require handcrafted features; rather, the architecture automatically extracts representations from the input sequence data. Results of our experiments with DeepSeq indicate significant improvements in terms of prediction accuracy when compared with other sequence-based methods. Our deep learning model achieves an overall validation accuracy of 86.72%, with an F1 score of 71.13%. Moreover, using the automatically learned features and without any changes to DeepSeq, we successfully solved a different problem i.e. protein function localization, with no human intervention. Finally, we discuss how this same architecture can be used to solve even more complicated problems such as prediction of 2D and 3D structure as well as protein-protein interactions."}, {"title": "\"Opposite-of\"-information improves similarity calculations in phenotype ontologies", "url": "https://www.biorxiv.org/content/early/2017/07/27/108977", "tag": "Bioinformatics", "abstract": "One of the most important use cases of ontologies is the calculation of similarity scores between a query and items annotated with classes of an ontology. The hierarchical structure of an ontology does not necessarily reflect all relevant aspects of the domain it is modelling, and this can reduce the performance of ontology-based search algorithms. For instance, the classes of phenotype ontologies may be arranged according to anatomical criteria, but individual phenotypic features may affect anatomic entities in opposite ways. Thus, \"opposite\" classes may be located in close proximity in an ontology; for example enlarged liver and small liver are grouped under abnormal liver size. Using standard similarity measures, these would be scored as being similar, despite in fact being opposites. In this paper, we use information about opposite ontology classes to extend two large phenotype ontologies, the human and the mammalian phenotype ontology. We also show that this information can be used to improve rankings based on similarity measures that incorporate this information. In particular, cosine similarity based measures show large improvements. We hypothesize this is due to the natural embedding of opposite phenotypes in vector space. We support the idea that the expressivity of semantic web technologies should be explored more extensively in biomedical ontologies and that similarity measures should be extended to incorporate more than the pure graph structure defined by the subclass or part-of relationships of the underlying ontologies."}, {"title": "DiGeST: Distributed Computing for Scalable Gene and Variant Ranking with Hadoop/Spark", "url": "https://www.biorxiv.org/content/early/2017/07/27/168633", "tag": "Bioinformatics", "abstract": "Background: The advent of next-generation sequencing technologies has opened new avenues for clinical genomics research. In particular, as sequencing costs continue to decrease, an ever-growing number of clinical genomics institutes now rely on DNA sequencing studies at varying scales - genome, exome, mendeliome - for uncovering disease-associated variants or genes, in both rare and non-rare diseases. A common methodology for identifying such variants or genes is to rely on genetic association studies (GAS), that test whether allele or genotype frequencies differ between two groups of individuals, usually diseased subjects and healthy controls. Current bioinformatics tools for performing GAS are designed to run on standalone machines, and do not scale well with the increasing size of study designs and the search for multi-locus genetic associations. More efficient distributed and scalable data analysis solutions are needed to address this challenge. Results: We developed a Big Data solution stack for distributing computations in genetic association studies, that address both single and multi-locus associations. The proposed stack, called DiGeST (Distributed Gene/variant Scoring Tool) is divided in two main components: a Hadoop/Spark high-performance computing back-end for efficient data storage and distributed computing, and a Web front-end providing users with a rich set of options to filter, compare and explore exome data from different sample populations. Using exome data from the 1000 Genomes Project, we show that our distributed implementation smoothly scales with computing resources. We make the resulting software stack Open-Source, and provide virtualisation scripts to run the complete environment both on standalone machine or Hadoop-based cluster. Conclusions: Hadoop/Spark provides a powerful and well-suited distributed computing framework for genetic association studies. Our work illustrates the flexibility, ease of use and scalability of the framework, and more generally advocates for its wider adoption in bioinformatics pipelines."}, {"title": "A human-specific switch of alternatively spliced AFMID isoforms contributes to TP53 mutations and tumor recurrence in hepatocellular carcinoma", "url": "https://www.biorxiv.org/content/early/2017/07/26/169029", "tag": "Bioinformatics", "abstract": "Pre-mRNA splicing can contribute to the switch of cell identity that occurs in carcinogenesis. Here we analyze a large collection of RNA-Seq datasets and report that splicing changes in hepatocyte-specific enzymes, such as AFMID and KHK, are associated with HCC patients' survival and relapse. The switch of AFMID isoforms is an early event in HCC development, and is associated with driver mutations in TP53 and ARID1A. Finally, we show that the switch of AFMID isoforms is human-specific and not detectable in other species, including primates. The integrative analysis uncovers a mechanistic link between splicing switches, de novo NAD+ biosynthesis, driver mutations, and HCC recurrence."}, {"title": "Low-rank Similarity Matrix Optimization Identifies Subpopulation Structure and Orders Single Cells in Pseudotime", "url": "https://www.biorxiv.org/content/early/2017/07/26/168922", "tag": "Bioinformatics", "abstract": "Sequencing the transcriptomes of single cells has greatly advanced our understanding of the cellular composition of complex tissues. In many of these systems, the role of heterogeneity has risen to prominence as a determinant of cell type composition and lineage transitions. While much effort has gone into developing appropriate tools for the analysis and comprehension of single cell sequencing data, further advances are required. Optimization-based approaches are under-utilized in single cell analysis and hold much potential due to their ability to capture global properties of the system in low dimension. Here we present SoptSC: an optimization-based algorithm for the identification of subpopulation structure, transition paths, and pseudotemporal ordering within a cell population. Based on a measure of similarity between cells, SoptSC uses non-negative matrix factorization to create low dimensional representations of the data for analysis and visualization. We find that in several examples, the low-dimensional representations produced by SoptSC offer greater potential for insight than alternative methods. We tested our methods on a simulated dataset and four published single cell datasets from Homo sapiens and Mus musculus. SoptSC is able to recapitulate a simulated developmental trajectory with greater fidelity than com- parable methods. Applied to two datasets on early embryonic development, SoptSC recapitulates known trajectories with high accuracy. Analysis of murine epidermis reveals overall agreement with previous studies, but differs markedly regarding the composition and heterogeneity of the basal compartment. Analysis of murine myelopoiesis found that SoptSC can resolve complex hematopoietic subpopulation composition, and led to a new prediction regarding the asynchronous development of myeloid subpopulations during stem cell differentiation."}, {"title": "Identifying Simultaneous Rearrangements in Cancer Genomes", "url": "https://www.biorxiv.org/content/early/2017/07/26/164855", "tag": "Bioinformatics", "abstract": "The traditional view of cancer evolution states that a cancer genome accumulates a sequential ordering of mutations over a long period of time. However, in recent years it has been suggested that a cancer genome may instead undergo a one-time catastrophic event, such as chromothripsis, where a large number of mutations instead occur simultaneously. A number of potential signatures of chromothripsis have been proposed. In this work we provide a rigorous formulation and analysis of the \"ability to walk the derivative chromosome\" signature originally proposed by Korbel and Campbell (2013). In particular, we show that this signature, as originally envisioned, may not always be present in a chromothripsis genome and we provide a precise quantification of under what circumstances it would be present. We also propose a variation on this signature, the H/T alternating fraction, which allows us to overcome some of the limitations of the original signature. We apply our measure to both simulated data and a previously analyzed real cancer dataset and find that the H/T alternating fraction may provide useful signal for distinguishing genomes having acquired mutations simultaneously from those acquired in a sequential fashion. An implementation of the H/T alternating fraction is available at https://bitbucket.org/oesperlab/ht-altfrac."}, {"title": "Objective, Quantitative, Data-Driven Assessment of Chemical Probes", "url": "https://www.biorxiv.org/content/early/2017/07/25/168369", "tag": "Bioinformatics", "abstract": "Chemical probes are essential tools for understanding biological systems and for target validation, yet selecting tools for biomedical research is largely biased and subjective. Here we describe the Probe Miner: Chemical Probes Objective Assessment resource - capitalising on the plethora of public medicinal chemistry data to empower quantitative, objective, Big Data-driven assessment of chemical probes. We assess >1.8m compounds for their suitability as chemical tools against 2,220 human targets and dissect their biases and limitations."}, {"title": "PhenoSpD: an integrated toolkit for phenotypic correlation estimation and multiple testing correction using GWAS summary statistics", "url": "https://www.biorxiv.org/content/early/2017/07/25/148627", "tag": "Bioinformatics", "abstract": "Background: Identifying phenotypic correlations between complex traits and diseases can provide useful etiological insights. Restricted access to individual-level phenotype data makes it difficult to estimate large-scale phenotypic correlation across the human phenome. State-of-the-art methods, metaCCA and LD score regression, provide an alternative approach to estimate phenotypic correlation using genome-wide association study (GWAS) summary statistics. Results: Here, we present an integrated R toolkit, PhenoSpD, to 1) apply metaCCA (or LD score regression) to estimate phenotypic correlations using GWAS summary statistics; and 2) to utilize the estimated phenotypic correlations to inform correction of multiple testing for complex human traits using the spectral decomposition of matrices (SpD). The simulations suggest it is possible to estimate phenotypic correlation using samples with only a partial overlap, but as overlap decreases correlations will attenuate towards zero and multiple testing correction will be more stringent than in perfectly overlapping samples. In a case study, PhenoSpD using GWAS results suggested 324.4 independent tests among 452 metabolites, which is close to the 296 independent tests estimated using true phenotypic correlation. We further applied PhenoSpD to estimated 7,503 pair-wise phenotypic correlations among 123 metabolites using GWAS summary statistics from Kettunen et al. and PhenoSpD suggested 44.9 number of independent tests for theses metabolites. Conclusion: PhenoSpD integrates existing methods and provides a simple and conservative way to reduce dimensionality for complex human traits using GWAS summary statistics, which is particularly valuable for post-GWAS analysis of complex molecular traits. Availability: R code and documentation for PhenoSpD V1.0.0 is available online (https://github.com/MRCIEU/PhenoSpD)."}, {"title": "Computational proteogenomic identification and functional interpretation of translated fusions and micro structural variations in cancer", "url": "https://www.biorxiv.org/content/early/2017/07/25/168377", "tag": "Bioinformatics", "abstract": "Rapid advancement in high throughput genome and transcriptome sequencing (HTS) and mass spectrometry (MS) technologies has enabled the acquisition of the genomic, transcriptomic and proteomic data from the same tissue sample.In this paper we introduce a novel computational framework which can integratively analyze all three types of omics data to obtain a complete molecular profile of a tissue sample, in normal and disease conditions.Our framework includes MiStrVar, an algorithmic method we developed to identify micro structural variants (microSVs) on genomic HTS data. Coupled with deFuse, a popular gene fusion detection method we developed earlier, MiStrVar can provide an accurate profile of structurally aberrant transcripts in cancer samples.Given the breakpoints obtained by MiStrVar and deFuse, our framework can then identify all relevant peptides that span the breakpoint junctions and match them with unique proteomic signatures in the respective proteomics data sets. Our framework's ability to observe structural aberrations at three levels of omics data provides means of validating their presence. We have applied our framework to all The Cancer Genome Atlas (TCGA) breast cancer Whole Genome Sequencing (WGS) and/or RNA-Seq data sets, spanning all four major subtypes, for which proteomics data from Clinical Proteomic Tumor Analysis Consortium (CPTAC) have been released.A recent study on this dataset focusing on SNVs has reported many that lead to novel peptides.Complementing and significantly broadening this study, we detected 244 novel peptides from 432 candidate genomic or transcriptomic sequence aberrations. Many of the fusions and microSVs we discovered have not been reported in the literature.Interestingly, the vast majority of these translated aberrations (in particular, fusions) were private, demonstrating the extensive inter-genomic heterogeneity present in breast cancer.Many of these aberrations also have matching out-of-frame downstream peptides, potentially indicating novel protein sequence and structure. Moreover, the most significantly enriched genes involved in translated fusions are cancer-related.Furthermore a number of the somatic, translated microSVs are observed in tumor suppressor genes."}, {"title": "FaStore - a space-saving solution for raw sequencing data", "url": "https://www.biorxiv.org/content/early/2017/07/25/168096", "tag": "Bioinformatics", "abstract": "The affordability of DNA sequencing has led to the generation of unprecedented volumes of raw sequencing data. These data must be stored, processed, and transmitted, which poses significant challenges. To facilitate this effort, we introduce FaStore, a specialized compressor for FASTQ files. The proposed algorithm does not use any reference sequences for compression, and permits the user to choose from several lossy modes to improve the overall compression ratio, depending on the specific needs. We demonstrate through extensive simulations that FaStore achieves a significant improvement in compression ratio with respect to previously proposed algorithms for this task. In addition, we perform an analysis on the effect that the different lossy modes have on variant calling, the most widely used application for clinical decision making, especially important in the era of precision medicine. We show that lossy compression can offer significant compression gains, while preserving the essential genomic information and without affecting the variant calling performance."}, {"title": "A direct approach to estimating false discovery rates conditional on covariates", "url": "https://www.biorxiv.org/content/early/2017/07/25/035675", "tag": "Bioinformatics", "abstract": "Modern scientific studies from many diverse areas of research abound with multiple hypothesis testing concerns. The false discovery rate is one of the most commonly used error rates for measuring and controlling rates of false discoveries when performing multiple tests. Adaptive false discovery rates rely on an estimate of the proportion of null hypotheses among all the hypotheses being tested. This proportion is typically estimated once for each collection of hypotheses. Here we propose a regression framework to estimate the proportion of null hypotheses conditional on observed covariates. This may then be used as a multiplication factor with the Benjamini-Hochberg adjusted p-values, leading to a plug-in false discovery rate estimator. We provide both finite sample and asymptotic conditions under which this covariate-adjusted estimate is conservative - leading to appropriately conservative false discovery rate estimates. Our case study concerns a genome-wise association meta-analysis which considers associations with body mass index. In our framework, we are able to use the sample sizes for the individual genomic loci and the minor allele frequencies as covariates. We further evaluate our approach via a number of simulation scenarios."}, {"title": "Characterizing and Managing Missing Structured Data in Electronic Health Records", "url": "https://www.biorxiv.org/content/early/2017/07/24/167858", "tag": "Bioinformatics", "abstract": "Missing data is a challenge for all studies; however, this is especially true for electronic health record (EHR) based analyses. Failure to appropriately consider missing data can lead to biased results. Here, we provide detailed procedures for when and how to conduct imputation of EHR data. We demonstrate how the mechanism of missingness can be assessed, evaluate the performance of a variety of imputation methods, and describe some of the most frequent problems that can be encountered. We analyzed clinical lab measures from 602,366 patients in the Geisinger Health System EHR. Using these data, we constructed a representative set of complete cases and assessed the performance of 12 different imputation methods for missing data that was simulated based on 4 mechanisms of missingness. Our results show that several methods including variations of Multivariate Imputation by Chained Equations (MICE) and softImpute consistently imputed missing values with low error; however, only a subset of the MICE methods were suitable for multiple imputation. The analyses described provide an outline of considerations for dealing with missing EHR data, steps that researchers can perform to characterize missingness within their own data, and an evaluation of methods that can be applied to impute clinical data. While the performance of methods may vary between datasets, the process we describe can be generalized to the majority of structured data types that exist in EHRs and all of our methods and code are publicly available."}, {"title": "Best Practice Data Life Cycle Approaches for the Life Sciences", "url": "https://www.biorxiv.org/content/early/2017/07/24/167619", "tag": "Bioinformatics", "abstract": "Throughout history, the life sciences have been revolutionised by technological advances; in our era this is manifested by advances in instrumentation for data generation, and consequently researchers now routinely handle large amounts of heterogeneous data in digital formats. The simultaneous transitions towards biology as a data science and towards a 'life cycle' view of research data pose new challenges. Researchers face a bewildering landscape of data management requirements, recommendations and regulations, without necessarily being able to access data management training or possessing a clear understanding of practical approaches that can assist in data management in their particular research domain. Here we provide an overview of best practice data life cycle approaches for researchers in the life sciences/bioinformatics space with a particular focus on 'omics' datasets and computer-based data processing and analysis. We discuss the different stages of the data life cycle and provide practical suggestions for useful tools and resources to improve data management practices."}, {"title": "Grid-based computational methods for the design of constraint-based parsimonious chemical reaction networks to simulate metabolite production: GridProd", "url": "https://www.biorxiv.org/content/early/2017/07/24/166777", "tag": "Bioinformatics", "abstract": "Constraint-based metabolic flux analysis of knockout strategies is an efficient method to simulate the production of useful metabolites in microbes. Owing to the recent development of technologies for artificial DNA synthesis, it may become important in the near future to mathematically design minimum metabolic networks to simulate metabolite production. Accordingly, we have developed a computational method where parsimonious metabolic flux distribution is computed for designated constraints on growth and production rates which are represented by grids. When the growth rate of this obtained parsimonious metabolic network is maximized, higher production rates compared to those noted using existing methods are observed for many target metabolites. The set of reactions used in this parsimonious flux distribution consists of reactions included in the original genome scale model iAF1260. The computational experiments show that the grid size affects the obtained production rates. Under the conditions that the growth rate is maximized and the minimum cases of flux variability analysis are considered, the developed method produced more than 90% of metabolites, while the existing methods produced less than 50%. Mathematical explanations using examples are provided to demonstrate potential reasons for the ability of the proposed algorithm to identify design strategies that the existing methods could not identify. The source code is freely available, and is implemented in MATLAB and COBRA toolbox."}, {"title": "beachmat: a Bioconductor C++ API for accessing single-cell genomics data from a variety of R matrix types", "url": "https://www.biorxiv.org/content/early/2017/07/24/167445", "tag": "Bioinformatics", "abstract": "Recent advances in single-cell RNA sequencing have dramatically increased the number of cells that can be profiled in a single experiment. This provides unparalleled resolution to study cellular heterogeneity within biological processes such as differentiation. However, the explosion of data that are generated from such experiments poses a challenge to the existing computational infrastructure for statistical data analysis. In particular, large matrices holding expression values for each gene in each cell require sparse or file-backed representations for manipulation with the popular R programming language. Here, we describe a C++ interface named beachmat, which enables agnostic data access from various matrix representations. This allows package developers to write efficient C++ code that is interoperable with simple, sparse and HDF5-backed matrices, amongst others. We perform simulations to examine the performance of beachmat on each matrix representation, and we demonstrate how beachmat can be incorporated into the code of other packages to drive analyses of a very large single-cell data set."}, {"title": "Temporal expression divergence of network modules", "url": "https://www.biorxiv.org/content/early/2017/07/24/167734", "tag": "Bioinformatics", "abstract": "Here we propose new module-based approaches to identify differentially regulated network sub-modules combining temporal trajectories of expression profiles with static network skeletons. Starting from modules identified by network clustering of static networks, our analysis refines pre-defined genesets by partitioning them into smaller homogeneous sets by non-paramettric Bayesian methods. Especially for case-control time series data we developed multi-time point discriminative models and identified each network module as a mixture or admixture of dynamic discriminative functions. Our results shows that our proposed approach outperformed existing geneset enrichment methods in simulation studies. Moreover we applied the methods to neural stem cell differentiation data, and discovered novel modules differentially perturbed in different developmental stages."}, {"title": "Prominent Features Of The Amino Acid Mutation Landscape In Cancer", "url": "https://www.biorxiv.org/content/early/2017/07/24/136002", "tag": "Bioinformatics", "abstract": "Cancer can be viewed as a set of different diseases with distinctions based on tissue origin, driver mutations, and genetic signatures. Accordingly, each of these distinctions have been used to classify cancer subtypes and to reveal common features. Here, we present a different analysis of cancer based on amino acid mutation signatures. Non-negative Matrix Factorization and principal component analysis of 29 cancers revealed six amino acid mutation signatures, including four signatures that were dominated by either arginine to histidine (Arg>His) or glutamate to lysine (Glu>Lys) mutations. Sample-level analyses reveal that while some cancers are heterogeneous, others are largely dominated by one type of mutation. Using a non-overlapping set of samples from the COSMIC somatic mutation database, we validate five of six mutation signatures, including signatures with prominent arginine to histidine (Arg>His) or glutamate to lysine (Glu>Lys) mutations. This suggests that our classification of cancers based on amino acid mutation patterns may provide avenues of inquiry pertaining to specific protein mutations that may generate novel insights into cancer biology."}, {"title": "SNP genotyping and parameter estimation in polyploids using low-coverage sequencing data", "url": "https://www.biorxiv.org/content/early/2017/07/24/120261", "tag": "Bioinformatics", "abstract": "Motivation: Genotyping and parameter estimation using high throughput sequencing data are everyday tasks for population geneticists, but methods developed for diploids are typically not applicable to polyploid taxa. This is due to their duplicated chromosomes, as well as the complex patterns of allelic exchange that often accompany whole genome duplication (WGD) events. For WGDs within a single lineage (autopolyploids), inbreeding can result from mixed mating and/or double reduction. For WGDs that involve hybridization (allopolyploids), alleles are typically inherited through independently segregating subgenomes. Results: We present two new models for estimating genotypes and population genetic parameters from genotype likelihoods for auto- and allopolyploids. We then use simulations to compare these models to existing approaches at varying depths of sequencing coverage and ploidy levels. These simulations show that our models typically have lower levels of estimation error for genotype and parameter estimates, especially when sequencing coverage is low. Finally, we also apply these models to two empirical data sets from the literature. Overall, we show that the use of genotype likelihoods to model non-standard inheritance patterns is a promising approach for conducting population genomic inferences in polyploids. Availability: A C++ program, EBG, is provided to perform inference using the models we describe. It is available under the GNU GPLv3 on GitHub: https://github.com/pblischak/polyploid-genotyping."}, {"title": "Correcting for batch effects in case-control microbiome studies", "url": "https://www.biorxiv.org/content/early/2017/07/24/165910", "tag": "Bioinformatics", "abstract": "High-throughput data generation platforms, like mass-spectrometry, microarrays, and second-generation sequencing are susceptible to batch effects due to run-to-run variation in reagents, equipment, protocols, or personnel. Currently, batch correction methods are not commonly applied to microbiome sequencing datasets. In this paper, we compare multiple batch-correction methods applied to microbiome case-control studies. We introduce a model-free normalization procedure where features (i.e. bacterial taxa) in case samples are converted to percentiles of the equivalent features in control samples within a study prior to pooling data across studies. We look at how this percentile-normalization method compares to ComBat, a widely used batch-correction model developed for RNA microarray data, and traditional meta-analysis methods for combining independent p-values. Overall, we show that percentile-normalization is a simple, model-free approach for removing batch effects and improving sensitivity in case-control meta-analyses."}, {"title": "Splatter: Simulation Of Single-Cell RNA Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/07/24/133173", "tag": "Bioinformatics", "abstract": "As single-cell RNA sequencing technologies have rapidly developed, so have analysis methods. Many methods have been tested, developed and validated using simulated datasets. Unfortunately, current simulations are often poorly documented, their similarity to real data is not demonstrated, or reproducible code is not available. Here we present the Splatter Bioconductor package for simple, reproducible and well-documented simulation of single-cell RNA-seq data. Splatter provides an interface to multiple simulation methods including Splat, our own simulation, based on a gamma-Poisson distribution. Splat can simulate single populations of cells, populations with multiple cell types or differentiation paths."}, {"title": "TruePaiR: Software for the Accurate Identification of Complementary piRNA Read Pairs in High-Throughput Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/07/23/167452", "tag": "Bioinformatics", "abstract": "piRNAs and their biogenesis pathways are well-conserved in Metazoans (Grimson et al. 2008). piRNAs have been implicated in transcriptional, post-transcriptional, and translational regulation (Grivna et al. 2006; Lin & Yin 2008; Brennecke et al. 2008; Brennecke et al. 2007; Aravin et al. 2007). We analyze the signatures of a critical process in the primary and secondary mechanism of piRNA biogenesis, referred to as the amplification loop. The presence of U-1 and A-10 bias within piRNA populations is an indicator, but not an absolute measure of piRNA amplification. By further considering imperfect and perfect sequence complementarity within the first ten base pairs of piRNAs, the active site promoting secondary piRNA biogenesis, we developed practical and statistically powerful metrics to observe relative piRNA amplification. TruePaiR is a fast and effective general software tool to assess the relative utilization of piRNA amplification in high throughput sRNA sequencing data. The results of TruePaiR runs in seven species and five tissues serve as a benchmark for meaningful context of piRNA amplification. The TruePaiR metrics provide foundational data regarding the in terms of species specificity, tissue specificity, as well as the relative participation based upon origin-based piRNA subsets regarding piRNA amplification. The low degree of variability of same sample TruePaiR runs allows for metric reliability, reproducibility, as well as the ability to detect subtle differences in piRNA amplification within and between species and tissues. Given that TruePaiR serves as an effective and consistent metric of piRNA amplification across species, it can represent a new, meaningful standard in the degree of piRNA amplification in a specific organism and tissue that is or is not expected to undergo piRNA amplification."}, {"title": "Cross-linking BioThings APIs through JSON-LD to facilitate knowledge exploration", "url": "https://www.biorxiv.org/content/early/2017/07/23/167353", "tag": "Bioinformatics", "abstract": "Application Programming Interfaces (APIs) are now widely used to distribute biological data. And many popular biological APIs developed by many different research teams have adopted Javascript Object Notation (JSON) as their primary data format. While usage of a common data format offers significant advantages, that alone is not sufficient for rich integrative queries across APIs. Here, we have implemented JSON for Linking Data (JSON-LD) technology on the BioThings APIs that we have developed, MyGene.info, MyVariant.info and MyChem.info. JSON-LD provides a standard way to add semantic context to the existing JSON data structure, for the purpose of enhancing the interoperability between APIs. We demonstrated several use cases that were facilitated by semantic annotations using JSON-LD, including simpler and more precise query capabilities as well as API cross-linking. We believe that this pattern offers a generalizable solution for interoperability of APIs in the life sciences."}, {"title": "piClusterBusteR: Software For Automated Classification And Characterization Of piRNA Cluster Loci", "url": "https://www.biorxiv.org/content/early/2017/07/22/133009", "tag": "Bioinformatics", "abstract": "Background: Piwi-interacting RNAs (piRNAs) are sRNAs that have a distinct biogenesis and molecular function from siRNAs and miRNAs. The piRNA pathway is well-conserved and shown to play an important role in the regulatory capacity of germline cells in Metazoans. Significant subsets of piRNAs are generated from discrete genomic loci referred to as piRNA clusters. Given that the contents of piRNA clusters dictate the target specificity of primary piRNAs, and therefore the generation of secondary piRNAs, they are of great significance when considering transcriptional and post-transcriptional regulation on a genomic scale. A quantitative comparison of top piRNA cluster composition can provide further insight into piRNA cluster biogenesis and function. Results: We have developed software for general use, piClusterBusteR, which performs nested annotation of piRNA cluster contents to ensure high-quality characterization, provides a quantitative representation of piRNA cluster composition by feature, and makes available annotated and unannotated piRNA cluster sequences that can be utilized for downstream analysis. The data necessary to run piClusterBusteR and the skills necessary to execute this software on any species of interest are not overly burdensome for biological researchers. piClusterBusteR has been utilized to compare the composition of top piRNA generating loci amongst 13 Metazoan species. Characterization and quantification of cluster composition allows for comparison within piRNA clusters of the same species and between piRNA clusters of different species. Conclusions: We have developed a tool that accurately, automatically, and efficiently describes the contents of piRNA clusters in any biological system that utilizes the piRNA pathway. The results from piClusterBusteR have provided an in-depth description and comparison of the architecture of top piRNA clusters within and between 13 species, as well as a description of annotated and unannotated sequences from top piRNA cluster loci in these Metazoans. piClusterBusteR is available for download on GitHub: https://github.com/pschreiner/piClusterBuster"}, {"title": "Hierarchical Analysis of Multi-mapping RNA-Seq Reads Improves the Accuracy of Allele-specific Expression", "url": "https://www.biorxiv.org/content/early/2017/07/22/166900", "tag": "Bioinformatics", "abstract": "Allele-specific expression (ASE) refers to the differential abundance of the allelic copies of a transcript. Direct RNA sequencing (RNA-Seq) can provide quantitative estimates of ASE for genes with transcribed polymorphisms. However, estimating ASE is challenging due to ambiguities in read alignment. Current approaches do not account for the hierarchy of multiple read alignments to genes, isoforms, and alleles. We have developed EMASE (Expectation- Maximization for Allele Specific Expression), an integrated approach to estimate total gene expression, ASE, and isoform usage based on hierarchical allocation of multi-mapping reads. In simulations, EMASE outperforms standard ASE estimation methods. We apply EMASE to RNA-Seq data from F1 hybrid mice where we observe widespread ASE associated with cis-acting polymorphisms and a small number of parent-of-origin effects at known imprinted genes. The EMASE software is freely available under GNU license at https://github.com/churchill-lab/emase and it can be adapted to other sequencing applications."}, {"title": "Learning RNA secondary structure (only) from structure probing data", "url": "https://www.biorxiv.org/content/early/2017/07/22/152629", "tag": "Bioinformatics", "abstract": "We present CONTRAfold-SE, which extends CONTRAfold to model RNA structure-probing data as observations of possibly unknown secondary structures. This model can then be learned from datasets containing only structure-probing data, or a mix of known structures and probing data. We train CONTRAfold-SE on various combinations of structure probing data and complete structures and find that while genome-wide structure probing data provides modest improvement in prediction performance, with sufficiently dense probing data alone it is possible to learn a model that approaches the performance of energy-based methods. CONTRAfold-SE may be obtained from https://github.com/csfoo/contrafold-se."}, {"title": "Systems-level identification of transcription factors critical for mouse embryonic development", "url": "https://www.biorxiv.org/content/early/2017/07/21/167197", "tag": "Bioinformatics", "abstract": "Dynamic changes in the transcriptional regulatory circuit can influence the specification of distinct cell types. Numerous transcription factors (TFs) have been shown to function through dynamic rewiring during embryonic development but a comprehensive survey on the global regulatory network is still lacking. Here, we performed an integrated analysis of epigenomic and transcriptomic data to reveal key regulators from 2 cells to postnatal day 0 in mouse embryogenesis. We predicted 3D chromatin interactions including enhancer-promoter interactions in 12 tissues across 8 developmental stages, which facilitates linking TFs to their target genes for constructing genetic networks. To identify driver TFs particularly those not necessarily differentially expressed ones, we developed a new algorithm, dubbed as Taiji, to assess the global importance of TFs in development. Through comparative analysis across tissues and developmental stages, we systematically uncovered TFs that are critical for lineage-specific and stage-dependent tissue specification. Most interestingly, we have identified TF combinations that function in spatiotemporal order to form transcriptional waves regulating developmental progress and differentiation. Not only does our analysis provide the first comprehensive map of transcriptional regulatory circuits during mouse embryonic development, the identified novel regulators and the predicted 3D chromatin interactions also provide a valuable resource to guide further mechanistic studies."}, {"title": "Fast and memory-efficient noisy read overlapping with KD-trees", "url": "https://www.biorxiv.org/content/early/2017/07/21/166835", "tag": "Bioinformatics", "abstract": "Motivation: Third-generation sequencing technologies produce long, but noisy reads with increasing sequencing throughput and decreasing per-base costs. Detecting read-to-read overlaps in such data is the most computationally intensive step in de novo assembly. Recently, efficient algorithms were developed for this task; nearly all of these utilize long k-mers (>10 bp) to compare reads, but vary in their approaches to indexing, hashing, filtering, and dimensionality reduction. Results: We describe an algorithm for efficient overlap detection that directly compares the full spectrum of short k-mers, namely tetramers, through geometric embedding and approximate nearest neighbor search in multidimensional KD-trees. A proof of concept implementation detected read-to-read overlaps in bacterial PacBio and ONT datasets with notably lower memory consumption than state-of-the-art approaches and allowed downstream de novo assembly into single contigs. We also introduce a sequence-context dependent tagging scheme that contributes to memory and computational efficiency and could be used with other aligning and overlapping algorithms. Availability: A C++14 implementation is available under the open source Apache License 2.0 at: https://github.com/dzif/kd-tree-overlapper Contact: Alice.McHardy@helmholtz-hzi.de; pdmitri@hotmail.com"}, {"title": "Effective normalization for copy number variation in Hi-C data", "url": "https://www.biorxiv.org/content/early/2017/07/21/167031", "tag": "Bioinformatics", "abstract": "Normalization is essential to ensure accurate analysis and proper interpretation of sequencing data. Chromosome conformation data, such as Hi-C, is not different. The most widely used type of normalization of Hi-C data casts estimations of unwanted effects as a matrix balancing problem, relying on the assumption that all genomic regions interact as much as any other. Here, we show that these approaches, while very effective on fully haploid or diploid genome, fail to correct for unwanted effects in the presence of copy number variations. We propose a simple extension to matrix balancing methods that properly models the copy-number variation effects. Our approach can either retain the copy-number variation effects or remove it. We show that this leads to better downstream analysis of the three-dimensional organization of rearranged genome."}, {"title": "Probing tissue-scale deformation by in vivo force application reveals a fast tissue softening during early embryogenesis", "url": "https://www.biorxiv.org/content/early/2017/07/21/167155", "tag": "Bioinformatics", "abstract": "During development, cell-generated forces induce tissue-scale deformations to shape the organism. Here, we present a method that allows to quantitatively relate such tissue-scale deformations to spatially localized forces and measure mechanical properties of epithelia in vivo. Our approach is based on the application of controlled forces on microparticles embedded in individual cells of an embryo. Combining measurements of the bead displacement with the analysis of induced deformation fields in a continuum mechanics framework, we can quantify tissue material properties and follow their change over time. In particular, we uncover a rapid change in tissue response occuring during Drosophila cellularization, resulting from a softening of the blastoderm and an increase of external friction. Pharmacological treatments reveal that in addition to actomyosin, the microtubule cytoskeleton is a major contributor to epithelial mechanics at that stage. Overall, our method allows for measuring essential mechanical parameters governing tissue-scale deformations and flows occurring during morphogenesis."}, {"title": "Exact graph-based analysis of scientific articles on clinical trials", "url": "https://www.biorxiv.org/content/early/2017/07/21/164475", "tag": "Bioinformatics", "abstract": "This article describes Amorpha, a software package based on new concept of exact graph-based linguistic analysis. Analytical capabilities of Amorpha are demonstrated using analysis of scientific abstracts on clinical trials from PubMed. Current trends in therapy of breast cancer and psoriatic arthritis were analyzed using 400 abstracts on breast cancer and 131 abstracts on psoriatic arthritis. The spectrum of diseases that currently treated with paclitaxel was extracted from 400 most recent abstracts on paclitaxel. In addition to text representation, analytical results are presented as graph images showing essential concepts of a text. Amorpha is not designed specifically to analyze clinical trials and will be also useful for analysis of biological scientific articles and regulatory documents. Amorpha does not require any preliminary knowledge base, ensures full coverage of target text and 100% accuracy of obtained results."}, {"title": "A real-time compression library for microscopy images", "url": "https://www.biorxiv.org/content/early/2017/07/21/164624", "tag": "Bioinformatics", "abstract": "Fluorescence imaging techniques such as single molecule localization microscopy, high-content screening and light-sheet microscopy are producing ever-larger datasets, which poses increasing challenges in data handling and data sharing. Here, we introduce a real-time compression library that allows for very fast (beyond 1 GB/s) compression and de-compression of microscopy datasets during acquisition. In addition to an efficient lossless mode, our algorithm also includes a lossy option, which limits pixel deviations to the intrinsic noise level of the image and yields compression ratio of up to 100-fold. We present a detailed performance analysis of the different compression modes for various biological samples and imaging modalities."}, {"title": "PaSD-qc: Quality control for single cell whole-genome sequencing data using power spectral density estimation", "url": "https://www.biorxiv.org/content/early/2017/07/21/166637", "tag": "Bioinformatics", "abstract": "Single cell whole-genome sequencing (scWGS) is providing novel insights into the nature of genetic heterogeneity in normal and diseased cells. However, scWGS introduces DNA amplification-related biases that can confound downstream analysis. Here we present a statistical method, with an accompanying package PaSD-qc (Power Spectral Density-qc), that evaluates the quality of single cell libraries. It uses a modified power spectral density to assess amplification uniformity, amplicon size distribution, autocovariance, and inter-sample consistency as well as identifies aberrantly amplified chromosomes. We demonstrate the usefulness of this tool in evaluating scWGS protocols and in selecting high-quality libraries from low-coverage data for deep sequencing."}, {"title": "Enhancing the Representational Similarity Between Execution and Imagination of Movement Using Network-Based Brain Computer Interfacing", "url": "https://www.biorxiv.org/content/early/2017/07/21/166603", "tag": "Bioinformatics", "abstract": "Motor imagery-based brain computer interfacing (MI-BCI) as a neuro-rehabilitation tool aims at facilitating motor improvement using mental practice. However, the effectiveness of MI-BCI in producing clinically meaningful functional outcome is debated. Aside from computational shortcomings, a main limiting obstacle seems to be the substantial representational dissimilarity between movement imagination (MI) and movement execution (ME) on the level of engaged neural networks. This dissimilarity renders inducing functionally effective and long lasting changes in motor behavior through MI challenging. Moreover, the quality and intensity of imagination is highly prone to change on a trial-to-trial basis, based on the state of mind and mental fatigue. This leads to an inconsistent profile of neural activity throughout training, limiting learning in a Hebbian sense. To address these issues, we propose a neuroconnectivity-based paradigm, as a systematic priming technique to be utilized pre-BCI training. In the proposed paradigm, ME-idle representational dissimilarity network (RDN) features are used to detect MI in real-time. This means that to drive the virtual environment, an ME-like activation pattern has to be learned and generated in the brain through MI. This contrasts with conventional BCIs which consider a successful MI, one that results in higher than a threshold change in the power of sensorimotor rhythms. Our results show that four out of five participants achieved a consistent session-to-session enhancement in their net MI-ME network-level similarity (mean change rate of 6.16% \u00b1 4.64 per session). We suggest that the proposed paradigm, if utilized as a priming technique pre-BCI training, can potentially enhance the neural and functional effectiveness. This can be achieved through 1- shifting MI towards engaging ME-related networks to a higher extent, and 2- inducing consistency in MI quality by using the ME-related networks as the ground-truth and thus enhancing the robustness of the activity pattern in the brain. This would in turn lend to the clinical acceptability of BCI as a neurorehabilitation tool."}, {"title": "Automated analysis of internally programmed grooming behavior in Drosophila using a k-nearest neighbors classifier", "url": "https://www.biorxiv.org/content/early/2017/07/21/166561", "tag": "Bioinformatics", "abstract": "Despite being pervasive, the control of programmed grooming is poorly understood. We have addressed this gap in knowledge by developing a high-throughput platform that allows long-term detection of grooming in the fruit fly Drosophila melanogaster. Automatic classification of daily behavior shows flies spend 30% of their active time grooming. We show that a large proportion of this behavior is driven by two major internal programs. One of these programs is the circadian clock that modulates rhythms in daily grooming. The second program depends on cycle and clock and regulates the amount of time flies spend grooming. This emerging dual control model of programmed grooming in which one regulator controls the timing and another controls the duration, resembles the well-established two-process regulatory model of fly sleep. Together, our quantitative approach in Drosophila has revealed that grooming is an important internally driven behavior under the control of two regulatory programs."}, {"title": "Fusion detection and quantification by pseudoalignment", "url": "https://www.biorxiv.org/content/early/2017/07/20/166322", "tag": "Bioinformatics", "abstract": "RNA sequencing in cancer cells is a powerful technique to detect chromosomal rearrangements, allowing for de novo discovery of actively expressed fusion genes. Here we focus on the problem of detecting gene fusions from raw sequencing data, assembling the reads to define fusion transcripts and their associated breakpoints, and quantifying their abundances. Building on the pseudoalignment idea that simplifies and accelerates transcript quantification, we introduce a novel approach to fusion detection based on inspecting paired reads that cannot be pseudoaligned due to conflicting matches. The method and software, called pizzly, filters false positives, assembles new transcripts from the fusion reads, and reports candidate fusions. With pizzly, fusion detection from raw RNA-Seq reads can be performed in a matter of minutes, making the program suitable for the analysis of large cancer gene expression databases and for clinical use. pizzly is available at https://github.com/pmelsted/pizzly"}, {"title": "Vermont: a multi-perspective visual interactive platform for mutational analysis", "url": "https://www.biorxiv.org/content/early/2017/07/20/166348", "tag": "Bioinformatics", "abstract": "Background: A huge amount of data about genomes and sequence variation is available and continues to grow on a large scale, which makes experimentally characterizing these mutations infeasible regarding disease association and effects on protein structure and function. Therefore, reliable computational approaches are needed to support the understanding of mutations and their impacts. Here, we present VERMONT 2.0, a visual interactive platform that combines sequence and structural parameters with interactive visualizations to make the impact of protein point mutations more understandable. Results: We aimed to contribute a novel visual analytics oriented method to analyze and gain insight on the impact of protein point mutations. To assess the ability of VERMONT to do this, we visually examined a set of mutations that were experimentally characterized to determine if VERMONT could identify damaging mutations and why they can be considered so. Conclusions: VERMONT allowed us to understand mutations by interpreting position-specific structural and physicochemical properties. Additionally, we note some specific positions we believe have an impact on protein function/structure in the case of mutation."}, {"title": "Detection of long repeat expansions from PCR-free whole-genome sequence data", "url": "https://www.biorxiv.org/content/early/2017/07/20/093831", "tag": "Bioinformatics", "abstract": "Identifying large repeat expansions such as those that cause amyotrophic lateral sclerosis (ALS) and Fragile X syndrome is challenging for short-read (100-150 bp) whole genome sequencing (WGS) data. A solution to this problem is an important step towards integrating WGS into precision medicine. We have developed a software tool called ExpansionHunter that, using PCR-free WGS short-read data, can genotype repeats at the locus of interest, even if the expanded repeat is larger than the read length. We applied our algorithm to WGS data from 3,001 ALS patients who have been tested for the presence of the C9orf72 repeat expansion with repeat-primed PCR (RP-PCR). Taking the RP-PCR calls as the ground truth, our WGS-based method identified pathogenic repeat expansions with 98.1% sensitivity and 99.7% specificity. Further inspection identified that all 11 conflicts were resolved as errors in the original RP-PCR results. Compared against this updated result, ExpansionHunter correctly classified all (212/212) of the expanded samples as either expansions (208) or potential expansions (4). Additionally, 99.9% (2,786/2,789) of the wild type samples were correctly classified as wild type by this method with the remaining two identified as possible expansions. We further applied our algorithm to a set of 144 samples where every sample had one of eight different pathogenic repeat expansions including examples associated with fragile X syndrome, Friedreich's ataxia and Huntington's disease and correctly flagged all of the known repeat expansions. Finally, we tested the accuracy of our method for short repeats by comparing our genotypes with results from 860 samples sized using fragment length analysis and determined that our calls were >95% accurate. ExpansionHunter can be used to accurately detect known pathogenic repeat expansions and provides researchers with a tool that can be used to identify new pathogenic repeat expansions."}, {"title": "HTSSIP: an R package for analysis of high throughput sequencing data from nucleic acid stable isotope probing (SIP) experiments", "url": "https://www.biorxiv.org/content/early/2017/07/20/166009", "tag": "Bioinformatics", "abstract": "Combining high throughput sequencing with stable isotope probing (HTS-SIP) is a powerful method for mapping in situ metabolic processes to thousands of microbial taxa. However, accurately mapping metabolic processes to taxa is complex and challenging. Multiple HTS-SIP data analysis methods have been developed, including high-resolution stable isotope probing (HR-SIP), multi-window high-resolution stable isotope probing (MW-HR-SIP), quantitative stable isotope probing (q-SIP), and \u0394BD. Currently, the computational tools to perform these analyses are either not publicly available or lack documentation, testing, and developer support. To address this shortfall, we have developed the HTSSIP R package, a toolset for conducting HTS-SIP analyses in a straightforward and easily reproducible manner. The HTSSIP package, along with full documentation and examples, is available from CRAN at https://cran.r-project.org/web/packages/HTSSIP/index.html and Github at https://github.com/nick-youngblut/HTSSIP."}, {"title": "MediSyn: uncertainty-aware visualization of multiple biomedical datasets to support drug treatment selection", "url": "https://www.biorxiv.org/content/early/2017/07/20/165878", "tag": "Bioinformatics", "abstract": "Background: Dispersed biomedical databases limit user exploration to generate structured knowledge. Linked Data unifies data structures and makes the dispersed data easy to search across resources, but it lacks supporting human cognition to achieve insights. In addition, potential errors in the data are difficult to detect in their free formats. Devising a visualization that synthesizes multiple sources in such a way that links between data sources are transparent, and uncertainties, such as data conflicts, are salient is challenging. Results: To investigate the requirements and challenges of uncertainty-aware visualizations of linked data, we developed MediSyn, a system that synthesizes medical datasets to support drug treatment selection. It uses a matrix-based layout to visually link drugs, targets (e.g., mutations), and tumor types. Data uncertainties are salient in MediSyn; for example, (i) missing data are exposed in the matrix view of drug-target relations; (ii) inconsistencies between datasets are shown via overlaid layers; and (iii) data credibility is conveyed through links to data provenance. Conclusions: Through the synthesis of two manually curated datasets, cancer treatment biomarkers and drug-target bioactivities, a use case shows how MediSyn effectively supports the discovery of drug-repurposing opportunities. A study with six domain experts indicated that MediSyn benefited the drug selection and data inconsistency discovery. Though linked publication sources supported user exploration for further information, the causes of inconsistencies were not easy to find. Additionally, MediSyn could embrace more patient data to increase its informativeness. We derive design implications from the findings."}, {"title": "Allele-specific multi-sample copy number segmentation", "url": "https://www.biorxiv.org/content/early/2017/07/20/166017", "tag": "Bioinformatics", "abstract": "Allele-specific copy number alterations are commonly used to trace the evolution of tumours. A key step of the analysis is to segment genomic data into regions of constant copy number. For precise phylogenetic inference, breakpoints shared between samples need to be aligned to each other. Here we present asmultipcf, an algorithm for allele-specific segmentation of multiple samples that infers private and shared segment boundaries of phylogenetically related samples. The output of this algorithm can directly be used for allele-specific copy number calling using ASCAT. asmultipcf is available as part of the ASCAT R package (version > 2.5) from github.com/Crick-CancerGenomics/ascat"}, {"title": "SANe: The Seed Active Network For Mining Transcriptional Regulatory Programs of Seed Development", "url": "https://www.biorxiv.org/content/early/2017/07/20/165894", "tag": "Bioinformatics", "abstract": "Seed development is an evolutionarily important phase of the plant life cycle that governs the fate of next progeny. Distinct sub-regions within seeds have diverse roles in protecting and nourishing the embryo as it enlarges, and for the synthesis of storage reserves that serve as an important source of nutrients and energy for germination. Several studies have revealed that transcription factors (TFs) act in fine coordination to regulate target genes that ensure proper maintenance, metabolism, and development of the embryo. Here, we present genome-wide predictions of seed-specific regulatory interactions between TFs and their target genes in the model plant Arabidopsis thaliana. The network is based on a panel of high-resolution seed-specific gene expression datasets and takes the form of a module-regulatory network. TFs that are well studied in the literature were often found at the top of the predicted ranks for the module that corresponds to their validated function role. Furthermore, we brought together a dedicated web resource for the systematic analysis of transcriptional-level regulatory programs underlying the development of seeds (https://plantstress-pereira.uark.edu/SANe/). The platform will enable biologists to query a subset of modules, TFs of interest, as well as analyze new transcriptomes to find modules significantly perturbed in their experiment."}, {"title": "Wisdom of artificial crowds feature selection in untargeted metabolomics: An application to the development of a blood-based diagnostic test for thrombotic myocardial infarction", "url": "https://www.biorxiv.org/content/early/2017/07/19/165977", "tag": "Bioinformatics", "abstract": "Introduction: Heart disease remains a leading cause of global mortality. While acute myocardial infarction (colloquially: heart attack), has multiple proximate causes, proximate etiology cannot be determined by a blood-based diagnostic test. We enrolled a suitable patient cohort and conducted an untargeted quantification of plasma metabolites by mass spectrometry for developing a test that can differentiate between thrombotic MI, non-thrombotic MI, and stable disease. A significant challenge in developing such a diagnostic test is solving the NP-hard problem of feature selection for constructing an optimal statistical classifier. Objective: We employed a Wisdom of Artificial Crowds (WoAC) strategy for solving the feature selection problem and evaluated the accuracy and parsimony of downstream classifiers in comparison with embedded feature selection via the Lasso and Elastic Net. Materials and Methods: Artificial Crowd Wisdom was generated via aggregation of the best solutions from independent and diverse genetic algorithm populations that were initialized with bootstrapping and a random subspaces constraint. Results / Conclusions: WoAC feature selection performed favorably compared to Lasso and Elastic Net solutions. The classifier constructed following WoAC feature selection had a cross-validation estimated misclassification rate of 2.6% as compared to 26.3% via the Lasso and 18.5% via an Elastic Net. The classifier warrants further evaluation as a diagnostic test in an independent cohort."}, {"title": "SnakeChunks: modular blocks to build Snakemake workflows for reproducible NGS analyses", "url": "https://www.biorxiv.org/content/early/2017/07/19/165191", "tag": "Bioinformatics", "abstract": "Summary: Next-Generation Sequencing (NGS) is becoming a routine approach for most domains of life sciences, yet there is a crucial need to improve the automation of processing for the huge amounts of data generated and to ensure reproducible results. We present SnakeChunks, a collection of Snakemake rules enabling to compose modular and user-configurable workflows, and show its usage with analyses of transcriptome (RNA-seq) and genome-wide location (ChIP-seq) data. Availability: The code is freely available (github.com/SnakeChunks/SnakeChunks), and documented with tutorials and illustrative demos (snakechunks.readthedocs.io). Contact: claire.rioualen@inserm.fr, jacques.van-helden@univ-amu.fr Supplementary information: Supplementary data are available at Bioinformatics online."}, {"title": "Association Mapping From Sequencing Reads Using K-mers", "url": "https://www.biorxiv.org/content/early/2017/07/19/141267", "tag": "Bioinformatics", "abstract": "Genome wide association studies (GWAS) rely on microarrays, or more recently mapping of whole-genome sequencing reads, to genotype individuals. The reliance on prior sequencing of a reference genome for the organism on which the association study is to be performed limits the scope of association studies, and also precludes the identification of differences between cases and controls outside of the reference. We present an alignment free method for association studies that is based on counting k-mers in sequencing reads, testing for associations directly between k-mers and the trait of interest, and local assembly of the statistically significant k-mers to identify sequence differences. Results with simulated data and an analysis of the 1000 genomes data provide a proof of principle for the approach. In a pairwise comparison of the Toscani in Italia (TSI) and the Yoruba in Ibadan, Nigeria (YRI) populations we find that sequences identified by our method largely agree with results obtained using standard GWAS based on variant calling from mapped reads. However unlike standard GWAS, we find that our method identifies associations with structural variations and sites not present in the reference genome revealing sequences absent from the human reference genome. We also analyze data from the Bengali from Bangladesh (BEB) population to explore possible genetic basis of high rate of mortality due to cardiovascular diseases (CVD) among South Asians and find significant differences in frequencies of a number of non-synonymous variants in genes linked to CVDs between BEB and TSI samples, including the site rs1042034, which has been associated with higher risk of CVDs previously and the nearby rs676210 in the Apolipoprotein B (ApoB) gene."}, {"title": "l1kdeconv: an R package for peak calling analysis with LINCS L1000 data", "url": "https://www.biorxiv.org/content/early/2017/07/18/165258", "tag": "Bioinformatics", "abstract": "Background: LINCS L1000 is a high-throughput technology that allows gene expression measurement in a large number of assays. However, to fit the measurements of ~1000 genes in the ~500 color channels of LINCS L1000, every two landmark genes are designed to share a single channel. Thus, a deconvolution step is required to infer the expression values of each gene. Any errors in this step can be propagated adversely to the downstream analyses. Results: We presented a LINCS L1000 data peak calling R package l1kdeconv based on a new outlier detection method and an aggregate Gaussian mixture model (AGMM). Upon the remove of outliers and the borrowing information among similar samples, l1kdeconv showed more stable and better performance than methods commonly used in LINCS L1000 data deconvolution. Conclusions: Based on the benchmark using both simulated data and real data, the l1kdeconv package achieved more stable results than the commonly used LINCS L1000 data deconvolution methods"}, {"title": "Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks", "url": "https://www.biorxiv.org/content/early/2017/07/18/165183", "tag": "Bioinformatics", "abstract": "Regulatory sequences are not solely defined by their nucleic acid sequence but also by their relative distances to genomic landmarks such as transcription start site, exon boundaries, or polyadenylation site. Deep learning has become the approach of choice for modeling regulatory sequences because of its strength to learn complex sequence features. However, modeling relative distances to genomic landmarks in deep neural networks has not been addressed. Here we developed spline transformation, a neural network module based on splines to flexibly and robustly model distances. Modeling distances to various genomic landmarks with spline transformations significantly increased state-of-the-art prediction accuracy of in vivo RNA-binding protein binding sites for 114 out of 123 proteins. We also developed a deep neural network for human splice branchpoint based on spline transformations that outperformed the current best, already distance-based, machine learning model. Compared to piecewise linear transformation, as obtained by composition of rectified linear units, spline transformation yields higher prediction accuracy as well as faster and more robust training. As spline transformation can be applied to further quantities beyond distances, such as methylation or conservation, we foresee it as a versatile component in the genomics deep learning toolbox. Spline transformation is implemented as a Keras layer in the CONCISE python package: github.com/gagneurlab/concise. Analysis code is available at goo.gl/3yMY5w."}, {"title": "Unexpected Properties of Short Genomic Tandem Repeats", "url": "https://www.biorxiv.org/content/early/2017/07/18/165308", "tag": "Bioinformatics", "abstract": "Length polymorphisms in genomic short tandem repeats have been implicated in a variety of diseases, most notably human neurodegenerative disorders. Expansions of tandem repeats are also associated with genomic instability in cancer. Our previous study of length-3 tandem repeats uncovered a surprising pattern in the length distribution of certain such repeats in the non-coding regions of the human reference genome: a bias towards repeats of length 3n - 1, (n > 3). That is, the observed frequency of repeats of this length in the human genome is higher than expected by chance based on the frequency of shorter repeats. We have hypothesized that this pattern may be a general property of genomic DNA. If true, this could have implications with regard to the dynamics of repeat expansion generally. To test this hypothesis, we have analyzed the genomic sequences of a broad range of eukaryotic organisms as well as several complete human genomes and obtained a number of thought provoking results. We establish that this unexpected elevation in frequency of 3n - 1 long repeats is statistically significant. We also expanded this analysis to different classes of genomic regions and tandem repeats of length four and five. The specific pattern was found in 13 of the 20 organisms analyzed, including all chordate and insect genomes tested. The bias pattern, however, was not confined to a single branch of the evolutionary tree. For some genomes, such as Drosophila melanogaster, the repeat bias surprisingly was also identified in exons. The pattern is present in both small and large genomes. A similar pattern was also found in tetranucleotide and pentanucleotide repeats in the human genome. Another surprising property was identified for the flanking GC content for triplet repeats of length 3n. These findings indicate a puzzling new genomic phenomenon with possible evolutionary and disease-related implications."}, {"title": "Bayesian Unidimensional Scaling for visualizing uncertainty in high dimensional datasets with latent ordering of observations", "url": "https://www.biorxiv.org/content/early/2017/07/18/163915", "tag": "Bioinformatics", "abstract": "Background. Detecting patterns in high-dimensional multivariate datasets is non-trivial. Clustering and dimensionality reduction techniques often help in discerning inherent structures. In biological datasets such as microbial community composition or gene expression data, observations can be generated from a continuous process, often unknown. Estimating data points' `natural ordering' and their corresponding uncertainties can help researchers draw insights about the mechanisms involved. Results. We introduce a Bayesian Unidimensional Scaling (BUDS) technique which extracts dominant sources of variation in high dimensional datasets and produces their visual data summaries, facilitating the exploration of a hidden continuum. The method maps multivariate data points to latent one dimensional coordinates along their underlying trajectory, and provides estimated uncertainty bounds. By statistically modeling dissimilarities and applying a DiSTATIS registration method to their posterior samples, we are able to incorporate visualizations of uncertainties in the estimated data trajectory across different regions using confidence contours for individual data points. We also illustrate the estimated overall data density across different areas by including density clouds. One-dimensional coordinates recovered by BUDS help researchers discover sample attributes or covariates that are factors driving the main variability in a dataset. We demonstrated usefulness and accuracy of BUDS on a set of published microbiome 16S and RNA-seq and roll call data. Conclusions. Our method effectively recovers and visualizes natural orderings present in datasets. Automatic visualization tools for data exploration and analysis are available at: https://nlhuong.shinyapps.io/visTrajectory/"}, {"title": "unitas: the universal tool for annotation of small RNAs", "url": "https://www.biorxiv.org/content/early/2017/07/18/165175", "tag": "Bioinformatics", "abstract": "Background: Next generation sequencing is a key technique in small RNA biology research that has led to the discovery of functionally different classes of small non-coding RNAs in the past years. However, reliable annotation of the extensive amounts of small non-coding RNA data produced by high-throughput sequencing is time-consuming and requires robust bioinformatics expertise. Moreover, existing tools have a number of shortcomings including a lack of sensitivity under certain conditions, limited number of supported species or detectable sub-classes of small RNAs. Results: Here we introduce unitas, an out-of-the-box ready software for complete annotation of small RNA sequence datasets, supporting the wide range of species for which non-coding RNA reference sequences are available in the Ensembl databases (currently more than 800). unitas combines high quality annotation and numerous analysis features in a user-friendly manner. A complete annotation can be started with one simple shell command, making unitas particularly useful for researchers not having access to a bioinformatics facility. Noteworthy, the algorithms implemented in unitas are on par or even outperform comparable existing tools for small RNA annotation that base on available ncRNA sequence information. Conclusions: unitas brings together annotation and analysis features that hitherto required the installation of numerous different bioinformatics tools which can pose a challenge for the non-expert user. With this, unitas overcomes the problem of read normalization. Moreover, the high quality of sequence annotation and analysis, paired with the ease of use, make unitas a valuable tool for researchers in all fields connected to small RNA biology."}, {"title": "Combining multiple functional annotation tools increases completeness of metabolic annotation", "url": "https://www.biorxiv.org/content/early/2017/07/18/160887", "tag": "Bioinformatics", "abstract": "The dirty little secret behind genome-scale systems biology modeling efforts is that they are invariably based on very incomplete functional annotations. Annotated genomes typically contain 30-50% of genes with little or no functional annotation, severely limiting our knowledge of the \"parts lists\" that the organisms have at their disposal. In metabolic modeling, these incomplete annotations are often sufficient to derive a reasonably complete model of the core metabolism at least, typically consisting of well-studied (and thus well-annotated) metabolic pathways that are sufficient for growth in pure culture. However secondary metabolic pathways or pathways that are important for growth on unusual metabolites exchanged in complex microbial communities are often much less well understood, resulting in missing or lower confidence functional annotations in newly sequenced genomes. Here, we present preliminary results on a comprehensive reannotation of 27 bacterial Tier 1 and Tier 2 reference genomes from BioCyc, focusing on enzymes with EC numbers annotated by KEGG, RAST, EFICAz, and the Brenda enzyme database, and on membrane transport annotations by TransportDB, KEGG and RAST."}, {"title": "Sweep Dynamics (SD) plots: Computational identification of selective sweeps to monitor the adaptation of influenza A viruses", "url": "https://www.biorxiv.org/content/early/2017/07/18/110528", "tag": "Bioinformatics", "abstract": "Monitoring changes in the genome of influenza A viruses is crucial to understand its rapid evolution. These changes reflect how the virus adapts to changing environmental conditions such as establishment within a novel host species. Selective sweeps represent a rapid mode of adaptation and are typically observed in the evolution of human influenza A viruses. Here we describe a computational method named Sweep Dynamics (SD) plots. These combine phylogenetic algorithms with statistical techniques, allowing to characterize the molecular adaptation of rapidly evolving viruses from longitudinal sequence data. To our knowledge, it is the first method that allows not only to identify selective sweeps, but also the time periods in which these occurred and the associated amino acid changes that may have provided a selective advantage to the virus. Using SD plots, we studied the past genome-wide adaptation of the 2009 pandemic H1N1 influenza A (pH1N1) and seasonal H3N2 influenza A (sH3N2) viruses. The pH1N1 influenza virus showed simultaneous amino acid changes in various proteins, particularly in seasons of high pH1N1 activity. Some of these adaptive changes resulted in functional alterations that facilitated virus transmission by respiratory droplets, which is key for sustained human-to-human transmission, directly after pandemic emergence. In the evolution of sH3N2 influenza viruses since 1999, we detected a large number of amino acid changes characterizing vaccine strains. Amino acid changes found in antigenically novel strains rising to predominance were occasionally revealed in a selective sweep one season prior to the recommendation of the WHO, suggesting the value of the technique for the vaccine strain selection problem. Taken together, our results show that SD plots allow to monitor and characterize the adaptive evolution of influenza A viruses by identifying selective sweeps and their associated signatures."}, {"title": "Correcting batch effects in single-cell RNA sequencing data by matching mutual nearest neighbours.", "url": "https://www.biorxiv.org/content/early/2017/07/18/165118", "tag": "Bioinformatics", "abstract": "The presence of batch effects is a well-known problem in experimental data analysis, and single-cell RNA sequencing (scRNA-seq) is no exception. Large-scale scRNA-seq projects that generate data from different laboratories and at different times are rife with batch effects that can fatally compromise integration and interpretation of the data. In such cases, computational batch correction is critical for eliminating uninteresting technical factors and obtaining valid biological conclusions. However, existing methods assume that the composition of cell populations are either known or the same across batches. Here, we present a new strategy for batch correction based on the detection of mutual nearest neighbours in the high-dimensional expression space. Our approach does not rely on pre-defined or equal population compositions across batches, only requiring that a subset of the population be shared between batches. We demonstrate the superiority of our approach over existing methods on a range of simulated and real scRNA-seq data sets. We also show how our method can be applied to integrate scRNA-seq data from two separate studies of early embryonic development."}, {"title": "VolcanoR - web service to produce volcano plots and do basic enrichment analysis", "url": "https://www.biorxiv.org/content/early/2017/07/18/165100", "tag": "Bioinformatics", "abstract": "We introduce VolcanoR - web based tool to analyse results of differential gene expression. It takes a table containing gene name p-value and foldChange as input data. It can produce publication quality volcano plots, apply different p-value and fold change thresholds and do basic GeneOntology and KEGG enrichment analysis with selected gene set. For now it supports H.sapiens, R.norvegicus and M.musclus. Availability : VolcanoR is wtitten using R Shiny framework. It is publically available at http://volcanor.bioinf.su or stand-alone application, that can be downloaded at https://github.com/vovalive/volcanoR"}, {"title": "TASI: A software tool for spatial-temporal quantification of tumor spheroid dynamics", "url": "https://www.biorxiv.org/content/early/2017/07/17/164921", "tag": "Bioinformatics", "abstract": "Spheroid cultures derived from explanted cancer specimens are an increasingly utilized resource for studying complex biological processes like tumor cell invasion and metastasis, representing an important bridge between the simplicity and practicality of 2D monolayer cultures and the complexity and realism of in vivo animal models. Temporal imaging of spheroids can capture the dynamics of cell behaviors and microenvironments, and when combined with quantitative image analysis methods, enables deep interrogation of biological mechanisms. This paper presents a comprehensive open-source software framework for Temporal Analysis of Spheroid Imaging (TASI) that allows investigators to objectively characterize spheroid growth and invasion dynamics. TASI performs spatiotemporal segmentation of spheroid cultures, extraction of features describing spheroid morpho-phenotypes, mathematical modeling of spheroid dynamics, and statistical comparisons of experimental conditions. We demonstrate the utility of this tool in an analysis of non-small cell lung cancer spheroids that exhibit variability in metastatic and proliferative behaviors."}, {"title": "Knowledge-guided gene prioritization reveals new insights into the mechanisms of chemoresistance", "url": "https://www.biorxiv.org/content/early/2017/07/17/090027", "tag": "Bioinformatics", "abstract": "Background: Identification of genes whose basal mRNA expression predicts the sensitivity of tumor cells to cytotoxic treatments can play an important role in individualized cancer medicine. It enables detailed characterization of the mechanism of action of drugs. Furthermore, screening the expression of these genes in the tumor tissue may suggest the best course of chemotherapy or a combination of drugs to overcome drug resistance. Results: We developed a computational method called ProGENI to identify genes most associated with the variation of drug response across different individuals, based on gene expression data. In contrast to existing methods, ProGENI also utilizes prior knowledge of protein-protein and genetic interactions, using random walk techniques. Analysis of two relatively new and large datasets including gene expression data on hundreds of cell lines and their cytotoxic responses to a large compendium of drugs reveals a significant improvement in prediction of drug sensitivity using genes identified by ProGENI compared to other methods. Our siRNA knockdown experiments on ProGENI-identified genes confirmed the role of many new genes in sensitivity to three chemotherapy drugs: cisplatin, docetaxel and doxorubicin. Based on such experiments and extensive literature survey, we demonstrate that about 73% our top predicted genes modulate drug response in selected cancer cell lines. In addition, global analysis of genes associated with groups of drugs uncovered pathways of cytotoxic response shared by each group. Conclusions: Our results suggest that knowledge-guided prioritization of genes using ProGENI gives new insight into mechanisms of drug resistance and identifies genes that may be targeted to overcome this phenomenon."}, {"title": "pyABC: distributed, likelihood-free inference", "url": "https://www.biorxiv.org/content/early/2017/07/17/162552", "tag": "Bioinformatics", "abstract": "Likelihood-free methods are often required for inference in systems biology. While Approximate Bayesian Computation (ABC) provides a theoretical solution, its practical application has often been challenging due to its high computational demands. To scale likelihood-free inference to computationally demanding stochastic models we developed pyABC: a distributed and scalable ABC-Sequential Monte Carlo (ABC-SMC) framework. It implements computation-minimizing and scalable, runtime-minimizing parallelization strategies for multi-core and distributed environments scaling to thousands of cores. The framework is accessible to non-expert users and also enables advanced users to experiment with and to custom implement many options of ABC-SMC schemes, such as acceptance threshold schedules, transition kernels and distance functions without alteration of pyABC's source code. pyABC includes a web interface to visualize ongoing and finished ABC-SMC runs and exposes an API for data querying and post-processing. pyABC is written in Python 3 and is released under the GPLv3 license. The source code is hosted on https://github.com/neuralyzer/pyabc and the documentation on http://pyabc.readthedocs.io. It can be installed from the Python Package Index (PyPI)."}, {"title": "Comparative genomic analyses highlight the contribution of pseudogenized protein-coding genes to human lincRNAs", "url": "https://www.biorxiv.org/content/early/2017/07/17/163626", "tag": "Bioinformatics", "abstract": "Background: The regulatory roles of long intergenic noncoding RNAs (lincRNAs) in humans have been revealed through the use of advanced sequencing technology. Recently, three possible scenarios of lincRNA origin have been proposed: de novo origination from intergenic regions, duplication from long noncoding RNA, and pseudogenization from protein. The first two scenarios are largely studied and supported, yet few studies focused on the evolution from pseudogenized protein-coding sequence to lincRNA. Due to the non mutually exclusive nature that these three scenarios have, accompanied by the need of systematic investigation of lincRNA origination, we conduct a comparative genomics study to investigate the evolution of human lincRNAs. Results: Combining with syntenic analysis and stringent Blastn e-value cutoff, we found that the majority of lincRNAs are aligned to the intergenic regions of other species. Interestingly, 193 human lincRNAs could have protein-coding orthologs in at least two of nine vertebrates. Transposable elements in these conserved regions in human genome are much less than expectation. Moreover, 19% of these lincRNAs have overlaps with or are close to pseudogenes in the human genome. Conclusions: We suggest that a notable portion of lincRNAs could be derived from pseudogenized protein-coding genes. Furthermore, based on our computational analysis, we hypothesize that a subset of these lincRNAs could have potential to regulate their paralogs by functioning as competing endogenous RNAs. Our results provide evolutionary evidence of the relationship between human lincRNAs and protein-coding genes."}, {"title": "GAS Power Calculator: web-based power calculator for genetic association studies", "url": "https://www.biorxiv.org/content/early/2017/07/17/164343", "tag": "Bioinformatics", "abstract": "Motivation: Statistical power calculations are crucial in designing genetic association studies. They help guide tradeoffs between large sample sizes and detailed assessments of genotype and phenotype, help determine which studies are viable, and help interpret research findings. To facilitate widespread use of power analysis in the design and interpretation of genetic studies, it is important to enable users to calculate power and visualize the effect of different models and design choices in convenient, interactive tools that are easily accessible. Results: We developed the Genetic Association Study (GAS) Power Calculator to provide users with a simple interface that can be compute the power of genetic association studies in a convenient browser based interface. Availability: The GAS Power Calculator can be accessed from the web interface at http://csg.sph.umich.edu/abecasis/gas_power_calculator/. Source code is available at https://github.com/jenlij/GAS-power-calculator."}, {"title": "A novel Word2vec based tool to estimate semantic similarity of genes by using Gene Ontology terms", "url": "https://www.biorxiv.org/content/early/2017/07/16/103648", "tag": "Bioinformatics", "abstract": "The Gene Ontology (GO) database contains GO terms that describe biological functions of genes and proteins in the cell. A GO term contains one or two sentences describing a biological aspect. GO database is used in many applications. One application is the comparison of two genes or two proteins by first comparing semantic similarity of the GO terms that annotate them. Previous methods for this task have relied on the fact that GO terms are organized into a tree structure. In this old paradigm, the locations of two GO terms in the tree dictate their similarity score. In this paper, we introduce a new solution to the problem of comparing two GO terms. Our method uses natural language processing (NLP) and does not rely on the GO tree. In our approach, we use the Word2vec model to compare two words. Using this model as the key building-block, we compare two sentences, and definitions of two GO terms. Because a gene or protein is annotated by a set of GO terms, we can apply our method to compare two genes or two proteins. We validate our method in two ways. In the first experiment, we measure the similarity of genes in the same regulatory pathways. In the second experiment, we test the model's ability to differentiate a true protein-protein network from a randomly generated network. Our results are equivalent to those of previous methods which depend on the GO tree. This gives promise to the development of NLP methods in comparing GO terms."}, {"title": "Characteristics of functional enrichment and gene expression level of human putative transcriptional target genes", "url": "https://www.biorxiv.org/content/early/2017/07/16/085654", "tag": "Bioinformatics", "abstract": "Background: Transcriptional target genes show functional enrichment of genes. However, how many and how significantly transcriptional target genes include functional enrichments are still unclear. To address these issues, I predicted human transcriptional target genes using open chromatin regions, ChIP-seq data and DNA binding sequences of transcription factors in databases, and examined functional enrichment and gene expression level of putative transcriptional target genes. Results: Gene Ontology annotations showed four times larger numbers of functional enrichments in putative transcriptional target genes than gene expression information alone, independent of transcriptional target genes. To compare the number of functional enrichments of putative transcriptional target genes between cells or search conditions, I normalized the number of functional enrichment by calculating its ratios in the total number of transcriptional target genes. With this analysis, native putative transcriptional target genes showed the largest normalized number of functional enrichments, compared with target genes including 5-60% of randomly selected genes. The normalized number of functional enrichments was changed according to the criteria of enhancer-promoter interactions such as distance from transcriptional start sites and orientation of CTCF-binding sites. Forward-reverse orientation of CTCF-binding sites showed significantly higher normalized number of functional enrichments than the other orientations. Journal papers showed that the top five frequent functional enrichments were related to the cellular functions in the three cell types. The median expression level of transcriptional target genes changed according to the criteria of enhancer-promoter assignments (i.e. interactions) and was correlated with the changes of the normalized number of functional enrichments of transcriptional target genes. Conclusions: Human putative transcriptional target genes showed significant functional enrichments. Functional enrichments were related to the cellular functions. The normalized number of functional enrichments of human putative transcriptional target genes changed according to the criteria of enhancer-promoter assignments and correlated with the median expression level of the target genes. These analyses and characters of human putative transcriptional target genes would be useful to examine the criteria of enhancer-promoter assignments and to predict the novel mechanisms and factors such as DNA binding proteins and DNA sequences of enhancer-promoter interactions."}, {"title": "Quantius: Generic, high-fidelity human annotation of scientific images at 105-clicks-per-hour", "url": "https://www.biorxiv.org/content/early/2017/07/15/164087", "tag": "Bioinformatics", "abstract": "We describe Quantius, a crowd-based image annotation platform that provides an accurate alternative to task-specific computational algorithms for difficult image analysis problems. We use Quantius to quantify a variety of computationally challenging medium-throughput tasks with ~50x and 30x savings in analysis time and cost respectively, relative to a single expert annotator. We show equivalent deep learning performance for Quantius- and expert-derived annotations, bridging towards scalable integration with tailored machine-learning algorithms."}, {"title": "IW-Scoring: an Integrative Weighted Scoring framework for annotating and prioritizing genetic variations in the noncoding genome", "url": "https://www.biorxiv.org/content/early/2017/07/15/163311", "tag": "Bioinformatics", "abstract": "IW-Scoring represents a new Integrative Weighted Scoring model to annotate and prioritise functionally relevant noncoding variations. The pipeline integrates 11 popular algorithms and outperforms individual methods in three independent data sets, including variants in ClinVar database and GWAS studies, and cancer mutations. Using IW-Scoring, we located 11 recurrently mutated noncoding regions enriched for at least three functional mutations in 14 follicular lymphoma genomes, and validated 9 clusters (82%) in the International Cancer Genome Consortium cohort (n=36), including promoter and enhancer regions of PAX5. IW-Scoring offers greater versatility to identify trait and disease associated noncoding variants."}, {"title": "riboSeed: leveraging prokaryotic genomic architecture to assemble across ribosomal regions", "url": "https://www.biorxiv.org/content/early/2017/07/14/159798", "tag": "Bioinformatics", "abstract": "The vast majority of bacterial genome sequencing has been performed using Illumina short reads. Because of the inherent difficulty of resolving repeated regions with short reads alone, only ~10 of sequencing projects have resulted in a closed genome. The most common repeated regions are those coding for ribosomal operons (rDNAs), which occur in a bacterial genome between 1 and 15 times and are typically used as sequence markers to classify and identify bacteria. Here, we show that the genomic context in which rDNAs occur is conserved across taxa and that, by utilizing the conserved nature of rDNAs across taxa and the uniqueness of their flanking regions, it is possible to improve assembly of these regions relative to de novo sequencing. We describe a method which constructs targeted pseudocontigs generated by iteratively assembling reads that map to a reference genome's rDNAs. These pseudocontigs are then used to more accurately assemble the newly-sequenced chromosome. We show that this method, implemented as riboSeed, correctly bridges across adjacent contigs in bacterial genome assembly and, when used in conjunction with other genome polishing tools, can result in closure of a genome."}, {"title": "confFuse: high-confidence fusion gene detection across tumor entities", "url": "https://www.biorxiv.org/content/early/2017/07/14/163675", "tag": "Bioinformatics", "abstract": "Background: Fusion genes play an important role in the tumorigenesis of many cancers. Next-generation sequencing (NGS) technologies have been successfully applied in fusion gene detection for the last several years, and a number of NGS-based tools have been developed for identifying fusion genes during this period. Most fusion gene detection tools based on RNA-seq data report a large number of candidates (mostly false positives), making it hard to prioritize candidates for experimental validation and further analysis. Selection of reliable fusion genes for downstream analysis becomes very important in cancer research. We therefore developed confFuse, a scoring algorithm to reliably select high-confidence fusion genes which are likely to be biologically relevant. Results: ConfFuse takes multiple parameters into account in order to assign each fusion candidate a confidence score, of which score \u22658 indicates high-confidence fusion gene predictions. These parameters were manually curated based on our experience and on certain structural motifs of fusion genes. Compared with alternative tools, based on 96 published RNA-seq samples from different tumor entities, our method can significantly reduce the number of fusion candidates (301 high-confidence from 8,083 total predicted fusion genes) and keep high detection accuracy (recovery rate 85.7%). Validation of 18 novel, high-confidence fusions detected in three breast tumor samples resulted in a 100% validation rate. Conclusions: ConfFuse is a novel downstream filtering method that allows selection of highly reliable fusion gene candidates for further downstream analysis and experimental validations. confFuse is available at https://github.com/Zhiqin-HUANG/confFuse."}, {"title": "Intragenic differential expression in archaea transcriptomes revealed by computational analysis of tiling microarrays", "url": "https://www.biorxiv.org/content/early/2017/07/14/163436", "tag": "Bioinformatics", "abstract": "Recent advances, in high-throughput technologies allows whole transcriptome analysis, providing a complete and panoramic view of intragenic differential expression in eukaryotes. However, intragenic differential expression in prokaryotes still mystery and incompletely understood. In this study, we investigated and collected the evidence for intragenic differential expression in several archaeal transcriptomes such as, Halobacterium salinarum NRC-1, Pyrococcus furiosus, Methanococcus maripaludis, and Sulfolobus solfataricus, based on computational methods; specifically, by well-known self-organizing map (SOM) for cluster analysis, which transforms high dimensional data into low dimensional. We found 104 (3.86%) of genes in Halobacterium salinarum NRC-1, 59 (2.56%) of genes in Pyrococcus furiosus, 43 (2.41%) of genes Methanococcus maripaludis and 13 (0.42%) of genes in Sulfolobus solfataricus have two or more clusters, i.e., showed the intragenic differential expression at different conditions."}, {"title": "C-State: An interactive web app for simultaneous multi-gene visualization and comparative epigenetic pattern search", "url": "https://www.biorxiv.org/content/early/2017/07/14/163634", "tag": "Bioinformatics", "abstract": "Background: Comparative epigenomic analysis across multiple genes presents a bottleneck for bench biologists working with NGS data. Despite the development of standardized peak analysis algorithms, the identification of novel epigenetic patterns and their visualization across gene subsets remains a challenge. Results: We developed a fast and interactive web app, C-State (Chromatin-State), to query and plot chromatin landscapes across multiple loci and cell types. C-State has an interactive, JavaScript-based graphical user interface and runs locally in modern web browsers that are pre-installed on all computers, thus eliminating the need for cumbersome data transfer, pre-processing and prior programming knowledge. Conclusions: C-State is unique in its ability to extract and analyze multi-gene epigenetic information. It allows for powerful GUI-based pattern searching and visualization. We include a case study to demonstrate its potential for identifying user-defined epigenetic trends in context of gene expression profiles."}, {"title": "Repliscan: a tool for classifying replication timing regions", "url": "https://www.biorxiv.org/content/early/2017/07/13/094177", "tag": "Bioinformatics", "abstract": "Background: Replication timing experiments that use label incorporation and high throughput sequencing produce peaked data similar to ChIP-Seq experiments. However, the differences in experimental design, coverage density, and possible results make traditional ChIP-Seq analysis methods inappropriate for use with replication timing. Results: To accurately detect and classify regions of replication across the genome, we present Repliscan. Repliscan robustly normalizes, automatically removes outlying and uninformative data points, and classifies Repli-seq signals into discrete combinations of replication signatures. The quality control steps and self-fitting methods make Repliscan generally applicable and more robust than previous methods that classify regions based on thresholds. Conclusions: Repliscan is simple and effective to use on organisms with different genome sizes. Even with analysis window sizes as small as 1 kilobase, reliable profiles can be generated with as little as 2.4x coverage."}, {"title": "Proposal for minimum information guidelines to report and reproduce results of particle tracking and motion analysis", "url": "https://www.biorxiv.org/content/early/2017/07/13/155036", "tag": "Bioinformatics", "abstract": "The proposed Minimum Information About Particle Tracking Experiments (MIAPTE) reporting guidelines described here aim to deliver a set of rules representing the minimal information required to report and support interpretation and assessment of data arising from intracellular multiple particle tracking (MPT) experiments. Examples of such experiments are those tracking viral particles as they move from the site of entry to the site of replication within an infected cell, or those following vesicular dynamics during secretion, endocytosis, or exocytosis. By promoting development of community standards, we hope that MIAPTE will contribute to making MPT data FAIR (Findable Accessible Interoperable and Reusable). Ultimately, the goal of MIAPTE is to promote and maximize data access, discovery, preservation, re-use, and repurposing through efficient annotation, and ultimately to enable reproducibility of particle tracking experiments. This document introduces MIAPTE v0.2, which updates the version that was posted to Fairsharing.org in October 2016. MIAPTE v0.2 is presented with the specific intent of soliciting comments from the particle tracking community with the purpose of extending and improving the model. The MIAPTE guidelines are intended for different categories of users: 1) Scientists with the desire to make new results available in a way that can be interpreted unequivocally by both humans and machines. For this class of users, MIAPTE provides data descriptors to define data entry terms and the analysis workflow in a unified manner. 2) Scientists wishing to evaluate, replicate and re-analyze results published by others. For this class of users MIAPTE provides descriptors that define the analysis procedures in a manner that facilitates its reproduction. 3) Developers who want to take advantage of the schema of MIAPTE to produce MIAPTE compatible tools. MIAPTE consists of a list of controlled vocabulary (CV) terms that describe elements and properties for the minimal description of particle tracking experiments, with a focus on viral and vesicular traffic within cells. As part of this submission we provide entity relationship (ER) diagrams that show the relationship between terms. Finally, we also provide documents containing the MIAPTE-compliant XML schema describing the data model used by Open Microscopy Environment inteGrated Analysis (OMEGA), our novel particle tracking data analysis and management tool, which is reported in a separate manuscript. MIAPTE is structured in two sub-sections: 1) Section 1 contains elements, attributes and data structures describing the results of particle tracking, namely: particles, links, trajectories and trajectory segments. 2) Section 2 contains elements that provide details about the algorithmic procedure utilized to produce and analyze trajectories as well as the results of trajectory analysis. In addition MIAPTE includes those OME-XML elements that are required to capture the acquisition parameters and the structure of images to be subjected to particle tracking."}, {"title": "Incomplete inhibition of HIV infection results in more HIV infected lymph node cells by reducing cell death", "url": "https://www.biorxiv.org/content/early/2017/07/13/163352", "tag": "Bioinformatics", "abstract": "HIV has been reported to be cytotoxic in vitro and in lymph node infection models. Using a computational approach, we found that partial inhibition of transmission which involves multiple virions per cell could lead to increased numbers of live infected cells if the number of viral DNA copies remains above one after inhibition, as eliminating the surplus viral copies reduces cell death. Using a cell line, we observed increased numbers of live infected cells when infection was partially inhibited with the antiretroviral efavirenz or neutralizing antibody. We then used efavirenz at concentrations reported in lymph nodes to inhibit lymph node infection by partially resistant HIV mutants. We observed more live infected lymph node cells, but with fewer HIV DNA copies per cell, relative to no drug. Hence, counterintuitively, limited attenuation of HIV transmission per cell may increase live infected cell numbers in environments where the force of infection is high."}, {"title": "Principal Metabolic Flux Mode Analysis", "url": "https://www.biorxiv.org/content/early/2017/07/13/163055", "tag": "Bioinformatics", "abstract": "Motivation: In the analysis of metabolism using omics data, two distinct and complementary approaches are frequently used: Principal component analysis (PCA) and Stoichiometric flux analysis. PCA is able to capture the main modes of variability in a set of experiments and does not make many prior assumptions about the data, but does not inherently take into account the flux mode structure of metabolism. Stoichiometric flux analysis methods, such as Flux Balance Analysis (FBA) and Elementary Mode Analysis, on the other hand, produce results that are readily interpretable in terms of metabolic flux modes, however, they are not best suited for exploratory analysis on a large set of samples. Results: We propose a new methodology for the analysis of metabolism, called Principal Metabolic Flux Mode Analysis (PMFA), which marries the PCA and Stoichiometric flux analysis approaches in an elegant regularized optimization framework. In short, the method incorporates a variance maximization objective form PCA coupled with a Stoichiometric regularizer, which penalizes projections that are far from any flux modes of the network. For interpretability, we also introduce a sparse variant of PMFA that favours flux modes that contain a small number of reactions. Our experiments demonstrate the versatility and capabilities of our methodology."}, {"title": "TriPoly: a haplotype estimation approach for polyploids using sequencing data of related individuals", "url": "https://www.biorxiv.org/content/early/2017/07/13/163162", "tag": "Bioinformatics", "abstract": "Knowledge of \"haplotypes\", i.e. phased and ordered marker alleles on a chromosome, is essential to answer many questions in genetics and genomics. By generating short pieces of DNA sequence, high-throughput modern sequencing technologies make estimation of haplotypes possible for single individuals. In polyploids, however, haplotype estimation methods usually require deep coverage to achieve sufficient accuracy. This often renders sequencing-based approaches too costly to be applied to large populations needed in studies of Quantitative Trait Loci (QTL). We propose a novel haplotype estimation method for polyploids, TriPoly, that combines sequencing data with Mendelian inheritance rules to infer haplotypes in parent-offspring trios. Using realistic simulations of short-read sequencing data for potato (Solanum tuberosum) and banana (Musa acuminata) trios, we show that TriPoly yields more accurate progeny haplotypes at low coverages compared to the existing methods that work on single individuals."}, {"title": "HECIL: A Hybrid Error Correction Algorithm for Long Reads with Iterative Learning", "url": "https://www.biorxiv.org/content/early/2017/07/13/162917", "tag": "Bioinformatics", "abstract": "Second-generation sequencing techniques generate short reads that can result in fragmented genome assemblies. Third-generation sequencing platforms mitigate this limitation by producing longer reads that span across complex and repetitive regions. Currently, the usefulness of such long reads is limited, however, because of high sequencing error rates. To exploit the full potential of these longer reads, it is imperative to correct the underlying errors. We propose HECIL \u2014 Hybrid Error Correction with Iterative Learning \u2014 a hybrid error correction framework that determines a correction policy for erroneous long reads, based on optimal combinations of decision weights obtained from short read alignments. We demonstrate that HECIL outperforms state-of-the-art error correction algorithms for an overwhelming majority of evaluation metrics on diverse real data sets including E. coli, S. cerevisiae, and the malaria vector mosquito A. funestus. We further improve the performance of HECIL by introducing an iterative learning paradigm that improves the correction policy at each iteration by incorporating knowledge gathered from previous iterations via confidence metrics assigned to prior corrections."}, {"title": "An exact transformation of convolutional kernels applied directly to DNA/RNA sequences", "url": "https://www.biorxiv.org/content/early/2017/07/13/163220", "tag": "Bioinformatics", "abstract": "Motivation: The powerful learning ability of a convolutional neural network (CNN) to perform functional classification of DNA/RNA sequences could provide valuable clues for the discovery of underlying biological mechanisms. Currently, however, the only way to interpret the direct application of a convolutional kernel to DNA/RNA sequences is the heuristic construction of a position weight matrix (PWM) from fragments scored highly by that kernel; whether the resulting PWM still performs the sequence classification well is unclear. Results: We developed a novel kernel-to-PWM transformation whose result is theoretically provable. Specifically, we proved that the log-likelihood of the resulting PWM of any DNA/RNA sequence is exactly the sum of a constant and the convolution of the original kernel on the same sequence. Importantly, we further proved that the resulting PWM demonstrates the same performance, in theory, as the original kernel under popular CNN frameworks. Surprisingly, our PWMs almost always outperformed heuristic ones at sequence classification, whether the discriminative motif was sequence- or structure-conserved. These results compelled us to further develop a maximum likelihood estimation of the optimal PWM for each kernel and a back-transformation of predefined PWMs into kernels. These tools can benefit the biological interpretation of kernel signals. Availability: Python scripts for the transformation from kernel to PWM, the inverted transformation from PWM to kernel, and the maximum likelihood estimation of optimal PWM are available through ftp://ftp.cbi.pku.edu.cn/pub/software/CBI/k2p."}, {"title": "An Algorithm for Cellular Reprogramming", "url": "https://www.biorxiv.org/content/early/2017/07/13/162974", "tag": "Bioinformatics", "abstract": "The day we understand the time evolution of subcellular elements at a level of detail comparable to physical systems governed by Newton's laws of motion seems far away. Even so, quantitative approaches to cellular dynamics add to our understanding of cell biology, providing data-guided frameworks that allow us to develop better predictions about, and methods for, control over specific biological processes and system-wide cell behavior. In this paper, we describe an approach to optimizing the use of transcription factors (TFs) in the context of cellular reprogramming. We construct an approximate model for the natural evolution of a cell cycle synchronized population of human fibroblasts, based on data obtained by sampling the expression of 22,083 genes at several time points along the cell cycle. In order to arrive at a model of moderate complexity, we cluster gene expression based on the division of the genome into topologically associating domains (TADs) and then model the dynamics of the TAD expression levels. Based on this dynamical model and known bioinformatics, such as transcription factor binding sites (TFBS) and functions, we develop a methodology for identifying the top transcription factor candidates for a specific cellular reprogramming task. The approach used is based on a device commonly used in optimal control. Our data-guided methodology identifies a number of transcription factors previously validated for reprogramming and/or natural differentiation. Our findings highlight the immense potential of dynamical models, mathematics, and data-guided methodologies for improving strategies for control over biological processes."}, {"title": "Aether: Leveraging Linear Programming For Optimal Cloud Computing In Genomics", "url": "https://www.biorxiv.org/content/early/2017/07/13/162883", "tag": "Bioinformatics", "abstract": "Across biology we are seeing rapid developments in scale of data production without a corresponding increase in data analysis capabilities. Here, we present Aether (http://aether.kosticlab.org), an intuitive, easy-to-use, cost-effective, and scalable framework that uses linear programming (LP) to optimally bid on and deploy combinations of underutilized cloud computing resources. Our approach simultaneously minimizes the cost of data analysis while maximizing its efficiency and speed. As a test, we used Aether to de novo assemble 1572 metagenomic samples, a task it completed in merely 13 hours with cost savings of approximately 80% relative to comparable methods."}, {"title": "Blazing Signature Filter: a library for fast pairwise similarity comparisons", "url": "https://www.biorxiv.org/content/early/2017/07/12/162750", "tag": "Bioinformatics", "abstract": "Identifying similarities between datasets is a fundamental task in data mining and has become an integral part of modern scientific investigation. Whether the task is to identify co-expressed genes in large-scale expression surveys or to predict combinations of gene knockouts which would elicit a similar phenotype, the underlying computational task is often a multi-dimensional similarity test. As datasets continue to grow, improvements to the efficiency, sensitivity or specificity of such computation will have broad impacts as it allows scientists to more completely explore the wealth of scientific data. A significant practical drawback of large-scale data mining is that the vast majority of pairwise comparisons are unlikely to be relevant, meaning that they do not share a signature of interest. It is therefore essential to efficiently identify these unproductive comparisons as rapidly as possible and exclude them from more time-intensive similarity calculations. The Blazing Signature Filter (BSF) is a highly efficient pairwise similarity algorithm which enables extensive data mining within a reasonable amount of time. The algorithm transforms datasets into binary metrics, allowing it to utilize the computationally efficient bit operators and provide a coarse measure of similarity. As a result, the BSF can scale to high dimensionality and rapidly filter unproductive pairwise comparison. Two bioinformatics applications of the tool are presented to demonstrate the ability to scale to billions of pairwise comparisons and the usefulness of this approach."}, {"title": "CRAVAT 4: Cancer-Related Analysis of Variants Toolkit", "url": "https://www.biorxiv.org/content/early/2017/07/12/162859", "tag": "Bioinformatics", "abstract": "Cancer sequencing studies are increasingly comprehensive and well-powered, returning long lists of somatic mutations that can be difficult to sort and interpret. Diligent analysis and quality control can require multiple computational tools of distinct utility and producing disparate output, creating additional challenges for the investigator. The Cancer-Related Analysis of Variants Toolkit (CRAVAT) is an evolving suite of informatics tools for mutation interpretation that includes mutation projecting and quality control, impact prediction and extensive annotation, gene- and mutation-level interpretation including joint prioritization of all nonsilent consequence types, and structural and mechanistic visualization. Results from CRAVAT submissions are explored in an interactive, user-friendly web-environment with dynamic filtering and sorting designed to highlight the most informative mutation, even in the context of very large studies. CRAVAT can be run on a public web-portal, in the cloud, or downloaded for local use, and is easily integrated with other methods for cancer omics analysis."}, {"title": "Sequanix: A Dynamic Graphical Interface for Snakemake Workflows", "url": "https://www.biorxiv.org/content/early/2017/07/12/162701", "tag": "Bioinformatics", "abstract": "We designed a PyQt graphical user interface -- Sequanix -- aiming at democratizing the use of Snakemake pipelines. Although the primary goal of Sequanix was to facilitate the execution of NGS Snakemake pipelines available in the Sequana project (http://sequana.readthedocs.io), it can also handle any Snakemake pipelines. Therefore, Sequanix should be useful to all Snakemake developers willing to expose their pipelines to a wider audience."}, {"title": "Deep convolutional neural networks allow analysis of cell motility during stem cell differentiation and neoplastic transformation", "url": "https://www.biorxiv.org/content/early/2017/07/12/159202", "tag": "Bioinformatics", "abstract": "Cells in culture display diverse motility behaviors. In multiple contexts, motility behaviors reflect broader cell function, providing motivation to discriminate between different motility behaviors. Current methods to do so rely upon manual feature engineering. However, the types of features necessary to distinguish between motility behaviors can vary greatly depending on the biological context, and it is not always clear which features may be most predictive in each setting for distinguishing particular cell types or disease states. Convolutional neural networks (CNNs) are a class of machine learning models ideally suited to the analysis of spatial data, allowing for relevant spatial features to be learned as parameters of a model. Given that motility data is inherently spatial, CNNs are a promising approach to learn relevant features for motility analysis from data, rather than requiring a domain expert to engineer features by hand. Here, we apply CNNs to classify different motility behaviors by representing motility as a 3D space with markers denoting a cell location at each time point. 3D CNNs provide accurate classification of several simulated motility behaviors, the motility behaviors of multiple cell types, and characteristic motility behaviors of commitment states in myogenic cells. Autoencoders were trained effectively to learn representations of these 3D motility spaces in an unsupervised manner. We show that this approach can achieve reliable detection of differentiation state for muscle stem cells and better-than-chance detection of neoplastic transformation in a cancer cell model. The variety of cell type differences we can detect suggests that the algorithm is generally applicable to novel cell types. While we have applied these methods to the analysis of cell motility, our scheme for representing motion spatially for analysis by CNNs is generalizable to other motion classification problems."}, {"title": "wft4galaxy: A Workflow Tester for Galaxy", "url": "https://www.biorxiv.org/content/early/2017/07/12/132001", "tag": "Bioinformatics", "abstract": "Motivation. Workflow managers for scientific analysis provide a high-level programming platform facilitating standardization, automation, collaboration and access to sophisticated computing resources. The Galaxy workflow manager provides a prime example of this type of platform. As compositions of simpler tools, workflows effectively comprise specialized computer programs implementing often very complex analysis procedures. To date, no simple way exists to automatically test Galaxy workflows and ensure their correctness. Results. With wft4galaxy we offer a tool to bring automated testing to Galaxy workflows, making it feasible to bring continuous integration to their development and ensuring that defects are detected promptly. wft4galaxy can be easily installed as a regular Python program or launched directly as a Docker container -- the latter reducing installation effort to a minimum. Availability. wft4galaxy is available online at https://github.com/phnmnl/wft4galaxy."}, {"title": "QuASAR-MPRA: Accurate allele-specific analysis for massively parallel reporter assays", "url": "https://www.biorxiv.org/content/early/2017/07/12/105627", "tag": "Bioinformatics", "abstract": "The majority of the human genome is composed of non-coding regions containing regulatory elements such as enhancers, which are crucial for controlling gene expression. Many variants associated with complex traits are in these regions, and may disrupt gene regulatory sequences. Consequently, it is important to not only identify true enhancers but also to test if a variant within an enhancer affects gene regulation. Recently, allele-specific analysis in high-throughput reporter assays, such as massively parallel reporter assays (MPRA), have been used to functionally validate non-coding variants. However, we are still missing high-quality and robust data analysis tools for these datasets. We have further developed our method for allele-specific analysis QuASAR (quantitative allele-specific analysis of reads) to analyze allele-specific signals in barcoded read counts data from MPRAs. Using this approach, we can take into account the uncertainty on the original plasmid proportions, over-dispersion, and sequencing errors. The provided allelic skew estimate and its standard error also simplifies meta-analysis of replicate experiments. Additionally, we show that a beta-binomial distribution better models the variability present in the allelic imbalance of these synthetic reporters and results in a test that is statistically well calibrated under the null. Applying this approach to the MPRA data by Tewhey et al (2016), we find 602 SNPs with significant (FDR 10%) allele-specific regulatory function in LCLs. We also show that we can combine MPRA with QuASAR estimates to validate existing experimental and computational annotations of regulatory variants. Our study shows that by having the appropriate data analysis tools, we can greatly improve the power to detect allelic effects in high throughput reporter assays."}, {"title": "Text mining of 15 million full-text scientific articles", "url": "https://www.biorxiv.org/content/early/2017/07/11/162099", "tag": "Bioinformatics", "abstract": "Across academia and industry, text mining has become a popular strategy for keeping up with the rapid growth of the scientific literature. Text mining of the scientific literature has mostly been carried out on collections of abstracts, due to their availability. Here we present an analysis of 15 million English scientific full-text articles published during the period 1823-2016. We describe the development in article length and publication sub-topics during these nearly 250 years. We showcase the potential of text mining by extracting published protein-protein, disease-gene, and protein subcellular associations using a named entity recognition system, and quantitatively report on their accuracy using gold standard benchmark data sets. We subsequently compare the findings to corresponding results obtained on 16.5 million abstracts included in MEDLINE and show that text mining of full-text articles consistently outperforms using abstracts only."}, {"title": "2-Way k-Means as a Model for Microbiome Samples", "url": "https://www.biorxiv.org/content/early/2017/07/11/161935", "tag": "Bioinformatics", "abstract": "Microbiome sequencing allows defining clusters of samples with shared composition. However, this paradigm poorly accounts for samples whose composition is a mixture of cluster-characterizing ones, and therefore lie in-between them in cluster space. This paper addresses unsupervised learning of 2-way clusters. It defines a mixture model that allows 2-way cluster assignment and describes a variant of generalized $k$-means for learning such a model. We demonstrate applicability to microbial 16S rDNA sequencing data from the Human Vaginal Microbiome Project."}, {"title": "omniCLIP: Bayesian identification of protein-RNA interactions from CLIP-Seq data", "url": "https://www.biorxiv.org/content/early/2017/07/11/161877", "tag": "Bioinformatics", "abstract": "High-throughput immunoprecipitation methods to analyze RNA binding protein-RNA interactions and modifications have great potential to further the understanding of post-transcriptional gene regulation. Due to the differences between individual approaches, each of a diverse number of computational methods can typically be applied to only one specific sequencing protocol. Here, we present a Bayesian model called omniCLIP that can be applied to data from all protocols to detect regulatory elements in RNAs. omniCLIP greatly simplifies the data analysis, increases the reliability of results and paves the way for integrative studies based on data from different sources."}, {"title": "Science Game Lab: tool for the unification of biomedical games with a purpose", "url": "https://www.biorxiv.org/content/early/2017/07/10/156141", "tag": "Bioinformatics", "abstract": "Games with a purpose and other kinds of citizen science initiatives demonstrate great potential for advancing biomedical science and improving STEM education. Articles documenting the success of projects such as Fold.it and Eyewire in high impact journals have raised wide interest in new applications of the distributed human intelligence that these systems have tapped into. However, the path from a good idea to a successful citizen science game remains highly challenging. Apart from the scientific difficulties of identifying suitable problems and appropriate human-powered solutions, the games still need to be created, need to be fun, and need to reach a large audience that remain engaged for the long-term. Here, we describe Science Game Lab (SGL) (https://sciencegamelab.org), a platform for bootstrapping the production, facilitating the publication, and boosting both the fun and the value of the user experience for scientific games with a purpose."}, {"title": "GEM: A manifold learning based framework for reconstructing spatial organizations of chromosomes", "url": "https://www.biorxiv.org/content/early/2017/07/10/161208", "tag": "Bioinformatics", "abstract": "Decoding the spatial organizations of chromosomes has crucial implications for studying eukaryotic gene regulation. Recently, Chromosomal conformation capture based technologies, such as Hi-C, have been widely used to uncover the interaction frequencies of genomic loci in high-throughput and genome-wide manner and provide new insights into the folding of three-dimensional (3D) genome structure. In this paper, we develop a novel manifold learning framework, called GEM (Genomic organization reconstructor based on conformational Energy and Manifold learning), to elucidate the underlying 3D spatial organizations of chromosomes from Hi-C data. Unlike previous chromatin structure reconstruction methods, which explicitly assume specific relationships between Hi-C interaction frequencies and spatial distances between distal genomic loci, GEM is able to reconstruct an ensemble of chromatin conformations by directly embedding the neighboring affinities from Hi-C space into 3D Euclidean space based on a manifold learning strategy that considers both the fitness of Hi-C data and the biophysical feasibility of the modeled structures, which are measured by the conformational energy derived from our current biophysical knowledge about the 3D polymer model. Extensive validation tests on both simulated interaction frequency data and experimental Hi-C data of yeast and human demonstrated that GEM not only greatly outperformed other state-of-art modeling methods but also reconstructed accurate chromatin structures that agreed well with the hold-out or independent Hi-C data and sparse geometric restraints derived from the previous fluorescence in situ hybridization (FISH) studies. In addition, as GEM can generate accurate spatial organizations of chromosomes by integrating both experimentally-derived spatial contacts and conformational energy, we for the first time extended our modeling method to recover long-range genomic interactions that are missing from the original Hi-C data. All these results indicated that GEM can provide a physically and physiologically valid 3D representations of the organizations of chromosomes and thus serve as an effective and useful genome structure reconstructor."}, {"title": "SRSF shape analysis for sequencing data reveal new differentiating patterns", "url": "https://www.biorxiv.org/content/early/2017/07/10/161448", "tag": "Bioinformatics", "abstract": "Motivation: Sequencing-based methods to examine fundamental features of the genome, such as gene expression and chromatin structure, rely on inferences from the abundance and distribution of reads derived from Illumina sequencing. Drawing sound inferences from such experiments relies on appropriate mathematical methods to model the distribution of reads along the genome, which has been challenging due to the scale and nature of these data. Results: We propose a new framework (SRSFseq) based on Square Root Slope Functions shape analysis to analyse Illumina sequencing data. In the new approach the basic unit of information is the density of mapped reads over region of interest located on the known reference genome. The densities are interpreted as shapes and a new shape analysis model is proposed. An equivalent of a Fisher test is used to quantify the significance of shape differences in read distribution patterns between groups of density functions in different experimental conditions. We evaluated the performance of this new framework to analyze RNA-seq data at the exon level, which enabled the detection of variation in read distributions and abundances between experimental conditions not detected by other methods. Thus, the method is a suitable supplement to the state of the are count based techniques. The variety of density representations and flexibility of mathematical design allow the model to be easily adapted to other data types or problems in which the distribution of reads is to be tested. The functional interpretation and SRSF phase-amplitude separation technique gives an efficient noise reduction procedure improving the sensitivity and specificity of the method."}, {"title": "Lineage: Visualizing Multivariate Clinical Data in Genealogy Graphs", "url": "https://www.biorxiv.org/content/early/2017/07/09/128579", "tag": "Bioinformatics", "abstract": "The majority of diseases that are a significant challenge for public and individual heath are caused by a combination of hereditary and environmental factors. In this paper we introduce Lineage, a novel visual analysis tool designed to support domain experts that study such multifactorial diseases in the context of genealogies. Incorporating familial relationships between cases can provide insights on shared genomic variants that could be implicated in diseases, but also into shared environmental exposures. We introduce a data and task abstraction and argue that the problem of analyzing such diseases based on genealogical, clinical, and genetic data can be mapped to a multivariate graph visualization problem. The main contribution of our design study is a novel visual representation for tree-like, multivariate graphs, which we apply to genealogies and clinical data about the individuals in these families. We introduce data-driven aggregation methods to scale to multiple families. By designing the genealogy graph layout to align with a tabular view, we are able to incorporate extensive, multivariate attributes in the analysis of the genealogy without cluttering the graph. We validate our designs using an illustrative example based on real-world data, and report on feedback from domain experts."}, {"title": "HiPiler: Visual Exploration Of Large Genome Interaction Matrices With Interactive Small Multiples", "url": "https://www.biorxiv.org/content/early/2017/07/09/123588", "tag": "Bioinformatics", "abstract": "This paper presents an interactive visualization interface - HiPiler - for the exploration and visualization of regions-of-interest in large genome interaction matrices. Genome interaction matrices approximate the physical distance of pairs of genomic regions to each other and can contain up to 3 million rows and columns with many sparse regions. Traditional matrix aggregation or pan-and-zoom interfaces largely fail in supporting search, inspection, and comparison of local regions-of-interest (ROIs). ROIs can be defined, e.g., by sets of adjacent rows and columns, or by specific visual patterns in the matrix. ROIs are first-class objects in HiPiler, which represents them as thumbnail-like \u201csnippets\u201d. Snippets can be laid out automatically based on their data and meta attributes. They are linked back to the matrix and can be explored interactively. The design of HiPiler is based on a series of semi-structured interviews with 10 domain experts involved in the analysis and interpretation of genome interaction matrices. We describe six exploration tasks that are crucial for analysis of interaction matrices and demonstrate how HiPiler supports these tasks. We report on a user study with a series of data exploration sessions with domain experts to assess the usability of HiPiler as well as to demonstrate respective findings in the data."}, {"title": "Gene isoforms as expression-based biomarkers predictive of drug response in vitro", "url": "https://www.biorxiv.org/content/early/2017/07/08/160937", "tag": "Bioinformatics", "abstract": "Background: One of the main challenges in precision medicine is the identification of molecular features associated to drug response to provide clinicians with tools to select the best therapy for each individual cancer patient. The recent adoption of next-generation sequencing technologies enables accurate profiling of not only gene expression but also alternatively-spliced transcripts in large-scale pharmacogenomic studies. Given that altered mRNA splicing has been shown to be prominent in cancers, linking this feature to drug response will open new avenues of research in biomarker discovery. Methods: To address the lack of reproducibility of drug sensitivity measurements across studies, we developed a meta-analytical framework combining the pharmacological data generated within the Cancer Cell Line Encyclopedia (CCLE) and the Genomics of Drug Sensitivity in Cancer (GDSC). Predictive models are fitted with CCLE RNA-seq data as predictor variables, controlled for tissue type, and combined GDSC and CCLE drug sensitivity values as dependent variables. Results: We first validated the biomarkers identified from GDSC and CCLE using an existing pharmacogenomic dataset of 70 breast cancer cell lines. We further selected four drugs with the most promising biomarkers to test whether their predictive value is robust to change in pharmacological assay. We successfully validated 10 isoform-based biomarkers predictive of drug response in breast cancer, including TGFA-001 for the MEK tyrosine kinase inhibitor (TKI) AZD6244, DUOX-001 for the EGFR inhibitor erlotinib, and CPEB4-001 transcript expression associated with lack of sensitivity to paclitaxel. Conclusion: The results of our meta-analysis of pharmacogenomic data suggest that isoforms represent a rich resource for biomarkers predictive of response to chemo- and targeted therapies. Our study also showed that the validation rate for this type of biomarkers is low (<50%) for most drugs, supporting the requirements for independent datasets to identify reproducible predictors of response to anticancer drugs."}, {"title": "Brain xQTL Map: Integrating The Genetic Architecture Of The Human Brain Transcriptome And Epigenome", "url": "https://www.biorxiv.org/content/early/2017/07/07/142927", "tag": "Bioinformatics", "abstract": "We perform quantitative trait locus (xQTL) analyses on a multi-omic dataset, comprising RNA sequence, DNA methylation, and histone acetylation ChIP sequence data from the dorsolateral prefrontal cortex of 411 older adult individuals. We identify SNPs that are significantly associated with gene expression, DNA methylation, and histone modification levels. Many SNPs influence more than one type of molecular feature, and epigenetic features are shown to mediate eQTLs in a number of (9%) such loci. We illustrate the utility of our new resource, xQTL Serve, in prioritizing the cell type most affected by an xQTL and in enhancing genome wide association studies (GWAS) as we report 18 additional CNS disease susceptibility loci after re-analyzing published studies."}, {"title": "Separable Fully Connected Layers Improve Deep Learning Models For Genomics", "url": "https://www.biorxiv.org/content/early/2017/07/07/146431", "tag": "Bioinformatics", "abstract": "Convolutional neural networks are rapidly gaining popularity in regulatory genomics. Typically, these networks have a stack of convolutional and pooling layers, followed by one or more fully connected layers. In genomics, the same positional patterns are often present across multiple convolutional channels. Therefore, in current state-of-the-art networks, there exists significant redundancy in the representations learned by standard fully connected layers. We present a new separable fully connected layer that learns a weights tensor that is the outer product of positional weights and cross-channel weights, thereby allowing the same positional patterns to be applied across multiple convolutional channels. Decomposing positional and cross-channel weights further enables us to readily impose biologically-inspired constraints on positional weights, such as symmetry. We also propose a novel regularizer and constraint that act on curvature in the positional weights. Using experiments on simulated and in vivo datasets, we show that networks that incorporate our separable fully connected layer outperform conventional models with analogous architectures and the same number of parameters. Additionally, our networks are more robust to hyperparameter tuning, have more informative gradients, and produce importance scores that are more consistent with known biology than conventional deep neural networks. Availability: Implementation: https://github.com/kundajelab/keras/tree/keras_1. A gist illustrating model setup is at: goo.gl/gYooaa."}, {"title": "Completing bacterial genome assemblies with multiplex MinION sequencing", "url": "https://www.biorxiv.org/content/early/2017/07/07/160614", "tag": "Bioinformatics", "abstract": "Illumina sequencing platforms have enabled widespread bacterial whole genome sequencing. While Illumina data is appropriate for many analyses, its short read length limits its ability to resolve genomic structure. This has major implications for tracking the spread of mobile genetic elements, including those which carry antimicrobial resistance determinants. Fully resolving a bacterial genome requires long-read sequencing such as those generated by Oxford Nanopore Technologies (ONT) platforms. Here we describe our use of the ONT MinION to sequence 12 isolates of Klebsiella pneumoniae on a single flow cell. We assembled each genome using a combination of ONT reads and previously available Illumina reads, and little to no manual intervention was needed to achieve fully resolved assemblies using the Unicycler hybrid assembler. Assembling only ONT reads with Canu was less effective, resulting in fewer resolved genomes and higher error rates even following error correction with Nanopolish. We demonstrate that multiplexed ONT sequencing is a valuable tool for high-throughput bacterial genome finishing. Specifically, we advocate the use of Illumina sequencing as a first analysis step, followed by ONT reads as needed to resolve genomic structure."}, {"title": "Axe: rapid, competitive sequence read demultiplexing using a trie", "url": "https://www.biorxiv.org/content/early/2017/07/07/160606", "tag": "Bioinformatics", "abstract": "Here we implement a rapid algorithm for demultiplexing DNA sequence reads with in-read indixces. Axe selects the optimal index present in a sequence read, even in the presence of sequencing errors. The algorithm is able to handle combinatorial indexing, indices of differing length, and several mismatches per index sequence."}, {"title": "PIVOT: Platform for Interactive Analysis and Visualization of Transcriptomics Data", "url": "https://www.biorxiv.org/content/early/2017/07/06/053348", "tag": "Bioinformatics", "abstract": "Many R packages have been developed for transcriptome analysis but their use often requires familiarity with R and integrating results of different packages is difficult. Here we present PIVOT, an R-based application with a uniform user interface and graphical data management that allows non-programmers to conveniently access various bioinformatics tools and interactively explore transcriptomics data. PIVOT supports many popular open source packages for transcriptome analysis and provides an extensive set of tools for statistical data manipulations. A graph-based visual interface is used to represent the links between derived datasets, allowing easy tracking of data versions. PIVOT further supports automatic report generation, publication-quality plots, and program/data state saving, such that all analysis can be saved, shared and reproduced."}, {"title": "Toward A Scalable Exploratory Framework for Complex High-Dimensional Phenomics Data", "url": "https://www.biorxiv.org/content/early/2017/07/06/159954", "tag": "Bioinformatics", "abstract": "Motivation: Phenomics is an emerging branch of modern biology, which uses high throughput phenotyping tools to capture multiple environment and phenotypic trait measurements, at a massive scale. The resulting high dimensional data sets represent a treasure trove of information for providing an indepth understanding of how multiple factors interact and contribute to control the growth and behavior of different plant crop genotypes. However, computational tools that can parse through such high dimensional data sets and aid in extracting plausible hypothesis are currently lacking. In this paper, we present a new algorithmic approach to effectively decode and characterize the role of environment on phenotypic traits, from complex phenomic data. To the best of our knowledge, this effort represents the first application of topological data analysis on phenomics data. Results: We applied this novel algorithmic approach on a real-world maize data set. Our results demonstrate the ability of our approach to delineate emergent behavior among subpopulations, as dictated by one or more environmental factors; notably, our approach shows how the environment plays a key role in determining the phenotypic behavior of one of the two genotypes. Availability: Downloadable Source code and test data are freely available with instruction set at https://xperthut.github.io/HYPPO-X."}, {"title": "RNA Transcriptome Mapping with GraphMap", "url": "https://www.biorxiv.org/content/early/2017/07/06/160085", "tag": "Bioinformatics", "abstract": "Next generation sequencing technologies have made RNA sequencing widely accessible and applicable in many areas of research. In recent years, 3rd generation sequencing technologies have matured and are slowly replacing NGS for DNA sequencing. This paper presents a novel tool for RNA mapping guided by gene annotations. The tool is an adapted version of a previously developed DNA mapper - GraphMap, tailored for third generation sequencing data, such as those produced by Pacific Biosciences or Oxford Nanopore Technologies devices. It uses gene annotations to generate a transcriptome, uses a DNA mapping algorithm to map reads to the transcriptome, and finally transforms the mappings back to genome coordinates. Modified version of GraphMap is compared on several synthetic datasets to the state-of-the-art RNAseq mappers enabled to work with third generation sequencing data. The results show that our tool outperforms other tools in general mapping quality."}, {"title": "Uncovering genomic trajectories with heterogeneous genetic and environmental backgrounds across single-cells and populations", "url": "https://www.biorxiv.org/content/early/2017/07/05/159913", "tag": "Bioinformatics", "abstract": "Pseudotime algorithms can be employed to extract latent temporal information from cross-sectional data sets allowing dynamic biological processes to be studied in situations where the collection of genuine time series data is challenging or prohibitive. Computational techniques have arisen from areas such as single-cell 'omics and in cancer modelling where pseudotime can be used to learn about cellular differentiation or tumour progression. However, methods to date typically assume homogenous genetic and environmental backgrounds, which becomes particularly limiting as datasets grow in size and complexity. As a solution to this we describe a novel statistical framework that learns pseudotime trajectories in the presence of non-homogeneous genetic, phenotypic, or environmental backgrounds. We demonstrate that this enables us to identify interactions between such factors and the underlying genomic trajectory. By applying this model to both single-cell gene expression data and population level cancer studies we show that it uncovers known and novel interaction effects between genetic and enironmental factors and the expression of genes in pathways. We provide an R implementation of our method PhenoPath at https://github.com/kieranrcampbell/phenopath."}, {"title": "Network simulations of interneuron circuits in the honeybee primary auditory center", "url": "https://www.biorxiv.org/content/early/2017/07/05/159533", "tag": "Bioinformatics", "abstract": "Processing of airborne vibration signals in the auditory system is essential for honeybee communication through the waggle dance language. Properties of neurons in the honeybee primary auditory center suggest a circuitry of excitatory and inhibitory neurons encoding these communication signals. To test this assumption, we simulated this network and analyzed the predicted responses for different types of inputs. In particular, we investigated the effect of specific inhibitory connections in the network. The results indicate that the experimentally observed responses of certain interneuron types are compatible with an inhibitory network of vibration processing in the primary auditory center of the honeybee."}, {"title": "Anti-HIV activity prediction and enrichment identify novel human targets for Anti-HIV therapy", "url": "https://www.biorxiv.org/content/early/2017/07/04/159236", "tag": "Bioinformatics", "abstract": "Human immunodeficiency virus (HIV) relies heavily on the host proteins to facilitate its entry and replication. Currently, more than 4000 human proteins are recorded to be involved in the HIV-1 life cycle. Identifying appropriate anti-HIV targets from so many host proteins is crucial to anti-HIV drug development, but a challenging work. Here we combined anti-HIV activity prediction and enrichment analysis to identify novel human targets for anti-HIV therapy. We firstly developed an accurate prediction tool named Anti-HIV-Predictor (AUC > 0.96) to predict the anti-HIV activity of given compounds. Using this tool, we predicted 10488 anti-HIV compounds from ChEMBLdb. Then, based on this result and relationships of targets and compounds, we inferred 73 anti-HIV targets that enriched with anti-HIV compounds. The functional annotation and network analysis revealed that they directly or indirectly interact with 20 HIV proteins through neuropeptide signaling, GPCR signaling, cell surface signaling pathway, and so on. Nearly half of these targets overlap with the NCBI HIV dataset. However, the percentage of known therapeutic targets in these targets is significantly higher than that in the NCBI HIV dataset. After a series of feature analysis, we identified 13 novel human targets with high potential as anti-HIV targets, the inhibitors of which have experimentally confirmed anti-HIV activity. It is noteworthy that the inhibitors of REN and CALCA have better anti-HIV activity than CCR5 inhibitors. Taken together, our findings provide novel human targets for the host-oriented anti-HIV drug development and should significantly advance current anti-HIV research."}, {"title": "KinFin: Software for taxon-aware analysis of clustered protein sequences", "url": "https://www.biorxiv.org/content/early/2017/07/03/159145", "tag": "Bioinformatics", "abstract": "The field of comparative genomics is concerned with the study of similarities and differences between the information encoded in the genomes of organisms. A common approach is to define gene families by clustering protein sequences based on sequence similarity, and analyse protein cluster presence and absence in different species groups as a guide to biology. Due to the high dimensionality of these data, downstream analysis of protein clusters inferred from large numbers of species, or species with many genes, is non-trivial, and few solutions exist for transparent, reproducible and customisable analyses. We present KinFin, a streamlined software solution capable of integrating data from common file formats and delivering aggregative annotation of protein clusters. KinFin delivers analyses based on systematic taxonomy of the species analysed, or on user-defined groupings of taxa, for example sets based on attributes such as life history traits, organismal phenotypes, or competing phylogenetic hypotheses. Results are reported through graphical and detailed text output files. We illustrate the utility of the KinFin pipeline by addressing questions regarding the biology of filarial nematodes, which include parasites of veterinary and medical importance. We resolve the phylogenetic relationships between the species and explore functional annotation of proteins in clusters in key lineages and between custom taxon sets, identifying gene families of interest. KinFin can easily be integrated into existing comparative genomic workflows and promotes transparent and reproducible analysis of clustered protein data."}, {"title": "Column subset selection for single-cell RNA-Seq clustering", "url": "https://www.biorxiv.org/content/early/2017/07/03/159079", "tag": "Bioinformatics", "abstract": "The first step in the analysis of single-cell RNA sequencing (scRNA-Seq) is dimensionality reduction, which reduces noise and simplifies data visualization. However, techniques such as principal components analysis (PCA) fail to preserve non-negativity and sparsity structures present in the original matrices, and the coordinates of projected cells are not easily interpretable. Commonly used thresholding methods avoid those pitfalls, but ignore collinearity and covariance in the original matrix. We show that a deterministic column subset selection (DCSS) method possesses many of the favorable properties of PCA and common thresholding methods, while avoiding pitfalls from both. We derive new spectral bounds for DCSS. We apply DCSS to two measures of gene expression from two scRNA-Seq experiments with different clustering workflows, and compare to three thresholding methods. In each case study, the clusters based on the small subset of the complete gene expression profile selected by DCSS are similar to clusters produced from the full set. The resulting clusters are informative for cell type."}, {"title": "MassBlast: A workflow to accelerate RNA-seq and DNA database analysis", "url": "https://www.biorxiv.org/content/early/2017/07/03/131953", "tag": "Bioinformatics", "abstract": "Current workflows for sequence analysis heavily depend on user input and manual curation. New specialized tools and methods are appearing all the time, but the actions required for a full analysis are disconnected and very time-consuming. The software we propose, MassBlast, com-bines BLAST+ and an automated workflow analysis to filter the results and significantly improve the annotation of multiple sequencing databases for exploring new biosynthetic pathways and new protein families, among other applications. MassBlast is fully configurable and reproducible. Availability and Implementation: The MassBlast package is written in Ruby. Source code and re-leases are freely available from Github (https://github.com/averissimo/mass-blast) for all major platforms (Linux, MS Windows and OS X) under the GPLv3 license."}, {"title": "Evaluating Metagenome Assembly on a Simple Defined Community with Many Strain Variants", "url": "https://www.biorxiv.org/content/early/2017/07/03/155358", "tag": "Bioinformatics", "abstract": "We evaluate the performance of three metagenome assemblers, IDBA, MetaSPAdes, and MEGAHIT, on short-read sequencing of a defined \u201cmock\u201d community containing 64 genomes (Shakya et al. (2013)). We update the reference metagenome for this mock community and detect several additional genomes in the read data set. We show that strain confusion results in significant loss in assembly of reference genomes that are otherwise completely present in the read data set. In agreement with previous studies, we find that MEGAHIT performs best computationally; we also show that MEGAHIT tends to recover larger portions of the strain variants than the other assemblers."}, {"title": "PHASIS: A computational suite for de novo discovery and characterization of phased, siRNA-generating loci and their miRNA triggers", "url": "https://www.biorxiv.org/content/early/2017/07/03/158832", "tag": "Bioinformatics", "abstract": "Phased, secondary siRNAs (phasiRNAs) are found widely in plants, from protein-coding transcripts and long, non-coding RNAs; animal piRNAs are also phased. Integrated methods characterizing \u201cPHAS\u201d loci are unavailable, and existing methods are quite limited and inefficient in handling large volumes of sequencing data. The PHASIS suite described here provides complete tools for the computational characterization of PHAS loci, with an emphasis on plants, in which these loci are numerous. Benchmarked comparisons demonstrate that PHASIS is sensitive, highly scalable and fast. Importantly, PHASIS eliminates the requirement of a sequenced genome and PARE/degradome data for discovery of phasiRNAs and their miRNA triggers."}, {"title": "PEPA test: fast and powerful differential analysis from relative quantitative proteomics data using shared peptides", "url": "https://www.biorxiv.org/content/early/2017/06/30/158212", "tag": "Bioinformatics", "abstract": "We propose a new hypothesis test for the differential abundance of proteins in mass-spectrometry based relative quantification. An important feature of this type of high-throughput analyses is that it involves an enzymatic digestion of the sample proteins into peptides prior to identification and quantification. Due to numerous homology sequences, different proteins can lead to peptides with identical amino acid chains, so that their parent protein is ambiguous. These so-called shared peptides make the protein-level statistical analysis a challenge, so that they are often not accounted for. In this article, we use a linear model describing peptide-protein relationships to build a likelihood ratio test of differential abundance for proteins. We show that the likelihood ratio statistic can be computed in linear time with the number of peptides. We also provide the asymptotic null distribution of a regularized version of our statistic. Experiments on both real and simulated datasets show that our procedures outperforms state-of-the-art methods. The procedures are available via the pepa.test function of the DAPAR Bioconductor R package."}, {"title": "Detecting known repeat expansions with standard protocol next generation sequencing, towards developing a single screening test for neurological repeat expansion disorders", "url": "https://www.biorxiv.org/content/early/2017/06/30/157792", "tag": "Bioinformatics", "abstract": "Background: Repeat expansions cause over 20 neurogenetic disorders that can present with overlapping clinical phenotypes, making molecular diagnosis challenging. Single gene or small panel PCR-based methods are employed to identify the precise genetic cause, but can be slow and costly, and often yield no result. Genomic analysis via whole exome and whole genome sequencing (WES and WGS) is being increasingly performed to diagnose genetic disorders. However, until recently analysis protocols could not identify repeat expansions in these datasets. Methods: A new method for the identification of repeat expansions using either WES or WGS was developed. Four retrospective cohorts of individuals with eight different known repeat expansion disorders were analysed with the new method. Results were assessed by comparing to the known disease status. Performance was also compared to a recently published genotyping-based method, ExpansionHunter. Findings: Expansion repeats were successfully identified in WES and WGS datasets. The new method demonstrated very high predictive capabilities, achieving a median area under the curve (AUC) of 0.9. The new robust method achieved a median specificity and sensitivity of 0.99 and 0.75 respectively, compared to ExpansionHunter, a recently published genotyping-based method (median specificity = 0.99, median sensitivity = 0.56). Interpretation: The new method, called exSTRa (expanded STR algorithm), is available from https://github.com/bahlolab/exSTRa. It can be applied to existing WES or WGS data to identify likely repeat expansions. We demonstrate that exSTRa can be effectively utilized as a screening tool to interrogate WES and WGS sequencing data generated with PCR-based library preparations which can then be followed up with specific diagnostic tests."}, {"title": "zingeR: unlocking RNA-seq tools for zero-inflation and single cell applications", "url": "https://www.biorxiv.org/content/early/2017/06/30/157982", "tag": "Bioinformatics", "abstract": "Dropout in single cell RNA-seq (scRNA-seq) applications causes many transcripts to go undetected. It induces excess zero counts, which leads to power issues in differential expression (DE) analysis and has triggered the development of bespoke scRNA-seq DE tools that cope with zero-inflation. Recent evaluations, however, have shown that dedicated scRNA-seq tools provide no advantage compared to traditional bulk RNA-seq tools. We introduce zingeR, a zero-inflated negative binomial model that identifies excess zero counts and generates observation weights to unlock bulk RNA-seq pipelines for zero-inflation, boosting performance in scRNA-seq differential expression analysis."}, {"title": "A benchmarking of workflows for detecting differential splicing and differential expression at isoform level in human RNA-seq studies", "url": "https://www.biorxiv.org/content/early/2017/06/30/156752", "tag": "Bioinformatics", "abstract": "Over the last few years, RNA-seq has been used to study alterations in alternative splicing related to several diseases. Bioinformatics workflows used to perform these studies can be divided into two groups, those finding changes in the absolute isoform expression and those studying differential splicing. Many computational methods for transcriptomics analysis have been developed, evaluated and compared; however, there are not enough reports of systematic and objective assessment of processing pipelines as a whole. Moreover, comparative studies have been performed considering separately the changes in absolute or relative isoform expression levels. Consequently, no consensus exists about the best practices and appropriate workflows to analyse alternative and differential splicing. To assist the adequate pipeline choice, we present here a benchmarking of nine commonly used workflows to detect differential isoform expression and splicing. We evaluated the workflows performance over three different experimental scenarios where changes in absolute and relative isoform expression occurred simultaneously. In addition, the effect of the number of isoforms per gene, and the magnitude of the expression change over pipeline performances were also evaluated. Our results suggest that workflow performance is influenced by the number of replicates per condition and the conditions heterogeneity. In general, workflows based on DESeq, DEXSeq, Limma and NOISeq performed well over a wide range of transcriptomics experiments. In particular, we suggest the use of workflows based on Limma when high precision is required, and DESeq2 and DEXseq pipelines to prioritize sensitivity. When several replicates per condition are available, NOISeq and Limma pipelines are indicated."}, {"title": "Patchwork of contrasting medication cultures across the USA", "url": "https://www.biorxiv.org/content/early/2017/06/30/156281", "tag": "Bioinformatics", "abstract": "Health care in the United States is markedly heterogeneous, with large disparities in treatment choices and health spending. Drug prescription is one major component of health care \u2014 reflecting the accuracy of diagnosis, the adherence to evidence-based guidelines, susceptibility to drug marketing, and regulatory factors. Using medical claims data covering nearly half of the USA population, we have developed a framework to compare prescription rates of 600 popular drugs in 2,334 counties. Our approach uncovers geographically separated sub-Americas, where patients receive treatment for different diseases, and where physicians choose different drugs for the same disease. The geographical variation suggests influences of racial composition, state-level health care laws, and wealth. Some regions consistently prefer more expensive drugs, even when they have not been proven more efficacious than cheaper alternatives. Our study underlines the benefit of aggregating massive information on medical practice into a summarized and actionable form. We hope that our methodology and conclusions will guide policy measures for aligning prescriptions with best-practice guidelines."}, {"title": "GIGGLE: a search engine for large-scale integrated genome analysis", "url": "https://www.biorxiv.org/content/early/2017/06/29/157735", "tag": "Bioinformatics", "abstract": "GIGGLE is a genomics search engine that identifies and ranks the significance of shared genomic loci between query features and thousands of genome interval files. GIGGLE scales to billions of intervals, is faster (+1,000X) than existing methods, and its speed extends the accessibility and utility of resources such as ENCODE, Roadmap Epigenomics, and GTEX by facilitating data integration and hypothesis generation. GIGGLE is available at https://github.com/ryanlayer/giggle."}, {"title": "Identification and visualization of differential isoform expression in RNA-seq time series", "url": "https://www.biorxiv.org/content/early/2017/06/29/155135", "tag": "Bioinformatics", "abstract": "Motivation: As sequencing technologies improve their capacity to detect distinct transcripts of the same gene and to address complex experimental designs such as longitudinal studies, there is a need to develop statistical methods for the analysis of isoform expression changes in time series data. Results: Iso-maSigPro is a new functionality of the R package maSigPro for transcriptomics time series data analysis. Iso-maSigPro identifies genes with a differential isoform usage across time. The package also includes new clustering and visualization functions that allow grouping of genes with similar expression patterns at the isoform level, as well as those genes with a shift in major expressed isoform. Availability: The package is freely available under the LGPL license from the Bioconductor web site."}, {"title": "Inferring multi-scale neural mechanisms with brain network modelling", "url": "https://www.biorxiv.org/content/early/2017/06/28/157263", "tag": "Bioinformatics", "abstract": "The neurophysiological processes underlying non-invasive brain activity measurements are not well understood. Here, we developed a novel connectome-based brain network model that integrates individual structural and functional data with neural population dynamics to support multi-scale neurophysiological inference. Simulated populations were linked by structural connectivity and, as a novelty, driven by electroencephalography (EEG) source activity. Simulations not only predicted subjects\u2019 individual resting-state functional magnetic resonance imaging (fMRI) time series and spatial network topologies over 20 minutes of activity, but more importantly, they also revealed precise neurophysiological mechanisms that underlie and link six empirical observations from different scales and modalities: (1) slow resting-state fMRI oscillations, (2) spatial topologies of functional connectivity networks, (3) excitation-inhibition balance, (4, 5) pulsed inhibition on short and long time scales, and (6) fMRI power-law scaling. These findings underscore the potential of this new modelling framework for general inference and integration of neurophysiological knowledge to complement empirical studies."}, {"title": "Automated Recommendation Of Metabolite Substructures From Mass Spectra Using Frequent Pattern Mining", "url": "https://www.biorxiv.org/content/early/2017/06/28/134189", "tag": "Bioinformatics", "abstract": "Despite the increasing importance of metabolomics approaches, the structural elucidation of metabolites from mass spectral data remains a challenge. Although several reliable tools to identify known metabolites exist, identifying compounds that have not been previously seen is a challenging task that still eludes modern bioinformatics tools. Here, we describe an automated method for substructure recommendation from mass spectra using pattern mining techniques. Based on previously seen recurring substructures our approach succeeds in identifying parts of unknown metabolites. An important advantage of this approach is that it does not require any prior information concerning the metabolites to be identified, and therefore it can be used for the (partial) identification of unknown unknowns. Using rules extracted by pattern mining we are able to recommend valid substructures even for those metabolites for which no match can be found in spectral libraries. We further demonstrate how this approach is complementary to existing metabolite identification tools, achieving improved identification results. The method is called MESSAR (MEtabolite SubStructure Auto-Recommender) and is implemented as a free online web service available at http://www.biomina.be/apps/MESSAR/."}, {"title": "GrapHi-C: Graph-based visualization of Hi-C Datasets", "url": "https://www.biorxiv.org/content/early/2017/06/27/156679", "tag": "Bioinformatics", "abstract": "Background: Hi-C is a proximity-based ligation reaction used to detect regions of the genome that are close in 3D space (or \u201cinteracting\u201d). Typically, results from Hi-C experiments (whole-genome contact maps) are visualized as heatmaps or Circos plots. While informative, these visualizations do not intuitively represent the complex organization and folding of the genome in 3D space, making the interpretation of the underlying 3D genomic organization difficult. Our objective was to utilize existing tools to generate a graph-based representation of a whole-genome contact map that leads to a more intuitive visualization. Methodology: Whole-genome contact maps were converted into graphs where each vertex represented a genomic region and each edge represented a detected or known interaction between two vertices. Three types of interactions were represented in the graph: linear, intra-chromosomal (cis-), and inter-chromosomal (trans-) interactions. Each edge had an associated weight related to the linear distance (Hi-C experimental resolution) or the associated interaction frequency from the contact map. Graphs were generated based on this representation scheme for whole-genome contact maps from a fission yeast dataset where yeast mutants were used to identify specific principles influencing genome organization (GEO accession: GSE56849). Graphs were visualized in Cytoscape with an edge-weighted spring embedded layout where vertices and linear interaction edges were coloured according to their corresponding chromosome. Results: The graph-based visualizations (compared to the equivalent heatmaps) more intuitively represented the effects of the rad21 mutant on genome organization. Specifically, the graph based visualizations clearly highlighted the loss of structural globules and a greater intermingling of chromosomes in the mutant strain when compared to the wild-type. The graph-based representation and visualization protocol developed here will aid in understanding the complex organization and folding of the genome."}, {"title": "De novo discovery of structural motifs in RNA 3D structures through clustering", "url": "https://www.biorxiv.org/content/early/2017/06/27/155580", "tag": "Bioinformatics", "abstract": "As functional components in three-dimensional conformation of an RNA, the RNA structural motifs provide an easy way to associate the molecular architectures with their biological mechanisms. In the past years, many computational tools have been developed to search motif instances by using the existing knowledge of well-studied families. Recently, with the rapidly increasing number of resolved RNA 3D structures, there is an urgent need to discover novel motifs with the newly presented information. In this work, we classify all the loops in non-redundant RNA 3D structures to detect plausible RNA structural motif families by using a clustering pipeline. Compared with other clustering approaches, our method has two benefits: first, the underlying alignment algorithm is tolerant to the variations in 3D structures; second, sophisticated downstream analysis has been performed to ensure the clusters are valid and easily applied to further research. The final clustering results contain many interesting new variants of known motif families, such as GNAA tetraloop, kink-turn, sarcin-ricin, and T-loop. We have also discovered potential novel functional motifs conserved in ribosomal RNA, sgRNA, SRP RNA, riboswitch, and ribozyme."}, {"title": "Ultrafast comparison of personal genomes", "url": "https://www.biorxiv.org/content/early/2017/06/27/130807", "tag": "Bioinformatics", "abstract": "We present an ultra-fast method for comparing personal genomes. We transform the standard genome representation (lists of variants relative to a reference) into 'genome fingerprints' that can be readily compared across sequencing technologies and reference versions. Because of their reduced size, computation on the genome fingerprints is fast and requires little memory. This enables scaling up a variety of important genome analyses, including quantifying relatedness, recognizing duplicative sequenced genomes in a set, population reconstruction, and many others. The original genome representation cannot be reconstructed from its fingerprint; the method thus has significant implications for privacy-preserving genome analytics."}, {"title": "Detecting repeated cancer evolution in human tumours from multi-region sequencing data", "url": "https://www.biorxiv.org/content/early/2017/06/27/156729", "tag": "Bioinformatics", "abstract": "Carcinogenesis is an evolutionary process driven by the accumulation of genomic aberrations. Recurrent sequences of genomic changes, both between and within patients, reflect repeated evolution that is valuable for anticipating cancer progression. Multi-region sequencing and phylogenetic analysis allow inference of the partial temporal order of genomic changes within a patient's tumour. However, the inherent stochasticity of the evolutionary process makes phylogenetic trees from different patients appear very distinct, preventing the robust identification of recurrent evolutionary trajectories. Here we present a novel quantitative method based on a machine learning approach called Transfer Learning (TL) that allows overcoming the stochastic effects of cancer evolution and highlighting hidden recurrences in cancer patient cohorts. When applied to multi-region sequencing datasets from lung, breast and renal cancer (708 samples from 160 patients), our method detected repeated evolutionary trajectories that determine novel patient subgroups, which reproduce in large single- sample cohorts (n=2,641) and have prognostic value. Our method provides a novel patient classification measure that is grounded in the cancer evolution paradigm, and which reveals repeated evolution during tumorigenesis, with implications for our ability to anticipate malignant evolution."}, {"title": "ADAGE signature analysis: differential expression analysis with data-defined gene sets", "url": "https://www.biorxiv.org/content/early/2017/06/27/156620", "tag": "Bioinformatics", "abstract": "Background: Gene set enrichment analysis and overrepresentation analyses are commonly used methods to determine the biological processes affected by a differential expression experiment. This approach requires biologically relevant gene sets, which are currently curated manually, limiting their availability and accuracy in many organisms without extensively curated resources. New feature learning approaches can now be paired with existing data collections to directly extract functional gene sets from big data. Results: Here we introduce a method to identify perturbed processes. In contrast with methods that use curated gene sets, this approach uses signatures extracted from public expression data. We first extract expression signatures from public data using ADAGE, a neural network-based feature extraction approach. We next identify signatures that are differentially active under a given treatment. Our results demonstrate that these signatures represent biological processes that are perturbed by the experiment. Because these signatures are directly learned from data without supervision, they can identify uncurated or novel biological processes. We implemented ADAGE signature analysis for the bacterial pathogen Pseudomonas aeruginosa. For the convenience of different user groups, we implemented both an R package (ADAGEpath) and a web server (http://adage.greenelab.com) to run these analyses. Both are open-source to allow easy expansion to other organisms or signature generation methods. We applied ADAGE signature analysis to an example dataset in which wild-type and \u0394anr mutant cells were grown as biofilms on the Cystic Fibrosis genotype bronchial epithelial cells. We mapped active signatures in the dataset to KEGG pathways and compared with pathways identified using GSEA. The two approaches generally return consistent results; however, ADAGE signature analysis also identified a signature that revealed the molecularly supported link between the MexT regulon and Anr. Conclusions: We designed ADAGE signature analysis to perform gene set analysis using data-defined functional gene signatures. This approach addresses an important gap for biologists studying non-traditional model organisms and those without extensive curated resources available. We built both an R package and web server to provide ADAGE signature analysis to the community."}, {"title": "A dual transcript-discovery approach to improve the delimitation of gene features from RNA-seq data in the chicken model", "url": "https://www.biorxiv.org/content/early/2017/06/27/156406", "tag": "Bioinformatics", "abstract": "The sequence of the chicken genome, like several other draft genome sequences, is presently not fully covered. Gaps, contigs assigned with low confidence and uncharacterized chromosomes result in gene fragmentation and imprecise gene annotation. Transcript abundance estimation from RNA sequencing (RNA-seq) data relies on read quality, library complexity and expression normalization. In addition, the quality of the genome sequence used to map sequencing reads and the gene annotation that defines gene features must also be taken into account. Partially covered genome sequence causes the loss of sequencing reads from the mapping step, while an inaccurate definition of gene features induces imprecise read counts from the assignment step. Both steps can significantly bias interpretation of RNA-seq data. Here, we describe a dual transcript-discovery approach combining a genome-guided gene prediction and a de novo transcriptome assembly. This dual approach enabled us to increase the assignment rate of RNA-seq data by nearly 20% as compared to when using only the chicken reference annotation, contributing therefore to a more accurate estimation of transcript abundance. More generally, this strategy could be applied to any organism with partial genome sequence and/or lacking a manually-curated reference annotation in order to improve the accuracy of gene expression studies."}, {"title": "MoMo: Discovery of post-translational modification motifs", "url": "https://www.biorxiv.org/content/early/2017/06/27/153882", "tag": "Bioinformatics", "abstract": "Post-translational modifications (PTMs) of proteins are associated with many significant biological functions and can be identified in high throughput using tandem mass spectrometry. Many PTMs are associated with short sequence patterns called \u201cmotifs\u201d that help localize the modifying enzyme. Accordingly, many algorithms have been designed to identify these motifs from mass spectrometry data. MoMo is a software tool for identifying motifs among sets of PTMs. The program re-implements two previously described algorithms, motif-x and MoDL, packaging them in a web-accessible user interface. In addition to reading sequence files in FASTA format, MoMo is capable of directly parsing output files produced by commonly used mass spectrometry search engines. The resulting motifs are presented to the user in an HTML summary with motif logos and linked text files in MEME motif format. Source code and web server available at http://meme-suite.org."}, {"title": "Patternize: An R Package For Quantifying Color Pattern Variation", "url": "https://www.biorxiv.org/content/early/2017/06/26/121962", "tag": "Bioinformatics", "abstract": "The use of image data to quantify, study and compare variation in the colors and patterns of organisms requires the alignment of images to establish homology, followed by color-based segmentation of images. Here we describe an R package for image alignment and segmentation that has applications to quantify color patterns in a wide range of organisms. patternize is an R package that quantifies variation in color patterns obtained from image data. patternize first defines homology between pattern positions across specimens either through manually placed homologous landmarks or automated image registration. Pattern identification is performed by categorizing the distribution of colors using an RGB threshold, k-means clustering or watershed transformation. We demonstrate that patternize can be used for quantification of the color patterns in a variety of organisms by analyzing image data for butterflies, guppies, spiders and salamanders. Image data can be compared between sets of specimens, visualized as heatmaps and analyzed using principal component analysis (PCA). patternize has potential applications for fine scale quantification of color pattern phenotypes in population comparisons, genetic association studies and investigating the basis of color pattern variation across a wide range of organisms."}, {"title": "powsimR: Power analysis for bulk and single cell RNA-seq experiments", "url": "https://www.biorxiv.org/content/early/2017/06/26/117150", "tag": "Bioinformatics", "abstract": "Power analysis is essential to optimize the design of RNA-seq experiments and to assess and compare the power to detect differentially expressed genes in RNA-seq data. PowsimR is a flexible tool to simulate and evaluate differential expression from bulk and especially single-cell RNA-seq data making it suitable for a priori and posterior power analyses."}, {"title": "Resistome SNP Calling via Read Colored de Bruijn Graphs", "url": "https://www.biorxiv.org/content/early/2017/06/26/156174", "tag": "Bioinformatics", "abstract": "The microbiome and resistome, which refers to all the antimicrobial resistant (AMR) genes in pathogenic and non-pathogenic bacteria, are frequently studied using shotgun metagenomics data. Unfortunately, there are few methods capable of identifying single nucleotide polymorphisms (SNPs) in metagenomics data, and to the best of our knowledge, there are no methods that identify SNPs in AMR genes. Nonetheless, the identification of SNPs in AMR genes is an important problem since it allows these genes, which confer resistance to antibiotics, to be \u201cfingerprinted\u201d and tracked across multiple samples or time periods. In this paper, we present LueVari, which allows SNPs to be identified in AMR genes from metagenomes data. LueVari is based on the read colored de Bruijn graph, an extension of the traditional de Bruijn graph that we present and formally define in this paper. We show that read coloring allows regions longer than the k-mer length and shorter than the read length to be identified unambiguously. In addition to this theoretical concept, we present a succinct data structure that allows for large datasets to be analyzed in a reasonable amount of time and space. Our experiments demonstrate LueVari was the only SNP caller that reliably reported sequences that spanned on average 47.5% of the AMR gene. Competing methods (GATK and SAMtools) only reported specific loci and require a reference to do so. This feature, along with the high accuracy of LueVari, allows distinct AMR genes to be detected reliably in a de novo fashion."}, {"title": "BPG: Seamless, Automated and Interactive Visualization of Scientific Data", "url": "https://www.biorxiv.org/content/early/2017/06/26/156067", "tag": "Bioinformatics", "abstract": "We introduce BPG, an easy-to-use framework for generating publication-quality, highly-customizable plots in the R statistical environment. This open-source package includes novel methods of displaying high-dimensional datasets and facilitates generation of complex multi-panel figures, making it ideal for complex datasets. A web-based interactive tool allows online figure customization, from which R code can be downloaded for seamless integration with computational pipelines. BPG is available at http://labs.oicr.on.ca/boutros-lab/software/bpg."}, {"title": "Combining Semantic Similarity and GO Enrichment for Computation of Functional Similarity", "url": "https://www.biorxiv.org/content/early/2017/06/26/155689", "tag": "Bioinformatics", "abstract": "Functional similarity between genes is widely used in many bioinformatics applications including detecting molecular pathways, finding co-expressed genes, predicting protein-protein interactions, and prioritization of candidate genes. Methods evaluating functional similarity of genes are mostly based on semantic similarity of gene ontology (GO) terms. Though there are hundreds of functional similarity measures available in the literature, none of them considers the enrichment of the GO terms by the querying gene pair. We propose a novel method to incorporate GO enrichment into the existing functional similarity measures. Our experiments show that the inclusion of gene enrichment significantly improves the performance of 44 widely used functional similarity measures, especially in the prediction of sequence homologies, gene expression correlations, and protein-protein interactions."}, {"title": "Not All Experimental Questions Are Created Equal: Accelerating Biological Data to Knowledge Transformation (BD2K) via Science Informatics, Active Learning and Artificial Intelligence", "url": "https://www.biorxiv.org/content/early/2017/06/25/155150", "tag": "Bioinformatics", "abstract": "The fathers of Artificial Intelligence (AI) dreamt about the application of AI to Scientific Discovery. We propose a new twist on these early dreams and describe a novel approach aimed at remodeling of the biomedical research infrastructure and catalyze gene function determination. We aim to start a bold discussion of new ideas aimed towards increasing the efficiency of the allocation of research capacities, reproducibility, provenance tracking, removing redundancy and catalyzing knowledge gain with each experiment. In particular, we describe a tractable computational framework and infrastructure that can help researchers assess the potential information gain of millions of experiments before conducting them. The utility of experiments in this paper is modeled as the predictive knowledge (formalized as information) to be gained as a result of performing an experiment. The experimentalist would then be empowered to select experiments that maximize information gain if they wished, recognizing that there are frequently other considerations, such as a specific technological or medical utility, that might override the priority of maximizing information gain. The conceptual approach we develop is general, and here we apply it to the study of gene function."}, {"title": "Xolik: finding cross-linked peptides with maximum paired scores in linear time", "url": "https://www.biorxiv.org/content/early/2017/06/24/155069", "tag": "Bioinformatics", "abstract": "Motivation: Cross-linking technique coupled with mass spectrometry (MS) is widely used in the analysis of protein structures and protein-protein interactions. In order to identify cross-linked peptides from MS data, we need to consider all pairwise combinations of peptides, which is computationally prohibitive when the sequence database is large. To alleviate this problem, some heuristic screening strategies are used to reduce the number of peptide pairs during the identification. However, heuristic screening criteria may ignore true findings. Results: We directly tackle the combination challenge without using any screening strategies. With the additive scoring function and the data structure of double-ended queue, the proposed algorithm reduces the quadratic time complexity of exhaustive searching down to the linear time complexity. We implement the algorithm in a tool named Xolik, and the running time of Xolik is validated using databases with different number of proteins. Experiments using synthetic and empirical datasets show that Xolik outperforms existing tools in terms of running time and statistical power. Availability: Source code and binaries of Xolik are freely available at http://bioinformatics.ust.hk/Xolik.html."}, {"title": "Computational approaches for discovery of mutational signatures in cancer", "url": "https://www.biorxiv.org/content/early/2017/06/24/154716", "tag": "Bioinformatics", "abstract": "The accumulation of somatic mutations in a genome is the result of the activity of one or more mutagenic processes, each of which leaves its own imprint. The study of these DNA fingerprints, termed mutational signatures, holds important potential for furthering our understanding of the causes and evolution of cancer, and can provide insights of relevance for cancer prevention and treatment. In this review, we focus our attention on the mathematical models and computational techniques that have driven recent advances in the field."}, {"title": "In silico identification of non-coding RNAs in Halobacterium salinarum NRC-1 model archeon organism", "url": "https://www.biorxiv.org/content/early/2017/06/23/152439", "tag": "Bioinformatics", "abstract": "In addition to the regulatory elements already known, for instance, transcription factors or post-translation modifications, there is growing interests in the regulatory role played by non-coding RNA molecules (ncRNA) whose functions can be performed on different level of biological information processing. Model organisms allow a convenient way to work on laboratory and different research groups aiming to guide their studies for a mutual and wide understanding of the cellular mechanisms present on these organisms. Although some ncRNAs elements have been found in Halobacterium salinarum model organism we believe that not enough is knowing about these genomic regions. In these context, an in silico analysis for ncRNAs identification was applied to H. salinarum NRC-1. Considering a data integration perspective and some available methodologies, several machine learning models was built and used to designate candidate ncRNAs genome regions. According to achieve results, 42 new ncRNAs could be identified. Combing analysis with other available tools, it had been observed that some suggested candidates also was found with different methodologies and thus, it highlights the proposed results."}, {"title": "Twelve Elements of Visualization and Analysis for Tertiary and Quaternary Structure of Biological Molecules", "url": "https://www.biorxiv.org/content/early/2017/06/23/153528.1", "tag": "Bioinformatics", "abstract": "During the last decades, 3D Molecular Graphics in Life Sciences has been used almost exclusively by experts through complex software and applications ranging from Structural Biology to Computer Aided Drug Design. The emergence of JavaScript and WebGL as a viable platform has enabled 3D visualization of biomolecular structures through Web browsers, without any need for specialized software. Although still in its infancy, Web Molecular Graphics opens new perspectives. This white paper, proposes a set of Twelve Elements to consider to enable 3D visualization and structural analyses of biological systems in Web molecular viewers. The Elements go beyond 3D graphics and propose an integrated approach to visualize and analyze molecular entities and their interactions in multiple dimensions, at multiple levels of details, for diverse users. The bridging of 1D sequence browsers and 3D structure viewers, possible under a Web browser, enables information flow where molecular biologists can use structural information directly at the sequence level. Given the tsunami of sequence information linked to diseases from next generation sequencing \u2014 in need for interpretation \u2014 making structural information readily available to research scientists is a tremendous opportunity for medical discovery. The Twelve Elements are conceptual and are intended to entice developers to architect software components and APIs, and to gather together as a community around common goals and open source software. A few features of emerging viewers, all available as open source, are highlighted. Speed and quality of 3D graphics for large molecular systems, the interoperability of Web components, and the instantaneous sharing of annotated visualizations through the Web, are some of the most amazing and promising capabilities of 3D Web viewing, opening bright perspectives for Life Sciences research."}, {"title": "Vasomotor Symptoms Monitoring with a Commercial Activity Tracking Watch", "url": "https://www.biorxiv.org/content/early/2017/06/23/154658", "tag": "Bioinformatics", "abstract": "Continuous tracking of electrodermal activity (EDA), also known as galvanic skin response (GSR), values with commercial fitness devices for individuals with vasomotor symptoms (hot flashes) provides a path forward for future studies with fine resolution monitoring. This can improve upon the current reliance on the use of personal diaries. There are multiple conditions associated with vasomotor symptoms including menopause/early menopausal transition, medications (Quinestrol, tramadol, etc.), chemotherapy and Tamoxifen, hyperthyroidism, infections (Inflammatory Bowel Disease: IBD, etc.), and more. Multiple studies have characterized hot flashes in premenopausal and menopausal women using self reported methods, laboratory polysomnographic recording, and some specially designed devices. However, it is difficult for an individual to track and accurately report nighttime vasomotor symptoms without the aid of a physiological monitoring device. Emerging commercial and custom devices with EDA meters will greatly facilitate the nighttime monitoring of hot flashes for individuals for more informative longitudinal studies of conditions with associated vasomotor symptoms. This report illustrates the potential fine-resolution monitoring of nighttime vasomotor symptoms using commercially available activity-tracking devices with an EDA sensor."}, {"title": "Correction of copy number induced false positives in CRISPR screens", "url": "https://www.biorxiv.org/content/early/2017/06/23/151985", "tag": "Bioinformatics", "abstract": "Cell autonomous cancer dependencies are now routinely identified using CRISPR loss-of-function screens. However, a bias exists that makes it difficult to assess the true essentiality of genes located in amplicons, since the entire amplified region can exhibit lethal scores. These false-positive hits can either be discarded from further analysis, which in cancer models can represent a significant number of hits, or methods can be developed to rescue the true-positives within amplified regions. We propose two methods to rescue true positive hits in amplified regions by correcting for this copy number artefact. The Local Drop Out (LDO) method uses the relative lethality scores within genomic regions to assess true essentiality and does not require additional orthogonal data (e.g. copy number value). LDO is meant to be used in screens covering a dense region of the genome (e.g. a whole chromosome or the whole genome). The General Additive Model (GAM) method models the screening data as a function of the known copy number values and removes the systematic effect from the measured lethality. GAM does not require the same density as LDO, but does require prior knowledge of the copy number values. Both methods have been developed with single sample experiments in mind so that the correction can be applied even in smaller screens. Here we demonstrate the efficacy of both methods at removing the copy number effect and rescuing hits from some of the amplified regions. We estimate a 70-80% decrease of false positive hits in regions of high copy number with either method."}, {"title": "Informatics for Cancer Immunotherapy", "url": "https://www.biorxiv.org/content/early/2017/06/22/152264", "tag": "Bioinformatics", "abstract": "The rapid development of immunomodulatory cancer therapies has led to a concurrent increase in the application of informatics techniques to the analysis of tumors, the tumor microenvironment, and measures of systemic immunity. In this review, the use of tumors to gather genetic and expression data will first be explored. Next, techniques to assess tumor immunity are reviewed, including HLA status, predicted neoantigens, immune microenvironment deconvolution and T-cell receptor (TCR) sequencing. Attempts to integrate these data are in early stages of development and are discussed next. Finally, we review the application of these informatics strategies to therapy development, with a focus on vaccines, adoptive cell transfer, and checkpoint blockade therapies."}, {"title": "Positional effects revealed in Illumina Methylation Array and the impact on analysis", "url": "https://www.biorxiv.org/content/early/2017/06/22/153858", "tag": "Bioinformatics", "abstract": "With the evolution of rapid epigenetic research, Illumina Infinium HumanMethylation BeadChips have been widely used to study DNA methylation. However, in evaluating the accuracy of this method, we found that the commonly used Illumina HumanMethylation BeadChips are substantially affected by positional effects; the DNA sample's location in a chip affects the measured methylation levels. We analyzed three HumanMethylation450 and three HumanMethylation27 datasets by using four methods to prove the existence of positional effects. Three datasets were analyzed further for technical replicate analysis or differential methylation CpG sites analysis. The pre- and post- correction comparisons indicate that the positional effects could alter the measured methylation values and downstream analysis results. Nevertheless, ComBat, linear regression and functional normalization could all be used to minimize such artifact. We recommend performing ComBat to correct positional effects followed by the correction of batch effects in data preprocessing as this procedure slightly outperforms the others. In addition, randomizing the sample placement should be a critical laboratory practice for using such experimental platforms. Code for our method is freely available at: https://github.com/ChuanJ/posibatch."}, {"title": "Mapping-free variant calling using haplotype reconstruction from k-mer frequencies", "url": "https://www.biorxiv.org/content/early/2017/06/22/153619", "tag": "Bioinformatics", "abstract": "Motivation: The standard protocol for detecting variation in DNA is to map millions of short sequence reads to a known reference and find loci that differ. While this approach works well, it cannot be applied where the sample contains dense variants or is too distant from known references. De novo assembly or hybrid methods can recover genomic variation, but the cost of computation is often much higher. We developed a novel k-mer algorithm and software implementation, Kestrel, capable of characterizing densely-packed SNPs and large indels without mapping, assembly, or de Bruijn graphs. Results: When applied to mosaic penicillin binding protein (PBP) genes in Streptococcus pneumoniae, we found near perfect concordance with assembled contigs at a fraction of the CPU time. Multilocus sequence typing (MLST) with this approach was able to bypass de novo assemblies. Kestrel has a very low false-positive rate when calling variants over the whole genome, but limitations of a purely k-mer based approach affect sensitivity. Availability: Source code and documentation for a Java implementation of Kestrel can be found at https://github.com/paudano/kestrel. All test code for this publication is located at https://github.com/paudano/kescases."}, {"title": "CD-HIT-OTU-MiSeq, an Improved Approach for Clustering and Analyzing Paired End MiSeq 16S rRNA Sequences", "url": "https://www.biorxiv.org/content/early/2017/06/22/153783", "tag": "Bioinformatics", "abstract": "In recent years, Illumina MiSeq sequencers replaced pyrosequencing platforms and became dominant in 16S rRNA sequencing. One unique feature of MiSeq technology, compared with Pyrosequencing, is the Paired End (PE) reads, with each read can be sequenced to 250-300 bases to cover multiple variable regions on the 16S rRNA gene. However, the PE reads need to be assembled into a single contig at the beginning of the analysis. Although there are many methods capable of assembling PE reads into contigs, a big portion of PE reads can not be accurately assembled because the poor quality at the 3' ends of both PE reads in the overlapping region. This causes that many sequences are discarded in the analysis. In this study, we developed a novel approach for clustering and annotation MiSeq-based 16S sequence data, CD-HIT-OTU-MiSeq. This new approach has four distinct novel features. (1) The package can clustering PE reads without joining them into contigs. (2) Users can choose a high quality portion of the PE reads for analysis (e.g. first 200 / 150 bases from forward / reverse reads), according to base quality profile. (3) We implemented a tool that can splice out the target region (e.g. V3-V4) from a full-length 16S reference database into the PE sequences. CD-HIT-OTU-MiSeq can cluster the spliced PE reference database together with samples, so we can derive Operational Taxonomic Units (OTUs) and annotate these OTUs concurrently. (4) Chimeric sequences are effectively identified through de novo approach. The package offers high speed and high accuracy. The software package is freely available as open source package and is distributed along with CD-HIT from http://cd-hit.org. Within the CD-HIT package, CD-HIT-OTU-MiSeq is within the usecase folder."}, {"title": "A Bayesian approach to multivariate and multilevel modelling with non-random missingness for hierarchical clinical proteomics data", "url": "https://www.biorxiv.org/content/early/2017/06/21/153049", "tag": "Bioinformatics", "abstract": "High throughput mass-spectrometry-based proteomics data from clinical studies brings challenges to statistical analysis. The challenges originate from the hierarchical levels of protein abundance data and interactions between clinical study design and experimental design. The non-random missingness of the measurements from a vast amount of information also adds complexity in data analysis. We propose multivariate multilevel models to analyse protein abundances and to handle abundance-dependent missingness within a Bayesian framework. The proposed model enables the variance decomposition at different levels of the data hierarchy and provides shrinkage of protein-level estimates for a group of proteins. A logistic missingness and censored model with informative prior is used to handle incomplete data. Hamiltonian MC/No-U-Turn Sampling and Gibb MCMC algorithms are created to derive the posterior distribution of study parameters; Hamiltonian MC is demonstrated to gain more efficiency for these high-dimensional correlated data. Improvements of the proposed missing data model is compared to the univariate mixed effect model and the multivariate-multilevel model using complete data in a simulated study and a clinical proteomics study. The proposed model framework can be used in other types of data with similar structure and Non Random Missingness mechanism (MNAR)."}, {"title": "Epigenome-Based Drug Repositioning in Acute Myeloid Leukemia", "url": "https://www.biorxiv.org/content/early/2017/06/21/152157", "tag": "Bioinformatics", "abstract": "Background. Repositioning approved drugs for the treatment of new indications is a promising avenue to reduce the burden of drug development. Most currently available computational methods based on molecular evidence can only utilize gene expression for repositioning despite a growing interest in the epigenome in human disease. We recently described a novel repositioning method, ksRepo, that enables investigators to move beyond microarray-based gene expression and utilize a variety of other sources of molecular evidence, such as DNA methylation differences. Methods. We downloaded differential DNA methylation data from two publicly available acute myeloid leukemia (AML) datasets, a cancer with known, extensive epigenomic perturbations. We consolidated CpGs-level to non-directional gene-level differential methylation using Brown's correction to Fisher's method. We then used ksRepo, which ignores directionality in disease- and gene-drug associations, to mine the resulting prioritized gene lists and and the Comparative Toxicogenomics Database (CTD) for predicted repositioning candidates. Results. We successfully recovered four compounds that were significant (FDR < 0.05) in two AML datasets: cytarabine, alitretinoin, panobinostat, and progesterone. Cytarabine is the most commonly used frontline therapy for AML and alitretinoin, panobinostat, and progesterone have all been investigated for the treatment of AML. Conclusions. Combining a method for consolidating CpG methylation to the gene level with ksRepo provides a pipeline for deriving drug repositioning hypotheses from differential DNA methylation. We claim that our platform can be extended to other diseases with epigenetic perturbations and to other epigenomic modalities, such as ChIP-seq."}, {"title": "A Site Specific Model And Analysis Of The Neutral Somatic Mutation Rate In Whole-Genome Cancer Data", "url": "https://www.biorxiv.org/content/early/2017/06/21/122879", "tag": "Bioinformatics", "abstract": "Understanding and modelling the neutral mutational process in cancer cells is crucial for identifying the mutations that drive cancer development. The neutral mutational process is very complex: Whole-genome analyses have revealed that the mutation rate differs between cancer types, between patients, and along the genome depending on the genetic and epigenetic context. Therefore, methods that predict the number of different types of mutations in regions or specific genomic elements must consider local genomic explanatory variables. A major drawback of most methods is the need to average the explanatory variables across the entire region or genomic element. This procedure is particularly problematic if the explanatory variable varies dramatically in the element under consideration. Instead, we model the probabilities of the different types of mutations for each position in the genome by multinomial logistic regression. We apply our site-specific model to a data set of 505 cancer genomes from 14 different cancer types. We show that for 1000 randomly selected genomic positions, the site-specific model predicts the mutation rate much better than regional based models. We use a forward selection procedure to identify the most important explanatory variables. The procedure identifies site-specific conservation (phyloP), replication timing, and expression level as the best predictors for the mutation rate. Finally, our model confirms certain well-known mutational signatures. Our site-specific multinomial regression model can serve as the neutral null model for the mutational process; regions that deviate from the null model are candidates for elements that drive cancer development."}, {"title": "Accessible, curated metagenomic data through ExperimentHub", "url": "https://www.biorxiv.org/content/early/2017/06/21/103085", "tag": "Bioinformatics", "abstract": "We present curatedMetagenomicData, a Bioconductor and command-line interface to thousands of metagenomic profiles from the Human Microbiome Project and other publicly available datasets, and ExperimentHub, a platform for convenient cloud-based distribution of data to the R desktop. The resource provides standardized per-participant metadata linked to bacterial, fungal, archaeal, and viral taxonomic abundances, as well as quantitative metabolic functional profiles. The datasets can be immediately analyzed in R or other software with a minimum of bioinformatic expertise and no preprocessing of data. We demonstrate identification of taxonomic/functional correlations, an investigation of gut \"enterotypes\", and a comparison of the accuracy of disease classification from different data types. These documented analyses can be reproduced efficiently on a laptop, without the barriers of working with large-scale, raw sequencing data. The building and expansion of curatedMetagenomicData is based entirely on open source software and pipelines, to facilitate the addition of new microbiome datasets and methods."}, {"title": "A rapid and accurate approach for prediction of interactomes from co-elution data (PrInCE)", "url": "https://www.biorxiv.org/content/early/2017/06/20/152355", "tag": "Bioinformatics", "abstract": "Background: An organism's protein interactome, or complete network of protein-protein interactions, defines the protein complexes that drive cellular processes. Techniques for studying protein complexes have traditionally applied targeted strategies such as yeast two-hybrid or affinity purification-mass spectrometry to assess protein interactions. However, given the vast number of protein complexes, more scalable methods are necessary to accelerate interaction discovery and to construct whole interactomes. We recently developed a complementary technique based on the use of protein correlation profiling (PCP) and stable isotope labeling in amino acids in cell culture (SILAC) to assess chromatographic co-elution as evidence of interacting proteins. Importantly, PCP-SILAC is also capable of measuring protein interactions simultaneously under multiple biological conditions, allowing the detection of treatment-specific changes to an interactome. Given the uniqueness and high dimensionality of co-elution data, new tools are needed to compare protein elution profiles, control false discovery rates, and construct an accurate interactome. Results: Here we describe a freely available bioinformatics pipeline, PrInCE, for the analysis of co-elution data. PrInCE is a modular, open-source library that is computationally inexpensive, able to use label and label-free data, and capable of detecting tens of thousands of protein-protein interactions. Using a machine learning approach, PrInCE offers greatly reduced run time, better performance, prediction of protein complexes, and greater ease of use over previous bioinformatics tools for co-elution data. PrInCE is implemented in Matlab (version R2015b). Source code and standalone executable programs for Windows and Mac OSX are available at https://github.com/fosterlab/PrInCE, where usage instructions can be found. An example dataset and output are also provided for testing purposes. Conclusions: PrInCE is the first fast and easy-to-use data analysis pipeline that predicts interactomes and protein complexes from co-elution data. PrInCE allows researchers without bioinformatics proficiency to analyze high-throughput co-elution datasets."}, {"title": "ORIGAMI: A Software Suite for Activated Ion Mobility Mass Spectrometry (aIM-MS) Applied To Multimeric Protein Assemblies", "url": "https://www.biorxiv.org/content/early/2017/06/20/152686", "tag": "Bioinformatics", "abstract": "We present here a software suite (ORIGAMI) that facilitates the rapid acquisition and analysis of ion mobility data following collisional activation. ORIGAMI was developed for use on Waters Synapt instruments where data acquisition is achieved by interfacing WREnS (Waters Research Enabled Software) and MassLynx. Two components are presented, the first is ORIGAMIMS which enables activation of ions by sequential increase of collision voltages prior to ion mobility analysis. We demonstrate the use of ORIGAMI on the tetrameric assemblies formed by the proteins concanavalin A (103 kDa) and alcohol dehydrogenase (143 kDa). Activation is performed in the trap collision cell of the Synapt TriWave assembly, where the collision voltage can be ramped from 0-200 V. All of the acquired data is recorded in a single file which simplifies data acquisition. This substantially decreases the time needed to perform a typical activated IM-MS experiment on a single protein charge state from approx. 2 hours to ~25 minutes. Following data acquisition the data is analysed in the second component, ORIGAMIANALYSE, which allows the user to visualise the effect of activation on the mobility of the parent ion, as well as on any produced fragment ion. The user can export the data in the form of heat maps, waterfall or wire plots. In addition, tools implemented in ORIGAMI enable easy data extraction from single or multiple MassLynx .raw files, in-depth interrogation of large datasets, statistical analysis and figure creation capabilities. We demonstrate the use of ORIGAMI on concanavalin A and alcohol dehydrogenase acquired using the traditional protocols and the ORIGAMIMS method."}, {"title": "Reconstructing cancer karyotypes from short read data: the half full and half empty glass", "url": "https://www.biorxiv.org/content/early/2017/06/20/152447", "tag": "Bioinformatics", "abstract": "Background: During cancer progression genomes undergo point mutations as well as larger segmental changes. The latter include, among others, segmental deletions duplications, translocations and inversions. The result is a highly complex, patient-specific cancer karyotype. Using high-throughput technologies of deep sequencing and microarrays it is possible to interrogate a cancer genome and produce chromosomal copy number profiles and a list of breakpoints (\u201cjumps\u201d) relative to the normal genome. This information is very detailed but local, and does not give the overall picture of the cancer genome. One of the basic challenges in cancer genome research is to use such information to infer the cancer karyotype. We present here an algorithmic approach, based on graph theory and integer linear programming, that receives segmental copy number and breakpoint data as input and produces a cancer karyotype that is most concordant with them. We used simulations to evaluate the utility of our approach, and applied it to real data. Results: By using a simulation model, we were able to estimate the correctness and robustness of the algorithm in a spectrum of scenarios. Under our base scenario, designed according to observations in real data, the algorithm correctly inferred 69% of the karyotypes. However, when using less stringent correctness metrics that account for incomplete and noisy data, 87% of the reconstructed karyotypes were correct. Furthermore, in scenarios where the data were very clean and complete, accuracy rose to 90%-100%. Some examples of analysis of real data, and the karyotypes reconstructed by our algorithm, are also presented. Conclusion: While reconstruction of complete, perfect karyotype based on short read data is very hard, a large portion of the reconstruction will still be correct and can provide useful information."}, {"title": "The proBAM and proBed standard formats: enabling a seamless integration of genomics and proteomics data.", "url": "https://www.biorxiv.org/content/early/2017/06/20/152579", "tag": "Bioinformatics", "abstract": "On behalf of The Human Proteome Organization (HUPO) Proteomics Standards Initiative (PSI), we are here introducing two novel standard data formats, proBAM and proBed, that have been developed to address the current challenges of integrating mass spectrometry based proteomics data with genomics and transcriptomics information in proteogenomics studies. proBAM and proBed are adaptations from the well-defined, widely used file formats SAM/BAM and BED respectively, and both have been extended to meet specific requirements entailed by proteomics data. Therefore, existing popular genomics tools such as SAMtools and Bedtools, and several very popular genome browsers, can be used to manipulate and visualize these formats already out-of-the-box. We also highlight that a number of specific additional software tools, properly supporting the proteomics information available in these formats, are now available providing functionalities such as file generation, file conversion, and data analysis. All the related documentation to the formats, including the detailed file format specifications, and example files are accessible at http://www.psidev.info/probam and http://www.psidev.info/probed."}, {"title": "FORKS: Finding Orderings Robustly using K-means and Steiner trees", "url": "https://www.biorxiv.org/content/early/2017/06/20/132811", "tag": "Bioinformatics", "abstract": "Recent advances in single cell RNA-seq technologies have provided researchers with unprecedented details of transcriptomic variation across individual cells.However, it has not been straightforward to infer differentiation trajectories from such data, due to the parameter-sensitivity of existing methods. Here, we present Finding Orderings Robustly using k-means and Steiner trees (FORKS), an algorithm that pseudo-temporally orders cells and thereby infers bifurcating state trajectories. FORKS, which is a generic method, can be applied to both single-cell and bulk differentiation data. It is a semi-supervised approach, in that it requires the user to specify the starting point of the time course. We systematically benchmarked FORKS and eight other pseudo-time estimation algorithms on six benchmark datasets, and found it to be more accurate, more reproducible, and more memory-efficient than existing methods for pseudo-temporal ordering. Another major advantage of our approach is its robustness \u2212 FORKS can be used with default parameter settings on a wide range of datasets."}, {"title": "Gating mass cytometry data by deep learning", "url": "https://www.biorxiv.org/content/early/2017/06/20/054411", "tag": "Bioinformatics", "abstract": "Mass cytometry or CyTOF is an emerging technology for high-dimensional multiparameter single cell analysis that overcomes many limitations of fluorescence-based flow cytometry. New methods for analyzing CyTOF data attempt to improve automation, scalability, performance, and interpretation of data generated in large studies. Assigning individual cells into discrete groups of cell types (gating) involves time-consuming sequential manual steps, untenable for larger studies. We introduce DeepCyTOF, a standardization approach for gating, based on deep learning techniques. DeepCyTOF requires labeled cells from only a single sample. It is based on domain adaptation principles and is a generalization of previous work that allows us to calibrate between a target distribution and a source distribution in an unsupervised manner. We show that Deep- CyTOF is highly concordant (98%) with cell classification obtained by individual manual gating of each sample when applied to a collection of 16 biological replicates of primary immune blood cells, even when measured accross several instruments. Further, DeepCyTOF achieves very high accuracy on the semi-automated gating challenge of the FlowCAP-I competition as well as two CyTOF datasets generated from primary immune blood cells: (i) 14 subjects with a history of infection with West Nile virus (WNV), (ii) 34 healthy subjects of different ages. We conclude that deep learning in general, and DeepCyTOF specifically, offers a powerful computational approach for semi-automated gating of CyTOF and flow cytometry data."}, {"title": "A RESTful API to serve BAM file with OAuth2 compatible authorization", "url": "https://www.biorxiv.org/content/early/2017/06/19/151787", "tag": "Bioinformatics", "abstract": "Summary: Bam-server is an open-source RESTful service to query slices of BAM files securely and manage their user accesses. A typical use case is the visualization of local read alignments in a web interface for variant calling diagnostic, without exposing sensitive data to unauthorized users through the network, and without moving the original - heavy - file. Bam-server follows the standard implementation of a protected resource server in the context of a typical token-based authorization protocol, supporting HMAC- and RSA-hashed signatures from an authorization server of choice. Availability: The source code is available at https://github.com/chuv-ssrc/bam-server-scala, and a complete documentation can be found at http://bam-server-scala.readthedocs.io/en/latest/."}, {"title": "Application of High-Dimensional Statistics and Network based Visualization techniques on Arab Diabetes and Obesity data", "url": "https://www.biorxiv.org/content/early/2017/06/18/151621", "tag": "Bioinformatics", "abstract": "Background: Obesity and its co-morbidities are characterized by a chronic low-grade inflammatory state, uncontrolled expression of metabolic measurements and dis-regulation of various forms of stress response. However, the contribution and correlation of inflammation, metabolism and stress responses to the disease are not fully elucidated. In this paper a cross-sectional case study was conducted on clinical data comprising 117 human male and female subjects with and without type 2 diabetes (T2D). Characteristics such as anthropometric, clinical and bio-chemical measurements were collected. Methods: Association of these variables with T2D and BMI were assessed using penalized hierarchical linear and logistic regression. In particular, elastic net, hdi and glinternet were used as regularization models to distinguish between cases and controls. Differential network analysis using closed-form approach was performed to identify pairwise-interaction of variables that influence prediction of the phenotype. Results: For the 117 participants, physical variables such as PBF, HDL and TBW had absolute coefficients 0.75, 0.65 and 0.34 using the glinternet approach, biochemical variables such as MIP, ROS and RANTES were identified as determinants of obesity with some interaction between inflammatory markers such as IL-4, IL-6, MIP, CSF, Eotaxin and ROS. Diabetes was associated with a significant increase in thiobarbituric acid reactive substances (TBARS) which are considered as an index of endogenous lipid peroxidation and an increase in two inflammatory markers, MIP-1 and RANTES. Furthermore, we obtained 13 pairwise effects. The pairwise effects include pairs from and within physical, clinical and biochemical features, in particular metabolic, inflammatory, and oxidative stress markers. Conclusions: We showcase that markers of oxidative stress (derived from lipid peroxidation) such as MIP-1 and RANTES participate in the pathogenesis of diseases such as diabetes and obesity in the Arab population."}, {"title": "Uncovering thematic structure to link co-occurring taxa and predicted functional content in 16S rRNA marker gene surveys", "url": "https://www.biorxiv.org/content/early/2017/06/18/146126", "tag": "Bioinformatics", "abstract": "Analysis of microbiome data involves identifying co-occurring groups of taxa associated with sample features of interest (e.g., disease state). But elucidating key associations is often difficult since microbiome data are compositional, high dimensional, and sparse. Also, the configuration of co-occurring taxa may represent overlapping subcommunities that contribute to, for example, host status. Preserving the configuration of co-occurring microbes rather than detecting specific indicator species is more likely to facilitate biologically meaningful interpretations. In addition, analyses that utilize both taxonomic and predicted functional abundances typically independently characterize the taxonomic and functional profiles before linking them to sample information. This prevents investigators from identifying the specific functional components associate with which subsets of co-occurring taxa. We provide an approach to explore co-occurring taxa using \"topics\" generated via a topic model and then link these topics to specific sample classes (e.g., diseased versus healthy). Rather than inferring predicted functional content independently from taxonomic abundances, we instead focus on inference of functional content within topics, which we parse by estimating pathway-topic interactions through a multilevel, fully Bayesian regression model. We apply our methods to two large publically available 16S amplicon sequencing datasets: an inflammatory bowel disease (IBD) dataset from Gevers et al. and data from the American Gut (AG) project. When applied to the Gevers et al. IBD study, we demonstrate that a topic highly associated with Crohn's disease (CD) diagnosis is (1) dominated by a cluster of bacteria known to be linked with CD and (2) uniquely enriched for a subset of lipopolysaccharide (LPS) synthesis genes. In the AG data, our approach found that individuals with plant-based diets were enriched with Lachnospiraceae, Roseburia, Blautia, and Ruminococcaceae, as well as fluorobenzoate degradation pathways, whereas pathways involved in LPS biosynthesis were depleted. We introduce an approach for uncovering latent thematic structure in the context of sample features for 16S rRNA surveys. Using our topic-model approach, investigators can (1) capture groups of co-occurring taxa termed topics, (2) uncover within-topic functional potential, and (3) identify gene sets that may guide future inquiry. These methods have been implemented in a freely available R package https://github.com/EESI/themetagenomics."}, {"title": "A robust statistical framework to detect multiple sources of hidden variation in single-cell transcriptomes", "url": "https://www.biorxiv.org/content/early/2017/06/18/151217", "tag": "Bioinformatics", "abstract": "Single-cell RNA-Sequencing data often harbor variation from multiple correlated sources, which cannot be accurately detected by existing methods. Here we present a novel and robust statistical framework that can capture correlated sources of variation in an iterative fashion: iteratively adjusted surrogate variable analysis (IA-SVA). We demonstrate that IA-SVA accurately captures hidden variation in single cell RNA-Sequencing data arising from cell contamination, cell-cycle stage, and differences in cell types along with the marker genes associated with the source."}, {"title": "Biological classification with RNA-Seq data: Can alternative splicing enhance machine learning classifier?", "url": "https://www.biorxiv.org/content/early/2017/06/18/146340", "tag": "Bioinformatics", "abstract": "The extent to which the genes are expressed in the cell can be simplistically defined as a function of one or more factors of the environment, lifestyle, and genetics. RNA sequencing (RNA-Seq) is becoming a prevalent approach to quantify gene expression, and is expected to gain better insights to a number of biological and biomedical questions, compared to the DNA microarrays. Most importantly, RNA-Seq allows to quantify expression at the gene and alternative splicing isoform levels. However, leveraging the RNA-Seq data requires development of new data mining and analytics methods. Supervised machine learning methods are commonly used approaches for biological data analysis, and have recently gained attention for their applications to the RNA-Seq data. In this work, we assess the utility of supervised learning methods trained on RNA-Seq data for a diverse range of biological classification tasks. We hypothesize that the isoform-level expression data is more informative for biological classification tasks than the gene-level expression data. Our large-scale assessment is done through utilizing multiple datasets, organisms, lab groups, and RNA-Seq analysis pipelines. Overall, we performed and assessed 61 biological classification problems that leverage three independent RNA-Seq datasets and include over 2,000 samples that come from multiple organisms, lab groups, and RNA-Seq analyses. These 61 problems include predictions of the tissue type, sex, or age of the sample, healthy or cancerous phenotypes and, the pathological tumor stage for the samples from the cancerous tissue. For each classification problem, the performance of three normalization techniques and six machine learning classifiers was explored. We find that for every single classification problem, the isoform-based classifiers outperform or are comparable with gene expression based methods. The top-performing supervised learning techniques reached a near perfect classification accuracy, demonstrating the utility of supervised learning for RNA-Seq based data analysis."}, {"title": "Dissecting super-enhancer hierarchy based on chromatin interactions", "url": "https://www.biorxiv.org/content/early/2017/06/18/149583", "tag": "Bioinformatics", "abstract": "Recent studies have highlighted super-enhancers (SEs) as important regulatory elements for gene expression, but their intrinsic properties remain incompletely characterized. Through an integrative analysis of Hi-C and ChIP-seq data, we find that a significant fraction of SEs are hierarchically organized, containing both hub and non-hub enhancers. Hub enhancers share similar histone marks with non-hub enhancers, but are distinctly associated with cohesin and CTCF binding sites and disease-associated genetic variants. Genetic ablation of hub enhancers results in profound defects in gene activation and local chromatin landscape. As such, hub enhancers are the major constituents responsible for SE functional and structural organization."}, {"title": "simExTargId: An R package for real-time LC-MS metabolomic data analysis, instrument failure/drift notification and MS2 target identification", "url": "https://www.biorxiv.org/content/early/2017/06/16/151159", "tag": "Bioinformatics", "abstract": "The simExTargId R package provides real-time, autonomous, within-laboratory data analysis during a metabolomic LC-MS1-profiling experiment. Of concern to metabolomic investigators are instrumentation failure (especially for precious samples), outlier identification, instrument signal attenuation and pre-emptive feature identification for MS2 fragmentation. SimExTargId allows observation of an experiment in progress with PCA plot and peak table outputs and also two shiny applications targetId for MS2 target identification and peakMonitor for signal attenuation monitoring. SimExTargId is ideally utilised on a (temporarily) dedicated work-station or server which is networked to a LC-MS data directory. Features include: email notification for instrument stoppage/drift, file format conversion, peak-picking, pre-processing, PCA-based out-lier identification and statistical analysis. Additional MS1/MS2 experiments can be concatenated to a worklist or cleaning/recalibration undertaken if instrument drift is observed. All source code and a vignette with example data are available on GitHub https://github.com/WMBEdmands/simExTargId."}, {"title": "Addressing the looming identity crisis in single cell RNA-seq", "url": "https://www.biorxiv.org/content/early/2017/06/16/150524", "tag": "Bioinformatics", "abstract": "Single cell RNA-sequencing technology (scRNA-seq) provides a new avenue to discover and characterize cell types, but the experiment-specific technical biases and analytic variability inherent to current pipelines may undermine the replicability of these studies. Meta-analysis of rapidly accumulating data is further hampered by the use of ad hoc naming conventions. Here we demonstrate our replication framework, MetaNeighbor, that allows researchers to quantify the degree to which cell types replicate across datasets, and to rapidly identify clusters with high similarity for further testing. We first measure the replicability of neuronal identity by comparing more than 13 thousand individual scRNA-seq transcriptomes, then assess cross-dataset evidence for novel pyramidal neuron and cortical interneuron subtypes identified by scRNA-seq. We find that 24/45 cortical interneuron subtypes and 10/48 pyramidal neuron subtypes have evidence of replication in at least one other study. Identifying these putative replicates allows us to re-analyze the data for differential expression and provide lists of robust candidate marker genes. Across tasks we find that large sets of variably expressed genes can identify replicable cell types and subtypes with high accuracy, indicating many of the transcriptional changes characterizing cell identity are pervasive and easily detected."}, {"title": "GEOracle: Mining perturbation experiments using free text metadata in Gene Expression Omnibus", "url": "https://www.biorxiv.org/content/early/2017/06/16/150896", "tag": "Bioinformatics", "abstract": "There exists over 1.6 million publicly available gene expression samples across 79,000 data series in NCBI's Gene Expression Omnibus database. Due to the lack of the use of standardised ontology terms to annotate the experimental type and sample type, this database remains difficult to harness computationally without significant manual intervention. In this work, we present an interactive R/Shiny tool called GEOracle that utilises text mining and machine learning techniques to automatically identify perturbation experiments, group treatment and control samples and perform differential expression. We present applications of GEOracle to discover conserved signalling pathway target genes and identify an organ specific gene regulatory network."}, {"title": "K-mer clustering algorithm using a MapReduce framework: application to the parallelization of the Inchworm module of Trinity", "url": "https://www.biorxiv.org/content/early/2017/06/16/149948", "tag": "Bioinformatics", "abstract": "De novo transcriptome assembly is an important technique for understanding gene expression in non-model organisms. Many de novo assemblers using the de Bruijn graph of a set of the RNA sequences rely on in-memory representation of this graph. However, current methods analyse the complete set of read-derived k-mer sequence at once, resulting in the need for computer hardware with large shared memory. We introduce a novel approach that clusters k-mers as the first step. The clusters correspond to small sets of gene products, which can be processed quickly to give candidate transcripts. We implement the clustering step using the MapReduce approach for parallelising the analysis of large datasets, which enables the use of compute clusters. The computational task is distributed across the compute system, and no specialised hardware is required. Using this approach, we have re-implemented the Inchworm module from the widely used Trinity pipeline, and tested the method in the context of the full Trinity pipeline. Validation tests on a range of real datasets show large reductions in the runtime and per-node memory requirements, when making use of a compute cluster. Our study shows that MapReduce-based clustering has great potential for distributing challenging sequencing problems, without loss of accuracy. Although we have focussed on the Trinity package, we propose that such clustering is a useful initial step for other assembly pipelines."}, {"title": "Inter-Tools: a toolkit for interactome analysis", "url": "https://www.biorxiv.org/content/early/2017/06/15/150706", "tag": "Bioinformatics", "abstract": "Interactome analysis is an increasingly useful tool for discovering disease pathways. However, protein interaction data is scattered among different sources, and researchers must often sift through several large repositories before beginning a project. Inter-Tools allows users to search databases in the PSI-MITAB standard format for interactions meeting specified criteria, then combine them into a single, species-specific interactome dataset. It also produces graph files of interactome networks and performs basic analysis on user-input gene sets. Inter-Tools reduces the barriers to preliminary interactome investigation and hypothesis generation."}, {"title": "xCell: Digitally portraying the tissue cellular heterogeneity landscape", "url": "https://www.biorxiv.org/content/early/2017/06/15/114165", "tag": "Bioinformatics", "abstract": "Tissues are a complex milieu consisting of numerous cell types. For example, understanding the cellular heterogeneity the tumor microenvironment is an emerging field of research. Numerous methods have been published in recent years for the enumeration of cell subsets from tissue expression profiles. However, the available methods suffer from three major problems: inferring cell subset based on gene sets learned and verified from limited sources; displaying only partial portrayal of the full cellular heterogeneity; and insufficient validation in mixed tissues. To address these issues we developed xCell, a novel gene-signature based method for inferring 64 immune and stroma cell types. We first curated and harmonized 1,822 transcriptomic profiles of pure human cell types from various sources, employed a curve fitting approach for linear comparison of cell types, and introduced a novel spillover compensation technique for separating between closely related cell types. We test the ability of our model learned from pure cell types to infer enrichments of cell types in mixed tissues, using both comprehensive in silico analyses, and by comparison to cytometry immunophenotyping to show that our scores outperform previously published methods. Finally, we explore the cell type enrichments in tumor samples and show that the cellular heterogeneity of the tumor microenvironment uniquely characterizes different cancer types. We provide our method for inferring cell type abundances as a public resource to allow researchers to portray the cellular heterogeneity landscape of tissue expression profiles: http://xCell.ucsf.edu/."}, {"title": "Inferring synteny between genome assemblies: a systematic evaluation", "url": "https://www.biorxiv.org/content/early/2017/06/14/149989", "tag": "Bioinformatics", "abstract": "Identification of synteny between genomes of closely related species is an important aspect of comparative genomics. However, it is unknown to what extent draft assemblies lead to errors in such analysis. To investigate this, we fragmented genome assemblies of model nematodes to various extents and conducted synteny identification and downstream analysis. We first show that synteny between species can be underestimated up to 40% and find disagreements between popular tools that infer synteny blocks. This inconsistency and further demonstration of erroneous gene ontology enrichment tests throws into question the robustness of previous synteny analysis when gold standard genome sequences remain limited. In addition, determining the true evolutionary relationship is compromised by assembly improvement using a reference guided approach with a closely related species. Annotation quality, however, has minimal effect on synteny if the assembled genome is highly contiguous. Our results highlight the need for gold standard genome assemblies for synteny identification and accurate downstream analysis."}, {"title": "LIONS: Analysis Suite for Detecting and Quantifying Transposable Element Initiated Transcription from RNA-seq", "url": "https://www.biorxiv.org/content/early/2017/06/13/149864", "tag": "Bioinformatics", "abstract": "Transposable Elements (TEs), which comprise almost half of the human genome, can contribute to the evolution of novel transcriptional circuitry. Specific and biologically meaningful interpretation of TE-initiated transcripts has been marred by computational and methodological deficiencies. We developed the software suite LIONS (www.github.com/ababaian/LIONS) to analyze paired-end RNA-seq data for detecting and quantifying significant TE-contributions to a transcriptome. The LIONS suite serves as a platform for TE RNA-seq analysis and can be applied to a broad set of data sets for the study of development, stress treatments, aging and cancer."}, {"title": "Predicting double-strand DNA breaks using epigenome marks or DNA at kilobase resolution", "url": "https://www.biorxiv.org/content/early/2017/06/12/149039", "tag": "Bioinformatics", "abstract": "Double-strand breaks (DSBs) result from the attack of both DNA strands by multiple sources, including exposure to ionizing radiation or reactive oxygen species. DSBs can cause abnormal chromosomal rearrangements which are linked to cancer development, and hence represent an important issue. Recent techniques allow the genome-wide mapping of DSBs at high resolution, enabling the comprehensive study of DSB origin. However these techniques are costly and challenging. Hence we devised a computational approach to predict DSBs using the epigenomic and chromatin context, for which public data are available from the ENCODE project. We achieved excellent prediction accuracy (AUC=0.97) at high resolution (<1 kb), and showed that only chromatin accessibility and H3K4me1 mark were sufficient for highly accurate prediction (AUC=0.95). We identified chromatin accessibility, activity and long-range contacts as best predictors. In addition, our work represents the first step toward unveiling the \u201cDNA repairing\u201d code underlying DSBs, paving the way for future studies of cis elements involved in DNA damage and repair."}, {"title": "DeepARG: A deep learning approach for predicting antibiotic resistance genes from metagenomic data", "url": "https://www.biorxiv.org/content/early/2017/06/12/149328", "tag": "Bioinformatics", "abstract": "Growing concerns regarding increasing rates of antibiotic resistance call for global monitoring efforts. Monitoring of environmental media (e.g., wastewater, agricultural waste, food, and water) is of particular interest as these media can serve as sources of potential novel antibiotic resistance genes (ARGs), as hot spots for ARG exchange, and as pathways for the spread of ARGs and human exposure. Next-generation sequence-based monitoring has recently enabled direct access and profiling of the total metagenomic DNA pool, where ARGs are identified or predicted based on the best hits of homology searches against existing databases. Unfortunately, this approach tends to produce high rates of false negatives. To address such limitations, we propose here a deep leaning approach, taking into account a dissimilarity matrix created using all known categories of ARGs. Two models, deepARG-SS and deepARG-LS, were constructed for short read sequences and full gene length sequences, respectively. Performance evaluation of the deep learning models over 30 classes of antibiotics demonstrates that the deepARG models can predict ARGs with both high precision (>0.97) and recall (>0.90) for most of the antibiotic resistance categories. The models show advantage over the traditional best hit approach by having consistently much lower false negative rates and thus higher overall recall (>0.9). As more data become available for under-represented antibiotic resistance categories, the deepARG models performance can be expected to be further enhanced due to the nature of the underlying neural networks. The deepARG models are available both in command line version and via a Web server at http://bench.cs.vt.edu/deeparg. Our newly developed ARG database, deepARG-DB, containing predicted ARGs with high confidence and high degree of manual curation, greatly expands the current ARG repository. DeepARG-DB can be downloaded freely to benefit community research and future development of antibiotic resistance-related resources."}, {"title": "Tumor subclonal progression model for cancer hallmark acquisition", "url": "https://www.biorxiv.org/content/early/2017/06/12/149252", "tag": "Bioinformatics", "abstract": "Recent advances in the methodologies of reconstructing cancer evolutionary trajectories opened the horizon for deciphering the subclonal populations and their evolutionary architectures under the cancer ecosystems. An important challenge of the cancer evolution studies is connecting genetic aberrations in subclones to clinically interpretable and actionable target of subclones for individual patients. In this paper, we present a novel method for constructing tumor subclonal progression model for cancer hallmark acquisition using multi-regional sequencing data. We prepare a subclonal evolutionary tree inferred from variant allele frequencies and estimate the pathway alternation probabilities from large scale cohort genomic data. We then construct an evolutionary tree of pathway alternation that takes account of selectivity of pathway alternations by the notion of probabilistic causality. We show the effectiveness of our method using a dataset of clear cell renal cell carcinomas."}, {"title": "Critical Assessment of Metagenome Interpretation \u2212 a benchmark of computational metagenomics software", "url": "https://www.biorxiv.org/content/early/2017/06/12/099127", "tag": "Bioinformatics", "abstract": "In metagenome analysis, computational methods for assembly, taxonomic profiling and binning are key components facilitating downstream biological data interpretation. However, a lack of consensus about benchmarking datasets and evaluation metrics complicates proper performance assessment. The Critical Assessment of Metagenome Interpretation (CAMI) challenge has engaged the global developer community to benchmark their programs on datasets of unprecedented complexity and realism. Benchmark metagenomes were generated from ~700 newly sequenced microorganisms and ~600 novel viruses and plasmids, including genomes with varying degrees of relatedness to each other and to publicly available ones and representing common experimental setups. Across all datasets, assembly and genome binning programs performed well for species represented by individual genomes, while performance was substantially affected by the presence of related strains. Taxonomic profiling and binning programs were proficient at high taxonomic ranks, with a notable performance decrease below the family level. Parameter settings substantially impacted performances, underscoring the importance of program reproducibility. While highlighting current challenges in computational metagenomics, the CAMI results provide a roadmap for software selection to answer specific research questions."}, {"title": "Repeat aware evaluation of scaffolding tools", "url": "https://www.biorxiv.org/content/early/2017/06/12/148932", "tag": "Bioinformatics", "abstract": "Genomic sequences are assembled into a variable, but large number of contigs that should be scaffolded (ordered and oriented) for facilitating comparative or functional analysis. Finding scaffolding is computationally challenging due to misassemblies, inconsistent coverage across the genome, and long repeats. An accurate assessment of scaffolding tools should take into account multiple locations of the same contig on the reference scaffolding rather than matching a repeat to a single best location. This makes mapping of inferred scaffoldings onto the reference a computationally challenging problem. This paper formulates the repeat-aware scaffolding evaluation problem which is to find a mapping of the inferred scaffolding onto the reference maximizing number of correct links and proposes a scalable algorithm capable of handling large whole-genome datasets. Our novel scaffolding validation pipeline has been applied to assess the most of state-of-the-art scaffolding tools on the representative subset of GAGE datasets."}, {"title": "BugBuilder - An Automated Microbial Genome Assembly and Analysis Pipeline", "url": "https://www.biorxiv.org/content/early/2017/06/11/148783", "tag": "Bioinformatics", "abstract": "Summary: BugBuilder is a framework for hands-free assembly and annotation of microbial genomes. It produces outputs suitable either for database submission or downstream finishing processes. It is configurable to work with most command-line assembly and scaffolding tools which are selectable at run-time, and supports all common sequence types used in microbial genome assembly. Availability and Implementation: BugBuilder is implemented in Perl and is available under the Artistic License from http://www.imperial.ac.uk/bioinformatics-data-science-group/resources/software/bugbuilder, A virtual machine image is available pre-configured with the relevant freely-redistributable dependencies."}, {"title": "A new multi-genomic approach for the study of biogeochemical cycles at global scale: the molecular reconstruction of the sulfur cycle", "url": "https://www.biorxiv.org/content/early/2017/06/11/148775", "tag": "Bioinformatics", "abstract": "Despite the great advances in microbial ecology and the explosion of high throughput sequencing, our ability to understand and integrate the global biogeochemical cycles is still limited. Here we propose a novel approach to summarize the complexity of the Sulfur cycle based on the minimum ecosystem concept, the microbial mat model and the relative entropy of protein domains involved in S-metabolism. This methodology produces a single value, called the Sulfur Score (SS), which informs about the specific S-related molecular machinery. After curating an inventory of microorganisms, pathways and genes taking part in this cycle, we benchmark the performance of the SS on a collection of 2,107 non-redundant RefSeq genomes, 900 metagenomes from MG-RAST and 35 metagenomes analyzed for the first time. We find that the SS is able to correctly classify microorganisms known to be involved in the S-cycle, yielding an Area Under the ROC Curve of 0.985. Moreover, when sorting environments the top-scoring metagenomes were hydrothermal vents, microbial mats and deep-sea sediments, among others. This methodology can be generalized to the analysis of other biogeochemical cycles or processes. Provided that an inventory of relevant pathways and microorganisms can be compiled, entropy-based scores could be used to detect environmental patterns and informative samples in multi-genomic scale."}, {"title": "Graphtyper: Population-scale genotyping using pangenome graphs", "url": "https://www.biorxiv.org/content/early/2017/06/09/148403", "tag": "Bioinformatics", "abstract": "A fundamental requisite for genetic studies is an accurate determination of sequence variation. While human genome sequence diversity is increasingly well characterized, there is a need for efficient ways to utilize this knowledge in sequence analysis. Here we present Graphtyper, a publicly available novel algorithm and software for discovering and genotyping sequence variants. Graphtyper realigns short-read sequence data to a pangenome, a variation-aware graph structure that encodes sequence variation within a population by representing possible haplotypes as graph paths. Our results show that Graphtyper is fast, highly scalable, and provides sensitive and accurate genotype calls. Graphtyper genotyped 89.4 million sequence variants in whole-genomes of 28,075 Icelanders using less than 100,000 CPU days, including detailed genotyping of six human leukocyte antigen (HLA) genes. We show that Graphtyper is a valuable tool in characterizing sequence variation in population-scale sequencing studies."}, {"title": "iDEP: An integrated web application for differential expression and pathway analysis", "url": "https://www.biorxiv.org/content/early/2017/06/09/148411", "tag": "Bioinformatics", "abstract": "iDEP (integrated Differential Expression and Pathway analysis) is a web application that reads in gene expression data from DNA microarray or RNA-Seq and performs exploratory data analysis (EDA), differential expression, and pathway analysis. The key idea of iDEP is to make many powerful R/Bioconductor packages easily accessible by wrapping them under a graphical interface, alongside annotation databases. For EDA, it performs hierarchical clustering, k-means clustering, and principal component analysis (PCA). iDEP detects differentially expressed genes using the limma and DESeq2 packages. For a group of co-expressed genes, it identifies enriched gene ontology (GO) terms as well as transcription factor binding motifs in promoters. Pathway analysis can be performed using several packages like GAGE, GSEA, PGSEA, or ReactomePA. iDEP can also detect chromosomal gain or loss using the PREDA package. iDEP uses annotation of 69 metazoa and 44 plant genomes in Ensembl for ID mapping and functional categorization. Pathway information was also compiled from databases like KEGG, Reactome, MSigDB, GSKB, and araPath. As an example, we extensively analyzed an RNA-Seq dataset involving siRNA-mediated Hoxa1 knockdown in lung fibroblasts, and identified the down-regulation of cell-cycle genes, in agreement with previous findings. Our analyses also reveal the possible roles of E2F1 and its target genes, including microRNAs, in blocking G1/S transition, and the upregulation of genes related to cytokines, lysosome, neuronal parts. By integrating many R and Bioconductor packages with comprehensive annotation databases, iDEP (http://ge-lab.org/idep/) enables users to conduct in-depth bioinformatics analysis of transcriptomic data through a graphical interface."}, {"title": "OneD: increasing reproducibility of Hi-C Samples with abnormal karyotypes", "url": "https://www.biorxiv.org/content/early/2017/06/09/148254", "tag": "Bioinformatics", "abstract": "The three-dimensional conformation of genomes is an essential component of their biological activity. The advent of the Hi-C technology enabled an unprecedented progress in our understanding of genome structures. However, Hi-C is subject to systematic biases that can compromise downstream analyses. Several strategies have been proposed to remove those biases, but the issue of abnormal karyotypes received little attention. Many experiments are performed in cancer cell lines, which typically harbor large-scale copy number variations that create visible defects on the raw Hi-C maps. The consequences of these widespread artifacts on the normalized maps are mostly unexplored. We observed that current normalization methods are not robust to the presence of large-scale copy number variations, potentially obscuring biological differences and enhancing batch effects. To address this issue, we developed an alternative approach designed to take into account chromosomal abnormalities. The method, called OneD, increases reproducibility among replicates of Hi-C samples with abnormal karyotype, outperforming previous methods significantly. On normal karyotypes, OneD fared equally well as state-of-the-art methods, making it a safe choice for Hi-C normalization. OneD is fast and scales well in terms of computing resources for resolutions up to 1 kbp. OneD is implemented as an R package available at http://www.github.com/qenvio/dryhic."}, {"title": "Indoril: An I-PV Add-On For Visualization Of Point Mutations On 3D Cartesian Coordinates", "url": "https://www.biorxiv.org/content/early/2017/06/09/148122", "tag": "Bioinformatics", "abstract": "One of the main goals of proteomics is to understand how point mutations impact on the protein structure. Visualization and clustering of point mutations on user-defined 3 dimensional space can allow researchers to have new insights and hypothesis about the mutation's mechanism of action. Availability and Implementation: We have developed an interactive I-PV add-on called INDORIL to visualize point mutations. Indoril can be downloaded from http://www.i-pv.org."}, {"title": "Integrating long-range connectivity information into de Bruijn graphs", "url": "https://www.biorxiv.org/content/early/2017/06/09/147777", "tag": "Bioinformatics", "abstract": "Motivation: The de Bruijn graph is a simple and efficient data structure that is used in many areas of sequence analysis including genome assembly, read error correction and variant calling. The data structure has a single parameter k, is straightforward to implement and is tractable for large genomes with high sequencing depth. It also enables representation of multiple samples simultaneously to facilitate comparison. However, unlike the string graph, a de Bruijn graph does not retain long range information that is inherent in the read data. For this reason, applications that rely on de Bruijn graphs can produce sub-optimal results given their input. Results: We present a novel assembly graph data structure: the Linked de Bruijn Graph (LdBG). Constructed by adding annotations on top of a de Bruijn graph, it stores long range connectivity information through the graph. We show that with error-free data it is possible to losslessly store and recover sequence from a Linked de Bruijn graph. With assembly simulations we demonstrate that the LdBG data structure outperforms both the de Bruijn graph and the String Graph Assembler (SGA). Finally we apply the LdBG to Klebsiella pneumoniae short read data to make large (12 kbp) variant calls, which we validate using PacBio sequencing data, and to characterise the genomic context of drug-resistance genes. Availability: Linked de Bruijn Graphs and associated algorithms are implemented as part of McCortex, available under the MIT license at https://github.com/mcveanlab/mccortex."}, {"title": "Segway 2.0: Gaussian mixture models and minibatch training", "url": "https://www.biorxiv.org/content/early/2017/06/08/147470", "tag": "Bioinformatics", "abstract": "Segway performs semi-automated genome annotation, discovering joint patterns across multiple genomic signal datasets. We discuss a major new version of Segway and highlight its ability to model data with substantially greater accuracy. Segway 2.0 models data with a mixture of Gaussians, enabling capture of arbitrarily complex signal distributions. Segway 2.0 includes minibatch training, leading to better learned parameters. Availability and Implementation: Segway and its source code are freely available for download at https://segway.hoffmanlab.org. We have made available scripts (https://doi.org/10.5281/zenodo.802939) and datasets (https://doi.org/10.5281/zenodo.802906) for this paper's analysis."}, {"title": "WebMeV: A Cloud Platform for Analyzing and Visualizing Cancer Genomic Data", "url": "https://www.biorxiv.org/content/early/2017/06/08/147884", "tag": "Bioinformatics", "abstract": "Although large, complex genomic data sets are increasingly easy to generate, and the number of publicly available data sets in cancer and other diseases is rapidly growing, the lack of intuitive, easy to use analysis tools has remained a barrier to the effective use of such data. WebMeV (https://mev.tm4.org) is an open-source, web-based tool that gives users access to sophisticated tools for analysis of RNA-Seq and other data in an interface designed to democratize data access. WebMeV combines cloud-based technologies with a simple user interface to allow users to access large public data sets such as that from The Cancer Genome Atlas (TCGA) or to upload their own. The interface allows users to visualize data and to apply advanced data mining analysis methods to explore the data and draw biologically meaningful conclusions. We provide an overview of WebMeV and demonstrate two simple use cases that illustrate the value of putting data analysis in the hands of those looking to explore the underlying biology of the systems being studied."}, {"title": "Evaluation of RNAi and CRISPR technologies by large-scale gene expression profiling in the Connectivity Map", "url": "https://www.biorxiv.org/content/early/2017/06/08/147504", "tag": "Bioinformatics", "abstract": "The application of RNA interference (RNAi) to mammalian cells has provided the means to perform phenotypic screens to determine the functions of genes. Although RNAi has revolutionized loss of function genetic experiments, it has been difficult to systematically assess the prevalence and consequences of off-target effects. The Connectivity Map (CMAP) represents an unprecedented resource to study the gene expression consequences of expressing short hairpin RNAs (shRNAs). Analysis of signatures for over 13,000 shRNAs applied in 9 cell lines revealed that miRNA-like off-target effects of RNAi are far stronger and more pervasive than generally appreciated. We show that mitigating off-target effects is feasible in these datasets via computational methodologies to produce a Consensus Gene Signature (CGS). In addition, we compared RNAi technology to clustered regularly interspaced short palindromic repeat (CRISPR)-based knockout by analysis of 373 sgRNAs in 6 cells lines, and show that the on-target efficacies are comparable, but CRISPR technology is far less susceptible to systematic off-target effects. These results will help guide the proper use and analysis of loss-of-function reagents for the determination of gene function."}, {"title": "FUSTr: a tool to find gene Families Under Selection in Transcriptomes", "url": "https://www.biorxiv.org/content/early/2017/06/07/146951.1", "tag": "Bioinformatics", "abstract": "FUSTr is a tool for finding genes in transcriptomic datasets under strong positive selection that automatically detects isoform designation patterns in transcriptome assemblies to maximize phylogenetic independence in downstream analysis. When applied to previously studied spider toxin families as well as simulated data, FUSTr successfully grouped coding sequences into proper gene families as well as correctly identified those under strong positive selection. FUSTr provides a tool capable of utilizing multi-processor high-performance computational facilities and is scalable for large transcriptomic biodiversity datasets."}, {"title": "Prediction of RNA-protein sequence and structure binding preferences using deep convolutional and recurrent neural networks", "url": "https://www.biorxiv.org/content/early/2017/06/07/146175", "tag": "Bioinformatics", "abstract": "RNA regulation is significantly dependent on its binding protein partner, which is known as the RNA-binding proteins (RBPs). Unfortunately, the binding preferences for most RBPs are still not well characterized, especially on the structure point of view. Informative signals hiding and interdependencies between sequence and structure specificities are two challenging problems for both predicting RBP binding sites and accurate sequence and structure motifs mining. In this study, we propose a deep learning-based method, iDeepS, to simultaneously identify the binding sequence and structure motifs from RNA sequences using convolutional neural networks (CNNs) and a bidirectional long short term memory network (BLSTM). We first perform one-hot encoding for both the sequence and predicted secondary structure, which are appropriate for subsequent convolution operations. To reveal the hidden binding knowledge from the observations, the CNNs are applied to learn the abstract motif features. Considering the close relationship between sequences and predicted structures, we use the BLSTM to capture the long range dependencies between binding sequence and structure motifs identified by the CNNs. Finally, the learned weighted representations are fed into a classification layer to predict the RBP binding sites. We evaluated iDeepS on verified RBP binding sites derived from large-scale representative CLIP-seq datasets, and the results demonstrate that iDeepS can reliably predict the RBP binding sites on RNAs, and outperforms the state-of-the-art methods. An important advantage is that iDeepS is able to automatically extract both binding sequence and structure motifs, which will improve our transparent understanding of the mechanisms of binding specificities of RBPs. iDeepS is available at https://github.com/xypan1232/iDeepS."}, {"title": "3D clustering analysis of super-resolution microscopy data by 3D Voronoi tessellations", "url": "https://www.biorxiv.org/content/early/2017/06/07/146456", "tag": "Bioinformatics", "abstract": "Single-molecule localization microscopy (SMLM) can play an important role in integrated structural biology approaches for example at the interface of cryo electron microscopy (cryo-EM), X-ray crystallography, NMR and fluorescence imaging to identify, localize and determine the 3D structure of cellular structures. While many tools exist for the 3D analysis and visualisation of crystal or cryo-EM structures little exists for 3D SMLM data which can provide fascinating insights but are particularly challenging to analyze in three dimensions especially in a dense cellular context. We developed 3DClusterViSu, a method based on 3D Voronoi tessellations that allows local density estimation, segmentation & quantification of 3D SMLM data and visualization of protein clusters within a 3D tool. We show its robust performance on microtubules and histone proteins H2B and CENP-A with distinct spatial distributions. 3DClusterViSu will favor multi-scale and multi-resolution synergies to allow integrating molecular and cellular levels in the analysis of macromolecular complexes."}, {"title": "Genetic variation in human drug-related genes", "url": "https://www.biorxiv.org/content/early/2017/06/07/147108", "tag": "Bioinformatics", "abstract": "Variability in drug efficacy and adverse effects are observed in clinical practice. While the extent of genetic variability in classical pharmacokinetic genes is rather well understood, the role of genetic variation in drug targets is typically less studied. Based on 60,706 human exomes from the ExAC dataset, we performed an in-depth computational analysis of the prevalence of functional-variants in in 806 drug-related genes, including 628 known drug targets. We find that most genetic variants in these genes are very rare (f < 0.1%) and thus likely not observed in clinical trials. Overall, however, four in five patients are likely to carry a functional-variant in a target for commonly prescribed drugs and many of these might alter drug efficacy. We further computed the likelihood of 1,236 FDA approved drugs to be affected by functional-variants in their targets and show that the patient-risk varies for many drugs with respect to geographic ancestry. A focused analysis of oncological drug targets indicates that the probability of a patient carrying germline variants in oncological drug targets is with 44% high enough to suggest that not only somatic alterations, but also germline variants carried over into the tumor genome should be included in therapeutic decision-making."}, {"title": "PureCLIP: Capturing target-specific protein-RNA interaction footprints from single-nucleotide CLIP-seq data", "url": "https://www.biorxiv.org/content/early/2017/06/07/146704", "tag": "Bioinformatics", "abstract": "iCLIP and eCLIP techniques facilitate the detection of protein-RNA interaction sites at high resolution, based on diagnostic events at crosslink sites. However, previous methods do not explicitly model the specifics of iCLIP and eCLIP truncation patterns and possible biases. We developed PureCLIP, a hidden Markov model based approach, which simultaneously performs peak calling and individual crosslink site detection. It explicitly incorporates RNA abundances and, for the first time, non-specific sequence biases. On both simulated and real data, PureCLIP is more accurate in calling crosslink sites than other state-of-the-art methods and has a higher agreement across replicates. Link: https://github.com/skrakau/PureCLIP."}, {"title": "MMseqs2: sensitive protein sequence searching for the analysis of massive data sets", "url": "https://www.biorxiv.org/content/early/2017/06/07/079681", "tag": "Bioinformatics", "abstract": "Sequencing costs have dropped much faster than Moore's law in the past decade, and sensitive sequence searching has become the main bottleneck in the analysis of large metagenomic datasets. We therefore developed the open-source software MMseqs2 (mmseqs.org), which improves on current search tools over the full range of speed-sensitivity trade-off, achieving sensitivities better than PSI-BLAST at more than 400 times its speed."}, {"title": "Combining 16S rRNA gene variable regions enables high-resolution microbial community profiling", "url": "https://www.biorxiv.org/content/early/2017/06/06/146738", "tag": "Bioinformatics", "abstract": "Background: Most of our knowledge about the remarkable microbial diversity on Earth comes from sequencing the 16S rRNA gene. The use of next-generation sequencing methods has increased sample number and sequencing depth, but the read length of the most widely used sequencing platforms today is quite short, requiring the researcher to choose a subset of the gene to sequence (typically 16-33% of the total length). Thus, many bacteria may share the same amplified region and the resolution of profiling is inherently limited. Platforms that offer ultra long read lengths, whole genome shotgun sequencing approaches, and computational frameworks formerly suggested by us and by others, all allow different ways to circumvent this problem yet suffer various shortcomings. There is need for a simple and low cost 16S rRNA gene based profiling approach that harnesses the short read length to provide a much larger coverage of the gene to allow for high resolution, even in harsh conditions of low bacterial biomass and fragmented DNA. Results: This manuscript suggests Short MUltiple Regions Framework (SMURF), a method to combine sequencing results from different PCR-amplified regions to provide one coherent profiling. The de facto amplicon length is the total length of all amplified regions, thus providing much higher resolution compared to current techniques. Computationally, the method solves a convex optimization problem that allows extremely fast reconstruction and requires only moderate memory. We demonstrate the increase in resolution by in silico simulations and by profiling two mock mixtures and real-world biological samples. Reanalyzing a mock mixture from the Human Microbiome Project achieved about two-fold improvement in resolution when combing two independent regions. Using a custom set of six primer pairs spanning about 1200bp (80%) of the 16S rRNA gene we were able to achieve ~100 fold improvement in resolution compared to a single region, over a mock mixture of common human gut bacterial isolates. Finally, profiling of a Drosophila melanogaster microbiome using the set of six primer pairs provided a ~100 fold increase in resolution, and thus enabling efficient downstream analysis. Conclusions: SMURF enables identification of near full-length 16S rRNA gene sequences in microbial communities, having resolution superior compared to current techniques. It may be applied to standard sample preparation protocols with very little modifications. SMURF also paves the way to high-resolution profiling of low-biomass and fragmented DNA, e.g., in the case of Formalin-fixed and Paraffin-embedded samples, fossil-derived DNA or DNA exposed to other degrading conditions. The approach is not restricted to combining amplicons of the 16S rRNA gene and may be applied to any set of amplicons, e.g., in Multilocus Sequence Typing (MLST)."}, {"title": "HMMER Cut-off Threshold Tool (HMMERCTTER): Supervised Classification of Superfamily Protein Sequences with a reliable Cut-off Threshold", "url": "https://www.biorxiv.org/content/early/2017/06/06/130443", "tag": "Bioinformatics", "abstract": "Protein superfamilies can be divided into subfamilies of proteins with different functional characteristics. Their sequences can be classified hierarchically, which is part of sequence function assignation. Typically, there are no clear subfamily hallmarks that would allow pattern-based function assignation by which this task is mostly achieved based on the similarity principle. This is hampered by the lack of a score cut-off that is both sensitive and specific. HMMER Cut-off Threshold Tool (HMMERCTTER) adds a reliable cut-off threshold to the popular HMMER. Using a high quality superfamily phylogeny, it clusters a set of training sequences such that the cluster-specific HMMER profiles show 100% precision and recall (P&R), thereby generating a specific threshold as inclusion cut-off. Profiles and threshold are then used as classifiers to screen a target dataset. Iterative inclusion of novel sequences to clusters and the corresponding HMMER profiles results in high sensitivity while specificity is maintained by imposing 100% P&R. In three presented case studies of protein superfamilies, classification of large datasets with 100% P&R was achieved with over 95% coverage. Limits and caveats are presented and explained. HMMERCTTER is a promising protein superfamily sequence classifier provided high quality training datasets are used. It provides a decision support system that aids in the difficult task of sequence function assignation in the twilight zone of sequence similarity."}, {"title": "Assembling Metagenomes, One Community At A Time", "url": "https://www.biorxiv.org/content/early/2017/06/06/120154", "tag": "Bioinformatics", "abstract": "Background: Metagenomics allows unprecedented access to uncultured environmental microorganisms. The analysis of metagenomic sequences facilitates gene prediction and annotation, and enables the assembly of draft genomes, including uncultured members of a community. However, while several platforms have been developed for this critical step, there is currently no clear framework for the assembly of metagenomic sequence data. Results: To assist with selection of an appropriate metagenome assembler we evaluated the capabilities of nine prominent assembly tools on nine publicly-available environmental metagenomes, as well as three simulated datasets. Overall, we found that SPAdes provided the largest contigs and highest N50 values across 6 of the 9 environmental datasets, followed by MEGAHIT and metaSPAdes. MEGAHIT emerged as a computationally inexpensive alternative to SPAdes, assembling the most complex dataset using less than 500 GB of RAM and within 10 hours. Conclusions: We found that assembler choice ultimately depends on the scientific question, the available resources and the bioinformatic competence of the researcher. We provide a concise workflow for the selection of the best assembly tool."}, {"title": "speaq 2.0: a complete workflow for high-throughput 1D NMR spectra processing and quantification", "url": "https://www.biorxiv.org/content/early/2017/06/06/138503", "tag": "Bioinformatics", "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy is, together with liquid chromatography-mass spectrometry (LC-MS), the most established platform to perform metabolomics. In contrast to LC-MS however, NMR data is predominantly being processed with commercial software. Meanwhile its data processing remains tedious and dependent on user interventions. As a follow-up to speaq, a previously released workflow for NMR spectral alignment and quantitation, we present speaq 2.0. This completely revised framework to automatically analyze 1D NMR spectra uses wavelets to efficiently summarize the raw spectra with minimal information loss or user interaction. The tool offers a fast and easy workflow that starts with the common approach of peak-picking, followed by grouping. This yields a matrix consisting of features, samples and peak values that can be conveniently processed either by using included multivariate statistical functions or by using many other recently developed methods for NMR data analysis. speaq 2.0 facilitates robust and high-throughput metabolomics based on 1D NMR but is also compatible with other NMR frameworks or complementary LC-MS workflows. The methods are benchmarked using two publicly available datasets. speaq 2.0 is distributed through the existing speaq R package to provide a complete solution for NMR data processing. The package and the code for the presented case studies are freely available on CRAN (https://cran.r-project.org/package=speaq) and GitHub (https://github.com/beirnaert/speaq)."}, {"title": "BEARscc determines robustness of single-cell clusters using simulated technical replicates", "url": "https://www.biorxiv.org/content/early/2017/06/05/118919", "tag": "Bioinformatics", "abstract": "Technical variance is a major confounding factor in single-cell RNA sequencing, not least because it is not possible to replicate measurements on the same cell. We present BEARscc, a tool that uses RNA spike-in controls to simulate experiment-specific technical replicates. We demonstrate that the tool improves the unsupervised classification of cells and facilitates the biological interpretation of single-cell RNA-seq experiments."}, {"title": "Reconstructing cell cycle and disease progression using deep learning", "url": "https://www.biorxiv.org/content/early/2017/06/05/081364", "tag": "Bioinformatics", "abstract": "We show that deep convolutional neural networks combined with non-linear dimension reduction enable reconstructing biological processes based on raw image data. We demonstrate this by reconstructing the cell cycle of Jurkat cells and disease progression in diabetic retinopathy. In further analysis of Jurkat cells, we detect and separate a subpopulation of dead cells in an unsupervised manner and, in classifying discrete cell cycle stages, we reach a 6-fold reduction in error rate compared to a recent approach based on boosting on image features. In contrast to previous methods, deep learning based predictions are fast enough for on-the-fly analysis in an imaging flow cytometer."}, {"title": "The importance of Chargaff's second parity rule for genomic signatures in metagenomics", "url": "https://www.biorxiv.org/content/early/2017/06/04/146001", "tag": "Bioinformatics", "abstract": "An important problem in metagenomic data analysis is to identify the source organism, or at least taxon, of each sequence. Most methods tackle this problem in two steps by using an alignment-free approach: first the DNA sequences are represented as points of a real n-dimensional space via a mapping function then either clustering or classification algorithms are applied. Those mapping functions require to be genomic signatures: the dissimilarity between the mapped points must reflect the degree of phylogenetic similarity of the source species. Designing good signatures for metagenomics can be challenging due to the special characteristics of metagenomic sequences; most of the existing signatures were not designed accordingly and they were tested only on error-free sequences sampled from a few dozens of species. In this work we analyze comparatively the goodness of existing and novel signatures based on tetranucleotide frequencies via statistical models and computational experiments; we also study how they are affected by the generalized Chargaff's second parity rule (GCSPR), which states that in a given sequence longer than 50 kbp, inverse oligonucleotides are approximately equally frequent. We analyze 38 million sequences of 150 bp-1,000 bp with 1% base-calling error, sampled from 1,284 microbes. Our models indicate that GCSPR reduces strand-dependence of signatures, that is, their values are less affected by the source strand; GCSPR is further exploited by some signatures to reduce the intra-species dispersion. Two novel signatures stand out both in the models and in the experiments: the combination signature and the operation signature. The former achieves strand-independence without grouping oligonucleotides; this could be valuable for alignment-free sequence comparison methods when distinguishing inverse oligonucleotides matters. Operation signature sums the frequencies of reverse, complement, and inverse tetranucleotides; having 72 features it reduces the computational intensity of the analysis."}, {"title": "Improving the value of public RNA-seq expression data by phenotype prediction", "url": "https://www.biorxiv.org/content/early/2017/06/03/145656", "tag": "Bioinformatics", "abstract": "Background: Publicly available genomic data are a valuable resource for studying normal human variation and disease, but these data are often not well labeled or annotated. The lack of phenotype information for public genomic data severely limits their utility for addressing targeted biological questions. Results: We develop an in silico phenotyping approach for predicting critical missing annotation directly from genomic measurements using, well-annotated genomic and phenotypic data produced by consortia like TCGA and GTEx as training data. We apply in silico phenotyping to a set of 70,000 RNA-seq samples we recently processed on a common pipeline as part of the recount2 project (https://jhubiostatistics.shinyapps.io/recount/). We use gene expression data to build and evaluate predictors for both biological phenotypes (sex, tissue, sample source) and experimental conditions (sequencing strategy). We demonstrate how these predictions can be used to study cross-sample properties of public genomic data, select genomic projects with specific characteristics, and perform downstream analyses using predicted phenotypes. The methods to perform phenotype prediction are available in the phenopredict R package (https://github.com/leekgroup/phenopredict) and the predictions for recount2 are available from the recount R package (https://bioconductor.org/packages/release/bioc/html/recount.html). Conclusion: Having leveraging massive public data sets to generate a well-phenotyped set of expression data for more than 70,000 human samples, expression data is available for use on a scale that was not previously feasible."}, {"title": "GOLabeler: Improving Sequence-based Large-scale Protein Function Prediction by Learning to Rank", "url": "https://www.biorxiv.org/content/early/2017/06/03/145763", "tag": "Bioinformatics", "abstract": "Motivation: Gene Ontology (GO) has been widely used to annotate functions of proteins and understand their biological roles. Currently only <1% of more than 70 million proteins in UniProtKB have experimental GO annotations, implying the strong necessity of automated function prediction (AFP) of proteins, where AFP is a hard multi-label classification problem due to one protein with a diverse number of GO terms. Most of these proteins have only sequences as input information, indicating the importance of sequence-based AFP (SAFP: sequences are the only input). Furthermore, homology-based SAFP tools are competitive in AFP competitions, while they do not necessarily work well for so-called difficult proteins, which have <60% sequence identity to proteins with annotations already. Thus, the vital and challenging problem now is to develop a method for SAFP, particularly for difficult proteins. Methods: The key of this method is to extract not only homology information but also diverse, deep-rooted information/evidence from sequence inputs and integrate them into a predictor in an efficient and also effective manner. We propose GOLabeler, which integrates five component classifiers, trained from different features, including GO term frequency, sequence alignment, amino acid trigram, domains and motifs, and biophysical properties, etc., in the framework of learning to rank (LTR), a new paradigm of machine learning, especially powerful for multi-label classification. Results: The empirical results obtained by examining GOLabeler extensively and thoroughly by using large-scale datasets revealed numerous favorable aspects of GOLabeler, including significant performance advantage over state-of-the-art AFP methods."}, {"title": "Guiding the design of bacterial signaling interactions using a coevolutionary landscape", "url": "https://www.biorxiv.org/content/early/2017/06/02/116947", "tag": "Bioinformatics", "abstract": "The selection of amino acid identities that encode new interactions between two-component signaling (TCS) proteins remains a significant challenge. Recent work constructed a co- evolutionary landscape that can be used to select mutations to maintain signal transfer interactions between partner TCS proteins without introducing signal transfer between non-partners (crosstalk). A bigger challenge is to introduce mutations between non-natural partner TCS proteins using the landscape to enhance, suppress, or have a neutral effect on their basal signal transfer rates. This study focuses on the selection of mutations to a response regulator (RR) from Bacilus subtilis and its effect on phosphotransfer with a histidine kinase (HK) from Escherichia Coli. Twelve single-point mutations of the RR protein are selected from the landscape and experimentally expressed to directly test the theoretical predictions on the effect of signal transfer. Differential Scanning Calorimetry is used to monitor any protein stability effects caused by the mutations, which could be detrimental to proper protein function. Of these proteins, seven mutants successfully perturb phosphoryl transfer activity in the computationally predicted manner between the TCS proteins. Furthermore, brute-force exhaustive mutagenesis approaches indicate that only 1% of mutations result in enhanced activity. In comparison, of the six mutations predicted to enhance phosphotransfer, two mutations exhibit a significant enhancement while two mutations are comparable to the wild-type. Thus co-evolutionary landscape theory offers significant improvement over traditional large-scale mutational studies in the efficiency of selecting mutations for protein engineering and design."}, {"title": "Accurate And Fast Feature Selection Workflow For High-Dimensional Omics Data", "url": "https://www.biorxiv.org/content/early/2017/06/02/144162", "tag": "Bioinformatics", "abstract": "We are moving into the age of \u201cBig Data\u201d in biomedical research and bioinformatics. This trend could be encapsulated in this simple formula: D = S x F, where the volume of data generated (D) increases in both dimensions: the number of samples (S) and the number of sample features (F). Frequently, a typical bioinformatics problem (e.g. classification) includes redundant and irrelevant features that can result, in the worst-case scenario, in false positive results. Then, Feature Selection (FS) constitutes an enormous challenge. Despite the number and diversity of algorithms available, the proper choice of an approach for facing a specific problem often falls in a \u201cgrey zone\u201d. In this study, we select a subset of FS methods to develop an efficient workflow and an R package for bioinformatics machine learning problems. We cover relevant issues concerning FS, ranging from domains problems to algorithm solutions and computational tools. Finally, we use seven different proteomics and gene expression datasets to evaluate the workflow and guide the FS process."}, {"title": "Pair Matcher (PaM): Fast Model-Based Optimisation Of Treatment/Case-Control Matches Using Demographic And Genetic Data", "url": "https://www.biorxiv.org/content/early/2017/06/02/145177", "tag": "Bioinformatics", "abstract": "In clinical trials, individuals are matched for demographic criteria, paired, and then randomly assigned to treatment and control groups to determine a drug's efficacy. The successful completion of pilot trials is a prerequisite to larger and more expensive Phase III trials. One of the chief causes for the irreproducibility of results across pilot to Phase III trials is population stratification bias caused by the uneven distribution of ancestries in the treatment and control groups. Pair Matcher (PaM) addresses stratification bias by optimising pairing assignments a priori and/or posteriori to the trial using both genetic and demographic criteria. Using simulated and real datasets, we show that PaM identifies ideal and near-ideal pairs that are more genetically homogeneous than those identified based on racial criteria or Principal Component Analysis (PCA) alone. Homogenising the treatment (or case) and control groups can be expected to improve the accuracy and reproducibility of the study. PaM's ability to infer the ancestry of the participants further allows identifying subgroup of responders and developing a precision medicine approach to treatment. PaM is simple to execute, fast, and can be used for clinical trials and association studies. PaM is freely available via R scripts and a web interface."}, {"title": "KmerFinderJS: A Client-Server Method For Fast Species Typing Of Bacteria Over Slow Internet Connections", "url": "https://www.biorxiv.org/content/early/2017/06/02/145284", "tag": "Bioinformatics", "abstract": "Motivation: KmerFinder is a program based on K-mer statistics for identifying bacterial species in whole genome data, that as a web server that have been used more than 10,000 times. KmerFind-erJS is a development of the KmerFinder that benefits from the downsampling of data using a prefix filtering used by KmerFinder, to minimize amount of data that needs to be transferred between the client and the server. Results: KmerFinderJS replaces the Python based hash structure for holding the databases with a true Key-value database. These improvements are shown to lead to a many-fold speed up of species identification with the internet transfer speeds that are realistic to expect today. It is also shown that the method can find the true content of an artificial metagenomic cocktail with no false positives. Availability: The method is freely available at https://cge.cbs.dtu.dk/services/KmerFinderJS/ and as a source code at https://bitbucket.org/genomicepidemiology/kmerfinderjs."}, {"title": "Haplotype And Repeat Separation In Long Reads", "url": "https://www.biorxiv.org/content/early/2017/06/02/145474", "tag": "Bioinformatics", "abstract": "Resolving the correct structure and succession of highly similar sequence stretches is one of the main open problems in genome assembly. For non haploid genomes this includes determining the sequences of the different haplotypes. For all but the smallest genomes it also involves separating different repeat instances. In this paper we discuss methods for resolving such problems in third generation long reads by classifying alignments between long reads according to whether they represent true or false read overlaps. The main problem in this context is the high error rate found in such reads, which greatly exceeds the amount of difference between the similar regions we want to separate. Our methods can separate read classes stemming from regions with as little as 1% difference."}, {"title": "Effective Online Bayesian Phylogenetics Via Sequential Monte Carlo With Guided Proposals", "url": "https://www.biorxiv.org/content/early/2017/06/02/145219", "tag": "Bioinformatics", "abstract": "Modern infectious disease outbreak surveillance produces continuous streams of sequence data which require phylogenetic analysis as data arrives. Current software packages for Bayesian phylogenetic inference are unable to quickly incorporate new sequences as they become available, making them less useful for dynamically unfolding evolutionary stories. This limitation can be addressed by applying a class of Bayesian statistical inference algorithms called sequential Monte Carlo (SMC) to conduct online inference, wherein new data can be continuously incorporated to update the estimate of the posterior probability distribution. In this paper we describe and evaluate several different online phylogenetic sequential Monte Carlo (OPSMC) algorithms. We show that proposing new phylogenies with a density similar to the Bayesian prior suffers from poor performance, and we develop \u2018guided\u2019 proposals that better match the proposal density to the posterior. Furthermore, we show that the simplest guided proposals can exhibit pathological behavior in some situations, leading to poor results, and that the situation can be resolved by heating the proposal density. The results demonstrate that relative to the widely-used MCMC-based algorithm implemented in MrBayes, the total time required to compute a series of phylogenetic posteriors as sequences arrive can be significantly reduced by the use of OPSMC, without incurring a significant loss in accuracy."}, {"title": "Exhaustive capture of biological variation in RNA-seq data through k-mer decomposition", "url": "https://www.biorxiv.org/content/early/2017/06/02/122937", "tag": "Bioinformatics", "abstract": "Each individual cell produces its own set of transcripts, which is the combined result of genetic variation, transcription regulation and post-transcriptional processing. Due to this combinatorial nature, obtaining the exhaustive set of full-length transcripts for a given species is a never-ending endeavor. Yet, each RNA deep sequencing experiment produces a variety of transcripts that depart from the reference transcriptome and should be properly identified. To address this challenge, we introduce a k-mer-based software protocol for capturing local RNA variation from a set of standard RNA-seq libraries, independently of a reference genome or transcriptome. Our software, called DE-kupl, analyzes k-mer contents and detects k-mers with differential abundance directly from the raw data files, prior to assembly or mapping. This enables to retrieve the virtually complete set of unannotated variation lying in an RNA-seq dataset. This variation is subsequently assigned to biological events such as differential lincRNAs, antisense RNAs, splice and polyadenylation variants, introns, expressed repeats, and SNV-harboring or exogenous RNA. We applied DE-kupl to public RNA-seq datasets, including an Epythelial-Mensenchymal Transition model and different human tissues. DE-kupl identified abundant novel events and showed excellent reproducibility when applied to independent deep sequencing experiments. DE-kupl is a new paradigm for analyzing differential RNA-seq data with no preconception on target events, which can also provide fresh insights into existing RNA-seq repositories."}, {"title": "Driver Pattern Identification Over The Gene Co-Expression Of Drug Response In Ovarian Cancer By Integrating High Throughput Genomics Data", "url": "https://www.biorxiv.org/content/early/2017/06/02/145268", "tag": "Bioinformatics", "abstract": "The multiple types of high throughput genomics data create a potential opportunity to identify driver pattern in ovarian cancer, which will acquire some novel and clinical biomarkers for appropriate diagnosis and treatment to cancer patients. However, it is a great challenging work to integrate omics data, including somatic mutations, Copy Number Variations (CNVs) and gene expression profiles, to distinguish interactions and regulations which are hidden in drug response dataset of ovarian cancer. To distinguish the candidate driver genes and the corresponding driving pattern for resistant and sensitive tumor from the heterogeneous data, we combined gene co-expression modules and mutation modulators and proposed the identification driver patterns method. Firstly, co-expression network analysis is applied to explore gene modules for gene expression profiles via weighted correlation network analysis (WGCNA). Secondly, mutation matrix is generated by integrating the CNVs and somatic mutations, and a mutation network is constructed from this mutation matrix. The candidate modulators are selected from the significant genes by clustering the vertex of the mutation network. At last, regression tree model is utilized for module networks learning in which the achieved gene modules and candidate modulators are trained for the driving pattern identification and modulator regulatory exploring. Many of the candidate modulators identified are known to be involved in biological meaningful processes associated with ovarian cancer, which can be regard as potential driver genes, such as CCL11, CCL16, CCL18, CCL23, CCL8, CCL5, APOB, BRCA1, SLC18A1, FGF22, GADD45B, GNA15, GNA11 and so on, which can help to facilitate the discovery of biomarkers, molecular diagnostics, and drug discovery."}, {"title": "Causal Modeling Dissects Tumour\u2013Microenvironment Interactions In Breast Cancer", "url": "https://www.biorxiv.org/content/early/2017/06/01/144832", "tag": "Bioinformatics", "abstract": "Elucidating interactions between cancer cells and their microenvironment is a key goal of cancer research with implications for understanding cancer evolution and improving immunotherapy. Previous studies used association-based approaches to infer relationships in transcriptomic data, but could not infer the direction of interaction. Here we present a causal modeling approach that infers directed interactions between signaling pathway activity and immune activity by anchoring the analysis on somatic genomic changes. Our approach integrates copy number profiles, transcriptomic data, image data and a protein-protein interaction network to infer directed relationships. As a result, we propose 11 novel genomic drivers of T cell phenotypes in the breast cancer tumour microenvironment and validate them in independent cohorts and orthogonal data types. Our framework is flexible and provides a generally applicable way to extend association-based analysis in other cancer types and to other data and clinical parameters."}, {"title": "Reproducible Bioconductor Workflows Using Browser-Based Interactive Notebooks And Containers", "url": "https://www.biorxiv.org/content/early/2017/06/01/144816", "tag": "Bioinformatics", "abstract": "Objective: Bioinformatics publications typically include complex software workflows that are difficult to describe in a manuscript. We describe and demonstrate the use of interactive software notebooks to document and distribute bioinformatics research. We provide a user-friendly tool, BiocImageBuilder, to allow users to easily distribute their bioinformatics protocols through interactive notebooks uploaded to either a GitHub repository or a private server. Materials and methods: We present three different interactive Jupyter notebooks using R and Bioconductor workflows to infer differential gene expression, analyze cross-platform datasets and process RNA-seq data. These interactive notebooks are available on GitHub. The analytical results can be viewed in a browser. Most importantly, the software contents can be executed and modified. This is accomplished using Binder, which runs the notebook inside software containers, thus avoiding the need for installation of any software and ensuring reproducibility. All the notebooks were produced using custom files generated by BiocImageBuilder. Results: BiocImageBuilder facilitates the publication of workflows with a point-and-click user interface. We demonstrate that interactive notebooks can be used to disseminate a wide range of bioinformatics analyses. The use of software containers to mirror the original software environment ensures reproducibility of results. Parameters and code can be dynamically modified, allowing for robust verification of published results and encouraging rapid adoption of new methods. Conclusion: Given the increasing complexity of bioinformatics workflows, we anticipate that these interactive software notebooks will become as ubiquitous and necessary for documenting software methods as traditional laboratory notebooks have been for documenting bench protocols."}, {"title": "Software For The Integration Of Multi-Omics Experiments In Bioconductor", "url": "https://www.biorxiv.org/content/early/2017/06/01/144774", "tag": "Bioinformatics", "abstract": "Multi-omics experiments are increasingly commonplace in biomedical research, and add layers of complexity to experimental design, data integration, and analysis. R and Bioconductor provide a generic framework for statistical analysis and visualization, as well as specialized data classes for a variety of high-throughput data types, but methods are lacking for integrative analysis of multi-omics experiments. The MultiAssayExperiment software package, implemented in R and leveraging Bioconductor software and design principles, provides for the coordinated representation of, storage of, and operation on multiple diverse genomics data. We provide all of the multiple 'omics data for each cancer tissue in The Cancer Genome Atlas (TCGA) as ready-to-analyze MultiAssayExperiment objects, and demonstrate in these and other datasets how the software simplifies data representation, statistical analysis, and visualization. The MultiAssayExperiment Bioconductor package reduces major obstacles to efficient, scalable and reproducible statistical analysis of multi-omics data and enhances data science applications of multiple omics datasets."}, {"title": "Prior Knowledge And Sampling Model Informed Learning With Single Cell RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/05/31/142398", "tag": "Bioinformatics", "abstract": "Single cell RNA-seq (scRNA-seq) experiments can provide a wealth of information about heterogeneous, multi-cellular systems. However, this information has to be inferred computationally from sequencing reads which constitute a sparse and noisy sub-sampling of the actual cellular transcriptomes. Here we present UNCURL (https://github.com/mukhes3/UNCURL_release), a unified framework for scRNA-seq data visualization, cell type identification and lineage estimation that explicitly accounts for the sequencing process. The main algorithmic novelty is a non-negative matrix factorization method that uses knowledge of the distribution resulting from the sequencing process to more accurately model the underlying cell state matrix. We also develop a systematic way for incorporating prior biological information such as bulk RNA expression profiles into the cell state matrix. We find that UNCURL dramatically improves performance over state-of-the-art methods both in the absence and presence of prior knowledge. Finally we demonstrate that using UNCURL as a data preprocessing tool significantly improves the performance of existing scRNA-seq analysis algorithms."}, {"title": "SCENIC: Single-Cell Regulatory Network Inference And Clustering", "url": "https://www.biorxiv.org/content/early/2017/05/31/144501", "tag": "Bioinformatics", "abstract": "Single-cell RNA-seq allows building cell atlases of any given tissue and infer the dynamics of cellular state transitions during developmental or disease trajectories. Both the maintenance and transitions of cell states are encoded by regulatory programs in the genome sequence. However, this regulatory code has not yet been exploited to guide the identification of cellular states from single-cell RNA-seq data. Here we describe a computational resource, called SCENIC (Single Cell rEgulatory Network Inference and Clustering), for the simultaneous reconstruction of gene regulatory networks (GRNs) and the identification of stable cell states, using single-cell RNA-seq data. SCENIC outperforms existing approaches at the level of cell clustering and transcription factor identification. Importantly, we show that cell state identification based on GRNs is robust towards batch-effects and technical-biases. We applied SCENIC to a compendium of single-cell data from the mouse and human brain and demonstrate that the proper combinations of transcription factors, target genes, enhancers, and cell types can be identified. Moreover, we used SCENIC to map the cell state landscape in melanoma and identified a gene regulatory network underlying a proliferative melanoma state driven by MITF and STAT and a contrasting network controlling an invasive state governed by NFATC2 and NFIB. We further validated these predictions by showing that two transcription factors are predominantly expressed in early metastatic sentinel lymph nodes. In summary, SCENIC is the first method to analyze scRNA-seq data using a network-centric, rather than cell-centric approach. SCENIC is generic, easy to use, and flexible, and allows for the simultaneous tracing of genomic regulatory programs and the mapping of cellular identities emerging from these programs. Availability: SCENIC is available as an R workflow based on three new R/Bioconductor packages: GENIE3, RcisTarget and AUCell. As scalable alternative to GENIE3, we also provide GRNboost, paving the way towards the network analysis across millions of single cells."}, {"title": "Compositional Canonical Correlation Analysis", "url": "https://www.biorxiv.org/content/early/2017/05/31/144584", "tag": "Bioinformatics", "abstract": "The study of the relationships between two compositions by means of canonical correlation analysis is addressed. A compositional version of canonical correlation analysis is developed, and called CODA-CCO. We consider two approaches, using the centred log-ratio transformation and the calculation of all possible pairwise log-ratios within sets. The relationships between both approaches are pointed out, and their merits are discussed. The related covariance matrices are structurally singular, and this is efficiently dealt with by using generalized inverses. We develop compositional canonical biplots and detail their properties. The canonical biplots are shown to be powerful tools for discovering the most salient relationships between two compositions. Some guidelines for compositional canonical biplots construction are discussed. A geological data set with X-ray fluorescence spectrometry measurements on major oxides and trace elements is used to illustrate the proposed method. The relationships between an analysis based on centred log-ratios and on isometric log-ratios are also shown."}, {"title": "ANNOgesic: A Pipeline To Translate Bacterial/Archaeal RNA-Seq Data Into High-Resolution Genome Annotations", "url": "https://www.biorxiv.org/content/early/2017/05/29/143081", "tag": "Bioinformatics", "abstract": "To understand the gene regulation of an organism of interest, a comprehensive genome annotation is essential. While some features, such as coding sequences, can be computationally predicted with high accuracy based purely on the genomic sequence, others, such as promoter elements or non-coding RNAs are harder to detect. RNA-Seq has proven to be an efficient method to identify these genomic features and to improve genome annotations. However, processing and integrating RNA-Seq data in order to generate high-resolution annotations is challenging, time consuming and requires numerous different steps. We have constructed a powerful and modular pipeline called ANNOgesic that provides the required analyses and simplifies RNA-Seq-based bacterial and archaeal genome annotation. It predicts and annotates numerous features, including small non-coding RNAs, with high precision. The software is available under an open source license (ISCL) at https://pythonhosted.org/ANNOgesic/"}, {"title": "dms2dfe: Comprehensive Workflow for Analysis of Deep Mutational Scanning Data", "url": "https://www.biorxiv.org/content/early/2017/05/28/072645", "tag": "Bioinformatics", "abstract": "High throughput genotype to phenotype (G2P) data is increasingly being generated by widely applicable Deep Mutational Scanning (DMS) method. dms2dfe is a comprehensive end-to-end workflow that addresses critical issue with noise reduction and offers variety of crucial downstream analyses. Noise reduction is carried out by normalizing counts of mutants by depth of sequencing and subsequent dispersion shrinkage at the level of calculation of preferential enrichments. In downstream analyses, dms2dfe workflow provides identification of relative selection pressures, potential molecular constraints and generation of data-rich visualizations. Availability: dms2dfe is implemented as a python package and it is available at https://kc-lab.github.io/dms2dfe ."}, {"title": "Dr.seq2: A Quality Control And Analysis Pipeline For Parallel Single Cell Transcriptome And Epigenome Data", "url": "https://www.biorxiv.org/content/early/2017/05/28/143271", "tag": "Bioinformatics", "abstract": "An increasing number of single cell transcriptome and epigenome technologies, including single cell ATAC-seq (scATAC-seq), have been recently developed as powerful tools to analyze the features of many individual cells simultaneously. However, the methods and software were designed for one certain data type and only for single cell transcriptome data. A systematic approach for epigenome data and multiple types of transcriptome data is needed to control data quality and to perform cell-to-cell heterogeneity analysis on these ultra-high-dimensional transcriptome and epigenome datasets. Here we developed Dr.seq2, a Quality Control (QC) and analysis pipeline for multiple types of single cell transcriptome and epigenome data, including scATAC-seq and Drop-ChIP data. Application of this pipeline provides four groups of QC measurements and different analyses, including cell heterogeneity analysis. Dr.seq2 produced reliable results on published single cell transcriptome and epigenome datasets. Overall, Dr.seq2 is a systematic and comprehensive QC and analysis pipeline designed for parallel single cell transcriptome and epigenome data. Dr.seq2 is freely available at: http://www.tongji.edu.cn/~zhanglab/drseq2/ and https://github.com/ChengchenZhao/DrSeq2."}, {"title": "CAM: A Quality Control Pipeline For MNase-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/05/28/143263", "tag": "Bioinformatics", "abstract": "Nucleosome organization affects the accessibility of cis-elements to trans-acting factors. Micrococcal nuclease digestion followed by high-throughput sequencing (MNase-seq) is the most popular technology used to profile nucleosome organization on a genome-wide scale. Evaluating the data quality of MNase-seq data remains challenging, especially in mammalian. There is a strong need for a convenient and comprehensive approach to obtain dedicated quality control (QC) for MNase-seq data analysis. Here we developed CAM, which is a comprehensive QC pipeline for MNase-seq data. The CAM pipeline provides multiple informative QC measurements and nucleosome organization profiles on different potentially functional regions for given MNase-seq data. CAM also includes 268 historical MNase-seq datasets from human and mouse as a reference atlas for unbiased assessment. CAM is freely available at: http://www.tongji.edu.cn/~zhanglab/CAM"}, {"title": "Bias, Robustness And Scalability In Differential Expression Analysis Of Single-Cell RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/05/28/143289", "tag": "Bioinformatics", "abstract": "Background: As single-cell RNA-seq (scRNA-seq) is becoming increasingly common, the amount of publicly available data grows rapidly, generating a useful resource for computational method development and extension of published results. Although processed data matrices are typically made available in public repositories, the procedure to obtain these varies widely between data sets, which may complicate reuse and cross-data set comparison. Moreover, while many statistical methods for performing differential expression analysis of scRNA-seq data are becoming available, their relative merits and the performance compared to methods developed for bulk RNA-seq data are not sufficiently well understood. Results: We present conquer, a collection of consistently processed, analysis-ready public single-cell RNA-seq data sets. Each data set has count and transcripts per million (TPM) estimates for genes and transcripts, as well as quality control and exploratory analysis reports. We use a subset of the data sets available in conquer to perform an extensive evaluation of the performance and characteristics of statistical methods for differential gene expression analysis, evaluating a total of 30 statistical approaches on both experimental and simulated scRNA-seq data. Conclusions: Considerable differences are found between the methods in terms of the number and characteristics of the genes that are called differentially expressed. Pre-filtering of lowly expressed genes can have important effects on the results, particularly for some of the methods originally developed for analysis of bulk RNA-seq data. Generally, however, methods developed for bulk RNA-seq analysis do not perform notably worse than those developed specifically for scRNA-seq."}, {"title": "Opportunities And Obstacles For Deep Learning In Biology And Medicine", "url": "https://www.biorxiv.org/content/early/2017/05/28/142760", "tag": "Bioinformatics", "abstract": "Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine."}, {"title": "Reverse-Engineering Biological Networks From Large Data Sets", "url": "https://www.biorxiv.org/content/early/2017/05/28/142034", "tag": "Bioinformatics", "abstract": "Much of contemporary systems biology owes its success to the abstraction of a network, the idea that diverse kinds of molecular, cellular, and organismal species and interactions can be modeled as relational nodes and edges in a graph of dependencies. Since the advent of high-throughput data acquisition technologies in fields such as genomics, metabolomics, and neuroscience, the automated inference and reconstruction of such interaction networks directly from large sets of activation data, commonly known as reverse-engineering, has become a routine procedure. Whereas early attempts at network reverse-engineering focused predominantly on producing maps of system architectures with minimal predictive modeling, reconstructions now play instrumental roles in answering questions about the statistics and dynamics of the underlying systems they represent. Many of these predictions have clinical relevance, suggesting novel paradigms for drug discovery and disease treatment. While other reviews focus predominantly on the details and effectiveness of individual network inference algorithms, here we examine the emerging field as a whole. We first summarize several key application areas in which inferred networks have made successful predictions. We then outline the two major classes of reverse-engineering methodologies, emphasizing that the type of prediction that one aims to make dictates the algorithms one should employ. We conclude by discussing whether recent breakthroughs justify the computational costs of large-scale reverse-engineering sufficiently to admit it as a mainstay in the quantitative analysis of living systems."}, {"title": "OMSV enables accurate and comprehensive identification of large structural variations from nanochannel-based single-molecule optical maps", "url": "https://www.biorxiv.org/content/early/2017/05/27/143040", "tag": "Bioinformatics", "abstract": "Human genomes contain structural variations (SVs) that are associated with various phenotypic variations and diseases. SV detection by sequencing is incomplete due to limited read length. Nanochannel-based optical mapping (OM) allows direct observation of SVs up to hundreds of kilobases in size on individual DNA molecules, making it a promising alternative technology for identifying large SVs. SV detection from optical maps is non-trivial due to complex types of error present in OM data, and no existing methods can simultaneously handle all these complex errors and the wide spectrum of SV types. Here we present a novel method, OMSV, for accurate and comprehensive identification of SVs from optical maps. OMSV detects both homozygous and heterozygous SVs, SVs of various types and sizes, and SVs with and without creating/destroying restriction sites. In an extensive series of tests based on real and simulated data, OMSV achieved both high sensitivity and specificity, with clear performance gains over the latest existing method. Applying OMSV to a human cell line, we identified hundreds of SVs >2kbp, with 65% of them missed by sequencing-based callers. Independent experimental validations confirmed the high accuracy of these SVs. We also demonstrate how OMSV can incorporate sequencing data to determine precise SV break points and novel sequences in the SVs not contained in the reference. We provide OMSV as open-source software to facilitate systematic studies of large SVs."}, {"title": "An Experiment in Learning the Language of Sequence Motifs: Sequence Logos vs. Finite-State Machines", "url": "https://www.biorxiv.org/content/early/2017/05/27/143024", "tag": "Bioinformatics", "abstract": "Position weight matrices (PWMs) are the standard way to model binding site affinities in bioinformatics. However, they assume that symbol occurrences are position independent and, hence, they do not take into account symbols co-occurrence at different sequence positions. To address this problem, we propose to construct finite-state machines (FSMs) instead. A modified version of the Evidence-Driven State Merging (EDSM) heuristic is used to reduce the number of states as FSMs grow too quickly as a function of the number of sequences to reveal any useful structure. We tested our approach on sequence data for the transcription factor HNF4 and found out that the constructed FSMs provide small representations and an intuitive visualization. Furthermore, the FSM was better than PWMs at discriminating the positive and negative sequences in our data set."}, {"title": "Vaxrank: A Computational Tool For Designing Personalized Cancer Vaccines", "url": "https://www.biorxiv.org/content/early/2017/05/27/142919", "tag": "Bioinformatics", "abstract": "Therapeutic vaccines targeting mutant tumor antigens (\u201cneoantigens\u201d) are an increasingly popular form of personalized cancer immunotherapy. Vaxrank is a computational tool for selecting neoantigen vaccine peptides from tumor mutations, tumor RNA data, and patient HLA type. Vaxrank is freely available at www.github.com/hammerlab/vaxrank under the Apache 2.0 open source license and can also be installed from the Python Package Index."}, {"title": "Large-Scale Evolutionary Patterns of Protein Domain Distributions in Eukaryotes", "url": "https://www.biorxiv.org/content/early/2017/05/27/142182", "tag": "Bioinformatics", "abstract": "The genomic inventory of protein domains is an important indicator of an organism regulatory and metabolic capabilities. Existing gene annotations, however, can be plagued by substantial ascertainment biases that make it difficult to obtain and compare quantitative domain data. We find that quantitative trends across the Eukarya can be investigated based on a combination of gene prediction and standard domain annotation pipelines. Species-specific training is required, however, to account for the genomic peculiarities in many lineages. In contrast to earlier studies we find wide-spread statistically significant avoidance of protein domains associated with distinct functional high-level gene-ontology terms."}, {"title": "A Novel Post Hoc Method For Detecting Index Switching Finds No Evidence For Increased Switching On The Illumina HiSeq X", "url": "https://www.biorxiv.org/content/early/2017/05/25/142356", "tag": "Bioinformatics", "abstract": "High throughput sequencing using the Illumina HiSeq platform is a pervasive and critical molecular ecology resource, and has provided the data underlying many recent advances. A recent study has suggested that \u2018index switching\u2019, where reads are misattributed to the wrong sample, may be higher in new versions of the HiSeq platform. This has the potential to invalidate both published and in-progress work across the field. Here, we test for evidence of index switching in an exemplar whole genome shotgun dataset sequenced on both the Illumina HiSeq 2500, which should not have the problem, and the Illumina HiSeq X, which may. We leverage unbalanced heterozygotes, which may be produced by index switching, and ask whether the under-sequenced allele is more likely to be found in other samples in the same lane than expected based on the allele frequency. Although we validate the sensitivity of this method using simulations, we find that neither the HiSeq 2500 nor the HiSeq X have evidence of index switching. This suggests that, thankfully, index switching may not be a ubiquitous problem in HiSeq X sequence data. Lastly, we provide scripts for applying our method so that index switching can be tested for in other datasets."}, {"title": "Serial Crystallography with Multi-stage Merging of 1000's of Images", "url": "https://www.biorxiv.org/content/early/2017/05/25/141770", "tag": "Bioinformatics", "abstract": "KAMO and Blend provide particularly effective tools to automatically manage the merging of large numbers of data sets from serial crystallography. The requirement for manual intervention in the process can be reduced by extending Blend to support additional clustering options to increase the sensitivity to differences in unit cell parameters and to allow for clustering of nearly complete datasets on the basis of intensity or amplitude differences. If datasets are already sufficiently complete to permit it, apply KAMO once, just for reflections. If starting from incomplete datasets, one applies KAMO twice, first using cell parameters. In this step either the simple cell vector distance of the original Blend is used, or the more sensitive NCDist, to find clusters to merge to achieve sufficient completeness to allow intensities or amplitudes to be compared. One then uses KAMO again using the correlation between the reflections at the common HKLs to merge clusters in a way sensitive to structural differences that may not perturb the cell parameters sufficiently to make meaningful clusters. Many groups have developed effective clustering algorithms that use a measurable physical parameter from each diffraction still or wedge to cluster the data into categories which can then be merged to, hopefully, yield the electron density from a single protein iso-form. What is striking about many of these physical parameters is that they are largely independent from one another. Consequently, it should be possible to greatly improve the efficacy of data clustering software by using a multi-stage partitioning strategy. Here, we have demonstrated one possible approach to multi-stage data clustering. Our strategy was to use unit-cell clustering until merged data was of sufficient completeness to then use intensity based clustering. We have demonstrated that, using this strategy, we were able to accurately cluster data sets from crystals that had subtle differences."}, {"title": "Pheno4J: A Gene To Phenotype Graph Database", "url": "https://www.biorxiv.org/content/early/2017/05/25/142257", "tag": "Bioinformatics", "abstract": "Efficient storage and querying of large amounts of genetic and phenotypic data is crucial to contemporary clinical genetic research. This introduces computational challenges for classical relational databases, due to the sparsity and sheer volume of the data. Our Java based solution loads annotated genetic variants and well phenotyped patients into a graph database to allow fast efficient storage and querying of large volumes of structured genetic and phenotypic data. This abstracts technical problems away and lets researchers focus on the science rather than the implementation. We have also developed an accompanying webserver with end-points to facilitate querying of the database. Availability and Implementation: The Java code and python code is available at https://github.com/phenopolis/pheno4j"}, {"title": "Modelling dropouts for feature selection in scRNASeq experiments", "url": "https://www.biorxiv.org/content/early/2017/05/25/065094", "tag": "Bioinformatics", "abstract": "A key challenge of single-cell RNASeq (scRNASeq) is the many genes with zero reads in some cells, but high expression in others. In full-transcript datasets modelling zeros using the Michaelis-Menten equation provides an equal or superior fit to existing scRNASeq datasets compared to other approaches and enables fast and accurate identification of features corresponding to differentially expressed genes without prior identification of cell subpopulations. For datasets tagged with unique molecular identifiers we introduce a depth adjusted negative binomial (DANB) to perform dropout-rate based feature selection. Applying our method to mouse preimplantation embryos revealed clusters corresponding to the inner cell mass and trophectoderm of the blastocyst. Our feature selection method overcomes batch effects to cluster cells from five different datasets by developmental stage rather than experimental origin."}, {"title": "Linclust: clustering billions of protein sequences per day on a single server", "url": "https://www.biorxiv.org/content/early/2017/05/25/104034", "tag": "Bioinformatics", "abstract": "Metagenomic datasets contain billions of protein sequences that could greatly enhance large-scale functional annotation and structure prediction. But clustering them with current algorithms is impractical because runtimes depend almost quadratically on input set size. Linclust's linear scaling overcomes this limitation, enabling us to cluster and assemble 1.6 billion sequence fragments from ~2200 metagenomic datasets in (10+30) hours on 28 cores into 711 million sequences. (Open-source software and Metaclust database: https://mmseqs.org/)."}, {"title": "StereoGene: Rapid Estimation of Genomewide Correlation of Continuous or Interval Feature Data", "url": "https://www.biorxiv.org/content/early/2017/05/25/059584", "tag": "Bioinformatics", "abstract": "Motivation: Genomics features with similar genomewide distributions are generally hypothesized to be functionally related, for example, colocalization of histones and transcription start sites indicate chromatin regulation of transcription factor activity. Therefore, statistical algorithms to perform spatial, genomewide correlation among genomic features are required. Results: Here, we propose a method, StereoGene, that rapidly estimates genomewide correlation among pairs of genomic features. These features may represent high throughput data mapped to reference genome or sets of genomic annotations in that reference genome. StereoGene enables correlation of continuous data directly, avoiding the data binarization and subsequent data loss. Correlations are computed among neighboring genomic positions using kernel correlation. Representing the correlation as a function of the genome position, StereoGene outputs the local correlation track as part of the analysis. StereoGene also accounts for confounders such as input DNA by partial correlation. We apply our method to numerous comparisons of ChIP-Seq datasets from the Human Epigenome Atlas and FANTOM CAGE to demonstrate its wide applicability. We observe the changes in the correlation between epigenomic features across developmental trajectories of several tissue types consistent with known biology, and find a novel spatial correlation of CAGE clusters with donor splice sites and with poly(A) sites. These analyses provide examples for the broad applicability of StereoGene for regulatory genomics. Availability: The StereoGene C++ source code, program documentation, Galaxy integration scripts and examples are available from the project homepage http://stereogene.bioinf.fbb.msu.ru/ Contact: favorov@sensi.org Supplementary information: Supplementary data are available online."}, {"title": "The Structure Of Small Beta Barrels", "url": "https://www.biorxiv.org/content/early/2017/05/24/140376", "tag": "Bioinformatics", "abstract": "The small beta barrel is a protein structural domain, highly conserved throughout evolution and hence exhibits a broad diversity of functions. Here we undertake a comprehensive review of the structural features of this domain. We begin with what characterizes the structure and the variable nomenclature that has been used to describe it. We then go on to explore the anatomy of the structure and how functional diversity is achieved, including through establishing a variety of multimeric states, which, if misformed, contribute to disease states. We conclude with work following from such a comprehensive structural study."}, {"title": "Matriarchy And Prehistory: A Statistical Method For Testing An Old Theory", "url": "https://www.biorxiv.org/content/early/2017/05/24/141374", "tag": "Bioinformatics", "abstract": "Mythological data and statistical methods have been used to reconstruct the phylogeny of matriarchal tales and its relationship to genetic trees. The results show a correlation between the two trees and allow to identify the proto-forms of this mythology."}, {"title": "Cancer Progression Models And Fitness Landscapes: A Many-To-Many Relationship", "url": "https://www.biorxiv.org/content/early/2017/05/24/141465", "tag": "Bioinformatics", "abstract": "The identification of constraints, due to gene interactions, in the order of accumulation of mutations during cancer progression can allow us to single out therapeutic targets. Cancer progression models (CPMs) use genotype frequency data from cross-sectional samples to try to identify these constraints, and return Directed Acyclic Graphs (DAGs) of genes. On the other hand, fitness landscapes, which map genotypes to fitness, contain all possible paths of tumor progression. Thus, we expect a correspondence between DAGs from CPMs and the fitness landscape where evolution happened. But many fitness landscapes \u2014 e.g., those with reciprocal sign epistasis \u2014 cannot be represented by CPMs. Using simulated data under 500 fitness landscapes, I show that CPMs' performance (prediction of genotypes that can exist) degrades with reciprocal sign epistasis. There is large variability in the DAGs inferred from each landscape, which is also affected by mutation rate, detection regime, and fitness landscape features, in ways that depend on CPM method. And the same DAG is often observed in very different landscapes, which differ in more than 50% of their accessible genotypes. Using a pancreatic data set, I show that this many-to-many relationship affects the analysis of empirical data. Fitness landscapes that are widely different from each other can, when evolutionary processes run repeatedly on them, both produce data similar to the empirically observed one, and lead to DAGs that are very different among themselves. Because reciprocal sign epistasis can be common in cancer, these results question the use and interpretation of CPMs."}, {"title": "Building Applications For Interactive Data Exploration In Systems Biology", "url": "https://www.biorxiv.org/content/early/2017/05/24/141630", "tag": "Bioinformatics", "abstract": "As the systems biology community generates and collects data at an unprecedented rate, there is a growing need for interactive data exploration tools to explore the datasets. These tools need to combine advanced statistical analyses, relevant knowledge from biological databases, and interactive visualizations in an application with clear user interfaces. To answer specific research questions tools must provide specialized user interfaces and visualizations. While these are application-specific, the underlying components of a data analysis tool can be shared and reused later. Application developers can therefore compose applications of reusable services rather than implementing a single monolithic application from the ground up for each project. Our approach for developing data exploration applications in systems biology builds on the microservice architecture. Microservice architectures separates an application into smaller components that communicate using language-agnostic protocols. We show that this design is suitable in bioinformatics applications where applications often use different tools, written in different languages, by different research groups. Packaging each service in a software container enables re-use and sharing of key components between applications, reducing development, deployment, and maintenance time. We demonstrate the viability of our approach through a web application, MIxT blood-tumor, for exploring and comparing transcriptional profiles from blood and tumor samples in breast cancer patients. The application integrates advanced statistical software, up-to-date information from biological databases, and modern data visualization libraries. The web application for exploring transcriptional profiles, MIxT, is online at mixt-blood-tumor.bci.mcgill.ca and open-sourced at github.com/fjukstad/mixt. Packages to build the supporting microservices are open-sourced as a part of Kvik at github.com/fjukstad/kvik."}, {"title": "FiberNET: An ensemble deep learning framework for clustering white matter fibers", "url": "https://www.biorxiv.org/content/early/2017/05/24/141036", "tag": "Bioinformatics", "abstract": "White matter tracts are commonly analyzed in studies of micro-structural integrity and anatomical connectivity in the brain. Over the last decade, it has been an open problem as to how best to cluster white matter fibers, extracted from whole-brain tractography, into anatomically meaningful groups. Some existing techniques use region of interest (ROI) based clustering, atlas-based labeling, or unsupervised spectral clustering. ROI-based clustering is popular for analyzing anatomical connectivity among a set of ROIs, but it does not always partition the brain into recognizable fiber bundles. Here we propose an approach using convolutional neural networks (CNNs) to learn shape features of the fiber bundles, which are then exploited to cluster white matter fibers. To achieve such clustering, we first need to re-parameterize the fibers in an intrinsic space. The clustering is performed in induced parameterized coordinates. To our knowledge, this is one of the first approaches for fiber clustering using deep learning techniques. The results show strong accuracy - on a par with or better than other state-of-the-art methods."}, {"title": "Curve Selection For Predicting Breast Cancer Metastasis From Prospective Gene Expression In Blood", "url": "https://www.biorxiv.org/content/early/2017/05/23/141325", "tag": "Bioinformatics", "abstract": "In this article we use gene expression measurements from blood samples to predict breast cancer metastasis. We compare several predictive models and propose a biologically motivated variable selection scheme. Curve selection is based on the assumption that gene expression intensity as a function of time should diverge between cases and controls: there should be a larger difference between case and control closer to diagnosis than years before. We obtain better predictions and more stable predictive signatures by using curve selection and show some evidence that metastasis can be detected in blood samples."}, {"title": "MATAM: Reconstruction Of Phylogenetic Marker Genes From Short Sequencing Reads In Metagenomes", "url": "https://www.biorxiv.org/content/early/2017/05/23/141176", "tag": "Bioinformatics", "abstract": "Advances in the sequencing of uncultured environmental samples, dubbed metagenomics, raise a growing need for accurate taxonomic assignment. Accurate identification of organisms present within a community is essential to understanding even the most elementary ecosystems. However, current high-throughput sequencing technologies generate short reads which partially cover full-length marker genes and this poses difficult bioinformatic challenges for taxonomy identification at high resolution. We designed MATAM, a software dedicated to the fast and accurate targeted assembly of short reads sequenced from a genomic marker of interest. The method implements a stepwise process based on construction and analysis of a read overlap graph. It is applied to the assembly of 16S rRNA markers and is validated on simulated, synthetic and genuine metagenomes. We show that MATAM outperforms other available methods in terms of low error rates and recovered genome fractions and is suitable to provide improved assemblies for precise taxonomic assignments."}, {"title": "UPS-indel: A Universal Positioning System For Indels", "url": "https://www.biorxiv.org/content/early/2017/05/23/133553", "tag": "Bioinformatics", "abstract": "Indels, though differing in allele sequence and position, are biologically equivalent when they lead to the same altered sequences. Storing biologically equivalent indels as distinct entries in databases causes data redundancy, and may mislead downstream analysis and interpretations. About 10% of the human indels stored in dbSNP are redundant. It is thus desirable to have a unified system for identifying and representing equivalent indels in publically available databases. Moreover, a unified system is also desirable to compare the indel calling results produced by different tools. This paper describes UPS-indel, a utility tool that creates a universal positioning system for indels so that equivalent indels can be uniquely determined by their coordinates in the new system, which also can be used to compare indel calling results produced by different tools. UPS-indel identifies nearly 15% indels in dbSNP (version 142) as redundant across all human chromosomes, higher than previously reported. When applied to COSMIC coding and noncoding indel datasets, UPS-indel identifies nearly 29% and 13% indels as redundant, respectively. Comparing the performance of UPS-indel with existing variant normalization tools vt normalize, BCFtools, and GATK LeftAlignAndTrimVariants shows that UPS-indel is able to identify 456,352 more redundant indels in dbSNP; 2,118 more in COSMIC coding, and 553 more in COSMIC noncoding indel dataset in addition to the ones reported jointly by these tools. Moreover, comparing UPS-indel to other state-of-the-art approaches for indel call set comparison demonstrates that UPS-indel is clearly superior to other approaches in finding indels in common among call sets. UPS-indel is theoretically proven to find all equivalent indels, and is thus exhaustive. UPS-indel is written in C++ and the command line version is freely available to download at http://ups-indel.sourceforge.net. The online version of UPS-indel is available at http://bench.cs.vt.edu/ups-indel/."}, {"title": "Overtraining Often Results In Topologically Incorrect Species Trees With Maximum Likelihood Methods", "url": "https://www.biorxiv.org/content/early/2017/05/22/140780", "tag": "Bioinformatics", "abstract": "Background: Overtraining occurs when an optimization process is applied for too many steps, leading to a model describing noise in addition to the signal present in the data. This effect may affect typical approaches for species tree reconstruction that use maximum likelihood optimization procedures on a small sample of concatenated genes. In this context, overtraining may result in trees better describing the specific evolutionary history of the sampled genes rather than the sought evolutionary relationships among the species. Results: Using a cross-validation-like approach on real and simulated datasets we showed that overtraining occurs in a significant fraction of cases, leading to species trees that are more distant from a gold-standard reference tree than a previously considered (and rejected) solution in the optimization process. However, we show that the shape of the likelihood curve is informative of the optimal stopping point. As expected, overtraining is aggravated in smaller gene samples and in datasets with increased levels of topological variation among gene trees, but occurs also in controlled, simulated scenarios where a common underlying topology is enforced. Conclusions: Overtraining is frequent in species tree reconstruction and leads to a final tree that is worse in describing the evolutionary relationships of the species under study than an earlier (and rejected) solution encountered during the likelihood optimization process. This result should help develop specific methods for species tree reconstruction in the future, and may improve our understanding of the complexity of tree likelihood landscapes."}, {"title": "SNaReSim: Synthetic Nanopore Read Simulator", "url": "https://www.biorxiv.org/content/early/2017/05/22/133652", "tag": "Bioinformatics", "abstract": "Nanopores represent the first commercial technology in decades to present a significantly different technique for DNA sequencing, and one of the first technologies to propose direct RNA sequencing. Despite significant differences with previous sequencing technologies, read simulators to date make similar assumptions with respect to error profiles and their analysis. This is a great disservice to both nanopore sequencing and to algorithm developers who seek to optimize their tools to the platform. Previous works have discussed the occurrence of some k-mer bias, but this discussion has been focused on homopolymers, leaving unanswered the question of whether k-mer bias exists over general k-mers, how it occurs, and what can be done to reduce the effects. In this work, we demonstrate that current read simulators fail to accurately represent k-mer error distributions, We explore the sources of k-mer bias in nanopore basecalls, and we present a model for predicting k-mers that are difficult to identify. We also propose a new SNaReSim, a new state-of-the-art simulator, and demonstrate that it provides higher accuracy with respect to 6-mer accuracy biases."}, {"title": "Telomerecat: A Ploidy-Agnostic Method For Estimating Telomere Length From Whole Genome Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/05/19/139972.1", "tag": "Bioinformatics", "abstract": "Telomere length is a risk factor in disease and the dynamics of telomere length are crucial to our understanding of cell replication and vitality. The proliferation of whole genome sequencing represents an unprecedented opportunity to glean new insights into telomere biology on a previously unimaginable scale. To this end, a number of approaches for estimating telomere length from whole-genome sequencing data have been proposed. Here we present Telomerecat, a novel approach to the estimation of telomere length. Previous methods have been dependent on the number of telomeres present in a cell being known, which may be problematic when analysing aneuploid cancer data and non-human samples. Telomerecat is designed to be agnostic to the number of telomeres present, making it suited for the purpose of estimating telomere length in cancer studies. Telomerecat also accounts for interstitial telomeric reads and presents a novel approach to dealing with sequencing errors. We show that Telomerecat performs well at telomere length estimation when compared to leading experimental and computational methods. Furthermore, we show that it detects expected patterns in longitudinal data, technical replicates, and cross-species comparisons. We also apply the method to a cancer cell data, uncovering an interesting relationship with the underlying telomerase genotype."}, {"title": "Interactive Phenotyping Of Large-Scale Histology Imaging Data With HistomicsML", "url": "https://www.biorxiv.org/content/early/2017/05/19/140236", "tag": "Bioinformatics", "abstract": "Whole-slide imaging of histologic sections captures tissue microenvironments and cytologic details in expansive high-resolution images. These images can be mined to extract quantitative features that describe histologic elements, yielding measurements for hundreds of millions of objects. A central challenge in utilizing this data is enabling investigators to train and evaluate classification rules for identifying objects related to processes like angiogenesis or immune response. Here we present HistomicsML, an interactive machine-learning framework for large whole-slide imaging data. HistomicsML uses active learning direct user feedback, making classifier training efficient and scalable in datasets containing 108+ histologic objects. We demonstrate how HistomicsML can be used to phenotype microvascular structures in gliomas to predict survival, and to explore the molecular pathways associated with these phenotypes. Our approach enables researchers to unlock phenotypic information from digital pathology datasets to investigate prognostic image biomarkers and genotype-phenotype associations."}, {"title": "Systematic Analysis Of RNA-Seq-Based Gene Co-Expression Across Multiple Plants", "url": "https://www.biorxiv.org/content/early/2017/05/19/139923", "tag": "Bioinformatics", "abstract": "The complex cellular network was formed by the interacting gene modules. Building the high-quality RNA-seq-based Gene Co-expression Network (GCN) is critical for uncovering these modules and understanding the phenotypes of an organism. Here, we established and analyzed the RNA-seq-based GCNs in two monocot species rice and maize, and two eudicot species Arabidopsis and soybean, and subdivided them into co-expressed modules. Taking rice as an example, we associated these modules with biological functions and agronomic traits by enrichment analysis, and discovered a large number of conditin-specific or tissue-specific modules. In addition, we also explored the regulatory mechanism of the modules by enrichment of the known cis-elements, transcription factors and miRNA targets. Their coherent enrichment with the inferred functions of the modules revealed their synergistic effect on the gene expression regulation. Moreover, the comparative analysis of gene co-expression was performed to identify conserved and species-specific functional modules across 4 plant species. We discovered that the modules shared across 4 plants participate in the basic biological processes, whereas the species-specific modules were involved in the spatiotemporal-specific processes linking the genotypes to phenotypes. Our research provides the massive modules relating to the cellular activities and agronomic traits in several model and crop plant species."}, {"title": "Rigid geometry solves \u201ccurse of dimensionality\u201d effects in clustering methods: An application to omics data", "url": "https://www.biorxiv.org/content/early/2017/05/19/094391", "tag": "Bioinformatics", "abstract": "The quality of samples preserved long term at ultralow temperatures has not been adequately studied. To improve our understanding, we need a strategy to analyze protein degradation and metabolism at subfreezing temperatures. To do this, we obtained liquid chromatography-mass spectrometry (LC/MS) data of calculated protein signal intensities in HEK-293 cells. Our first attempt at directly clustering the values failed, most likely due to the so-called \u201ccurse of dimensionality\u201d. The clusters were not reproducible, and the outputs differed with different methods. By utilizing rigid geometry with a prime ideal I-adic (p-adic) metric, however, we rearranged the sample clusters into a meaningful and reproducible order, and the results were the same with each of the different clustering methods tested. Furthermore, we have also succeeded in application of this method to expression array data in similar situations. Thus, we eliminated the \u201ccurse of dimensionality\u201d from the data set, at least in clustering methods. It is possible that our approach determines a characteristic value of systems that follow a Boltzmann distribution."}, {"title": "STAR Chimeric Post For Rapid Detection Of Circular RNA And Fusion Transcripts", "url": "https://www.biorxiv.org/content/early/2017/05/18/139808", "tag": "Bioinformatics", "abstract": "Motivation: The biological relevance of chimeric RNA alignments is now well established. Chimera arising as chromosomal fusions are often drivers of cancer, and recently discovered circular RNA are only now being characterized. While software already exists for fusion discovery and quantitation, high false positive rates and high run-times hamper scalable fusion discovery on large datasets. Furthermore, very little software is available for circular RNA detection and quantification. Results: Here we present STAR Chimeric Post (STARChip), a novel software package that processes chimeric alignments from the STAR aligner and produces annotated circular RNA and high precision fusions in a rapid, efficient, and scalable manner that is appropriate for high dimensional medical omics datasets. Availability and Implementation: STARChip is available at https://github.com/LosicLab/STARChip Supplementary Information: Supplementary figures and tables are available at Bioinformatics online."}, {"title": "Evaluation Of Classifier Performance For Multiclass Phenotype Discrimination In Untargeted Metabolomics", "url": "https://www.biorxiv.org/content/early/2017/05/18/139584", "tag": "Bioinformatics", "abstract": "Statistical classification is a critical component of utilizing metabolomics data for examining the molecular determinants of phenotypes and for furnishing diagnostic and prognostic phenotype predictions in medicine. Despite this, a comprehensive and rigorous evaluation of classification techniques for phenotype discrimination given metabolomics data has not been conducted. We conducted such an evaluation using both simulated and real metabolomics data, comparing Partial Least Squares-Discriminant Analysis (PLS-DA), Sparse PLS-DA, Random Forests, Support Vector Machines, and Neural Network classification techniques for discriminating phenotype. We evaluated the techniques on simulated data generated to mimic global untargeted metabolomics data by incorporating realistic block-wise correlation and partial correlation structures for mimicking the correlations and metabolite clustering generated by biological processes. Over the simulation studies, covariance structures, means, and effect sizes were randomly simulated to provide consistent estimates of classifier performance over a wide range of possible scenarios. The presence of non-normal error distributions and the effect of prior-significance filtering (dimension reduction) were evaluated. In each simulation, classifier parameters (such as the number of hidden nodes in a neural network) were tuned by cross-validation to minimize the probability of detecting spurious results due to poorly tuned classifiers. Classifier performance was then evaluated using real clinical metabolomics datasets of varying sample medium, sample size, and experimental design. We report that in the scenarios without a significant presence of non-normal error distributions over metabolite clusters, Neural Network and PLS-DA classifiers performed poorly relative to Sparse PLS-DA (sPLS-DA), Support Vector Machine (SVM), and Random Forest classifiers. When non-normal error distributions were introduced, the performance of PLS-DA classifiers deteriorated further relative to the remaining techniques. Simultaneously, while the relative performance of Neural Network classifiers improved relative to PLS-DA classifiers, Neural Network classifier performance remained poor compared sPLS-DA, SVM, and Random Forest classifiers. Over the real datasets, a trend of better performance of SVM and Random Forest classifier performance was observed."}, {"title": "All Fingers Are Not The Same: Handling Variable-Length Sequences In A Discriminative Setting Using Conformal Multi-Instance Kernels", "url": "https://www.biorxiv.org/content/early/2017/05/18/139618", "tag": "Bioinformatics", "abstract": "Most string kernels for comparison of genomic sequences are generally tied to using (absolute) positional information of the features in the individual sequences. This poses limitations when comparing variable-length sequences using such string kernels. For example, profiling chromatin interactions by 3C-based experiments results in variable-length genomic sequences (restriction fragments). Here, exact position-wise occurrence of signals in sequences may not be as important as in the scenario of analysis of the promoter sequences, that typically have a transcription start site as reference. Existing position-aware string kernels have been shown to be useful for the latter scenario. In this work, we propose a novel approach for sequence comparison that enables larger positional freedom than most of the existing approaches, can identify a possibly dispersed set of features in comparing variable-length sequences, and can handle both the aforementioned scenarios. Our approach, CoMIK, identifies not just the features useful towards classification but also their locations in the variable-length sequences, as evidenced by the results of three binary classification experiments, aided by recently introduced visualization techniques. Furthermore, we show that we are able to efficiently retrieve and interpret the weight vector for the complex setting of multiple multi-instance kernels."}, {"title": "Enhanced Guide-RNA Design And Targeting Analysis For Precise CRISPR Genome Editing Of Single And Consortia Of Industrially Relevant And Non-Model Organisms", "url": "https://www.biorxiv.org/content/early/2017/05/18/139626", "tag": "Bioinformatics", "abstract": "Motivation: Genetic diversity of non-model organisms offers a repertoire of unique phenotypic features for exploration and cultivation for synthetic biology and metabolic engineering applications. To realize this enormous potential, it is critical to have an efficient genome editing tool for rapid strain engineering of these organisms to perform novel programmed functions. Results: To accommodate the use of CRISPR/Cas systems for genome editing across organisms, we have developed a novel method, named CASPER (CRISPR Associated Software for Pathway Engineering and Research), for identifying on- and off-targets with enhanced predictability coupled with an analysis of non-unique (repeated) targets to assist in editing any organism with various endonucleases. Utilizing CASPER, we demonstrated a modest 2.4% and significant 30.2% improvement (F-test, p<0.05) over the conventional methods for predicting on- and off-target activities, respectively. Further we used CASPER to develop novel applications in genome editing: multitargeting analysis (i.e. simultaneous multiple-site modification on a target genome with a sole guide-RNA (gRNA) requirement) and multispecies population analysis (i.e. gRNA design for genome editing across a consortium of organisms). Our analysis on a selection of industrially relevant organisms revealed a number of non-unique target sites associated with genes and transposable elements that can be used as potential sites for multitargeting. The analysis also identified shared and unshared targets that enable genome editing of single or multiple genomes in a consortium of interest. We envision CASPER as a useful platform to enhance the precise CRISPR genome editing for metabolic engineering and synthetic biology applications."}, {"title": "Extending Chemical Perturbations Of The Ubiquitin Fitness Landscape In A Classroom Setting", "url": "https://www.biorxiv.org/content/early/2017/05/17/139352", "tag": "Bioinformatics", "abstract": "Although the primary protein sequence of ubiquitin (Ub) is extremely stable over evolutionary time, it is highly tolerant to mutation during selection experiments performed in the laboratory. We have proposed that this discrepancy results from the difference between fitness under laboratory culture conditions and the selective pressures in changing environments over evolutionary time scales. Building on our previous work (Mavor et al. 2016), we used deep mutational scanning to determine how twelve new chemicals reveal novel mutational sensitivities of ubiquitin residues. We found sensitization of Lys63 in eight new conditions. In total, our experiments have uncovered a highly sensitizing condition for every position in Ub except Ser57 and Gln62. By determining the Ubiquitin fitness landscape under different chemical constraints, our work helps to resolve the inconsistencies between deep mutational scanning experiments and sequence conservation over evolutionary timescales."}, {"title": "GeoDiver: Differential Gene Expression Analysis & Gene-Set Analysis For GEO Datasets", "url": "https://www.biorxiv.org/content/early/2017/05/17/127753", "tag": "Bioinformatics", "abstract": "Summary: GeoDiver is an online web application for performing Differential Gene Expression Analysis (DGEA) and Generally Applicable Gene-set Enrichment Analysis (GAGE) on gene expression datasets from the publicly available Gene Expression Omnibus (GEO). The output produced includes numerous high quality interactive graphics, allowing users to easily explore and examine complex datasets instantly. Furthermore, the results produced can be reviewed at a later date and shared with collaborators. Availability: GeoDiver is freely available online at http://www.geodiver.co.uk . The source code is available on Github: https://github.com/GeoDiver/GeoDiver and a docker image is available for easy installation."}, {"title": "Graph-Guided Assembly For Novel HLA Allele Discovery", "url": "https://www.biorxiv.org/content/early/2017/05/17/138826", "tag": "Bioinformatics", "abstract": "Accurate typing of human leukocyte antigen (HLA), a histocompatibility test, is important because HLA genes play various roles in immune responses, and have also been shown to be associated with many diseases such as cancer. The current gold standard for HLA typing uses DNA sequencing technology combined with sequence enrichment techniques using specially designed primers or probes, causing it to be slow and labor-intensive. Although there exist enrichment-free computational methods that use various types of sequencing data, hyper-polymorphism found in HLA region of the human genome makes it challenging to type HLA genes with high accuracy from whole genome sequencing data. Furthermore, these methods are database-matching approaches where their output is inherently limited by the completeness of already known types, forcing them to find the best matching known alleles from a database, thereby causing them to be unsuitable for discovery of rare or novel alleles. In order to ensure both high accuracy as well as the ability to type novel alleles, we have developed a graph-guided assembly technique for classical HLA genes, which is capable of assembling phased, full-length haplotype sequences of typing exons given high-coverage (>30-fold) whole genome sequencing data. Our method delivers highly accurate HLA typing, comparable to the current state-of-the-art database-matching methods. We also demonstrate that our method can type novel alleles by experimenting on various data including simulated, Illumina Platinum Genomes, and 1000 Genomes data."}, {"title": "Exploiting General Independence Criteria For Network Inference", "url": "https://www.biorxiv.org/content/early/2017/05/17/138669", "tag": "Bioinformatics", "abstract": "Inference of networks representing dependency relationships is a key tool for understanding data derived from biological systems. It has been shown that nonlinear relationships and non-Gaussian noise aid detection of directions of functional dependencies. In this study we explore how far generalised independence criteria for statistical independence proposed in the literature are better suited to the inference of networks compared to standard independence criteria based on linear relationships and Gaussian noise. We compare such criteria within the framework of the PC algorithm, a popular network inference algorithm for directed acyclic dependency graphs. We also propose and evaluate a method to apply unconditional independence criteria to assess conditional independence and a method to simulate data with desired properties from experimental data. Our main finding is that a recently proposed criterion based on distance covariance performs well compared to other independence criteria in terms of error rates, speed of computation, and need of fine-tuning parameters when applied to experimental biological datasets."}, {"title": "Perturbed human sub-networks by Fusobacterium nucleatum candidate virulence proteins", "url": "https://www.biorxiv.org/content/early/2017/05/16/094136", "tag": "Bioinformatics", "abstract": "F. nucleatum is a gram-negative anaerobic species residing in the oral cavity and implicated in several inflammatory processes in the human body. Although F. nucleatum abundance is increased in inflammatory bowel disease subjects and is prevalent in colorectal cancer patients, the causal role of the bacterium in gastrointestinal disorders and the mechanistic details of host cell functions subversion are not fully understood. We devised a computational strategy to identify putative secreted F. nucleatum proteins (FusoSecretome) and to infer their interactions with human proteins based on the presence of host molecular mimicry elements. FusoSecretome proteins share similar features with known bacterial virulence factors thereby highlighting their pathogenic potential. We show that they interact with human proteins that participate in infection-related cellular processes and localize in established cellular districts of the host-pathogen interface. Our network-based analysis identified 31 functional modules in the human interactome preferentially targeted by 138 FusoSecretome proteins, among which we selected 26 as main candidate virulence proteins, representing both putative and known virulence proteins. Finally, 6 of the preferentially targeted functional modules are implicated in the onset and progression of inflammatory bowel diseases and colorectal cancer. Overall, our computational analysis identified candidate virulence proteins potentially involved in the F. nucleatum - human cross-talk in the context of gastrointestinal diseases."}, {"title": "MetaMeta: Integrating Metagenome Analysis Tools To Improve Taxonomic Profiling", "url": "https://www.biorxiv.org/content/early/2017/05/16/138578", "tag": "Bioinformatics", "abstract": "Many metagenome analysis tools are presently available to classify sequences and profile environmental samples. In particular, taxonomic profiling and binning methods are commonly used for such tasks. Tools available among these two categories make use of several techniques, e.g. read mapping, k-mer alignment, and composition analysis. Variations on the construction of the corresponding reference sequence databases are also common. In addition, different tools provide good results in different datasets and configurations. All this variation creates a complicated scenario to researchers to decide which methods to use. Installation, configuration and execution can also be difficult especially when dealing with multiple datasets and tools. We propose MetaMeta: a pipeline to execute and integrate results from metagenome analysis tools. MetaMeta provides an easy workflow to run multiple tools with multiple samples, producing a single enhanced output profile for each sample. MetaMeta includes a database generation, pre-processing, execution, and integration steps, allowing easy execution and parallelization. The integration relies on the co-occurrence of organisms from different methods as the main feature to improve community profiling while accounting for differences in their databases. In a controlled case with simulated and real data we show that the integrated profiles of MetaMeta overcome the best single profile. Using the same input data, it provides more sensitive and reliable results with the presence of each organism being supported by several methods. MetaMeta uses Snakemake and has six pre-configured tools, all available at BioConda channel for easy installation (conda install -c bioconda metameta). The MetaMeta pipeline is open-source and can be downloaded at: https://github.com/pirovc/metameta"}, {"title": "Predicting Protein Producibility In Filamentous Fungi", "url": "https://www.biorxiv.org/content/early/2017/05/16/138560", "tag": "Bioinformatics", "abstract": "In this paper we study the problem of predicting the producibility of recombinant proteins in filamentous fungi, especially T. reesei, using machine learning methods. We train supervised and semi-supervised support vector machines with protein sequences, represented by their amino acid composition as well as protein family and domain information. Our results indicate, somewhat surprisingly, that quite modest amount of proteins with experimental data are required to build a state-of-the-art classifier and that additional unlabeled sequences in semi-supervised models do not bring increased predictive performance. Our experiments in cross-species prediction show that models trained for the filamentous fungus A. niger protein dataset can be generalized to predict protein producibility in T. reesei, and vice versa, without sacrificing too much accuracy, regardless of their approximately 500 millions years of divergence. However, predictors trained on E. coli and S. cerevisiae datasets gave variable performance when applied to the filamentous fungi datasets, indicating that while protein producibility prediction can be generalized across related species, fully generic prediction tools applicable to any protein production host may not be realistic to achieve."}, {"title": "Sim3C: Simulation Of HiC And Meta3C Proximity Ligation Sequencing Technologies", "url": "https://www.biorxiv.org/content/early/2017/05/15/134452", "tag": "Bioinformatics", "abstract": "Background: Chromosome conformation capture (3C) and HiC DNA sequencing methods have rapidly advanced our understanding of the spatial organization of genomes and metagenomes. Many variants of these protocols have been developed, each with their own strengths. Currently there is no systematic means for simulating sequence data from this family of sequencing protocols. Findings: We describe a computational simulator that, given reference genome sequences and some basic parameters, will simulate HiC sequencing on those sequences. The simulator models the basic spatial structure in genomes that is commonly observed in HiC and 3C datasets, including the distance-decay relationship in proximity ligation, differences in the frequency of interaction within and across chromosomes, and the structure imposed by cells. A means to model the 3D structure of topologically associating domains (TADs) is provided. The simulator also models several sources of error common to 3C and HiC library preparation and sequencing methods, including spurious proximity ligation events and sequencing error. Conclusions: We have introduced the first comprehensive simulator for 3C and HiC sequencing protocols. We expect the simulator to have use in testing of HiC data analysis algorithms, as well as more general value for experimental design, where questions such as the required depth of sequencing, enzyme choice, and other decisions must be made in advance in order to ensure adequate statistical power to test the relevant hypotheses."}, {"title": "Rainbowfish: A Succinct Colored de Bruijn Graph Representation", "url": "https://www.biorxiv.org/content/early/2017/05/15/138016", "tag": "Bioinformatics", "abstract": "The colored de Bruijn graph \u2014 a variant of the de Bruijn graph which associates each edge (i.e., k-mer) with some set of colors \u2014 is an increasingly important combinatorial structure in computational biology. Iqbal et al. demonstrated the utility of this structure for representing and assembling a collection (population) of genomes, and showed how it can be used to accurately detect genetic variants. Muggli et al. introduced VARI, a representation of the colored de Bruijn graph that adopts the BOSS representation for the de Bruijn graph topology and achieves considerable savings in space over Cortex, albeit with some sacrifice in speed. The memory-efficient representation of VARI allows the colored de Bruijn graph to be constructed and analyzed for large datasets, beyond what is possible with Cortex. In this paper, we introduce Rainbowfish, a succinct representation of the color information of the colored de Bruijn graph that reduces the space usage even further. Our representation also uses BOSS to represent the de Bruijn graph, but decomposes the color sets based on an equivalence relation and exploits the inherent skewness in the distribution of these color sets. The Rainbowfish representation is compressed based on the 0th-order entropy of the color sets, which can lead to a significant reduction in the space required to store the relevant information for each edge. In practice, Rainbowfish achieves up to a 20x improvement in space over VARI. Rainbowfish is written in C++11 and is available at https://github.com/ COMBINE-lab/rainbowfish."}, {"title": "High-Quality Rice RNA-Seq-Based Co-Expression Network For Predicting Gene Function And Regulation", "url": "https://www.biorxiv.org/content/early/2017/05/15/138040", "tag": "Bioinformatics", "abstract": "Inferring the genome-scale gene co-expression network is important for understanding genetic architecture underlying the complex and various biological phenotypes. The recent availability of large-scale RNA-seq sequencing data provides great potential for co-expression network inference. In this study, for the first time, we presented a novel heterogeneous ensemble pipeline integrating three frequently used inference methods, to build a high-quality RNA-seq-based Gene Co-expression Network (GCN) in rice, an important monocot species. The quality of the network obtained by our proposed method was first evaluated and verified with the curated positive and negative gene functional link datasets, which obviously outperformed each single method. Secondly, the powerful capability of this network for associating unknown genes with biological functions and agronomic traits was showed by enrichment analysis and case studies. Particularly, we demonstrated the potential applications of our proposed method to predict the biological roles of long non-coding RNA (lncRNA) and circular RNA (circRNA) genes. Our results provided a valuable data source for selecting candidate genes to further experimental validation during rice genetics research and breeding. To enhance identification of novel genes regulating important biological processes and agronomic traits in rice and other crop species, we released the source code of constructing high-quality RNA-seq-based GCN and rice RNA-seq-based GCN, which can be freely downloaded online at https://github.com/czllab/NetMiner."}, {"title": "Multiplexing droplet-based single cell RNA-sequencing using natural genetic barcodes", "url": "https://www.biorxiv.org/content/early/2017/05/15/118778", "tag": "Bioinformatics", "abstract": "Droplet-based single-cell RNA-sequencing (dscRNA-seq) has enabled rapid, massively parallel profiling of transcriptomes from tens of thousands of cells. Multiplexing samples for single cell capture and library preparation in dscRNA-seq would enable cost-effective designs of differential expression and genetic studies while avoiding technical batch effects, but its implementation remains challenging. Here, we introduce an in-silico algorithm demuxlet that harnesses natural genetic variation to discover the sample identity of each cell and identify droplets containing two cells. These capabilities enable multiplexed dscRNA-seq experiments where cells from unrelated individuals are pooled and captured at higher throughput than standard workflows. To demonstrate the performance of demuxlet, we sequenced 3 pools of peripheral blood mononuclear cells (PBMCs) from 8 lupus patients. Given genotyping data for each individual, demuxlet correctly recovered the sample identity of > 99% of singlets, and identified doublets at rates consistent with previous estimates. In PBMCs, we demonstrate the utility of multiplexed dscRNA-seq in two applications: characterizing cell type specificity and inter-individual variability of cytokine response from 8 lupus patients and mapping genetic variants associated with cell type specific gene expression from 23 donors. Demuxlet is fast, accurate, scalable and could be extended to other single cell datasets that incorporate natural or synthetic DNA barcodes."}, {"title": "Learning Structural Motif Representations For Efficient Protein Structure Search", "url": "https://www.biorxiv.org/content/early/2017/05/14/137828", "tag": "Bioinformatics", "abstract": "Understanding the relationship between protein structure and function is a fundamental problem in protein science. Given a protein of unknown function, fast identification of similar protein structures from the Protein Data Bank (PDB) is a critical step for inferring its biological function. Such structural neighbors can provide evolutionary insights into protein conformation, interfaces and binding sites that are not detectable from sequence similarity. However, the computational cost of performing pairwise structural alignment against all structures in PDB is prohibitively expensive. Alignment-free approaches have been introduced to enable fast but coarse comparisons by representing each protein as a vector of structure features or fingerprints and only computing similarity between vectors. As a notable example, FragBag represents each protein by a \u201cbag of fragments\u201d, which is a vector of frequencies of contiguous short backbone fragments from a predetermined library. Here we present a new approach to learning effective structural motif presentations using deep learning. We develop DeepFold, a deep convolutional neural network model to extract structural motif features of a protein structure. Similar to FragBag, DeepFold represents each protein structure or fold using a vector of learned structural motif features. We demonstrate that DeepFold substantially outperforms FragBag on protein structural search on a non-redundant protein structure database and a set of newly released structures. Remarkably, DeepFold not only extracts meaningful backbone segments but also finds important long-range interacting motifs for structural comparison. We expect that DeepFold will provide new insights into the evolution and hierarchical organization of protein structural motifs. The source code for generating DeepFold representation can be downloaded at https://github.com/largelymfs/DeepFold."}, {"title": "Deciphering HLA-I motifs across HLA peptidomes improves neo-antigen predictions and identifies allostery regulating HLA specificity", "url": "https://www.biorxiv.org/content/early/2017/05/13/098780", "tag": "Bioinformatics", "abstract": "The precise identification of Human Leukocyte Antigen class I (HLA-I) binding motifs plays a central role in our ability to understand and predict (neo-)antigen presentation in infectious diseases and cancer. Here, by exploiting co-occurrence of HLA-I alleles across ten newly generated as well as forty public HLA peptidomics datasets comprising more than 115,000 unique peptides, we show that we can rapidly and accurately identify many HLA-I binding motifs and map them to their corresponding alleles without any a priori knowledge of HLA-I binding specificity. Our approach recapitulates and refines known motifs for 43 of the most frequent alleles, uncovers new motifs for 9 alleles that up to now had less than five known ligands and provides a scalable framework to incorporate additional HLA peptidomics studies in the future. The refined motifs improve neo-antigen and cancer testis antigen predictions, indicating that unbiased HLA peptidomics data are ideal for in silico predictions of neo-antigens from tumor exome sequencing data. The new motifs further reveal allosteric modulation of the binding specificity of HLA-I alleles and we unravel the underlying mechanisms by protein structure analysis, mutagenesis and in vitro binding assays."}, {"title": "Quantitative Models Identify Histone Signatures Of Poised Genes Prior To Cellular Differentiation", "url": "https://www.biorxiv.org/content/early/2017/05/13/137646", "tag": "Bioinformatics", "abstract": "Background: Recent studies have shown that histone marks are involved in pre-programming gene fates during cellular differentiation. Bivalent domains (marked by both H3K4me3 and H3K27me3) have been proposed to act in the histone pre-patterning of poised genes. However, bivalent genes could also resolve into monovalent silenced states during differentiation. Thus, the histone signatures of poised genes need to be more precisely characterized. Results: Using a support vector machine (SVM), we show that the collective histone modification data from human blood hematopoietic cells (HSCs) could predict poised genes with fairly high predictive accuracy within the model of directed erythrocyte differentiation from HSCs. Surprisingly, models with single histone marks (e.g., H3K4me3 or H2A.Z) could reach comparable predictive powers to the full model built with all of the nine histone marks. We also derived an H2A.Z and H3K9me3-based Naive Bayesian model for inferring poised genes, and the validity of this model was supported by data from several other pluripotent or multipotent cells (including mouse ES cells)."}, {"title": "Integrated Computing And Tracking System For Centralized High-Throughput Genetic Analysis: A Case Study", "url": "https://www.biorxiv.org/content/early/2017/05/13/137596", "tag": "Bioinformatics", "abstract": "The Genetic Analysis Center (GAC) of the Hispanic Community Health Study/Study of Latinos (HCHS/SOL) developed an Integrated Computing and Tracking system (ICT) in order to perform genome-wide and other genetic association studies automatically and efficiently, while documenting all analysis specifications. This system provides easy-to-use analysis set-up and computing procedures, automatic reports, and analysis search functionality due to integration with an on-site database. In this paper we describe the ICT and demonstrate how it satisfies key principles of reproducible research, while respecting constraints and challenges arising from using very large, restricted access, human-subjects data. This case study may benefit other groups that have similar requirements for high-throughput analysis execution and management."}, {"title": "A Simple Representation Of Three-Dimensional Molecular Structure", "url": "https://www.biorxiv.org/content/early/2017/05/13/136705", "tag": "Bioinformatics", "abstract": "Statistical and machine learning approaches predict drug-to-target relationships from 2D small-molecule topology patterns. One might expect 3D information to improve these calculations. Here we apply the logic of the Extended Connectivity FingerPrint (ECFP) to develop a rapid, alignment-invariant 3D representation of molecular conformers, the Extended Three-Dimensional FingerPrint (E3FP). By integrating E3FP with the Similarity Ensemble Approach (SEA), we achieve higher precision-recall performance relative to SEA with ECFP on ChEMBL20, and equivalent receiver operating characteristic performance. We identify classes of molecules for which E3FP is a better predictor of similarity in bioactivity than is ECFP. Finally, we report novel drug-to-target binding predictions inaccessible by 2D fingerprints and confirm three of them experimentally with ligand efficiencies from 0.442 - 0.637 kcal/mol/heavy atom."}, {"title": "Modules Of Co-Occurrence In The Cyanobacterial Pan-Genome", "url": "https://www.biorxiv.org/content/early/2017/05/12/137398", "tag": "Bioinformatics", "abstract": "The increasing availability of fully sequenced cyanobacterial genomes opens unprecedented opportunities to investigate the manifold adaptations and functional relationships that determine the genetic content of individual bacterial species. Here, we use comparative genome analysis to investigate the cyanobacterial pan-genome, based on 77 strains whose complete genome sequence is available. Our focus is the co-occurrence of likely ortholog genes, denoted as CLOGs. We conjecture that co-occurrence of CLOGs is indicative of functional relationships between the respective genes. Going beyond the analysis of pair-wise co-occurrences, we introduce a network approach to identify modules of co-occurring CLOGs. Our results demonstrate that these modules exhibit a high degree of functional coherence and reveal known as well as previously unknown functional relationships. We argue that the high functional coherence observed for the extracted modules is a consequence of the similar-yet-diverse nature of the cyanobacterial phylum. We provide a simple toolbox that facilitates further analysis of our results with respect to specific cyanobacterial genes of interest."}, {"title": "Protein Features Identification For Machine Learning-Based Prediction Of Protein-Protein Interactions", "url": "https://www.biorxiv.org/content/early/2017/05/12/137257", "tag": "Bioinformatics", "abstract": "The long awaited challenge of post-genomic era and systems biology research is computational prediction of protein-protein interactions (PPIs) that ultimately lead to protein functions prediction. The important research questions is how protein complexes with known sequence and structure be used to identify and classify protein binding sites, and how to infer knowledge from these classification such as predicting PPIs of proteins with unknown sequence and structure. Several machine learning techniques have been applied for the prediction of PPIs, but the accuracy of their prediction wholly depends on the number of features being used for training. In this paper, we have performed a survey of protein features used for the prediction of PPIs. The open research challenges and opportunities in the area have also been discussed."}, {"title": "De Novo Annotation And Characterization Of The Translatome With Ribosome Profiling Data", "url": "https://www.biorxiv.org/content/early/2017/05/12/137216", "tag": "Bioinformatics", "abstract": "By capturing and sequencing the RNA fragments protected by translating ribosomes, ribosome profiling sketches the landscape of translation at subcodon resolution. We developed a new method, RiboCode, which uses ribosome profiling data to assess the translation of each RNA transcript genome-wide. As shown by multiple tests with simulated data and cell type-specific QTI-seq and mass spectrometry data, RiboCode exhibits superior efficiency, sensitivity, and accuracy for de novo annotation of the translatome, which covers various types of novel ORFs in the previously annotated coding and non-coding regions and overlapping ORFs. Finally, to showcase its application, we applied RiboCode on a published ribosome profiling dataset and assembled the context-dependent translatomes of yeast under normal condition, heat shock, and oxidative stress. Comparisons among these translatomes revealed stress-activated novel upstream and downstream ORFs, some of which are associated with potential translational dysregulations of the main protein coding ORFs in response to the stress signals."}, {"title": "The Sentieon Genomics Tools - A fast and accurate solution to variant calling from next-generation sequence data", "url": "https://www.biorxiv.org/content/early/2017/05/12/115717", "tag": "Bioinformatics", "abstract": "In the past six years worldwide capacity for human genome sequencing has grown by more than five orders of magnitude, with costs falling by nearly two orders of magnitude over the same period. The rapid expansion in the production of next-generation sequence data and the use of these data in a wide range of new applications has created a need for improved computational tools for data processing. The Sentieon Genomics tools provide an optimized reimplementation of the most accurate pipelines for calling variants from next-generation sequence data, resulting in more than a 10-fold increase in processing speed while providing identical results to best practices pipelines. Here we demonstrate the consistency and improved performance of Sentieon's tools relative to BWA, GATK, MuTect, and MuTect2 through analysis of publically available human exome, low-coverage genome, and high-depth genome sequence data."}, {"title": "TITER: predicting translation initiation sites by deep learning", "url": "https://www.biorxiv.org/content/early/2017/05/12/103374", "tag": "Bioinformatics", "abstract": "Motivation: Translation initiation is a key step in the regulation of gene expression. In addition to the annotated translation initiation sites (TISs), the translation process may also start at multiple alternative TISs (including both AUG and non-AUG codons), which makes it challenging to predict TISs and study the underlying regulatory mechanisms. Meanwhile, the advent of several high-throughput sequencing techniques for profiling initiating ribosomes at single-nucleotide resolution, e.g., GTI-seq and QTI-seq, provides abundant data for systematically studying the general principles of translation initiation and the development of computational method for TIS identification. Methods: We have developed a deep learning based framework, named TITER, for accurately predicting TISs on a genome-wide scale based on QTI-seq data. TITER extracts the sequence features of translation initiation from the surrounding sequence contexts of TISs using a hybrid neural network and further integrates the prior preference of TIS codon composition into a unified prediction framework. Results: Extensive tests demonstrated that TITER can greatly outperform the state-of-the-art prediction methods in identifying TISs. In addition, TITER was able to identify important sequence signatures for individual types of TIS codons, including a Kozak-sequence-like motif for AUG start codon. Furthermore, the TITER prediction score can be related to the strength of translation initiation in various biological scenarios, including the repressive effect of the upstream open reading frames (uORFs) on gene expression and the mutational effects influencing translation initiation efficiency."}, {"title": "Outlier detection for improved differential splicing quantification from RNA-Seq experiments with replicates", "url": "https://www.biorxiv.org/content/early/2017/05/11/104059", "tag": "Bioinformatics", "abstract": "A key component in many RNA-Seq based studies is contrasting multiple replicates from different experimental conditions. In this setup replicates play a key role as they allow to capture underlying biological variability inherent to the compared conditions, as well as experimental variability. However, what constitutes a \u201cbad\u201d replicate is not necessarily well defined. Consequently, researchers might discard valuable data or downstream analysis may be hampered by failed experiments. Here we develop a probability model to weigh a given RNA-Seq sample as a representative of an experimental condition when performing alternative splicing analysis. We demonstrate that this model detects outlier samples which are consistently and significantly different compared to other samples from the same condition. Moreover, we show that instead of discarding such samples the proposed weighting scheme can be used to downweight samples and specific splicing variations suspected as outliers, gaining statistical power. These weights can then be used for differential splicing (DS) analysis, where the resulting algorithm offers a generalization of the MAJIQ algorithm. Using both synthetic and real-life data we perform an extensive evaluation of the improved MAJIQ algorithm in different scenarios involving perturbed samples, mislabeled samples, no-signal groups, and different levels of coverage, showing it compares favorably to other tools. Overall, this work offers an outlier detection algorithm that can be combined with any splicing pipeline, a generalized and improved version of MAJIQ for differential splicing detection, and an evaluation pipeline researchers can use to evaluate which algorithm may work best for their needs."}, {"title": "smallrnaseq: Short Non Coding RNA-Seq Analysis With Python", "url": "https://www.biorxiv.org/content/early/2017/05/10/110585", "tag": "Bioinformatics", "abstract": "The use of next generation sequencing is now a standard approach to elucidate the small non-coding RNA species (sncRNAs) present in tissue and biofluid samples. This has revealed the wide variety of RNAs with regulatory functions the best studied of which are microRNAs. Profiling of sncRNAs by deep sequencing allows measures of absolute abundance and for the discovery of novel species that have eluded previous methods. Specific considerations must be made when quantifying and cataloging sncRNAs and multiple algorithms are now available, mostly focused on miRNA analysis. smallrnaseq is a Python package that implements some of the standard approaches for quantification and analysis of sncRNAs. This includes miRNA quantification and novel miRNA prediction. A command line interface makes the software accessible for general users."}, {"title": "Deep-Learning Based, Automated Segmentation Of Macular Edema In Optical Coherence Tomography", "url": "https://www.biorxiv.org/content/early/2017/05/09/135640", "tag": "Bioinformatics", "abstract": "Evaluation of clinical images is essential for diagnosis in many specialties and the development of computer vision algorithms to analyze biomedical images will be important. In ophthalmology, optical coherence tomography (OCT) is critical for managing retinal conditions. We developed a convolutional neural network (CNN) that detects intraretinal fluid (IRF) on OCT in a manner indistinguishable from clinicians. Using 1,289 OCT images, the CNN segmented images with a 0.911 cross-validated Dice coefficient, compared with segmentations by experts. Additionally, the agreement between experts and between experts and CNN were similar. Our results reveal that CNN can be trained to perform automated segmentations."}, {"title": "Barcode identification for single cell genomics", "url": "https://www.biorxiv.org/content/early/2017/05/09/136242", "tag": "Bioinformatics", "abstract": "Single-cell sequencing experiments use short DNA barcode \"tags\" to identify reads that originate from the same cell. In order to recover single-cell information from such experiments, reads must be grouped based on their barcode tag, a crucial processing step that precedes other computations. However, this step can be difficult due to high rates of mismatch and deletion errors that can afflict barcodes. Here we present an approach to identify and error-correct barcodes by traversing the de Bruijn graph of circularized barcode k-mers. This allows for assignment of reads to consensus fingerprints constructed from k-mers, and we show that for single-cell RNA-Seq this improves the recovery of accurate single-cell transcriptome estimates."}, {"title": "MAJIQ-SPEL: Web-Tool To Interrogate Classical And Complex Splicing Variations From RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/05/09/136077", "tag": "Bioinformatics", "abstract": "Analysis of RNA sequencing (RNA-Seq) data have highlighted the fact that most genes undergo alternative splicing (AS) and that these patterns are tightly regulated. Many of these events are complex, resulting in numerous possible isoforms that quickly become difficult to visualize, interpret, and experimentally validate. To address these challenges, We developed MAJIQ-SPEL, a web-tool that takes as input local splicing variations (LSVs) quantified from RNA-Seq data and provides users with visualization and quantification of gene isoforms associated with those. Importantly, MAJIQ-SPEL is able to handle both classical (binary) and complex (non-binary) splicing variations. Using a matching primer design algorithm it also suggests users possible primers for experimental validation by RT-PCR and displays those, along with the matching protein domains affected by the LSV, on UCSC Genome Browser for further downstream analysis. Program and code will be available at http://majiq.biociphers.org/majiq-spel"}, {"title": "DNA.Land: A Digital Biobank Using A Massive Crowdsourcing Approach", "url": "https://www.biorxiv.org/content/early/2017/05/09/135715", "tag": "Bioinformatics", "abstract": "Precision medicine necessitates large scale collections of genomes and phenomes. Despite decreases in the costs of genomic technologies, collecting these types of information at scale is still a daunting task that poses logistical challenges and requires consortium-scale resources. Here, we describe DNA.Land, a digital biobank to collect genome and phenomes with a fraction of the resources of traditional studies at the same scale. Our approach relies on crowd-sourcing data from the rapidly growing number of individuals that have access to their own genomic datasets through Direct-to-Consumer (DTC) companies. To recruit participants, we developed a series of automatic return-of-results features in DNA.Land that increase users' engagement while stratifying human subject research protection. So far, DNA.Land has collected over 43,000 genomes in 20 months of operation, orders of magnitude higher than previous digital attempts by academic groups. We report lessons learned in running a digital biobank, our technical framework, and our approach regarding ethical, legal, and social implications."}, {"title": "Deconvolving sequence features that discriminate between overlapping regulatory annotations", "url": "https://www.biorxiv.org/content/early/2017/05/09/100511", "tag": "Bioinformatics", "abstract": "Genomic loci with regulatory potential can be identified and annotated with various properties. For example, genomic sites may be annotated as being bound by a given transcription factor (TF) in one or more cell types. The same sites may be further labeled as being proximal or distal to known promoters. Given such a collection of labeled sites, it is natural to ask what sequence features are associated with each annotation label. However, discovering such label-specific sequence features is often confounded by overlaps between annotation labels; e.g. if regulatory sites specific to a given cell type are also more likely to be promoter-proximal, it is difficult to assess whether motifs identified in that set of sites are associated with the cell type or associated with promoters. In order to meet this challenge, we developed SeqUnwinder, a principled approach to deconvolving interpretable discriminative sequence features associated with overlapping annotation labels. We demonstrate the novel analysis abilities of SeqUnwinder using three examples. Firstly, we show SeqUnwinder's ability to unravel sequence features associated with the dynamic binding behavior of TFs during motor neuron programming from features associated with chromatin state in the initial embryonic stem cells. Secondly, we characterize distinct sequence properties of multi-condition and cell-specific TF binding sites after controlling for uneven associations with promoter proximity. Finally, we demonstrate the scalability of SeqUnwinder to discover cell-specific sequence features from over one hundred thousand genomic loci that display DNase I hypersensitivity in one or more ENCODE cell lines. Availability: https://github.com/seqcode/sequnwinder"}, {"title": "Deep Learning and Association Rule Mining for Predicting Drug Response in Cancer. A Personalised Medicine Approach.", "url": "https://www.biorxiv.org/content/early/2017/05/09/070490", "tag": "Bioinformatics", "abstract": "A major challenge in cancer treatment is predicting the clinical response to anti-cancer drugs for each individual patient. For complex diseases such as cancer, characterized by high inter-patient variance, the implementation of precision medicine approaches is dependent upon understanding the pathological processes at the molecular level. While the 'omics' era provides unique opportunities to dissect the molecular features of diseases, the ability to utilize it in targeted therapeutic efforts is hindered by both the massive size and diverse nature of the 'omics' data. Recent advances with Deep Learning Neural Networks (DLNNs), suggests that DLNN could be trained on large data sets to efficiently predict therapeutic responses in cancer treatment. We present the application of Association Rule Mining combined with DLNNs for the analysis of high-throughput molecular profiles of 1001 cancer cell lines, in order to extract cancer-specific signatures in the form of easily interpretable rules and use these rules as input to predict pharmacological responses to a large number of anti-cancer drugs. The proposed algorithm outperformed Random Forests (RF) and Bayesian Multitask Multiple Kernel Learning (BMMKL) classification which currently represent the state-of-the-art in drug-response prediction. Moreover, the in silico pipeline presented, introduces a novel strategy for identifying potential therapeutic targets, as well as possible drug combinations with high therapeutic potential. For the first time, we demonstrate that DLNNs trained on a large pharmacogenomics data-set can effectively predict the therapeutic response of specific drugs in different cancer types. These findings serve as a proof of concept for the application of DLNNs to predict therapeutic responsiveness, a milestone in precision medicine."}, {"title": "MutPred2: inferring the molecular and phenotypic impact of amino acid variants", "url": "https://www.biorxiv.org/content/early/2017/05/09/134981", "tag": "Bioinformatics", "abstract": "We introduce MutPred2, a tool that improves the prioritization of pathogenic amino acid substitutions, generates molecular mechanisms potentially causative of disease, and returns interpretable pathogenicity score distributions on individual genomes. While its prioritization performance is state-of-the-art, a novel and distinguishing feature of MutPred2 is the probabilistic modeling of variant impact on specific aspects of protein structure and function that can serve to guide experimental studies of phenotype-altering variants. We demonstrate the utility of MutPred2 in the identification of the structural and functional mutational signatures relevant to Mendelian disorders and the prioritization of de novo mutations associated with complex neurodevelopmental disorders. We then experimentally validate the functional impact of several variants identified in patients with such disorders. We argue that mechanism-driven studies of human inherited diseases have the potential to significantly accelerate the discovery of clinically actionable variants. Availability: http://mutpred.mutdb.org/"}, {"title": "OncoScore: An R Package To Measure The Oncogenic Potential Of Genes", "url": "https://www.biorxiv.org/content/early/2017/05/08/135483", "tag": "Bioinformatics", "abstract": "Motivation: We here present OncoScore, an open-source tool and R package that implements a novel text-mining method capable of ranking genes according to their association to cancer, based on available biomedical literature on PubMed. OncoScore can scan the biomedical literature with dynamically updatable web queries and measure the association to cancer of each gene by considering their citations. The output of the tool is a score that is measuring the strength of the association of the genes to cancer at the time of the analysis. Availability and Implementation: OncoScore is available on GitHub and as an R package on bioconductor.org. Furthermore, the queries to OncoScore can also be performed at http://www.galseq.com/oncoscore.html Contact: daniele.ramazzotti@stanford.edu or l.desano@campus.unimib.it Supplementary information: Supplementary data are available at Bioinformatics online."}, {"title": "Insight Into The Mechanism Of Protein Thermostability Based On The Residue Interaction Degrees", "url": "https://www.biorxiv.org/content/early/2017/05/08/135319", "tag": "Bioinformatics", "abstract": "Understanding the basis of protein thermostability raises a general question: which residue with specific interaction degrees is more important to the protein thermostability? A strictly selected dataset of 131 pairs of thermophilic (TPs) and mesophilic proteins (MPs) was constructed. There were 6.4% and 8.4% of the total residues in sequences did not interact with others in TPs and MPs. The amino acid contents in sequences are closest to those with the interaction degrees of 3 according to the Chi-squared distances. Only Glu, Gln and the amide residues showed significant differences in sequences, which was the same as identified at low residue interaction degrees. However, we observed significant Phe, Lys, Leu, Gln and the charged, aliphatic, aromatic, positive charged and small residues at high interaction degree. Among them, Phe was rarely reported previously although aromatic residues were well-known contributor to protein thermostability. Finally, we took aspartate transcarbamylases as an example to explain how a residue with various interaction degrees contributed differently to their thermostability. Our results clearly demonstrated the differences of amino acids in sequence between TPs and MPs could only represent those involved in low interaction degrees. Much more residues with significant differences existed at high interaction degrees even if they had few significant amino acids in sequences. The interaction degree-based method should be an alternative tool in extracting valuable eigenvalues for predicting proteins attributes in bioinformatics. It could also provide a new perspective for studying the thermostability of proteins and engineering novel thermostable proteins."}, {"title": "LEAP: A Generalization Of The Landau-Vishkin Algorithm With Custom Gap Penalties", "url": "https://www.biorxiv.org/content/early/2017/05/07/133157", "tag": "Bioinformatics", "abstract": "Motivation: Approximate String Matching is a pivotal problem in the field of computer science. It serves as an integral component for many string algorithms, most notably, DNA read mapping and alignment. The improved LV algorithm proposes an improved dynamic-programming strategy over the banded Smith-Waterman algorithm but suffers from support of a limited selection of scoring schemes. In this paper, we propose the Leaping Toad problem, a generalization of the approximate string matching problem, as well as LEAP, a generalization of the Landau-Vishkin's algorithm that solves the Leaping Toad problem under a broader selection of scoring schemes. Results: We benchmarked LEAP against 3 state-of-the-art approximate string matching implementations. We show that when using a bit-vectorized De Bruijn sequence based optimization, LEAP is up to 7.4x faster than the state-of-the-art bit-vector Levenshtein distance implementation and up to 32x faster than the state-of-the-art affine-gap-penalty parallel Needleman-Wunsch Implementation."}, {"title": "Homology Modeling In A Dynamical World", "url": "https://www.biorxiv.org/content/early/2017/05/06/135004", "tag": "Bioinformatics", "abstract": "A key concept in template-based modeling is the high correlation between sequence and structural divergence, with the practical consequence that homologous proteins that are similar at the sequence level will also be similar at the structural level. However, conformational diversity of the native state will reduce the correlation between structural and sequence divergence, because structural variation can appear without sequence diversity. In this work, we explore the impact that conformational diversity has on the relationship between structural and sequence divergence. We find that the extent of conformational diversity can be as high as the maximum structural divergence among families. Also, as expected, conformational diversity impairs the well-established correlation between sequence and structural divergence, which is nosier than previously suggested. However, we found that this noise can be resolved using a priori information coming from the structure-function relationship. We show that protein families with low conformational diversity show a well-correlated relationship between sequence and structural divergence, which is severely reduced in proteins with larger conformational diversity. This lack of correlation could impair Template-based modelling (TMB) results in highly dynamical proteins. Finally, we also find that the presence of order/disorder can provide useful beforehand information for better TBM performance."}, {"title": "Metagenomic binning through low density hashing", "url": "https://www.biorxiv.org/content/early/2017/05/05/133116", "tag": "Bioinformatics", "abstract": "Bacterial microbiomes of incredible complexity are found throughout the world, from exotic marine locations to the soil in our yards to within our very guts. With recent advances in Next-Generation Sequencing (NGS) technologies, we have vastly greater quantities of microbial genome data, but the nature of environmental samples is such that DNA from different species are mixed together. Here, we present Opal for metagenomic binning, the task of identifying the origin species of DNA sequencing reads. Our Opal method introduces low-density, even-coverage hashing to bioinformatics applications, enabling quick and accurate metagenomic binning. Our tool is up to two orders of magnitude faster than leading alignment-based methods at similar or improved accuracy, allowing computational tractability on large metagenomic datasets. Moreover, on public benchmarks, Opal is substantially more accurate than both alignment-based and alignment-free methods (e.g. on SimHC20.500, Opal achieves 95% F1-score while Kraken and CLARK achieve just 91% and 88%, respectively); this improvement is likely due to the fact that the latter methods cannot handle computationally-costly long-range dependencies, which our even-coverage, low-density fingerprints resolve. Notably, capturing these long-range dependencies drastically improves Opal's ability to detect unknown species that share a genus or phylum with known bacteria. Additionally, the family of hash functions Opal uses can be generalized to other sequence analysis tasks that rely on k-mer based methods to encode long-range dependencies."}, {"title": "How To Normalize Metatranscriptomic Count Data For Differential Expression Analysis", "url": "https://www.biorxiv.org/content/early/2017/05/05/134650", "tag": "Bioinformatics", "abstract": "Background: Differential expression analysis on the basis of RNA-Seq count data has become a standard tool in transcriptomics. Several studies have shown that prior normalization of the data is crucial for a reliable detection of transcriptional differences. Until now it is not clear whether and how the transcriptomic approach can be used for differential expression analysis in metatranscriptomics. The potential side effects that may result from direct application of transcriptomic tools to metatranscriptomic count data have not been studied so far. Methods: We propose a model for differential expression in metatranscriptomics that explicitly accounts for variations in the taxonomic composition of transcripts across different samples. As a main consequence the correct normalization of metatranscriptomic count data requires the taxonomic separation of the data into organism-specific bins. Then the taxon-specific scaling of organism profiles yields a valid normalization and allows to recombine the scaled profiles into a metatranscriptomic count matrix. This matrix can then be analyzed with statistical tools for transcriptomic count data. For taxon-specific scaling and recombination of scaled counts we provide a simple R script. Results: When applying transcriptomic tools for differential expression analysis directly to metatranscriptomic data the organism-independent (global) scaling of counts implies a high risk of falsely predicted functional differences. In simulation studies we show that incorrect normalization not only tends to loose significant differences but especially can produce a large number of false positives. In contrast, taxon-specific scaling can equalize the variation of relative library sizes from different organisms and therefore shows a reliable detection of significant differences in all simulations. On real metatranscriptomic data the results from taxon-specific and global scaling can largely differ. In our study, global scaling shows a high number of extra predictions which are not supported by single transcriptome analyses. Inspection of the scaling error suggests that these extra predictions may actually correspond to artifacts of an incorrect normalization. Conclusions: As in transcriptomics, a proper normalization of count data is also essential for differential expression analysis in metatranscriptomics. Our model implies a taxon-specific scaling of counts for normalization of the data. The application of taxon-specific scaling consequently removes taxonomic composition variations from functional profiles and therefore effectively prevents the risk of false predictions due to incorrect normalization."}, {"title": "Simulating The Dynamics Of Targeted Capture Sequencing With CapSim", "url": "https://www.biorxiv.org/content/early/2017/05/05/134510", "tag": "Bioinformatics", "abstract": "Motivation: Targeted sequencing using capture probes has become increasingly popular in clinical applications due to its scalability and cost-effectiveness. The approach also allows for higher sequencing coverage of the targeted regions resulting in better analysis statistical power. However, because of the dynamics of the hybridisation process, it is difficult to evaluate the efficiency of the probe design prior to the experiments which are time consuming and costly. Results: We developed CapSim, a software package for simulation of targeted sequencing. Given a genome sequence and a set of probes, CapSim simulates the fragmentation, the dynamics of probe hybridisation, and the sequencing of the captured fragments on Illumina and PacBio sequencing platforms. The simulated data can be used for evaluating the performance of the analysis pipeline, as well as the efficiency of the probe design. Parameters of the various stages in the sequencing process can also be evaluated in order to optimise the efficacy of the experiments."}, {"title": "Efficient repositioning of approved drugs as anti-HIV agents using Anti-HIV-Predictor", "url": "https://www.biorxiv.org/content/early/2017/05/05/087445", "tag": "Bioinformatics", "abstract": "Development of new, effective and affordable drugs against HIV is urgently needed. In this study, we developed a world's first web server called Anti-HIV-Predictor (http://bsb.kiz.ac.cn:70/hivpre) for predicting anti-HIV activity of given compounds. This server is rapid and accurate (accuracy >93% and AUC > 0.958). We applied the server to screen 1835 approved drugs for anti-HIV therapy. Totally 67 drugs were predicted to have anti-HIV activity, 25 of which are anti-HIV drugs. Then we experimentally evaluated 35 predicted new anti-HIV compounds by assays of syncytia formation, p24 quantification, cytotoxicity. Finally, we repurposed 7 approved drugs (cetrorelix, dalbavancin, daunorubicin, doxorubicin, epirubicin, idarubicin and valrubicin) as new anti-HIV agents. The original indication of these drugs is involved in a variety of diseases such as female infertility and cancer. Anti-HIV-Predictor and the 7 repurposed anti-HIV agents provided here demonstrate the efficacy of this strategy for discovery of new anti-HIV agents."}, {"title": "MotifMark: Finding Regulatory Motifs in DNA Sequences", "url": "https://www.biorxiv.org/content/early/2017/05/05/134296", "tag": "Bioinformatics", "abstract": "The interaction between proteins and DNA is a key driving force in a significant number of biological processes such as transcriptional regulation, repair, recombination, splicing, and DNA modification. The identification of DNA-binding sites and the specificity of target proteins in binding to these regions are two important steps in understanding the mechanisms of these biological activities. A number of high-throughput technologies have recently emerged that try to quantify the affinity between proteins and DNA motifs. Despite their success, these technologies have their own limitations and fall short in precise characterization of motifs, and as a result, require further downstream analysis to extract useful and interpretable information from a haystack of noisy and inaccurate data. Here we propose MotifMark, a new algorithm based on graph theory and machine learning, that can find binding sites on candidate probes and rank their specificity in regard to the underlying transcription factor. We developed a pipeline to analyze experimental data derived from compact universal protein binding microarrays and benchmarked it against two of the most accurate motif search methods. Our results indicate that MotifMark can be a viable alternative technique for prediction of motif from protein binding microarrays and possibly other related high-throughput techniques."}, {"title": "CircMarker: A Fast and Accurate Algorithm for Circular RNA Detection", "url": "https://www.biorxiv.org/content/early/2017/05/05/134411", "tag": "Bioinformatics", "abstract": "While RNA is often created from linear splicing during transcription, recent studies have found that non-canonical splicing sometimes occurs. Non-canonical splicing joins 3' and 5' and forms the so-called circular RNA. It is now believed that circular RNA plays important biological roles such as affecting susceptibility in some diseases. within these few years, several experimental methods have been developed to enrich circular RNA while degrade linear RNA. Although several useful software tools for circRNA detection have been developed as well, these tools may miss many circular RNA. Also, existing tools are slow for large data because those tools often depend on reads mapping. In this paper, we present a new computational approach, named CircMarker, based on k-mers rather than reads mapping for circular RNA detection. CircMarker takes advantage of transcriptome annotation files to create k-mer table for circular RNA detection. Empirical results show that CircMarker outperforms existing tools in circular RNA detection on accuracy and efficiency in many simulated and real datasets. CircMarker can be downloaded from https://github.com/lxwgcool/CircMarker."}, {"title": "taxMaps - Ultra-comprehensive and highly accurate taxonomic classification of short-read data in reasonable time", "url": "https://www.biorxiv.org/content/early/2017/05/05/134023", "tag": "Bioinformatics", "abstract": "High-throughput sequencing is a revolutionary technology for the analysis of metagenomic samples. However, querying large volumes of reads against comprehensive DNA/RNA databases in a sensitive manner can be compute-intensive. Here, we present taxMaps, a highly efficient, sensitive and fully scalable taxonomic classification tool, capable of delivering classification accuracy comparable to that of BLASTn, but at up to 3 orders of magnitude less computational cost. taxMaps is freely available for academic and non-commercial research purposes at https://github.com/nygenome/taxmaps."}, {"title": "In-Silico Read Normalization Using Set Multi-Cover Optimization", "url": "https://www.biorxiv.org/content/early/2017/05/04/133579", "tag": "Bioinformatics", "abstract": "De Bruijn graphs are a common assembly data structure for large sequencing datasets. But with the advances in sequencing technologies, assembling high coverage datasets has become a computational challenge. Read normalization, which removes redundancy in large datasets, is widely applied to reduce resource requirements. Current normalization algorithms, though efficient, provide no guarantee to preserve important k-mers that form connections between regions in the graph. Here, normalization is phrased as a set multi-cover problem on reads and a heuristic algorithm, ORNA, is proposed. ORNA normalizes to the minimum number of reads required to retain all k-mers and their relative kmer abundances from the original dataset. Hence, all connections and coverage information from the original graph are preserved. ORNA was tested on various RNA-seq datasets with different coverage values. It was compared to the current normalization algorithms and was found to be performing better. It is shown that combining read error correction and normalization allows more accurate and resource efficient RNA assemblies compared to the original dataset. Further, an application was proposed in which multiple datasets were combined and normalized to predict novel transcripts that would have been missed otherwise. Finally, ORNA is a general purpose normalization algorithm that is fast and significantly reduces datasets with little loss of assembly quality. ORNA is freely available at https://github.com/SchulzLab/ORNA"}, {"title": "Real-Time Demultiplexing Nanopore Barcoded Sequencing Data With npBarcode", "url": "https://www.biorxiv.org/content/early/2017/05/04/134155", "tag": "Bioinformatics", "abstract": "Motivation: The recently introduced barcoding protocol to Oxford Nanopore sequencing has increased the versatility of the technology. Several bioinformatic tools have been developed to demultiplex the barcoded reads, but none of them support the streaming analysis. This limits the use of pooled sequencing in real-time applications, which is one of the main advantages of the technology. Results: We introduced npBarcode, an open source and cross platform tool for barcode demultiplex in streaming fashion. npBarcode can be seamlessly integrated into a streaming analysis pipeline. The tool also provides a friendly graphical user interface through npReader, allowing the real-time visual monitoring of the sequencing progress of barcoded samples. We show that npBarcode achieves comparable accuracies to the other alternatives. Availability: npBarcode is bundled in Japsa - a Java tools kit for genome analysis, and is freely available at https://github.com/hsnguyen/npBarcode"}, {"title": "A tandem simulation framework for predicting mapping quality", "url": "https://www.biorxiv.org/content/early/2017/05/04/103952", "tag": "Bioinformatics", "abstract": "Read alignment is the first step in most sequencing data analyses. Because a read's point of origin can be ambiguous, aligners report a mapping quality: the probability the reported alignment is incorrect. Despite its importance, there is no established and general method for calculating mapping quality. We describe a framework for predicting mapping qualities that works by simulating a set of tandem reads, similar to the input reads in important ways, but for which the true point of origin is known. We implement this in an accurate and low-overhead tool called Qtip, which is compatible with popular aligners."}, {"title": "Impact Of Sequencing Depth And Read Length On Single Cell RNA Sequencing Data: Lessons From T Cells", "url": "https://www.biorxiv.org/content/early/2017/05/04/134130", "tag": "Bioinformatics", "abstract": "Single cell RNA sequencing (scRNA-seq) has shown great potential in measuring the gene expression profiles of heterogeneous cell populations. In immunology, scRNA-seq allowed the characterisation of transcript sequence diversity of functionally relevant sub-populations of T cells, and notably the identification of the full length T cell receptor (TCR\u03b1\u03b2), which defines the specificity against cognate antigens. Several factors, such as RNA library capture, cell quality, and sequencing output have been suggested to affect the quality of scRNA-seq data, but these factors have not been systematically examined. We studied the effect of read length and sequencing depth on the quality of gene expression profiles, cell type identification, and TCR\u03b1\u03b2 reconstruction, utilising 1,305 publically available scRNA-seq datasets, and simulation-based analyses. Gene expression was characterised by an increased number of unique genes identified with short read lengths (<50 bp), but these featured higher technical variability compared to profiles from longer reads. TCR\u03b1\u03b2 were detected in 1,027 cells (79%), with a success rate between 81% and 100% for datasets with at least 250,000 (PE) reads of length >50 bp. Sufficient read length and sequencing depth can control technical noise to enable accurate identification of TCR\u03b1\u03b2 and gene expression profiles from scRNA-seq data of T cells."}, {"title": "Spike Discharge Prediction Based On Neuro-Fuzzy System", "url": "https://www.biorxiv.org/content/early/2017/05/03/133967", "tag": "Bioinformatics", "abstract": "This paper presents the development and evaluation of different versions of Neuro-Fuzzy model for prediction of spike discharge patterns. We aim to predict the spike discharge variation using first spike latency and frequency-following interval. In order to study the spike discharge dynamics, we analyzed the Cerebral Cortex data of the cat from http://oto2.wustl.edu/bbears/arnie/catcrtx.htm. Adaptive Neuro-Fuzzy Inference Systems (ANFIS), Wang and Mendel (WM), Dynamic evolving neural-fuzzy inference system (DENFIS), Hybrid neural Fuzzy Inference System (HyFIS), genetic for lateral tuning and rule selection of linguistic fuzzy system (GFS.LT.RS) and subtractive clustering and fuzzy c-means (SBC) algorithms are applied for data. Among these algorithms, ANFIS and GFS.LT.RS models have better performance. On the other hand, ANFIS and GFS.LT.RS algorithms can be used to predict the spike discharge dynamics as a function of first spike latency and frequency with a higher accuracy compared to other algorithms."}, {"title": "Gene2Function: An Integrated Online Resource For Gene Function Discovery", "url": "https://www.biorxiv.org/content/early/2017/05/03/133975", "tag": "Bioinformatics", "abstract": "One of the most powerful ways to develop hypotheses regarding biological functions of conserved genes in a given species, such as in humans, is to first look at what is known about function in another species. Model organism databases (MODs) and other resources are rich with functional information but difficult to mine. Gene2Function (G2F) addresses a broad need by integrating information about conserved genes in a single online resource."}, {"title": "SArKS: Discovering Gene Expression Regulatory Motifs And Domains By Suffix Array Kernel Smoothing", "url": "https://www.biorxiv.org/content/early/2017/05/03/133934", "tag": "Bioinformatics", "abstract": "Experiments designed to assess differential gene expression represent a rich resource for discovering how DNA regulatory sequences influence transcription. Results derived from such experiments are usually quantified as continuous scores, such as fold changes, test statistics and p-values. We present a de novo motif discovery algorithm, SArKS, which uses a nonparametric kernel smoothing approach to identify promoter motifs correlated with elevated differential expression scores. SArKS has the capability to smooth over both motif sequence similarity and, in a second pass, over spatial proximity of multiple motifs to identify longer regions enriched in correlative motifs. We applied SArKS to simulated data, illustrating how SArKS can be used to find motifs embedded in random background sequences, and to two published RNA-seq expression data sets, one probing S. cerevisiae transcriptional response to anti-fungal agents and the other comparing gene expression profiles among cortical neuron subtypes in M. musculus. For both RNA-seq sets we successfully identified motifs whose kernel-smoothed scores were significantly elevated compared to the permutation-estimated background distributions. We found strong similarities between these identified motifs and known, biologically meaningful sequence elements which may help to provide additional context for the results previously published regarding these data sets. Finally, because eukaryotic transcription regulation is highly combinatorial, we also outline how SArKS methods might be extended to discover synergistic motifs."}, {"title": "BugBase Predicts Organism Level Microbiome Phenotypes", "url": "https://www.biorxiv.org/content/early/2017/05/02/133462", "tag": "Bioinformatics", "abstract": "Shotgun metagenomics and marker gene amplicon sequencing can be used to directly measure or predict the functional repertoire of the microbiota en masse, but current methods do not readily estimate the functional capability of individual microorganisms. Here we present BugBase, an algorithm that predicts organism-level coverage of functional pathways as well as biologically interpretable phenotypes such as oxygen tolerance, Gram staining, and pathogenic potential, within complex microbiomes using either whole-genome shotgun or marker gene sequencing data. We find the organism-level pathway coverage of BugBase predictions to be statistically higher powered than current bag-of-genes approaches for discerning functional changes in both host-associated and environmental microbiomes."}, {"title": "Coal-Miner: A Coalescent-Based Method For GWA Studies Of Quantitative Traits With Complex Evolutionary Origins", "url": "https://www.biorxiv.org/content/early/2017/05/02/132951", "tag": "Bioinformatics", "abstract": "Association mapping (AM) methods are used in genome-wide association (GWA) studies to test for statistically significant associations between genotypic and phenotypic data. The genotypic and phenotypic data share common evolutionary origins -- namely, the evolutionary history of sampled organisms -- introducing covariance which must be distinguished from the covariance due to biological function that is of primary interest in GWA studies. A variety of methods have been introduced to perform AM while accounting for sample relatedness. However, the state of the art predominantly utilizes the simplifying assumption that sample relatedness is effectively fixed across the genome. In contrast, population genetic theory and empirical studies have shown that sample relatedness can vary greatly across different loci within a genome; this phenomena -- referred to as local genealogical variation -- is commonly encountered in many genomic datasets. New AM methods are needed to better account for local variation in sample relatedness within genomes. We address this gap by introducing Coal-Miner, a new statistical AM method. The Coal-Miner algorithm takes the form of a methodological pipeline. The initial stages of Coal-Miner seek to detect candidate loci, or loci which contain putatively causal markers. Subsequent stages of Coal-Miner perform test for association using a linear mixed model with multiple effects which account for sample relatedness locally within candidate loci and globally across the entire genome. Using synthetic and empirical datasets, we compare the statistical power and type I error control of Coal-Miner against state-of-the-art AM methods. The simulation conditions reflect a variety of genomic architectures for complex traits and incorporate a range of evolutionary scenarios, each with different evolutionary processes that can generate local genealogical variation. The empirical benchmarks include a large-scale dataset that appeared in a recent high-profile publication. Across the datasets in our study, we find that Coal-Miner consistently offers comparable or typically better statistical power and type I error control compared to the state-of-art methods."}, {"title": "Gene annotation bias impedes biomedical research", "url": "https://www.biorxiv.org/content/early/2017/05/02/133108", "tag": "Bioinformatics", "abstract": "We found tremendous inequality across gene and protein annotation resources. We observe that this bias leads biomedical researchers to focus on richly annotated genes instead of those with the strongest molecular data. We advocate for researchers to reduce these biases by pursuing data-driven hypotheses."}, {"title": "The Landscape Of Human Mutually Exclusive Splicing", "url": "https://www.biorxiv.org/content/early/2017/05/02/133215", "tag": "Bioinformatics", "abstract": "Mutually exclusive splicing of exons is a mechanism of functional gene and protein diversification with pivotal roles in organismal development and diseases such as Timothy syndrome, cardiomyopathy and cancer in humans. In order to obtain a first genome-wide estimate of the extent and biological role of mutually exclusive splicing in humans we predicted and subsequently validated mutually exclusive exons (MXEs) using 515 publically available RNA-seq datasets. Here, we provide evidence for the expression of over 855 MXEs, 42% of which represent novel exons, increasing the annotated human mutually exclusive exome more than five-fold. The data provides strong evidence for the existence of large and multi-cluster MXEs in higher vertebrates and offers new insights into MXE splicing mechanics and evolution. Finally, MXEs are significantly enriched in pathogenic mutations and their spatio-temporal expression predicts human disease pathology."}, {"title": "Identifying differential isoform abundance with RATs: a universal tool and a warning", "url": "https://www.biorxiv.org/content/early/2017/05/02/132761", "tag": "Bioinformatics", "abstract": "Motivation: The biological importance of changes in gene and transcript expression is well recognised and is reflected by the wide variety of tools available to characterise these changes. Regulation via Differential Transcript Usage (DTU) is emerging as an important phenomenon. Several tools exist for the detection of DTU from read alignment or assembly data, but options for detection of DTU from alignment-free quantifications are limited. Results: We present an R package named RATs - (Relative Abundance of Transcripts) - that identifies DTU transcriptome-wide directly from transcript abundance estimations. RATs is agnostic to quantification methods and exploits bootstrapped quantifications, if available, to inform the significance of detected DTU events. RATs contextualises the DTU results and shows good False Discovery performance (median FDR \u22640.05) at all replication levels. We applied RATs to a human RNA-seq dataset associated with idiopathic pulmonary fibrosis with three DTU events validated by qRT-PCR. RATs found all three genes exhibited statistically significant changes in isoform proportions based on Ensembl v60 annotations, but the DTU for two were not reliably reproduced across bootstrapped quantifications. RATs also identified 500 novel DTU events that are enriched for eleven GO terms related to regulation of the response to stimulus, regulation of immune system processes, and symbiosis/parasitism. Repeating this analysis with the Ensembl v87 annotation showed the isoform abundance profiles of two of the three validated DTU genes changed radically. RATs identified 414 novel DTU events that are enriched for five GO terms, none of which are in common with those previously identified. Only 141 of the DTU evens are common between the two analyses, and only 8 are among the 248 reported by the original study. Furthermore, the original qRT-PCR probes no longer match uniquely to their original transcripts, calling into question the interpretation of these data. We suggest parallel full-length isoform sequencing, annotation pre-filtering and sequencing of the transcripts captured by qRT-PCR primers as possible ways to improve the validation of RNA-seq results in future experiments. Availability: The package is available through Github at https://github.com/bartongroup/Rats ."}, {"title": "Non-Parametric Estimation Of Population Size Changes From The Site Frequency Spectrum", "url": "https://www.biorxiv.org/content/early/2017/05/02/125351", "tag": "Bioinformatics", "abstract": "The variability in population size is a key quantity for understanding the evolutionary history of a species. We present a new method, CubSFS, for estimating the changes in population size of a panmictic population from the site frequency spectrum. First, we provide a straightforward proof for the expression of the expected site frequency spectrum depending only on the population size. Our derivation is based on an eigenvalue decomposition of the instantaneous coalescent rate matrix. Second, we solve the inverse problem of determining the variability in population size from an observed SFS. Our solution is based on a cubic spline for the population size. The cubic spline is determined by minimizing the weighted average of two terms, namely (i) the goodness of fit to the SFS, and (ii) a penalty term based on the smoothness of the changes. The weight is determined by cross-validation. The new method is validated on simulated demographic histories and applied on data from nine different human populations."}, {"title": "PaperBLAST: Text-mining papers for information about homologs", "url": "https://www.biorxiv.org/content/early/2017/05/02/133041", "tag": "Bioinformatics", "abstract": "Large-scale genome sequencing has identified millions of protein-coding genes whose function is unknown. Many of these proteins are similar to characterized proteins from other organisms, but much of this information is missing from annotation databases and is hidden in the scientific literature. To make this information accessible, PaperBLAST uses EuropePMC to search the full text of scientific articles for references to genes. PaperBLAST also takes advantage of curated resources that link protein sequences to scientific articles (Swiss-Prot, GeneRIF, and EcoCyc). PaperBLAST's database includes over 700,000 scientific articles that mention over 400,000 different proteins. Given a protein of interest, PaperBLAST quickly finds similar proteins that are discussed in the literature and presents snippets of text from relevant articles or from the curators. PaperBLAST is available at http://papers.genomics.lbl.gov/."}, {"title": "BasecRAWller: Streaming Nanopore Basecalling Directly from Raw Signal", "url": "https://www.biorxiv.org/content/early/2017/05/01/133058", "tag": "Bioinformatics", "abstract": "All current nanopore basecalling applications begin with the segmentation of raw signal into discrete events, which are ultimately processed into called bases. We propose the basecRAWller algorithm, a pair of unidirectional recurrent neural networks that enables the calling of DNA bases in real time directly from the rawest form of nanopore output. This shift in nanopore basecalling provides a number of advantages over current processing pipelines including: 1) streaming basecalling, 2) tunable ratio of insertions to deletions, and 3) potential for streaming detection of modified bases. Key to the streaming basecalling capability is sequence prediction at a delay of less than 1/100th of a second, allowing future signal to continuously modulate sequence prediction. BasecRAWller is computationally efficient enabling basecalling at speeds faster than current nanopore instrument measurement speeds on a single core. Further, basecalling can be paused and resumed without any change in the resulting predicted sequence, transforming the potential applications for dynamic read rejection capabilities. The basecRAWller algorithm provides an alternative approach to nanopore basecalling at comparable accuracy and provides the community with the capacity to train their own basecRAWller neural networks with minimal effort."}, {"title": "FastNet: Fast and accurate inference of phylogenetic networks using large-scale genomic sequence data", "url": "https://www.biorxiv.org/content/early/2017/05/01/132795", "tag": "Bioinformatics", "abstract": "Advances in next-generation sequencing technologies and phylogenomics have reshaped our understanding of evolutionary biology. One primary outcome is the emerging discovery that interspecific gene flow has played a major role in the evolution of many different organisms across the Tree of Life. To what extent is the Tree of Life not truly a tree reflecting strict \"vertical\" divergence, but rather a more general graph structure known as a phylogenetic network which also captures \"horizontal\" gene flow? The answer to this fundamental question not only depends upon densely sampled and divergent genomic sequence data, but also computational methods which are capable of accurately and efficiently inferring phylogenetic networks from large-scale genomic sequence datasets. Recent methodological advances have attempted to address this gap. However, in a recent performance study, we demonstrated that the state of the art falls well short of the scalability requirements of existing phylogenomic studies. The methodological gap remains: how can phylogenetic networks be accurately and efficiently inferred using genomic sequence data involving many dozens or hundreds of taxa? In this study, we address this gap by proposing a new phylogenetic divide-and-conquer method which we call FastNet. Using synthetic and empirical data spanning a range of evolutionary scenarios, we demonstrate that FastNet outperforms state-of-the-art methods in terms of computational efficiency and topological accuracy. We predict an imminent need for new computational methodologies that can cope with dataset scale at the next order of magnitude, involving thousands of genomes or more. We consider FastNet to be a next step in this direction. We conclude with thoughts on the way forward through future algorithmic enhancements."}, {"title": "AASRA: An Anchor Alignment-Based Small RNA Annotation Pipeline", "url": "https://www.biorxiv.org/content/early/2017/05/01/132928", "tag": "Bioinformatics", "abstract": "SncRNA-Seq has become a routine for sncRNA profiling; however, software packages currently available are either exclusively for miRNA or piRNA annotation (e.g., miRDeep, miRanalyzer, Shortstack, PIANO), or for direct mapping of the sequence reads to the genome (e.g., Bowtie 2, SOAP and BWA), which tend to generate inaccurate counting due to repetitive matches to the genome or sncRNA homologs. Moreover, novel sncRNA variants in the sequencing reads, including those bearing small overhangs or internal insertions, deletions or mutations, are totally excluded from counting by these algorithms, leading to potential quantification bias. To overcome these problems, a comprehensive software package that can annotate all known small RNA species with adjustable tolerance towards small mismatches is needed. AASRA is based on our unique anchor alignment algorithm, which not only avoids repetitive or ambiguous counting, but also distinguishes mature miRNA from precursor miRNA reads. Compared to all existing pipelines for small RNA annotation, AASRA is superior in the following aspects: 1) AASRA can annotate all known sncRNA species simultaneously with the capability of distinguishing mature and precursor miRNAs; 2) AASRA can identify and allow for inclusion of sncRNA variants with small overhangs and/or internal insertions/deletions into the final counts; 3) AASRA is the fastest among all small RNA annotation pipelines tested. AASRA represents an all-in-one sncRNA annotation pipeline, which allows for high-speed, simultaneous annotation of all known sncRNA species with the capability to distinguish mature from precursor miRNAs, and to identify novel sncRNA variants in the sncRNA-Seq sequencing reads."}, {"title": "Detecting Presence Of Mutational Signatures In Cancer With Confidence", "url": "https://www.biorxiv.org/content/early/2017/05/01/132597", "tag": "Bioinformatics", "abstract": "Cancers arise as the result of somatically acquired changes in the DNA of cancer cells. However, in addition to the mutations that confer a growth advantage, cancer genomes accumulate a large number of somatic mutations resulting from normal DNA damage and repair processes as well as mutations triggered by carcinogenic exposures or cancer related aberrations of DNA maintenance machinery. These mutagenic processes often produce characteristic mutational patterns called mutational signatures. Decomposition of cancer's mutation catalog into mutations consistent with such signatures can provide valuable information about cancer etiology. However, the results from different decomposition methods are not always consistent. Hence, one needs to not only be able to decompose a patient's mutational profile into signatures but also to establish the accuracy of such decomposition. We proposed two complementary ways of measuring confidence and stability of decomposition results and applied them to analyze mutational signatures in breast cancer genomes. We identified very stable and highly unstable signatures, as well as signatures that have been missed altogether. We also provided additional support for the novel signatures. Our results emphasize the importance of assessing the confidence and stability of inferred signature contributions. All tools developed in this paper have been implemented in an R package, called SignatureEstimation, which is available from https://www.ncbi.nlm.nih.gov/CBBresearch/Przytycka/index.cgi#signatureestimation."}, {"title": "CRAST Leads to Homologous-ncRNA Search in Genomic Scale", "url": "https://www.biorxiv.org/content/early/2017/04/30/127738", "tag": "Bioinformatics", "abstract": "Motivation: Non-coding RNAs (ncRNAs) play important roles in various biological processes. In past, homologous-ncRNA search in genomic scale (e.g., search all house mouse ncRNAs for several human ones) is difficult since explicit consideration of secondary structure in alignment leads to impractical complexity on both of time and space. Results: In this study, building the program CRAST (Context RNA Alignment Search Tool, available at \"https: //github.com/heartsh/crast\" including the used validation/test set), we developed the CRAST algorithm, a \"seed-and-extend\" alignment one based on adaptive seed and RNA secondary structure context (motif probabilities). The algorithm is O(n : a sum of lengths of target sequences) on time through help of adaptive seed, implicitly considering both of sequence and secondary structure; it provides computation time comparable with other BLAST-like tools, significantly reduced from any variant of the Sankoff algorithm for alignment with the explicit consideration. It detects homologs as many as other BLAST-like tools and the lowest number of non-homologous ncRNAs."}, {"title": "Identification of Candidate Drugs for Heart Failure using Tensor Decomposition-Based Unsupervised Feature Extraction Applied to Integrated Analysis of Gene Expression between Heart Failure and DrugMatrix Datasets", "url": "https://www.biorxiv.org/content/early/2017/04/29/117465", "tag": "Bioinformatics", "abstract": "Identifying drug target genes in gene expression profiles is not straightforward. Because a drug targets not mRNAs but proteins, mRNA expression of drug target genes is not always altered. In addition, the interaction between a drug and protein can be context dependent; this means that simple drug incubation experiments on cell lines do not always reflect the real situation during active disease. In this paper, I apply tensor decomposition-based unsupervised feature extraction to the integrated analysis of gene expression between heart failure and the DrugMatrix dataset where comprehensive data on gene expression during various drug treatments of rats were reported. I found that this strategy, in a fully unsupervised manner, enables us to identify a combined set of genes and compounds, for which various associations with heart failure were reported."}, {"title": "Automated Detection of Records in Biological Sequence Databases that are Inconsistent with the Literature", "url": "https://www.biorxiv.org/content/early/2017/04/29/101246", "tag": "Bioinformatics", "abstract": "We investigate and analyse the data quality of nucleotide sequence databases with the objective of automatic detection of data anomalies and suspicious records. Specifically, we demonstrate that the published literature associated with each data record can be used to automatically evaluate its quality, by cross-checking the consistency of the key content of the database record with the referenced publications. Focusing on GenBank, we describe a set of quality indicators based on the relevance paradigm of information retrieval (IR). Then, we use these quality indicators to train an anomaly detection algorithm to classify records as \"confident\" or \"suspicious\". Our experiments on the PubMed Central collection show assessing the coherence between the literature and database records, through our algorithms, is an effective mechanism for assisting curators to perform data cleansing. Although fewer than 0.25% of the records in our data set are known to be faulty, we would expect that there are many more in GenBank that have not yet been identified. By automated comparison with literature they can be identified with a precision of up to 10% and a recall of up to 30%, while strongly outperforming several baselines. While these results leave substantial room for improvement, they reflect both the very imbalanced nature of the data, and the limited explicitly labelled data that is available. Overall, the obtained results show promise for the development of a new kind of approach to detecting low-quality and suspicious sequence records based on literature analysis and consistency. From a practical point of view, this will greatly help curators in identifying inconsistent records in large-scale sequence databases by highlighting records that are likely to be inconsistent with the literature."}, {"title": "Transcriptome-wide splicing quantification in single cells", "url": "https://www.biorxiv.org/content/early/2017/04/29/098517", "tag": "Bioinformatics", "abstract": "Single cell RNA-seq (scRNA-seq) has revolutionised our understanding of transcriptome variability, with profound implications both fundamental and translational. While scRNA-seq provides a comprehensive measurement of stochasticity in transcription, the limitations of the technology have prevented its application to dissect variability in RNA processing events such as splicing. Here we present BRIE (Bayesian Regression for Isoform Estimation), a Bayesian hierarchical model which resolves these problems by learning an informative prior distribution from multiple single cells. BRIE combines the mixture modelling approach for isoform quantification with a regression approach to learn sequence features which are predictive of splicing events. We validate BRIE on several scRNA-seq data sets, showing that BRIE yields reproducible estimates of exon inclusion ratios in single cells and provides an effective tool for differential isoform quantification between scRNA-seq data sets. BRIE therefore expands the scope of scRNA-seq experiments to probe the stochasticity of RNA-processing."}, {"title": "Signaling Pathway Activities Improve Prognosis for Breast Cancer", "url": "https://www.biorxiv.org/content/early/2017/04/29/132357", "tag": "Bioinformatics", "abstract": "With the advent of high-throughput technologies for genome-wide expression profiling, a large number of methods have been proposed to discover gene-based signatures as biomarkers to guide cancer prognosis. However, it is often difficult to interpret the list of genes in a prognostic signature regarding the underlying biological processes responsible for disease progression or therapeutic response. A particularly interesting alternative to gene-based biomarkers is mechanistic biomarkers, derived from signaling pathway activities, which are known to play a key role in cancer progression and thus provide more informative insights into cellular functions involved in cancer mechanism. In this study, we demonstrate that pathway-level features, such as the activity of signaling circuits, outperform conventional gene-level features in prediction performance in breast cancer prognosis. We also show that the proposed classification scheme can even suggest, in addition to relevant signaling circuits related to disease outcome, a list of genes that do not code for signaling proteins whose contribution to cancer prognosis potentially supplements the mechanisms detected by pathway analysis."}, {"title": "Exploring the Diversity of Bacillus whole genome sequencing projects using Peasant, the Prokaryotic Assembly and Annotation Tool", "url": "https://www.biorxiv.org/content/early/2017/04/28/132084", "tag": "Bioinformatics", "abstract": "The persistent decrease in cost and difficulty of whole genome sequencing of microbial organisms has led to a dramatic increase in the number of species and strains characterized from a wide variety of environments. Microbial genome sequencing can now be conducted by small laboratories and as part of undergraduate curriculum. While sequencing is routine in microbiology, assembly, annotation and downstream analyses still require computational resources and expertise, often necessitating familiarity with programming languages. To address this problem, we have created a light-weight, user-friendly tool for the assembly and annotation of microbial sequencing projects. The Prokaryotic Assembly and Annotation Tool, Peasant, automates the processes of read quality control, genome assembly, and annotation for microbial sequencing projects. High-quality assemblies and annotations can be generated by Peasant without the need of programming expertise or high-performance computing resources. Furthermore, statistics are calculated so that users can evaluate their sequencing project. To illustrate the computational speed and accuracy of Peasant, the SRA records of 322 Illumina platform whole genome sequencing assays for Bacillus species were retrieved from NCBI, assembled and annotated on a single desktop computer. From the assemblies and annotations produced, a comprehensive analysis of the diversity of over 200 high-quality samples was conducted, looking at both the 16S rRNA phylogenetic marker as well as the Bacillus core genome. Peasant provides an intuitive solution for high-quality whole genome sequence assembly and annotation for users with limited programing experience and/or computational resources. The analysis of the Bacillus whole genome sequencing projects exemplifies the utility of this tool. Furthermore, the study conducted here provides insight into the diversity of the species, the largest such comparison conducted to date."}, {"title": "Side-By-Side Analysis Of Alternative Approaches On Multi-Level RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/04/28/131862", "tag": "Bioinformatics", "abstract": "Background: RNA sequencing (RNA-seq) is widely used for RNA quantification across environmental, biological and medical sciences; it enables the description of genome-wide patterns of expression and the deduction of regulatory interactions and networks. The aim of computational analyses is to achieve an accurate output, i.e. rigorous quantification of genes/transcripts to allow a reliable prediction of differential expression (DE), despite the variable levels of noise and biases present in sequencing data. The evaluation of sequencing quality and normalization are essential components of this process. Results: We investigate the discriminative power of existing approaches for the quality checking of mRNA-seq data and also propose additional, quantitative, quality checks. To accommodate the analysis of a nested, multi-level design using data on D. melanogaster, we incorporated the sample layout into the analysis. We describe a \u201csubsampling without replacement\u201d-based normalization and identification of DE that accounts for the experimental design i.e. the hierarchy and amplitude of effect sizes within samples. We also evaluate the differential expression call in comparison to existing approaches. To assess the broader applicability of these methods, we applied this series of steps to a published set of H. sapiens mRNA-seq samples. Conclusions: The dataset-tailored methods improved sample comparability and delivered a robust prediction of subtle gene expression changes. Overall, the proposed approach offers the potential to improve key steps in the analysis of RNA-seq data by incorporating the structure and characteristics of biological experiments into the data analysis."}, {"title": "Meffil: efficient normalisation and analysis of very large DNA methylation samples", "url": "https://www.biorxiv.org/content/early/2017/04/27/125963", "tag": "Bioinformatics", "abstract": "Background. Technological advances in high throughput DNA methylation microarrays have allowed dramatic growth of a new branch of epigenetic epidemiology. DNA methylation datasets are growing ever larger in terms of the number of samples profiled, the extent of genome coverage, and the number of studies being meta-analysed. Novel computational solutions are required to efficiently handle these data. Methods. We have developed meffil, an R package designed to quality control, normalize and perform epigenome-wide association studies (EWAS) efficiently on large samples of Illumina Infinium HumanMethylation450 and MethylationEPIC BeadChip microarrays. We tested meffil by applying it to 6000 450k microarrays generated from blood collected for two different datasets. Results. A complete reimplementation of functional normalization minimizes computational memory requirements to 5% of that required by other R packages, without increasing running time. Incorporating fixed and random fixed effects alongside functional normalization, and automated estimation of functional normalisation parameters reduces technical variation in DNA methylation levels, thus reducing false positive associations and improving power. We also demonstrate that the ability to normalize datasets distributed across physically different locations without sharing any biologically-based individual-level data may reduce heterogeneity in meta-analyses of epigenome-wide association studies. However, when batch is perfectly confounded with cases and controls functional normalization is unable to prevent spurious associations. Conclusions. meffil is available online (https://github.com/perishky/meffil/) along with tutorials covering typical use cases."}, {"title": "Fast and simple analysis of MiSeq amplicon sequencing data with MetaAmp", "url": "https://www.biorxiv.org/content/early/2017/04/27/131631", "tag": "Bioinformatics", "abstract": "Microbial community profiling by barcoded 16S rRNA gene amplicon sequencing currently has many applications in microbial ecology. The low costs of the parallel sequencing of multiplexed samples, combined with the relative ease of data processing and interpretation (compared to shotgun metagenomes) have made this an entry-level approach. Here we present the MetaAmp pipeline for processing of SSU rRNA gene and other non-coding or protein-coding amplicon sequencing data by investigators that are inexperienced with bioinformatics procedures. It accepts single-end or paired-end sequences in fasta or fastq format from various sequencing platforms. It includes read quality control, and merging of forward and reverse reads of paired-end reads. It makes use of UPARSE, Mothur, and the SILVA database for clustering, removal of chimeric reads, taxonomic classification and generation of diversity metrics. The pipeline has been validated with a mock community of known composition. MetaAmp provides a convenient web interface as well as command line interface. It is freely available at: http://ebg.ucalgary.ca/metaamp. Since its launch two years ago, MetaAmp has been used >2,800 times, by many users worldwide."}, {"title": "Inference Of The Human Polyadenylation Code", "url": "https://www.biorxiv.org/content/early/2017/04/27/130591", "tag": "Bioinformatics", "abstract": "Processing of transcripts at the 3'-end involves cleavage at a polyadenylation site followed by the addition of a poly(A)-tail. By selecting which polyadenylation site is cleaved, alternative polyadenylation enables genes to produce transcript isoforms with different 3'-ends. To facilitate the identification and treatment of disease-causing mutations that affect polyadenylation and to understand the underlying regulatory processes, a computational model that can accurately predict polyadenylation patterns based on genomic features is desirable. Previous works have focused on identifying candidate polyadenylation sites and classifying sites which may be tissue-specific. What is lacking is a predictive model of the underlying mechanism of site selection, competition, and processing efficiency in a tissue-specific manner. We develop a deep learning model that trains on 3'-end sequencing data and predicts tissue-specific site selection among competing polyadenylation sites in the 3' untranslated region of the human genome. Two neural network architectures are evaluated: one built on hand-engineered features, and another that directly learns from the genomic sequence. The hand-engineered features include polyadenylation signals, cis-regulatory elements, n-mer counts, nucleosome occupancy, and RNA-binding protein motifs. The direct-from-sequence model is inferred without prior knowledge on polyadenylation, based on a convolutional neural network trained with genomic sequences surrounding each polyadenylation site as input. Both models are trained using the TensorFlow library. The proposed polyadenylation code can predict site selection among competing polyadenylation sites in different tissues. Importantly, it does so without relying on evolutionary conservation. The model can distinguish pathogenic from benign variants that appear near annotated polyadenylation sites in ClinVar and inspect the genome to find candidate polyadenylation sites. We also provide an analysis on how different features affect the model's performance."}, {"title": "Predicting clinical outcomes from large scale cancer genomic profiles with deep survival models", "url": "https://www.biorxiv.org/content/early/2017/04/27/131367", "tag": "Bioinformatics", "abstract": "Translating the vast data generated by genomic platforms into accurate predictions of clinical outcomes is a fundamental challenge in genomic medicine. Many prediction methods face limitations in learning from the high-dimensional profiles generated by these platforms, and rely on experts to hand-select a small number of features for training prediction models. In this paper, we demonstrate how deep learning and Bayesian optimization methods that have been remarkably successful in general high-dimensional prediction tasks can be adapted to the problem of predicting cancer outcomes. We perform an extensive comparison of Bayesian optimized deep survival models and other state of the art machine learning methods for survival analysis, and describe a framework for interpreting deep survival models using a risk backpropagation technique. Finally, we illustrate that deep survival models can successfully transfer information across diseases to improve prognostic accuracy. We provide an open-source software implementation of this framework called SurvivalNet that enables automatic training, evaluation and interpretation of deep survival models."}, {"title": "Clustering gene expression time series data using an infinite Gaussian process mixture model", "url": "https://www.biorxiv.org/content/early/2017/04/26/131151", "tag": "Bioinformatics", "abstract": "Transcriptome-wide time series expression profiling is used to characterize the cellular response to environmental perturbations. The first step to analyzing transcriptional response data is often to cluster genes with similar responses. Here, we present a nonparametric model-based method, Dirichlet process Gaussian process mixture model (DPGP), which jointly models cluster number with a Dirichlet process and temporal dependencies with Gaussian processes. We demonstrate the accuracy of DPGP in comparison with state-of-the-art approaches using hundreds of simulated data sets. To further test our method, we apply DPGP to published microarray data from a microbial model organism exposed to stress and to novel RNA-seq data from a human cell line exposed to the glucocorticoid dexamethasone. We validate our clusters by examining local transcription factor binding and histone modifications. Our results demonstrate that jointly modeling cluster number and temporal dependencies can reveal novel regulatory mechanisms. DPGP software is freely available online at https://github.com/PrincetonUniversity/DP_GP_cluster."}, {"title": "Performance Evaluation of Empirical Mode Decomposition Algorithms for Mental Task Classification", "url": "https://www.biorxiv.org/content/early/2017/04/26/076646", "tag": "Bioinformatics", "abstract": "The electroencephalograph (EEG) signal is one of the monitoring techniques to observe brain functionality. EEG is most preferable technology not just because of its non-invasive and cost effective quality, but also it can detect the cognitive activity of human. Brain Computer Interface (BCI), a direct pathway between the human brain and computer, is one of the most pragmatic applications of EEG signal. Mental Task Classification (MTC) based on EEG signals is a demanding BCI. Success of application depends on the efficient analysis of these signal. Empirical Mode Decomposition (EMD) is one of filter based heuristic technique which is used to analyze EEG signal in recent past. There are several variants of EMD algorithms which have their own merits and demerits. In this paper, we have explored three different EMD algorithms on EEG data for MTC-based BCI named as Empirical Mode Decomposition (EMD),Ensemble Empirical Mode Decomposition (EEMD) and Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN). Features are extracted from EEG signal in two phases; in the first phase, the signal is decomposed into different oscillatory functions with the help of different EMD algorithm and in the second phase, eight different parameters (features) are calculated for the each function for compact representation. In this paper a new feature known as Hurst Exponent along with other feature have been investigated for mental task classification. These features are fed up into Support Vector Machine (SVM) classifier to classify the different mental tasks. We have formulated two different types of MTC, the first one is binary and second one is multi-MTC. The proposed work outperforms the existing work for both binary and multi mental tasks classification."}, {"title": "Sparse Tensor Decomposition For Haplotype Assembly Of Diploids And Polyploids", "url": "https://www.biorxiv.org/content/early/2017/04/26/130930", "tag": "Bioinformatics", "abstract": "A framework that formulates haplotype assembly as sparse tensor decomposition is proposed. The problem is cast as that of decomposing a tensor having special structural constraints and missing a large fraction of its entries into a product of two factors, U and V; tensor V reveals haplotype information while U is a sparse matrix encoding the origin of erroneous sequencing reads. An algorithm, AltHap, which reconstructs haplotypes of either diploid or polyploid organisms by solving this decomposition problem is proposed. Starting from a judiciously selected initial point, AltHap alternates between two optimization tasks to recover U and V by relying on a modified gradient descent search that exploits salient structural properties of U and V. The performance and convergence properties of AltHap are theoretically analyzed and, in doing so, guarantees on the achievable minimum error correction scores and correct phasing rate are established. AltHap was tested in a number of different scenarios and was shown to compare favorably to state-of-the-art methods in applications to haplotype assembly of diploids, and significantly outperform existing techniques when applied to haplotype assembly of polyploids."}, {"title": "GATTACA: Lightweight Metagenomic Binning With Compact Indexing Of Kmer Counts And MinHash-based Panel Selection", "url": "https://www.biorxiv.org/content/early/2017/04/26/130997", "tag": "Bioinformatics", "abstract": "We introduce GATTACA, a framework for rapid and accurate binning of metagenomic contigs from a single or multiple metagenomic samples into clusters associated with individual species. The clusters are computed using co-abundance profiles within a set of reference metagnomes; unlike previous methods, GATTACA estimates these profiles from k-mer counts stored in a highly compact index. On multiple synthetic and real benchmark datasets, GATTACA produces clusters that correspond to distinct bacterial species with an accuracy that matches earlier methods, while being up to 20x faster when the reference panel index can be computed offline and 6x faster for online co-abundance estimation. Leveraging the MinHash technique to quickly compare metagenomic samples, GATTACA also provides an efficient way to identify publicly-available metagenomic data that can be incorporated into the set of reference metagenomes to further improve binning accuracy. Thus, enabling easy indexing and reuse of publicly-available metagenomic datasets, GATTACA makes accurate metagenomic analyses accessible to a much wider range of researchers."}, {"title": "Leveraging population-based clinical quantitative phenotyping for drug repositioning", "url": "https://www.biorxiv.org/content/early/2017/04/25/130799", "tag": "Bioinformatics", "abstract": "Computational drug repositioning methods can scalably nominate approved drugs for new diseases, with reduced risk of unforeseen side effects. The majority of methods eschew individual-level phenotypes despite the promise of biomarker-driven repositioning. In this study, we propose a framework for discovering serendipitous interactions between drugs and routine clinical phenotypes in cross-sectional observational studies. Key to our strategy is the use of a healthy and non-diabetic population derived from the National Health and Nutrition Examination Survey, mitigating risk for confounding by indication. We combine complementary diagnostic phenotypes (fasting glucose and glucose response) and associate them with prescription drug usage. We then sought confirmation of phenotype-drug associations in un-identifiable member claims data from Aetna using a retrospective self-controlled case analysis approach. We identify bupropion hydrochloride as a plausible antidiabetic agent, suggesting that surveying otherwise healthy individuals cross-sectional studies can discover new drug repositioning hypotheses that have applicability to longitudinal clinical practice."}, {"title": "Ultra-Accurate Complex Disorder Prediction: Case Study Of Neurodevelopmental Disorders", "url": "https://www.biorxiv.org/content/early/2017/04/25/129775", "tag": "Bioinformatics", "abstract": "Early prediction of complex disorders (e.g., autism and other neurodevelopmental disorders) is one of the fundamental goals of precision medicine and personalized genomics. An early prediction of complex disorders can have a significant impact on increasing the effectiveness of interventions and treatments in improving the prognosis and, in many cases, enhancing the quality of life in the affected patients. Considering the genetic heritability of neurodevelopmental disorders, we are proposing a novel framework for utilizing rare coding variation for early prediction of these disorders. We provide a novel formulation for the Ultra-Accurate Disorder Prediction (UADP) problem and develop a novel combinatorial framework for solving this problem. The primary goal of this novel framework, denoted as Odin (Oracle for DIsorder predictioN), is to make an accurate prediction for a subset of affected cases while having virtually zero false positive predictions for unaffected samples. Note that in the Odin framework we will take advantage of the available functional information (e.g., pairwise coexpression of genes during brain development) to increase the prediction power beyond genes with recurrent variants. Application of our method accurately recovers an additional 8% of autism cases without known recurrent mutated genes in the training set and with a less than 0.5% false positive prediction based on our analysis of unaffected controls. Furthermore, Odin predicted a set of 391 genes that severe variants in these genes can cause autism or other developmental delay disorders."}, {"title": "Parameter Estimation In Mathematical Models Of Viral Infections Using R", "url": "https://www.biorxiv.org/content/early/2017/04/25/130674", "tag": "Bioinformatics", "abstract": "In recent years, mathematical modeling approaches have played a central role to understand and to quantify mechanisms in different viral infectious diseases. In this approach, biological-based hypotheses are expressed via mathematical relations and then tested based on empirical data. The simulation results can be used to either identify underlying mechanisms, provide predictions on infection outcomes, or evaluate the efficacy of a treatment. Conducting parameter estimation for mathematical models is not an easy task. Here we detail an approach to conduct parameter estimation and to evaluate the results using the free software R. The method is applicable to influenza virus dynamics at different complexity levels, widening experimentalists capabilities in understanding their data. The parameter estimation approach presented here can be also applied to other viral infections or biological applications."}, {"title": "Quantification of Cancer Cell Migration with an Integrated Experimental-Computational Pipeline", "url": "https://www.biorxiv.org/content/early/2017/04/25/130526", "tag": "Bioinformatics", "abstract": "We describe an integrated experimental-computational pipeline for quantifying cell migration in vitro. This pipeline is robust to image noise, open source, and user friendly. The experimental component uses the Oris cell migration assay (Platypus Technologies) to create migration regions. The computational component of the pipeline creates masks in Matlab (MathWorks) to cell-covered regions, uses a genetic algorithm to automatically select the migration region, and outputs a metric to quantify the migration of cells. In this work we demonstrate the utility of our pipeline by quantifying the effects of a drug (Taxol) and of the secreted Anterior Gradient 2 (sAGR2) protein in the migration of MDA-MB-231 cells (a breast cancer cell line). In particular, we show that blocking sAGR2 reduces migration of MDA-MB-231 cells."}, {"title": "Iroki: automatic customization for phylogenetic trees", "url": "https://www.biorxiv.org/content/early/2017/04/25/106138.1", "tag": "Bioinformatics", "abstract": "Background Phylogenetic trees are an important analytical tool for examining species and community diversity, and the evolutionary history of species. In the case of microorganisms, decreasing sequencing costs have enabled researchers to generate ever-larger sequence datasets, which in turn have begun to fill gaps in the evolutionary history of microbial groups. However, phylogenetic analyses of large sequence datasets present challenges to extracting meaningful trends from complex trees. Scientific inferences made by visual inspection of phylogenetic trees can be simplified and enhanced by customizing various parts of the tree, including label color, branch color, and other features. Yet, manual customization is time-consuming and error prone, and programs designed to assist in batch tree customization often require programming experience. To address these limitations, we developed Iroki, a program for fast, automatic customization of phylogenetic trees. Iroki allows the user to incorporate information on a broad range of metadata for each experimental unit represented in the tree. Results Iroki was applied to four existing microbial sequence datasets to demonstrate its utility in data exploration and presentation. Specifically, we used Iroki to highlight connections between viral phylogeny and host taxonomy, explore the abundance of microbial groups associated with Shiga toxin-producing Escherichia coli (STEC) in cattle, examine short-term temporal dynamics of virioplankton communities, and to search for trends in the biogeography of Zetaproteobacteria. Conclusions Iroki is an easy-to-use application having both command line and web-browser implementations for fast, automatic customization of phylogenetic trees based on user-provided categorical or continuous metadata. Iroki enables hypothesis testing through improved visualization of phylogenetic trees, streamlining the process of biological sequence data exploration and presentation. Availability Iroki can be accessed through a web browser application or via installation through RubyGems, from source, or through the Iroki Docker image. All source code and documentation is available under the GPLv3 license at https://github.com/mooreryan/iroki. The Iroki web-app is accessible at www.iroki.net or through the VIROME portal (http://virome.dbi.udel.edu), and its source code is released under GPLv3 license at https://github.com/mooreryan/iroki_web. The Docker image can be found here: https://hub.docker.com/r/mooreryan/iroki."}, {"title": "Comprehensive investigation of temporal and autism-associated cell type composition-dependent and independent gene expression changes in human brains", "url": "https://www.biorxiv.org/content/early/2017/04/25/065292", "tag": "Bioinformatics", "abstract": "The functions of human brains highly depend on the precise temporal regulation of gene expression, and substantial transcriptome changes across lifespan have been observed. The substantial transcriptome alteration in neural disorders like autism has also been observed and is thought to be important for the pathology. While the cell type composition is known to be variable in brains, it remains unclear how it contributes to the temporal and pathological transcriptome changes in brains. Here, we applied the transcriptome deconvolution procedure to the age series RNA-seq data of healthy and autism samples, to quantify the contribution of cell type composition in shaping the temporal and autism pathological transcriptome in human brains. We estimated that composition change was the primary factor of both transcriptome changes. On the other hand, genes with substantial composition-independent expression changes were also observed in both cases. Those temporal and autism pathological composition-independent changes, many of which are related to synaptic functions, indicate the important intracellular regulatory changes in human brains in both processes."}, {"title": "Genome-Wide Differential Gene Network Analysis R Software And Its Application In LnCap Prostate Cancer", "url": "https://www.biorxiv.org/content/early/2017/04/24/129742", "tag": "Bioinformatics", "abstract": "We introduce an R software package for condition-specific gene regulatory network analysis based on DC3NET algorithm. We also present an application of it on a real prostate dataset and demonstrate the benefit of the software. We performed genome-wide differential gene network analysis with the software on the LnCap androgen stimulated and deprived prostate cancer gene expression datasets (GSE18684) and inferred the androgen stimulated prostate cancer specific differential network. As an outstanding result, CXCR7 along with CXCR4 appeared to have the most important role in the androgen stimulated prostate specific genome-wide differential network. This blind estimation is strongly supported from the literature. The critical roles for CXCR4, a receptor over-expressed in many cancers, and CXCR7 on mediating tumor metastasis, along with their contributions as biomarkers of tumor behavior as well as potential therapeutic target were studied in several other types of cancers. In fact, a pharmaceutical company had already developed a therapy by inhibiting CXCR4 to block non-cancerous immuno-suppressive and pro-angiogenic cells from populating the tumor for disrupting the cancer environment and restoring normal immune surveillance functions. Considering this strong confirmation, our inferred regulatory network might reveal the driving mechanism of LnCap androgen stimulated prostate cancer. Because, CXCR4 appeared to be in the center of the largest subnetwork of our inferred differential network. Moreover, enrichment analyses for the largest subnetwork of it appeared to be significantly enriched in terms of axon guidance, fc gamma R-mediated phagocytosis and endocytosis. This also conforms with the recent literature in the field of prostate cancer. We demonstrate how to derive condition-specific gene targets from expression datasets on genome-wide level using differential gene network analysis. Our results showed that differential gene network analysis worked well in a prostate cancer dataset, which suggest the use of this approach as essential part of current expression data processing. Availability: The introduced R software package available in CRAN at https://cran.r-project.org/web/packages/dc3net and also at https://github.com/altayg/dc3net"}, {"title": "Statistically robust methylation calling for whole-transcriptome bisulfite sequencing reveals distinct methylation patterns for mouse RNAs", "url": "https://www.biorxiv.org/content/early/2017/04/24/130419", "tag": "Bioinformatics", "abstract": "Cytosine-5 RNA methylation plays an important role in several biologically and pathologically relevant processes. However, owing to methodological limitations, the transcriptome-wide distribution of this mark has remained largely unknown. We previously established RNA bisulfite sequencing as a method for the analysis of RNA cytosine-5 methylation patterns at single-base resolution. More recently, next-generation sequencing has provided opportunities to establish transcriptome-wide maps of this modification. Here we present a computational approach that integrates tailored filtering and data-driven statistical modeling to eliminate many of the artifacts that are known to be associated with bisulfite sequencing. Using RNAs from mouse embryonic stem cells we performed a comprehensive methylation analysis of mouse tRNAs, rRNAs and mRNAs. Our approach identified all known methylation marks in tRNA and two previously unknown but evolutionary conserved marks in 28S rRNA. In addition, mRNAs were found to be very sparsely methylated or not methylated at all. Finally, the tRNA-specific activity of the DNMT2 methyltransferase could be resolved at single-base resolution, which provided important further validation. Our approach can be used to profile cytosine-5 RNA methylation patterns in many experimental contexts and will be important for understanding the function of cytosine-5 RNA methylation in RNA biology and in human disease."}, {"title": "Manifold Alignment Reveals Correspondence Between Single Cell Transcriptome and Epigenome Dynamics", "url": "https://www.biorxiv.org/content/early/2017/04/24/130336", "tag": "Bioinformatics", "abstract": "Single cell genomic techniques promise to yield key insights into the dynamic interplay between gene expression and epigenetic modification. However, the experimental difficulty of performing multiple measurements on the same cell currently limits efforts to combine multiple genomic data sets into a united picture of single cell variation. We show that it is possible to construct cell trajectories, reflecting the changes that occur in a sequential biological process, from single cell ATAC-seq, bisulfite sequencing, and ChIP-seq data. In addition, we present an approach called MATCHER that computationally circumvents the experimental difficulties inherent in performing multiple genomic measurements on a single cell by inferring correspondence between single cell transcriptomic and epigenetic measurements performed on different cells of the same type. MATCHER works by first learning a separate manifold for the trajectory of each kind of genomic data, then aligning the manifolds to infer a shared trajectory in which cells measured using different techniques are directly comparable. Using scM&T-seq data, we confirm that MATCHER accurately predicts true single cell correlations between DNA methylation and gene expression without using known cell correspondence information. We also used MATCHER to infer correlations among gene expression, chromatin accessibility, and histone modifications in single mouse embryonic stem cells. These results reveal the dynamic interplay between epigenetic changes and gene expression underlying the transition from pluripotency to differentiation priming. Our work is a first step toward a united picture of heterogeneous transcriptomic and epigenetic states in single cells."}, {"title": "Protein Structural Disorder Of The Envelope V3 Loop Contributes To The Switch In Human Immunodeficiency Virus Type 1 Cell Tropism", "url": "https://www.biorxiv.org/content/early/2017/04/24/130161", "tag": "Bioinformatics", "abstract": "Human immunodeficiency virus type 1 (HIV-1) envelope gp120 is partly an intrinsically disordered (unstructured/disordered) protein as it contains regions that do not fold into well-defined protein structures. These disordered regions play important roles in HIV's life cycle, particularly, V3 loop-dependent cell entry, which determines how the virus uses two coreceptors on immune cells, the chemokine receptors CCR5 (R5), CXCR4 (X4) or both (R5X4 virus). Most infecting HIV-1 variants utilise CCR5, while a switch to CXCR4-use occurs in the majority of infections. Why does this \u2018rewiring\u2019 event occur in HIV-1 infected patients? As changes in the charge of the V3 loop are associated with this receptor switch and it has been suggested that charged residues promote structure disorder, we hypothesise that the intrinsic disorder of the V3 loop plays a role in determining cell tropism. To test this we use three independent data sets of gp120 to analyse V3 loop disorder. We find that the V3 loop of X4 virus has significantly higher intrinsic disorder tendency than R5 and R5X4 virus, while R5X4 virus has the lowest. These results indicate that structural disorder plays an important role in determining HIV-1 cell tropism and CXCR4 binding. We speculate that changes in N-linked glycosylation associated with tropism change (from R5 to X4) are required to stabilise the V3 loop with increased disorder tendency during HIV-1 evolution. We discuss the potential evolutionary mechanisms leading to the fixation of disorder promoting mutations and the adaptive potential of protein structural disorder in viral host adaptation."}, {"title": "A Computational Study About The Mechanism Of Action Of Metformin On Hepatic Gluconeogenesis, Focused On Its Ability To Create Stable Pseudo-Aromatic Copper Complexes", "url": "https://www.biorxiv.org/content/early/2017/04/24/130211", "tag": "Bioinformatics", "abstract": "Metformin is the best therapeutic choice for treating type 2 Diabetes. Despite this, and the fact it has been prescribed worldwide for decades, its mechanism of gluconeogenesis inhibition is still unknown. In the following work a novel mechanism of inhibition is suggested: that metformin performs its action on the target enzyme not as a pure molecule but, after sequestering endogenous cellular copper, as a copper complex. This result was obtained using chemoinformatics methods including homology modeling for the creation of the target enzyme's tridimensional virtual structure, molecular docking for both the determination of the movement of the prosthetic group inside its cavity and for the identification of the best ligand poses for the metformin copper complexes, and eventually pharmacophore modeling and virtual screening to find alternative virtual leads that could achieve similar effects. The simulations show the complex binding as a non competitive inhibitor to the large exit of the mitochondrial glycerophosphate dehydrogenase enzyme's FAD cavity, preventing FAD movement inside the cavity and/or quinone interaction and therefore its electron transfer function. The proposed mechanism seems to be successful at explaining a wide range of existing experimental results, both regarding measurements of metformin non-competitive inhibition of GPD2 and the role of copper and pH in its action. The virtual screening outcome of at least two similarly active purchasable molecule hints to an easy way to experimentally test the proposed mechanisms. In fact, the virtual leads are very similar to the copper complex but quite different from metformin alone, and a laboratory confirmation of their activity should plausibly imply that metformin acts in synergy with copper, giving us the ability to design new antidiabetic drugs in a novel and more rational fashion, with significant savings in research costs and efforts."}, {"title": "NetREX: Network Rewiring using EXpression - Towards Context Specific Regulatory Networks", "url": "https://www.biorxiv.org/content/early/2017/04/24/126664", "tag": "Bioinformatics", "abstract": "Understanding gene regulation is a fundamental step towards understanding of how cells function and respond to environmental cues and perturbations. An important step in this direction is the ability to infer the transcription factor (TF)-gene regulatory network (GRN). However, gene regulatory networks are typically constructed disregarding the fact that regulatory programs are conditioned on tissue type, developmental stage, sex, and other factors. Due to lack of the biological context specificity, these context-agnostic networks may not provide insight for revealing the precise actions of genes for a specific biological system under concern. Collecting multitude of features required for a reliable construction of GRNs such as physical features (TF binding, chromatin accessibility) and functional features (correlation of expression or chromatin patterns) for every context of interest is costly. Therefore, we need methods that is able to utilize the knowledge about a context-agnostic network (or a network constructed in a related context) for construction of a context specific regulatory network. To address this challenge, we developed a computational approach that utilizes expression data obtained in a specific biological context such as a particular development stage, sex, tissue type and a GRN constructed in a different but related context (alternatively an incomplete or a noisy network for the same context) to construct a context specific GRN. Our method, NetREX, is inspired by network component analysis (NCA) that estimates TF activities and their influences on target genes given predetermined topology of a TF-gene network. To predict a network under a different condition, NetREX removes the restriction that the topology of the TF-gene network is fixed and allows for adding and removing edges to that network. To solve the corresponding optimization problem, which is non-convex and non-smooth, we provide a general mathematical framework allowing use of the recently proposed Proximal Alternative Linearized Maximization technique and prove that our formulation has the properties required for convergence. We tested our NetREX on simulated data and subsequently applied it to gene expression data in adult females from 99 hemizygotic lines of the Drosophila deletion (DrosDel) panel. The networks predicted by NetREX showed higher biological consistency than alternative approaches. In addition, we used the list of recently identified targets of the Doublesex (DSX) transcription factor to demonstrate the predictive power of our method."}, {"title": "Stationary Analysis the Alternative Splicing Profile Reveals the Splicing Code", "url": "https://www.biorxiv.org/content/early/2017/04/23/129866", "tag": "Bioinformatics", "abstract": "Recent works indicated that the regulatory function of RBPs showed context dependent manners, but the details of the regulatory function of most RBPs and importance are unknown. Here we integrated hundreds of eCLIP-seq and RNA-seq from ENCODE project and used RBP-position combinations on events to predict if the events are spliced out or not and the results showed that only a small of them can regulate the alternative splicing process in a degree. We observed SRSF1, LIN28B, FMR1, SRSF7, RBM22, PRPF8, SF3B4, TIA1 and hnRNP M are important features by binding the exon or intron region respectively. SF3B4 and RBM22 show opposite regulatory function when binding exons downstream and upstream intron. This supports the asymmetric exon decision model that state the exon exclusion pathway compete with the exon inclusion pathway to regulate splicing. We also observed that some peaks locate in exon-exon junction regions and indicate some splicing related RBPs also bind on mRNAs."}, {"title": "Using Neural Networks To Improve Single-Cell RNA-Seq Data Analysis", "url": "https://www.biorxiv.org/content/early/2017/04/23/129759", "tag": "Bioinformatics", "abstract": "While only recently developed, the ability to profile expression data in single cells (scRNA-Seq) has already led to several important studies and findings. However, this technology has also raised several new computational challenges including questions related to handling the noisy and sometimes incomplete data, how to identify unique group of cells in such experiments and how to determine the state or function of specific cells based on their expression profile. To address these issues we develop and test a method based on neural networks (NN) for the analysis and retrieval of single cell RNA-Seq data. We tested various NN architectures, some biologically motivated, and used these to obtain a reduced dimension representation of the single cell expression data. We show that the NN method improves upon prior methods in both, the ability to correctly group cells in experiments not used in the training and the ability to correctly infer cell type or state by querying a database of tens of thousands of single cell profiles. Such database queries (which can be performed using our web server) will enable researchers to better characterize cells when analyzing heterogeneous scRNA-Seq samples."}, {"title": "The closed-loop pathways of signaling molecules", "url": "https://www.biorxiv.org/content/early/2017/04/23/129841", "tag": "Bioinformatics", "abstract": "The pathways of signaling molecules are important to understanding how signaling molecules regulate physiological function and also in predicting the pathological development which is important to therapeutic strategy, however the thorough knowledge of these pathways is still lack. In this paper, we used the big data concept to analyze the pathways of signaling molecules and categorize these molecules into five groups according to their origin and effect on the five organs of heart-spleen-lung-kidney-liver. Heart group includes IGF, Ang and Mg; spleen group includes ANP, aldosterone, retinoic acid and ghrelin; lung group includes FGF7, VEGF, ascorbic acid and HIF; kidney group includes calcitonin, PTHrP, Wnt and NO; and liver group includes EPO, renin, SOD, AKR and GSH. We found that each group of molecules have assisting effect on the other organ in the order of heart-spleen-lung-kidney-liver-heart, and have regulating effect on the other organ in the order of heart-lung-liver-spleen-kidney-heart. Moreover, the pathways of molecules of each group also follow these two arrangements, in which the pathways of molecules form a closed-loop that may lead to new therapeutic strategies."}, {"title": "Estimation of immune cell content in tumour tissue using single-cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/04/21/127001", "tag": "Bioinformatics", "abstract": "As interactions between the immune system and tumour cells are governed by a complex network of cell-cell interactions, knowing the specific immune cell composition of a solid tumour may be essential to predict a patient's response to immunotherapy. Here, we analyse in depth how to derive the cellular composition of a solid tumour from bulk gene expression data by mathematical deconvolution, using indication- and cell type-specific reference gene expression profiles (RGEPs) from tumour-derived single-cell RNA sequencing data. We demonstrate that tumour-derived RGEPs are essential for the successful deconvolution and that RGEPs from peripheral blood are insufficient. We distinguish nine major cell types as well as three T cell subtypes. As the ratios of CD4+, CD8+ and regulatory T cells have been shown to predict overall survival, we extended our analysis to include the estimation of prognostic ratios that may enable the application in a clinical setting. Using the tumour derived RGEPs, we can estimate, for the first time, the content of cancer associated fibroblasts, endothelial cells and the malignant cells in a patient sample by a deconvolution approach. In addition, improved tumour cell gene expression profiles can be obtained by this method by computationally removing contamination from non-malignant cells. Given the difficulty around sample preparation and storage to obtain high quality single-cell RNA-seq data in the clinical context, the presented method represents a computational solution to derive the cellular composition of a tissue sample."}, {"title": "Power Analysis Provides Bounds for Genetic Architecture and Insights to Challenges for Rare Variant Association Studies", "url": "https://www.biorxiv.org/content/early/2017/04/21/100891", "tag": "Bioinformatics", "abstract": "Genome-wide association studies are now shifting focus from analysis of common to uncommon and rare variants with an anticipation to explain additional heritability of complex traits. As power for association testing for individual rare variants may often be low, various aggregate level association tests have been proposed to detect genetic loci that may contain clusters of susceptibility variants. Typically, power calculations for such tests require specification of large number of parameters, including effect sizes and allele frequencies of individual variants, making them difficult to use in practice. In this report, we approximate power to varying degree of accuracy using a smaller number of key parameters, including the total genetic variance explained by multiple variants within a locus. We perform extensive simulation studies to assess the accuracy of the proposed approximations in realistic settings. Using the simplified power calculation methods, we then develop an analytic framework to obtain bounds on genetic architecture of an underlying trait given results from a genome-wide study and observe important implications for the completely lack of or limited number of findings in many currently reported studies. Finally, we provide insights into the required quality of annotation/functional information for identification of likely causal variants to make meaningful improvement in power of subsequent association tests. A shiny application, Power Analysis for GEnetic AssociatioN Tests (PAGEANT), in R implementing the methods is made publicly available."}, {"title": "Automated Incorporation of Pairwise Dependency in Transcription Factor Binding Site Prediction Using Dinucleotide Weight Tensors", "url": "https://www.biorxiv.org/content/early/2017/04/21/078212", "tag": "Bioinformatics", "abstract": "Gene regulatory networks are ultimately encoded by the sequence-specific binding of (TFs) to short DNA segments. Although it is customary to represent the binding specificity of a TF by a position-specific weight matrix (PSWM), which assumes each position within a site contributes independently to the overall binding affinity, evidence has been accumulating that there can be significant dependencies between positions. Unfortunately, methodological challenges have so far hindered the development of a practical and generally-accepted extension of the PSWM model. On the one hand, simple models that only consider dependencies between nearest-neighbor positions are easy to use in practice, but fail to account for the distal dependencies that are observed in the data. On the other hand, models that allow for arbitrary dependencies are prone to overfitting, requiring regularization schemes that are difficult to use in practice for non-experts. Here we present a new regulatory motif model, called dinucleotide weight tensor (DWT), that incorporates arbitrary pairwise dependencies between positions in binding sites, rigorously from first principles, and free from tunable parameters. We demonstrate the power of the method on a large set of ChIP-seq data-sets, showing that DWTs outperform both PSWMs and motif models that only incorporate nearest-neighbor dependencies. We also demonstrate that DWTs outperform two previously proposed methods. Finally, we show that DWTs inferred from ChIP-seq data also outperform PSWMs on HT-SELEX data for the same TF, suggesting that DWTs capture inherent biophysical properties of the interactions between the DNA binding domains of TFs and their binding sites. We make a suite of DWT tools available at dwt.unibas.ch, that allow users to automatically perform `motif finding', i.e. the inference of DWT motifs from a set of sequences, binding site prediction with DWTs, and visualization of DWT `dilogo' motifs."}, {"title": "fusionDB: assessing microbial diversity and environmental preferences via functional similarity networks", "url": "https://www.biorxiv.org/content/early/2017/04/20/035923", "tag": "Bioinformatics", "abstract": "Microbial functional diversification is driven by environmental factors, i.e. microorganisms inhabiting the same environmental niche tend to be more functionally similar than those from different environments. In some cases, even closely phylogenetically related microbes differ more across environments than across taxa. While microbial similarities are often reported in terms of taxonomic relationships, no existing databases directly links microbial functions to the environment. We previously developed a method for comparing microbial functional similarities on the basis of proteins translated from the sequenced genomes. Here we describe fusionDB, a novel database that uses our functional data to represent 1,374 taxonomically distinct bacteria annotated with available metadata: habitat/niche, preferred temperature, and oxygen use. Each microbe is encoded as a set of functions represented by its proteome and individual microbes are connected via common functions. Users can search fusionDB via combinations of organism names and metadata. Moreover, the web interface allows mapping new microbial genomes to the functional spectrum of reference bacteria, rendering interactive similarity networks that highlight shared functionality. fusionDB provides a fast means of comparing microbes, identifying potential horizontal gene transfer events, and highlighting key environment-specific functionality. fusionDB is publicly available at http://services.bromberglab.org/fusiondb/."}, {"title": "Slingshot: Cell lineage and pseudotime inference for single-cell transcriptomics", "url": "https://www.biorxiv.org/content/early/2017/04/19/128843", "tag": "Bioinformatics", "abstract": "Single-cell transcriptomics allows researchers to investigate complex communities of heterogeneous cells. These methods can be applied to stem cells and their descendants in order to chart the progression from multipotent progenitors to fully differentiated cells. While a number of statistical and computational methods have been proposed for analyzing cell lineages, the problem of accurately characterizing multiple branching lineages remains difficult to solve. Here, we introduce a novel method, Slingshot, for inferring multiple developmental lineages from single-cell gene expression data. Slingshot is a uniquely robust and flexible tool for inferring developmental lineages and ordering cells to reflect continuous, branching processes."}, {"title": "GeneSeqToFamily: the Ensembl Compara GeneTrees pipeline as a Galaxy workflow", "url": "https://www.biorxiv.org/content/early/2017/04/19/096529", "tag": "Bioinformatics", "abstract": "Background: Gene duplication is a major factor contributing to evolutionary novelty, and the contraction or expansion of gene families has often been associated with morphological, physiological and environmental adaptations. The study of homologous genes helps us to understand the evolution of gene families. It plays a vital role in finding ancestral gene duplication events as well as identifying genes that have diverged from a common ancestor under positive selection. There are various tools available, such as MSOAR, OrthoMCL and HomoloGene, to identify gene families and visualise syntenic information between species, providing an overview of syntenic regions evolution at the family level. Unfortunately, none of them provide information about structural changes within genes, such as the conservation of ancestral exon boundaries amongst multiple genomes. The Ensembl GeneTrees computational pipeline generates gene trees based on coding sequences and provides details about exon conservation, and is used in the Ensembl Compara project to discover gene families. Findings: A certain amount of expertise is required to configure and run the Ensembl Compara GeneTrees pipeline via command line. Therefore, we have converted the command line Ensembl Compara GeneTrees pipeline into a Galaxy workflow, called GeneSeqToFamily, and provided additional functionality. This workflow uses existing tools from the Galaxy ToolShed, as well as providing additional wrappers and tools that are required to run the workflow. Conclusions: GeneSeqToFamily represents the Ensembl Compara pipeline as a set of interconnected Galaxy tools, so they can be run interactively within the Galaxy's user-friendly workflow environment while still providing the flexibility to tailor the analysis by changing configurations and tools if necessary. Additional tools allow users to subsequently visualise gene families, produced using the workflow, using the Aequatus.js interactive tool, which has been developed as part of the Aequatus software project."}, {"title": "Pheniqs: Fast and flexible quality-aware sequence demultiplexing", "url": "https://www.biorxiv.org/content/early/2017/04/19/128512", "tag": "Bioinformatics", "abstract": "Output from high throughput sequencing instruments often exceeds what is necessary to assay a single sample. To better utilize this capacity, multiple samples are independently tagged with a unique \"barcode\" sequence and are then pooled, or \"multiplexed\", and sequenced together. Classifying, or \"demultiplexing\", the reads involves decoding the barcode sequence. Although instruments estimate the probability of incorrectly calling each nucleobase, available demultiplexers do not consult those estimates or report classification error probabilities. We present Pheniqs, a fast and flexible sequence demultiplexer and quality analyzer. In addition to providing an efficient implementation of the widespread minimum distance decoder, Pheniqs introduces a novel Phred-adjusted maximum likelihood decoder that consults base calling quality scores and estimates the probability of a barcode decoding error. Setting an upper bound on the permissible error provides an intuitive way to control demultiplexing confidence and directly influence precision and recall. Pheniqs supports FASTQ and multiple Sequence Alignment/Map formats and uses auxiliary SAM tags to report both library classification and demultiplexing error probability. Evaluation on both real and semi-synthetic data indicates that Pheniqs is faster than existing demultiplexers, substantially when demultiplexing longer reads, and achieves greater accuracy by correctly reflecting quality measurements. Implemented in multithreaded C++ and available under the terms of the AGPL-3.0 license agreement at http://github.com/biosails/pheniqs. Manual and examples are available at http://biosails.github.io/pheniqs."}, {"title": "ConsensusDriver Improves Upon Individual Algorithms For Predicting Driver Alterations In Different Cancer Types And Individual Patients \u2014 A Toolbox For Precision Oncology", "url": "https://www.biorxiv.org/content/early/2017/04/18/127985", "tag": "Bioinformatics", "abstract": "Background: In recent years, several large-scale cancer genomics studies have helped generate detailed molecular profiling datasets for many cancer types and thousands of patients. These datasets provide a unique resource for studying cancer driver prediction methods and their utility for precision oncology, both to predict driver genetic alterations in patient subgroups (e.g. defined by histology or clinical phenotype) or even individual patients. Methods: We performed the most comprehensive assessment to date of 18 driver gene prediction methods, on more than 3,400 tumour samples, from 15 cancer types, to determine their suitability in guiding precision medicine efforts. These methods have diverse approaches, which can be classified into five categories: functional impact on proteins in general (FI) or specific to cancer (FIC), cohort-based analysis for recurrent mutations (CBA), mutations with expression correlation (MEC) and methods that use gene interaction network-based analysis (INA). Results: The performance of driver prediction methods varies considerably, with concordance with a gold-standard varying from 9% to 68%. FI methods show relatively poor performance (concordance <22%) while CBA methods provide conservative results, but require large sample sizes for high sensitivity. INA methods, through the integration of genomic and transcriptomic data, and FIC methods, by training cancer-specific models, provide the best trade-off between sensitivity and specificity. As the methods were found to predict different subsets of drivers, we propose a novel consensus-based approach, ConsensusDriver, which significantly improves the quality of predictions (20% increase in sensitivity). This tool can be applied to predict driver alterations in patient subgroups (e.g. defined by histology or clinical phenotype) or even individual patients. Conclusion: Existing cancer driver prediction methods are based on very different assumptions and each of them can only detect a particular subset of driver events. Consensus-based methods, like ConsensusDriver, are thus a promising approach to harness the strengths of different driver prediction paradigms."}, {"title": "Predicting Protein Binding Affinity With Word Embeddings And Recurrent Neural Networks", "url": "https://www.biorxiv.org/content/early/2017/04/18/128223", "tag": "Bioinformatics", "abstract": "At the core of our immunological system lies a group of proteins named Major Histocompatibility Complex (MHC), to which epitopes (also proteins sometimes named antigenic determinants), bind to eliciting a response. These responses are extremely varied and of widely different nature. For instance, Killer and Helper T cells are responsible for, respectively, counteracting viral pathogens and tumorous cells. Many other types exist, but their underlying structure can be very similar due to the fact that they all are proteins and bind to the MHC receptor in a similar fashion. With this framework in mind, being able to predict with precision the structure of a protein that will elicit a specific response in the human body represents a novel computational approach to drug discovery. Although many machine learning approaches have been used, no attempt to solve this problem using Recurrent Neural Networks (RNNs) exist. We extend the current efforts in the field by applying a variety of network architectures based on RNNs and word embeddings (WE). The code is freely available and under current development at https://github.com/carlomazzaferro/mhcPreds"}, {"title": "Detecting Gene Subnetworks Under Selection In Biological Pathways", "url": "https://www.biorxiv.org/content/early/2017/04/18/128306", "tag": "Bioinformatics", "abstract": "Advances in high throughput sequencing technologies have created a gap between data production and functional data analysis. Indeed, phenotypes result from interactions between numerous genes, but traditional methods treat loci independently, missing important knowledge brought by network-level emerging properties. Therefore, evidencing selection acting on multiple genes affecting the evolution of complex traits remains challenging. In this context, gene network analysis provides a powerful framework to study the evolution of adaptive traits and facilitates the interpretation of genome-wide data. To tackle this problem, we developed a method to analyse gene networks that is suitable to evidence polygenic selection. The general idea is to search biological pathways for subnetworks of genes that directly interact with each other and that present unusual evolutionary features. Subnetwork search is a typical combinatorial optimization problem that we solve using a simulated annealing approach. We have applied our methodology to find signals of adaptation to high-altitude in human populations. We show that this adaptation has a clear polygenic basis and is influenced by many genetic components. Our approach improves on classical tests for selection based on single genes by identifying both new candidate genes and new biological processes involved in adaptation to altitude."}, {"title": "Large-Scale Structure Prediction By Improved Contact Predictions And Model Quality Assessment", "url": "https://www.biorxiv.org/content/early/2017/04/18/128231", "tag": "Bioinformatics", "abstract": "Motivation: Accurate contact predictions can be used for predicting the structure of proteins. Until recently these methods were limited to very big protein families, decreasing their utility. However, recent progress by combining direct coupling analysis with machine learning methods has made it possible to predict accurate contact maps for smaller families. To what extent these predictions can be used to produce accurate models of the families is not known. Results: We present the PconsFold2 pipeline that uses contact predictions from PconsC3, the CONFOLD folding algorithm and model quality estimations to predict the structure of a protein. We show that the model quality estimation significantly increases the number of models that reliably can be identified. Finally, we apply PconsFold2 to 6379 Pfam families of unknown structure and find that PconsFold2 can, with an estimated 90% specificity, predict the structure of up to 558 Pfam families of unknown structure. Out of these 415 have not been reported before. Availability: Datasets as well as models of all the 558 Pfam families are available at http://c3.pcons.net/. All programs used here are freely available. Contact: arne@bioinfo.se Supplementary information: No supplementary data"}, {"title": "Uncovering Robust Patterns of MicroRNA Co-Expression across Cancers using Bayesian Relevance Networks", "url": "https://www.biorxiv.org/content/early/2017/04/16/115865", "tag": "Bioinformatics", "abstract": "Co-expression networks have long been used as a tool for investigating the molecular circuitry governing biological systems. However, most algorithms for constructing co-expression networks were developed in the microarray era, before high-throughput sequencing--with its unique statistical properties--became the norm for expression measurement. Here we develop Bayesian Relevance Networks, an algorithm that uses Bayesian reasoning about expression levels to account for the differing levels of uncertainty in expression measurements between highly- and lowly-expressed entities, and between samples with different sequencing depths. It combines data from groups of samples (e.g., replicates) to estimate group expression levels and confidence ranges. It then computes uncertainty-moderated estimates of cross-group correlations between entities, and uses permutation testing to assess their statistical significance. Using large scale miRNA data from The Cancer Genome Atlas, we show that our Bayesian update of the classical Relevance Networks algorithm provides improved reproducibility in co-expression estimates and lower false discovery rates in the resulting co-expression networks. Software is available at www.perkinslab.ca/Software.html."}, {"title": "SARNAclust: Semi-Automatic Detection Of RNA Protein Binding Motifs From Immunoprecipitation Data", "url": "https://www.biorxiv.org/content/early/2017/04/16/127878", "tag": "Bioinformatics", "abstract": "RNA-protein binding is critical to gene regulation, controlling fundamental processes including splicing, translation, localization and stability, and aberrant RNA-protein interactions are known to play a role in a wide variety of diseases. However, molecular understanding of RNA-protein interactions remains limited, and in particular identification of the RNA motifs that bind proteins has long been a difficult problem. To address this challenge, we have developed a novel semi-automatic algorithm, SARNAclust, to computationally identify combined structure/sequence motifs from immunoprecipitation data. SARNAclust is, to our knowledge, the first unsupervised method that can identify RNA motifs at full structural resolution while also being able to simultaneously deconvolve multiple motifs. SARNAclust makes use of a graph kernel to evaluate similarity between sequence/structure objects, and provides the ability to isolate the impact of specific features through a bulge graph formalism. SARNAclust also includes a key method for predicting RNA secondary structure at CLIP peaks, RNApeakFold, that we have verified to be effective on synthetic motif data. We applied SARNAclust to 30 ENCODE eCLIP datasets, identifying known motifs and novel predictions. Notably, we predicted a new motif for the protein ILF3 similar to that for the splicing factor hnRNPC, providing evidence for interaction between these proteins. To validate our predictions and test specific features that impact binding, we performed a directed RNA bind-n-seq assay for two proteins: ILF3 and SLBP, in each case revealing the combined importance of RNA sequence and structure to protein binding. Availability: https://github.com/idotu/SARNAclust"}, {"title": "CCFold: Rapid And Accurate Prediction Of Coiled-Coil Structures And Application To Modelling Intermediate Filaments", "url": "https://www.biorxiv.org/content/early/2017/04/14/123869", "tag": "Bioinformatics", "abstract": "Accurate molecular structure of the protein dimer representing the elementary building block of intermediate filaments (IFs) is essential towards the understanding of the filament assembly, rationalizing their mechanical properties and explaining the effect of disease-related IF mutations. The dimer contains a ~300-residue long \u03b1-helical coiled coil which is not assessable to either direct experimental structure determination or modelling using standard approaches. At the same time, coiled coils are well-represented in structural databases. Here we present CCFold, a generally applicable threading-based algorithm which produces coiled-coil models from protein sequence only. The algorithm is based on a statistical analysis of experimentally determined structures and can handle any hydrophobic repeat patterns in addition to the most common heptads. We demonstrate that CCFold outperforms general-purpose computational folding in terms of accuracy, while being faster by orders of magnitude. By combining the CCFold algorithm and Rosetta folding we generate representative dimer models for all IF protein classes. The source code is freely available at https://github.com/biocryst/IF"}, {"title": "ChIPWig: A Random Access-Enabling Lossless And Lossy Compression Method For ChIP-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/04/14/127464", "tag": "Bioinformatics", "abstract": "Motivation: The past decade has witnessed a rapid development of data acquisition technologies that enable integrative genomic and proteomic analysis. One such technology is chromatin immunoprecipitation sequencing (ChIP-seq), developed for analyzing interactions between proteins and DNA via next-generation sequencing technologies. As ChIP-seq experiments are inexpensive and time-efficient, massive datasets from this domain have been acquired, introducing significant storage and maintenance challenges. To address the resulting Big Data problems, we propose a state-of-the-art lossless and lossy compression framework specifically designed for ChIP-seq Wig data, termed ChIPWig. Wig is a standard file format, which in this setting contains relevant read density information crucial for visualization and downstream processing. ChIPWig may be executed in two different modes: lossless and lossy. Lossless ChIPWig compression allows for random access and fast queries in the file through careful variable-length block-wise encoding. ChIPWig also stores the summary statistics of each block needed for guided access. Lossy ChIPWig, in contrast, performs quantization of the read density values before feeding them into the lossless ChIPWig compressor. Nonuniform lossy quantization leads to further reductions in the file size, while maintaining the same accuracy of the ChIP-seq peak calling and motif discovery pipeline based on the NarrowPeaks method tailor-made for Wig files. The compressors are designed using new statistical modeling approaches coupled with delta and arithmetic encoding. Results: We tested the ChIPWig compressor on a number of ChIP-seq datasets generated by the ENCODE project. Lossless ChIPWig reduces the file sizes to merely 6% of the original, and offers an average 6-fold compression rate improvement compared to bigWig. The running times for compression and decompression are comparable to those of bigWig. The compression and decompression speed rates are of the order of 0.2 MB/sec using general purpose computers. ChIPWig with random access only slightly degrades the performance and running time when compared to the standard mode. In the lossy mode, the average file sizes reduce by 2-fold compared to the lossless mode. Most importantly, near-optimal nonuniform quantization with respect to mean-square distortion does not affect peak calling and motif discovery results on the data tested."}, {"title": "PB-kPRED: Knowledge-Based Prediction Of Protein Backbone Conformation Using A Structural Alphabet", "url": "https://www.biorxiv.org/content/early/2017/04/14/127423", "tag": "Bioinformatics", "abstract": "Libraries of structural prototypes that abstract protein local structures are known as structural alphabets and have proven to be very useful in various aspects of protein structure analyses and predictions. One such library, Protein Blocks (PBs), is composed of 16 standard 5-residues long structural prototypes. This form of analyzing proteins involves drafting its structure as a string of PBs. Thus, predicting the local structure of a protein in terms of protein blocks is a step towards the objective of predicting its 3-D structure. Here a new approach, kPred, is proposed towards this aim that is independent of the evolutionary information available. It involves (i) organizing the structural knowledge in the form of a database of pentapeptide fragments extracted from all protein structures in the PDB and (ii) apply a purely knowledge-based algorithm, not relying on secondary structure predictions or sequence alignment profiles, to scan this database and predict most probable backbone conformations for the protein local structures. Based on the strategy used for scanning the database, the method was able to achieve efficient mean Q16 accuracies between 40.8% and 66.3% for a non-redundant subset of the PDB filtered at 30% sequence identity cut-off. The impact of these scanning strategies on the prediction was evaluated and is discussed. A scoring function that gives a good estimate of the accuracy of prediction was further developed. This score estimates very well the accuracy of the algorithm (R2 of 0.82). An online version of the tool is provided freely for non-commercial usage at http://www.bo-protscience.fr/kpred/."}, {"title": "Describing the Local Structure of Sequence Graphs", "url": "https://www.biorxiv.org/content/early/2017/04/13/125542", "tag": "Bioinformatics", "abstract": "Analysis of genetic variation using graph structures is an emerging paradigm of genomics. However, defining genetic sites on sequence graphs remains an open problem. Paten's invention of the ultrabubble and snarl, special subgraphs of sequence graphs which can identified with efficient algorithms, represents important first step to segregating graphs into genetic sites. We extend the theory of ultrabubbles to a special subclass where every detail of the ultrabubble can be described in a series and parallel arrangement of genetic sites. We furthermore introduce the concept of bundle structures, which allows us to recognize the graph motifs created by additional combinations of variation in the graph, including but not limited to runs of abutting single nucleotide variants. We demonstrate linear-time identification of bundles in a bidirected graph. These two advances build on initial work on ultrabubbles in bidirected graphs, and define a more granular concept of genetic site."}, {"title": "HLA class I binding prediction via convolutional neural networks", "url": "https://www.biorxiv.org/content/early/2017/04/13/099358", "tag": "Bioinformatics", "abstract": "Many biological processes are governed by protein-ligand interactions. One such example is the recognition of self and nonself cells by the immune system. This immune response process is regulated by the major histocompatibility complex (MHC) protein which is encoded by the human leukocyte antigen (HLA) complex. Understanding the binding potential between MHC and peptides can lead to the design of more potent, peptide-based vaccines and immunotherapies for infectious autoimmune diseases. We apply machine learning techniques from the natural language processing (NLP) domain to address the task of MHC-peptide binding prediction. More specifically, we introduce a new distributed representation of amino acids, name HLA-Vec, that can be used for a variety of downstream proteomic machine learning tasks. We then propose a deep convolutional neural network architecture, name HLA-CNN, for the task of HLA class I-peptide binding prediction. Experimental results show combining the new distributed representation with our HLA-CNN architecture acheives state-of-the-art results in the majority of the latest two Immune Epitope Database (IEDB) weekly automated benchmark datasets. We further apply our model to predict binding on the human genome and identify 15 genes with potential for self binding. Codes are available at https://github.com/uci-cbcl/HLA-bind."}, {"title": "Contextual Autocomplete: A Novel User Interface Using Machine Learning to Improve Ontology Usage and Structured Data Capture for Presenting Problems in the Emergency Department", "url": "https://www.biorxiv.org/content/early/2017/04/12/127092", "tag": "Bioinformatics", "abstract": "Objective To determine the effect of contextual autocomplete, a user interface that uses machine learning, on the efficiency and quality of documentation of presenting problems (chief complaints) in the emergency department (ED). Materials and Method We used contextual autocomplete, a user interface that ranks concepts by their predicted probability, to help nurses enter data about a patient's reason for visiting the ED. Predicted probabilities were calculated using a previously derived model based on triage vital signs and a brief free text note. We evaluated the percentage and quality of structured data captured using a prospective before-and-after study design. Results A total of 279,231 patient encounters were analyzed. Structured data capture improved from 26.2% to 97.2% (p<0.0001). During the post-implementation period, presenting problems were more complete (3.35 vs 3.66; p=0.0004), as precise (3.59 vs. 3.74; p=0.1), and higher in overall quality (3.38 vs. 3.72; p=0.0002). Our system reduced the mean number of keystrokes required to document a presenting problem from 11.6 to 0.6 (p<0.0001), a 95% improvement. Discussion We have demonstrated a technique that captures structured data on nearly all patients. We estimate that our system reduces the number of man-hours required annually to type presenting problems at our institution from 92.5 hours to 4.8 hours. Conclusion Implementation of a contextual autocomplete system resulted in improved structured data capture, ontology usage compliance, and data quality."}, {"title": "Evaluation of the Angus ICD9-CM Sepsis Abstraction Criteria", "url": "https://www.biorxiv.org/content/early/2017/04/12/124289", "tag": "Bioinformatics", "abstract": "Objective: Validate the infection component of the Angus International Classification of Diseases, Ninth Revision, Clinical Modification (ICD9-CM) sepsis abstraction criteria Design: Observational cohort study Setting: 55,000 visits/year Adult Emergency Department (ED) Patients: All consecutive ED patient visits between 12/16/2011 and 08/13/2012 were included in the study. Patients were excluded if there was a missing outcome measure. Interventions: None. Measurements and Main Results: The primary outcome measure was suspected infection at conclusion of the ED work-up as judged by the physician. There were 34,796 patients who presented to the ED between 12/16/11 and 8/13/12, of which 31,755 (91%) patients were included and analyzed. The original Angus sepsis abstraction criteria had a sensitivity of 55%, specificity of 97%, PPV of 82%, NPV of 88%, accuracy of 87%, and a F1 score of 0.66. The modified Angus sepsis abstraction criteria which includes codes added after the original publication had a sensitivity of 65%, specificity of 96%, PPV of 81%, NPV of 91%, accuracy of 89%, and F1 score of 0.72. Conclusions: In our study, the Angus abstraction criteria have high specificity (97%), but moderate sensitivity (55%) in identifying patients with suspected infection as defined by physician at the time of disposition from the emergency department. Given these findings, it is likely that we are underestimating the true incidence of sepsis in the United States and worldwide."}, {"title": "Derivation and Validation of a Record Linkage Algorithm between EMS and the Emergency Department", "url": "https://www.biorxiv.org/content/early/2017/04/12/124313", "tag": "Bioinformatics", "abstract": "Background: Linking EMS electronic patient care reports (ePCRs) to ED records can provide clinicians access to vital information that can alter management. It can also create rich databases for research and quality improvement. Unfortunately, previous attempts at ePCR - ED record linkage have had limited success. Objective: To derive and validate an automated record linkage algorithm between EMS ePCR's and ED records using supervised machine learning. Methods: All consecutive ePCR's from a single EMS provider between June 2013 and June 2015 were included. A primary reviewer matched ePCR's to a list of ED patients to create a gold standard. Age, gender, last name, first name, social security number (SSN), and date of birth (DOB) were extracted. Data was randomly split into 80%/20% training and test data sets. We derived missing indicators, identical indicators, edit distances, and percent differences. A multivariate logistic regression model was trained using 5k fold cross-validation, using label k-fold, L2 regularization, and class re-weighting. Results: A total of 14,032 ePCRs were included in the study. Inter-rater reliability between the primary and secondary reviewer had a Kappa of 0.9. The algorithm had a sensitivity of 99.4%, a PPV of 99.9% and AUC of 0.99 in both the training and test sets. DOB match had the highest odd ratio of 16.9, followed by last name match (10.6). SSN match had an odds ratio of 3.8. Conclusions: We were able to successfully derive and validate a probabilistic record linkage algorithm from a single EMS ePCR provider to our hospital EMR."}, {"title": "miniMDS: 3D structural inference from high-resolution Hi-C data", "url": "https://www.biorxiv.org/content/early/2017/04/12/122473", "tag": "Bioinformatics", "abstract": "Motivation: Recent experiments have provided Hi-C data at resolution as high as 1 Kbp. However, 3D structural inference from high-resolution Hi-C datasets is often computationally unfeasible using existing methods. Results: We have developed miniMDS, an approximation of multidimensional scaling (MDS) that partitions a Hi-C dataset, performs high-resolution MDS separately on each partition, and then reassembles the partitions using low-resolution MDS. miniMDS is faster, more accurate, and uses less memory than existing methods for inferring the human genome at high resolution (10 Kbp). Availability: A Python implementation of miniMDS is available on GitHub: https://github.com/seqcode/miniMDS"}, {"title": "Consensus Development Of A Modern Ontology Of Emergency Department Presenting Problems: The HierArchical Presenting Problem OntologY (HaPPy)", "url": "https://www.biorxiv.org/content/early/2017/04/12/126870", "tag": "Bioinformatics", "abstract": "Objective: Numerous attempts have been made to create a standardized \u201cpresenting problem\u201d or \u201cchief complaint\u201d list to characterize the nature of an Emergency Department visit. Previous attempts have failed to gain widespread adoption as none were freely sharable and contained the right level of specificity, structure, and clinical relevance to gain acceptance by the larger emergency medicine community. Using real-world data, we constructed a presenting problem list that addresses these challenges. Materials and Methods: We prospectively captured the presenting problems for 112,612 consecutive emergency department patient encounters at an urban, academic, Level I trauma center. No patients were excluded. We used a modified Delphi consensus process to iteratively derive our system using real-world data. We used the first 95% of encounters to derive our ontology; the remaining 5% for validation. All concepts were mapped to SNOMED-CT. Results: Our system consists of a polyhierarchical ontology containing 690 unique concepts, 2,113 synonyms, and 30,605 non-visible descriptions to correct misspellings and non-standard terminology. Our ontology successfully captured structured data for 95.8% of visits in our validation dataset. Discussion and Conclusion: We present the HierArchical Presenting Problem ontologY (HaPPy). This ontology was empirically derived then iteratively validated by an expert consensus panel. HaPPy contains 690 presenting problem concepts, each concept being mapped to SNOMED-CT. This freely sharable ontology should help to facilitate presenting problem based quality metrics, research, and patient care."}, {"title": "Transcriptome-Wide Prediction Of lncRNA-RNA Interactions By A Thermodynamics Algorithm", "url": "https://www.biorxiv.org/content/early/2017/04/12/126946", "tag": "Bioinformatics", "abstract": "Motivation: The discovery of thousands of long noncoding RNAs (lncRNAs) in mammals raises a question about their functionality. It has been shown that some of them function post-transcriptionally via formation of inter-molecular duplexes. Sequence alignment tools are frequently used for transcriptome-wide prediction of RNA-RNA interactions. However, such approaches have poor prediction accuracy since they ignore RNA secondary structure and interaction energy. On the other hand, application of the thermodynamics- based algorithms to long transcripts is not computationally feasible on a large scale. Results: Here we describe a new computational pipeline ASSA that combines sequence alignment and thermodynamics tools for efficient prediction of RNA-RNA interactions between long transcripts. ASSA outperforms four other tools in terms of the Area Under the Curve. ASSA predictions for the lncRNA HOTAIR confirm that it binds to the chromatin through hybridization with the nascent transcripts. Analysis of the 49 murine lncRNA knockdown experiments reveals one transcript that may regulate its targets via RNA-RNA interactions. Availability: ASSA is available at https://assa.sourceforge.net/."}, {"title": "Characterizing RNA Pseudouridylation By Convolutional Neural Networks", "url": "https://www.biorxiv.org/content/early/2017/04/12/126979", "tag": "Bioinformatics", "abstract": "The most prevalent post-transcriptional RNA modification, pseudouridine (\u03a8), also known as the fifth ribonucleoside, is widespread in rRNAs, tRNAs, snRNAs, snoRNAs and mRNAs. Pseudouridines in RNAs are implicated in many aspects of post-transcriptional regulation, such as the maintenance of translation fidelity, control of RNA stability and stabilization of RNA structure. However, our understanding of the functions, mechanisms as well as precise distribution of pseudourdines (especially in mRNAs) still remains largely unclear. Though thousands of RNA pseudouridylation sites have been identified by high-throughput experimental techniques recently, the landscape of pseudouridines across the whole transcriptome has not yet been fully delineated. In this study, we present a highly effective model, called PULSE (PseudoUridyLation Sites Estimator), to predict novel \u03a8 sites from large-scale profiling data of pseudouridines and characterize the contextual sequence features of pseudouridylation. PULSE employs a deep learning framework, called convolutional neural network (CNN), which has been successfully and widely used for sequence pattern discovery in the literature. Our extensive validation tests demonstrated that PULSE can outperform conventional learning models and achieve high prediction accuracy, thus enabling us to further characterize the transcriptome-wide landscape of pseudouridine sites. Overall, PULSE can provide a useful tool to further investigate the functional roles of pseudouridylation in post-transcriptional regulation."}, {"title": "MAPseq: Improved Speed, Accuracy And Consistency In Ribosomal RNA Sequence Analysis", "url": "https://www.biorxiv.org/content/early/2017/04/12/126953", "tag": "Bioinformatics", "abstract": "Metagenomic sequencing has become crucial to studying microbial communities, but meaningful taxonomic analysis and inter-comparison of such data are still hampered by technical limitations, between-study design variability and inconsistencies between taxonomies used. Here we present MAPseq, a framework for reference-based rRNA metagenomic analysis that is up to 30% more accurate (F1/2 score) and up to one hundred times faster than existing solutions, providing in a single run multiple taxonomy classifications and hierarchical OTU mappings, for both amplicon and shotgun sequencing strategies, and for datasets of virtually any size. Availability: Source code and binaries are freely available at http://meringlab.org/software/mapseq/"}, {"title": "ASAP: a Web-based platform for the analysis and inter-active visualization of single-cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2017/04/11/096222", "tag": "Bioinformatics", "abstract": "Motivation: Single-cell RNA-sequencing (scRNA-seq) allows whole transcriptome profiling of thousands of individual cells, enabling the molecular exploration of tissues at the cellular level. Such analytical capacity is of great interest to many research groups in the world, yet, these groups often lack the expertise to handle complex scRNA-seq data sets. Results: We developed a fully integrated, web-based platform aimed at the complete analysis of scRNA-seq data post genome alignment: from the parsing, filtering, and normalization of the input count data files, to the visual repre-sentation of the data, identification of cell clusters, differentially expressed genes (including cluster-specific marker genes), and functional gene set enrichment. This Automated Single-cell Analysis Pipeline (ASAP) combines a wide range of commonly used algorithms with sophisticated visualization tools. Compared with existing scRNA-seq analy-sis platforms, researchers (including those lacking computational expertise) are able to interact with the data in a straightforward fashion and in real time. Furthermore, given the overlap between scRNA-seq and bulk RNA-seq anal-ysis workflows, ASAP should conceptually be broadly applicable to any RNA-seq dataset. As a validation, we demon-strate how we can use ASAP to simply reproduce the results from a single-cell study of 91 mouse cells involving five distinct cell types. Availability: The tool is freely available at http://asap.epfl.ch"}, {"title": "SuperTranscript: a data driven reference for analysis and visualisation of transcriptomes", "url": "https://www.biorxiv.org/content/early/2017/04/11/077750", "tag": "Bioinformatics", "abstract": "Numerous methods have been developed to analyse RNA sequencing data, but most rely on the availability of a reference genome, making them unsuitable for non-model organisms. De novo transcriptome assembly can build a reference transcriptome from the non-model sequencing data, but falls short of allowing most tools to be applied. Here we present superTranscripts, a simple but powerful solution to bridge that gap. SuperTranscripts are a substitute for a reference genome, consisting of all the unique exonic sequence, in transcriptional order, such that each gene is represented by a single sequence. We demonstrate how superTranscripts allow visualization, variant detection and differential isoform detection in non-model organisms, using widely applied methods that are designed to work with reference genomes. SuperTranscripts can also be applied to model organisms to enhance visualization and discover novel expressed sequence. We describe Lace, software to construct superTranscripts from any set of transcripts including de novo assembled transcriptomes. In addition we used Lace to combine reference and assembled transcriptomes for chicken and recovered the sequence of hundreds of gaps in the reference genome."}, {"title": "Evaluation of tools for long read RNA-seq splice-aware alignment", "url": "https://www.biorxiv.org/content/early/2017/04/11/126656", "tag": "Bioinformatics", "abstract": "Motivation: High-throughput sequencing has transformed the study of gene expression levels through RNA-seq, a technique that is now routinely used by various fields, such as genetic research or diagnostics. The advent of third generation sequencing technologies providing significantly longer reads opens up new possibilities. However, the high error rates common to these technologies set new bioinformatics challenges for the gapped alignment of reads to their genomic origin. In this study, we have explored how currently available RNA-seq splice-aware alignment tools cope with increased read lengths and error rates. All tested tools were initially developed for short NGS reads, but some have claimed support for long PacBio or even ONT MinION reads. Results: The tools were tested on synthetic and real datasets from the PacBio and ONT MinION technologies, and both alignment quality and resource usage were compared across tools. The effect of error correction of long reads was explored, both using self-correction and correction with an external short reads dataset. A tool was developed for evaluating RNA-seq alignment results. This tool can be used to compare the alignment of simulated reads to their genomic origin, or to compare the alignment of real reads to a set of annotated transcripts. Our tests show that while some RNA-seq aligners were unable to cope with long error-prone reads, others produced overall good results. We further show that alignment accuracy can be improved using error-corrected reads."}, {"title": "Generalising Better: Applying Deep Learning To Integrate Deleteriousness Prediction Scores For Whole-Exome SNV Studies", "url": "https://www.biorxiv.org/content/early/2017/04/11/126532", "tag": "Bioinformatics", "abstract": "Many automatic classifiers were introduced to aid inference of phenotypical effects of uncategorised nsSNVs (nonsynonymous Single Nucleotide Variations) in theoretical and medical applications. Lately, several meta-estimators have been proposed that combine different predictors, such as PolyPhen and SIFT, to integrate more information in a single score. Although many advances have been made in feature design and machine learning algorithms used, the shortage of high-quality reference data along with the bias towards intensively studied in vitro models call for improved generalisation ability in order to further increase classification accuracy and handle records with insufficient data. Since a meta-estimator basically combines different scoring systems with highly complicated nonlinear relationships, we investigated how deep learning (supervised and unsupervised), which is particularly efficient at discovering hierarchies of features, can improve classification performance. While it is believed that one should only use deep learning for high-dimensional input spaces and other models (logistic regression, support vector machines, Bayesian classifiers, etc) for simpler inputs, we still believe that the ability of neural networks to discover intricate structure in highly heterogenous datasets can aid a meta-estimator. We compare the performance with various popular predictors, many of which are recommended by the American College of Medical Genetics and Genomics (ACMG), as well as available deep learning-based predictors. Thanks to hardware acceleration we were able to use a computationally expensive genetic algorithm to stochastically optimise hyper-parameters over many generations. Overfitting was hindered by noise injection and dropout, limiting coadaptation of hidden units. Although we stress that this work was not conceived as a tool comparison, but rather an exploration of the possibilities of deep learning application in ensemble scores, our results show that even relatively simple modern neural networks can significantly improve both prediction accuracy and coverage. We provide open-access to our finest model at http://score.generesearch.ru"}, {"title": "Framework For Reanalysis Of Publicly Available Affymetrix\u00ae Genechip\u00ae Data Sets Based On Functional Regions Of Interest", "url": "https://www.biorxiv.org/content/early/2017/04/11/126573", "tag": "Bioinformatics", "abstract": "Background: Since the introduction of microarrays in 1995, researchers world-wide have used both commercial and custom-designed microarrays for understanding differential expression of transcribed genes. Public databases such as ArrayExpress and the Gene Expression Omnibus (GEO) have made millions of samples readily available. One main drawback to microarray data analysis involves the selection of probes to represent a specific transcript of interest, particularly in light of the fact that transcript-specific knowledge (notably alternative splicing) is dynamic in nature. Results: We therefore developed a framework for reannotating and reassigning probe groups for Affymetrix\u00ae GeneChip\u00ae technology based on functional regions of interest. This framework addresses three issues of Affymetrix\u00ae GeneChip\u00ae data analyses: removing nonspecific probes, updating probe target mapping based on the latest genome knowledge and grouping probes into gene, transcript and region-based (UTR, individual exon, CDS) probe sets. Updated gene and transcript probe sets provide more specific analysis results based on current genomic and transcriptomic knowledge. The framework selects unique probes, aligns them to gene annotations and generates a custom Chip Description File (CDF). The analysis reveals only 87% of the Affymetrix\u00ae GeneChip\u00ae HG-U133 Plus 2 probes uniquely align to the current hg38 human assembly without mismatches. We also tested new mappings on the publicly available data series using rat and human data from GSE48611 and GSE72551 obtained from GEO, and illustrate that functional grouping allows for the subtle detection of regions of interest likely to have phenotypical consequences. Conclusion: Through reanalysis of the publicly available data series GSE48611 and GSE72551, we profiled the contribution of UTR and CDS regions to the gene expression levels globally. The comparison between region and gene based results indicated that the detected expressed genes by gene-based and region-based CDFs show high consistency and regions based results allows us to detection of changes in transcript formation."}, {"title": "High Accuracy Base Calls in Nanopore Sequencing", "url": "https://www.biorxiv.org/content/early/2017/04/11/126680", "tag": "Bioinformatics", "abstract": "Nanopore sequencing has introduced the ability to sequence long stretches of DNA, enabling the resolution of repeating segments, or paired SNPs across long stretches of DNA. Unfortunately significant error rates >15%, introduced through systematic and random noise inhibit downstream analysis. We propose a novel method, using unsupervised learning, to correct biologically amplified reads before downstream analysis proceeds. We also demonstrate that our method has performance com- parable to existing techniques without limiting the detection of repeats, or the length of the input sequence."}, {"title": "BasePlayer: Versatile Analysis Software For Large-Scale Genomic Variant Discovery", "url": "https://www.biorxiv.org/content/early/2017/04/11/126482", "tag": "Bioinformatics", "abstract": "Next-generation sequencing (NGS) is being routinely applied in life sciences and clinical practice, where the interpretation of the resulting massive data has become a critical challenge. Computational workflows, such as the Broad GATK, have been established to take raw sequencing data and produce processed data for downstream analyses. Consequently, results of these computationally demanding workflows, consisting of e.g. sequence alignment and variant calling, are increasingly being provided for customers by sequencing and bioinformatics facilities. However, downstream variant analysis, whole-genome level in particular, has been lacking a multi-purpose tool, which could take advantage of rapidly growing genomic information and integrate genetic variant, sequence, genomic annotation and regulatory (e.g. ENCODE) data interactively and in a visual fashion. Here we introduce a highly efficient and user-friendly software, BasePlayer (http://baseplayer.fi), for biological discovery in large-scale NGS data. BasePlayer enables tightly integrated comparative variant analysis and visualization of thousands of NGS data samples and millions of variants, with numerous applications in disease, regulatory and population genomics. Although BasePlayer has been designed primarily for whole-genome and exome sequencing data, it is well-suited to various study settings, diseases and organisms by supporting standard and upcoming file formats. BasePlayer transforms an ordinary desktop computer into a large-scale genomic research platform, enabling also a non-technical user to perform complex comparative variant analyses, population frequency filtering and genome level annotations under intuitive, scalable and highly-responsive user interface to facilitate everyday genetic research as well as the search of novel discoveries."}, {"title": "JEPEGMIX2: improved gene-level joint analysis of eQTLs in cosmopolitan cohorts.", "url": "https://www.biorxiv.org/content/early/2017/04/11/126300", "tag": "Bioinformatics", "abstract": "ABSTRACT Motivation : To increase detection power, researchers use gene level analysis methods to aggregate weak marker signals. Due to gene expression controlling biological processes, researchers proposed aggregating signals for expression Quantitative Trait Loci (eQTL). Most gene-level eQTL methods make statistical inferences based on i) summary statistics from genome-wide association studies (GWAS) and ii) linkage disequilibrium (LD) patterns from a relevant reference panel. While most such tools assume homogeneous cohorts, our Gene-level Joint Analysis of functional SNPs in Cosmopolitan Cohorts (JEPEGMIX) method accommodates cosmopolitan cohorts by using heterogeneous panels. However, JEPGMIX relies on brain eQTLs from older gene expression studies and does not adjust for background enrichment in GWAS signals. Results: We propose JEPEGMIX2, an extension of JEPEGMIX. When compared to JPEGMIX, it uses i) cis-eQTL SNPs from the latest expression studies and ii) brains specific (sub)tissues and tissues other than brain. JEPEGMIX2 also i) avoids accumulating averagely enriched polygenic information by adjusting for background enrichment and ii), to avoid an increase in false positive rates for studies with numerous highly enriched (above the background) genes, it outputs gene q-values based on Holm adjustment of p-values."}, {"title": "De novo Identification of DNA Modifications Enabled by Genome-Guided Nanopore Signal Processing", "url": "https://www.biorxiv.org/content/early/2017/04/10/094672", "tag": "Bioinformatics", "abstract": "Advances in nanopore sequencing technology have enabled investigation of the full catalogue of covalent DNA modifications. We present the first algorithm for the identification of modified nucleotides without the need for prior training data along with the open source software implementation, nanoraw. Nanoraw accurately assigns contiguous raw nanopore signal to genomic positions, enabling novel data visualization, and increasing power and accuracy for the discovery of covalently modified bases in native DNA. Ground truth case studies utilizing synthetically methylated DNA show the capacity to identify three distinct methylation marks, 4mC, 5mC, and 6mA, in seven distinct sequence contexts without any changes to the algorithm. We demonstrate quantitative reproducibility simultaneously identifying 5mC and 6mA in native E. coli across biological replicates processed in different labs. Finally we propose a pipeline for the comprehensive discovery of DNA modifications in any genome without a priori knowledge of their chemical identities."}, {"title": "An image processing method for metagenomic binning: multi-resolution genomic binary patterns", "url": "https://www.biorxiv.org/content/early/2017/04/10/096719", "tag": "Bioinformatics", "abstract": "Bioinformatics methods typically use textual representations of genetic information, represented computationally as strings or sub-strings of the characters A, T, G and C. Image processing methods offer a rich source of alternative descriptors as they are designed to work in the presence of noisy data without the need for exact matching. We introduce a method, multi-resolution local binary patterns (MLBP) from image processing to extract local 'texture' changes from nucleotide sequence data. We apply this feature space to the alignment-free binning of metagenomic data. The effectiveness of MLBP is demonstrated using both simulated and real human gut microbial communities. The intuition behind our method is the MLBP feature vectors permit sequence comparisons without the need for explicit pairwise matching. Sequence reads or contigs can then be represented as vectors and their 'texture' compared efficiently using state-of-the-art machine learning algorithms to perform dimensionality reduction to capture eigengenome information and perform clustering (here using RSVD and BH-tSNE). We demonstrate this approach outperforms existing methods based on k-mer frequency. The image processing method, MLBP, thus offers a viable alternative feature space to textual representations of sequence data. The source code for our Multi-resolution Genomic Binary Patterns method can be found at https://github.com/skouchaki/MrGBP."}, {"title": "Dense And Accurate Whole-Chromosome Haplotyping Of Individual Genomes", "url": "https://www.biorxiv.org/content/early/2017/04/10/126136", "tag": "Bioinformatics", "abstract": "The diploid nature of the genome is neglected in many analyses done today, where a genome is perceived as a set of unphased variants with respect to a reference genome. Many important biological phenomena such as compound heterozygosity and epistatic effects between enhancers and target genes, however, can only be studied when haplotype-resolved genomes are available. This lack of haplotype-level analyses can be explained by a dearth of methods to produce dense and accurate chromosome-length haplotypes at reasonable costs. Here we introduce an integrative phasing strategy that combines global, but sparse haplotypes obtained from strand-specific single cell sequencing (Strand-seq) with dense, yet local, haplotype information available through long-read or linked-read sequencing. Our experiments provide comprehensive guidance on favorable combinations of Strand-seq libraries and sequencing coverages to obtain complete and genome-wide haplotypes of a single individual genome (NA12878) at manageable costs. We were able to reliably assign > 95% of alleles to their parental haplotypes using as few as 10 Strand-seq libraries in combination with 10-fold coverage PacBio data or, alternatively, 10X Genomics linked-read sequencing data. We conclude that the combination of Strand-seq with different sequencing technologies represents an attractive solution to chart the unique genetic variation of diploid genomes."}, {"title": "QuickRNASeq: Guide For Pipeline Implementation And For Interactive Results Visualization", "url": "https://www.biorxiv.org/content/early/2017/04/10/125856", "tag": "Bioinformatics", "abstract": "Sequencing of transcribed RNA molecules (RNA-seq) has been used wildly for studying cell transcriptomes in bulk or at the single-cell level and is becoming the de facto technology for investigating gene expression level changes in various biological conditions, on the time course, and under drug treatments. Furthermore, RNA-Seq data helped identify fusion genes that are related to certain cancers. Differential gene expression before and after drug treatments provides insights to mechanism of action, pharmacodynamics of the drugs, and safety concerns. Because each RNA-seq run generates tens to hundreds of millions of short reads with size ranging from 50bp-200bp, a tool that deciphers these short reads to an integrated and digestible analysis report is in high demand. QuickRNASeq is an application for large-scale RNA-seq data analysis and real-time interactive visualization of complex data sets. This application automates the use of several of the best open-source tools to efficiently generate user-friendly, easy to share, and ready to publish reports. The visualization features of the application have been further improved since its first publication in early 2016. The original QuickRNASeq publication provided details of background, software selection, and implementation. Here, we outline the steps required to implement QuickRNASeq in user own environment, as well as demonstrate some basic yet powerful utilities of the advanced interactive visualization modules in the report."}, {"title": "Faucet: streaming de novo assembly graph construction", "url": "https://www.biorxiv.org/content/early/2017/04/08/125658", "tag": "Bioinformatics", "abstract": "Motivation: We present Faucet, a 2-pass streaming algorithm for assembly graph construction. Faucet builds an assembly graph incrementally as each read is processed. Thus, reads need not be stored locally, as they can be processed while downloading data and then discarded. We demonstrate this functionality by performing streaming graph assembly of publicly available data, and observe that the ratio of disk use to raw data size decreases as coverage is increased. Results: Faucet pairs the de Bruijn graph obtained from the reads with additional meta-data derived from them. We show these metadata - coverage counts collected at junction k-mers and connections bridging between junction pairs - contain most salient information needed for assembly, and demonstrate they enable cleaning of metagenome assembly graphs, greatly improving contiguity while maintaining accuracy. We compared Faucet's resource use and assembly quality to state of the art metagenome assemblers, as well as leading resource-efficient genome assemblers. Faucet used orders of magnitude less time and disk space than the specialized metagenome assemblers MetaSPAdes and Megahit, while also improving on their memory use; this broadly matched performance of other assemblers optimizing resource efficiency - namely, Minia and LightAssembler. However, on metagenomes tested, Faucet's outputs had 14-110% higher mean NGA50 lengths compared to Minia, and 2-11-fold higher mean NGA50 lengths compared to LightAssembler, the only other streaming assembler available."}, {"title": "GAPPadder: A Sensitive Approach for Closing Gaps on Draft Genomes with Short Sequence Reads", "url": "https://www.biorxiv.org/content/early/2017/04/07/125534", "tag": "Bioinformatics", "abstract": "Closing gaps in draft genomes is an important post processing step in genome assembly. It leads to more complete genomes, which benefits downstream genome analysis such as annotation and genotyping. Several tools have been developed for gap closing. However, these tools don't fully utilize the information contained in the sequence data. For example, while it is known that many gaps are caused by genomic repeats, existing tools often ignore many sequence reads that originate from a repeat-related gap. In this paper, we propose a new approach called GAPPadder for gap closing. The main advantage of GAPPadder is that it uses more information in sequence data for gap closing. In particular, GAPPadder finds and uses reads that originate from repeate-related gaps. We show that these repeat-associated reads are useful for gap closing, even though they are ignored by all existing tools. Other main features of GAPPadder include utilizing the information in sequence reads with different insert sizes and performing two-stage local assembly of gap sequences. We compare GAPPadder with GapCloser, GapFiller and Sealer on one bacterial genome, human chromosome 14 and the human whole genome with paired-end and mate-paired reads with both short and long insert sizes. Empirical results show that GAPPadder can close more gaps than these existing tools. Besides closing gaps on draft genomes assembled only from short sequence reads, GAPPadder can also be used to close gaps for draft genomes assembled with long reads. We show GAPPadder can close gaps on the bed bug genome and the Asian sea bass genome that are assembled partially and fully with long reads respectively. We also show GAPPadder is efficient in both time and memory usage. The software tool, GAPPadder, is available for download at https://github.com/Reedwarbler/GAPPadder."}, {"title": "GSA-Genie: a web application for gene set analysis", "url": "https://www.biorxiv.org/content/early/2017/04/07/125443", "tag": "Bioinformatics", "abstract": "Gene set analysis is often used to interpret results from upstream analysis through predefined gene sets that are linked to biological features such as cell cycle or tumorgenesis. Gene sets have been defined in the literature via various criteria and are archived by numerous databases. We compiled over 2.3 million gene sets from 17 sources, and made them accessible through a web application, GSA-Genie. Selected gene sets can be analyzed online using one of 16 statistical methods. These methods can be grouped into two strategies: test of gene set over-representation within a gene list, or comparison of a gene-level statistics between gene set and background. GSA-Genie operates on a Shiny web server, hosted in a cloud instance within Amazon Web Services. GSA-Genie offers a broad selection of gene sets and statistical methods comparing to existing tools. GSA-Genie is freely available at http://gsagenie.awsomics.org."}, {"title": "DeepBound: Accurate Identification of Transcript Boundaries via Deep Convolutional Neural Fields", "url": "https://www.biorxiv.org/content/early/2017/04/07/125229", "tag": "Bioinformatics", "abstract": "Motivation: Reconstructing the full-length expressed transcripts (a.k.a. the transcript assembly problem) from the short sequencing reads produced by RNA-seq protocol plays a central role in identifying novel genes and transcripts as well as in studying gene expressions and gene functions. A crucial step in transcript assembly is to accurately determine the splicing junctions and boundaries of the expressed transcripts from the reads alignment. In contrast to the splicing junctions that can be efficiently detected from spliced reads, the problem of identifying boundaries remains open and challenging, due to the fact that the signal related to boundaries is noisy and weak. Results: We present DeepBound, an effective approach to identify boundaries of expressed transcripts from RNA-seq reads alignment. In its core DeepBound employs deep convolutional neural fields to learn the hidden distributions and patterns of boundaries. To accurately model the transition probabilities and to solve the label-imbalance problem, we novelly incorporate the AUC (area under the curve) score into the optimizing objective function. To address the issue that deep probabilistic graphical models requires large number of labeled training samples, we propose to use simulated RNA-seq datasets to train our model. Through extensive experimental studies on both simulation datasets of two species and biological datasets, we show that DeepBound consistently and significantly outperforms the two existing methods. Availability: DeepBound is freely available at https://github.com/realbigws/DeepBound. Contact: mingfu.shao@cs.cmu.edu, realbigws@gmail.com"}, {"title": "De novo assembly of microbial genomes from human gut metagenomes using barcoded short read sequences", "url": "https://www.biorxiv.org/content/early/2017/04/07/125211", "tag": "Bioinformatics", "abstract": "Shotgun short-read sequencing methods facilitate study of the genomic content and strain-level architecture of complex microbial communities. However, existing methodologies do not capture structural differences between closely related co-occurring strains such as those arising from horizontal gene transfer and insertion sequence mobilization. Recent techniques that partition large DNA molecules, then barcode short fragments derived from them, produce short-read sequences containing long-range information. Here, we present a novel application of these short-read barcoding techniques to metagenomic samples, as well as Athena, an assembler that uses these barcodes to produce improved metagenomic assemblies. We apply our approach to longitudinal samples from the gut microbiome of a patient with a hematological malignancy. This patient underwent an intensive regimen of multiple antibiotics, chemotherapeutics and immunosuppressants, resulting in profound disruption of the microbial gut community and eventual domination by Bacteroides caccae. We significantly improve draft completeness over conventional techniques, uncover strains of B. caccae differing in the positions of transposon integration, and find the abundance of individual strains to fluctuate widely over the course of treatment. In addition, we perform RNA sequencing to investigate relative transcription of genes in B. caccae, and find overexpression of antibiotic resistance genes in our de novo assembled draft genome of B. caccae coinciding with both antibiotic administration and the appearance of proximal transposons harboring a putative bacterial promoter region. Our approach produces overall improvements in contiguity of metagenomic assembly and enables assembly of whole classes of genomic elements inaccessible to existing short-read approaches."}, {"title": "RNA-Seq 2G: Online Analysis Of Differential Gene Expression With Comprehensive Options Of Statistical Methods", "url": "https://www.biorxiv.org/content/early/2017/04/07/122747", "tag": "Bioinformatics", "abstract": "RNA-seq has become the most prevalent technology for measuring genome-wide gene expression, but the best practices for processing and analysing RNA-seq data are still an open question. Many statistical methods have been developed to identify genes differentially expressed between sample groups from RNA-seq data. These methods differ by their data distribution assumptions, choice of statistical test, and computational resource requirements. Over 25 methods of differential expression detection were validated and made available through a user-friendly web portal, RNA-seq 2G. All methods are suitable for analysing differential gene expression between two groups of samples. They commonly use a read count matrix derived from RNA-seq data as input and statistically compare groups for each gene. The web portal uses a Shiny app front-end and is hosted by a cloud-based server provided by Amazon Web Service. The comparison of methods showed that the data distribution assumption is the major determinant of differences between methods. Most methods are more likely to find that longer genes are differentially expressed, which substantially impacts downstream gene set-level analysis. Combining results from multiple methods can potentially diminish this bias. RNA-seq 2G makes the analysis of RNA-seq data more accessible and efficient, and is freely available at http://rnaseq2g.awsomics.org."}, {"title": "ARIBA: rapid antimicrobial resistance genotyping directly from sequencing reads", "url": "https://www.biorxiv.org/content/early/2017/04/07/118000", "tag": "Bioinformatics", "abstract": "Antimicrobial resistance (AMR) is one of the major threats to human and animal health worldwide, yet few high-throughput tools exist to analyse and predict the resistance of a bacterial isolate from sequencing data. Here we present a new tool, ARIBA, that identifies AMR-associated genes and single nucleotide polymorphisms directly from short reads, and generates detailed and customisable output. The accuracy and advantages of ARIBA over other tools are demonstrated on three datasets from Gram-positive and Gram-negative bacteria, with ARIBA outperforming existing methods. ARIBA is available at https://github.com/sanger-pathogens/ariba."}, {"title": "Cycledash: a web application for the interactive analysis and exploration of variants", "url": "https://www.biorxiv.org/content/early/2017/04/06/125153", "tag": "Bioinformatics", "abstract": "As genomics begins to influence clinical care, validating the result of a somatic variant calling pipeline has become increasingly important. Cycledash is a web application which facilitates the validation and analysis of somatic variants, bringing together and streamlining existing tools and workflows for examining and verifying the existence of variants in patient samples."}, {"title": "Comprehensive statistical inference of the clonal structure of cancer from multiple biopsies", "url": "https://www.biorxiv.org/content/early/2017/04/06/125138", "tag": "Bioinformatics", "abstract": "A comprehensive characterization of tumor genetic heterogeneity is critical for understanding how cancers evolve and escape treatment. Although many algorithms have been developed for capturing tumor heterogeneity, they are designed for analyzing either a single type of genomic aberration or individual biopsies. Here we present THEMIS (Tumor Heterogeneity Extensible Modeling via an Integrative System), which allows for the joint analysis of different types of genomic aberrations from multiple biopsies taken from the same patient, using a dynamic graphical model. Simulation experiments demonstrate higher accuracy of THEMIS over its ancestor, TITAN. The heterogeneity analysis results from THEMIS are validated with single cell DNA sequencing from a clinical tumor biopsy. When THEMIS is used to analyze tumor heterogeneity among multiple biopsies from the same patient, it helps to reveal the mutation accumulation history, track cancer progression, and identify the mutations related to treatment resistance. We implement our model via an extensible modeling platform, which makes our approach open, reproducible, and easy for others to extend."}, {"title": "The value of prior knowledge in machine learning of complex network systems", "url": "https://www.biorxiv.org/content/early/2017/04/06/094151", "tag": "Bioinformatics", "abstract": "Our overall goal is to develop machine learning approaches based on genomics and other relevant accessible information for use in predicting how a patient will respond to a given proposed drug or treatment. Given the complexity of this problem, we begin by developing, testing, and analyzing learning methods using data from simulated systems, which allows us access to a known ground truth. We examine the benefits of using prior system knowledge and investigate how learning accuracy depends on various system parameters as well as the amount of training data available. The simulations are based on Boolean networks--directed graphs with 0/1 node states and logical node update rules--which are the simplest computational systems that can mimic the dynamic behavior of cellular systems. Boolean networks can be generated and simulated at scale, have complex yet cyclical dynamics, and as such provide a useful framework for developing machine learning algorithms for modular and hierarchical networks such as biological systems in general and cancer in particular. We demonstrate that utilizing prior knowledge (in the form of network connectivity information)--even without detailed state equations--greatly increases the power of machine learning algorithms to predict network steady state node values (\"phenotypes\") and perturbation responses (\"drug effects\")."}, {"title": "Unbiased Estimate Of Synonymous And Non-Synonymous Substitution Rates With Non-Stationary Base Composition", "url": "https://www.biorxiv.org/content/early/2017/04/06/124925", "tag": "Bioinformatics", "abstract": "The measure of synonymous and non-synonymous substitution rates (dS and dN) is useful for assessing selection operating on protein sequences or for investigating mutational processes affecting genomes. In particular, the ratio dN/dS is expected to be a good proxy of \u03c9, the probability of fixation of non-synonymous mutations relative to that of neutral mutations. Standard methods for estimating dN, dS or \u03c9 rely on the assumption that the base composition of sequences is at the equilibrium of the evolutionary process. In many clades, this assumption of stationarity is in fact incorrect, and we show here through simulations and through analyses of empirical data that non-stationarity biases the estimate of dN, dS and \u03c9. We show that the bias in the estimate of \u03c9 can be fixed by explicitly considering non-stationarity in the modeling of codon evolution, in a maximum likelihood framework. Moreover, we propose an exact method of estimate of dN and dS on branches, based on stochastic mapping, that can take into account non-stationarity. This method can be directly applied to any kind of model of evolution of codons, as long as neutrality is clearly parameterized."}, {"title": "SOMSC: Self-Organization-Map for High-Dimensional Single-Cell Data of Cellular States and Their Transitions", "url": "https://www.biorxiv.org/content/early/2017/04/06/124735", "tag": "Bioinformatics", "abstract": "Measurements of gene expression levels for multiple genes in single cells provide a powerful approach to study heterogeneity of cell populations and cellular plasticity. While the expression levels of multiple genes in each cell are available in such data, the potential connections among the cells (e.g. the lineage relationship) are not directly evident from the measurement. Classifying cellular states and identifying transitions among those states are challenging due to many factors, including the small number of cells versus the large number of genes collected in the data. In this paper we adapt a classical self-organizing-map approach to single-cell gene expression data, such as those based on qPCR and RNA-seq. In this method (SOMSC), a cellular state map (CSM) is derived and employed to identify cellular states inherited in a population of measured single cells. Cells located in the same basin of the CSM are considered as in one cellular state while barriers between the basins provide information on transitions among the cellular states. Consequently, paths of cellular state transitions (e.g. differentiation) and a temporal ordering of the measured single cells are obtained. Applied to a set of synthetic data, two single-cell qPCR data sets and two single-cell RNA-seq data sets for a simulated model of cell differentiation, and systems on the early embryo development, haematopoietic cell lineages, human preimplanation embryo development, and human skeletal muscle myoblasts differentiation, the SOMSC shows good capabilities in identifying cellular states and their transitions in the high-dimensional single-cell data. This approach will have broad applications in studying cell lineages and cellular fate specification."}, {"title": "Literature Evidence in Open Targets - a target validation platform", "url": "https://www.biorxiv.org/content/early/2017/04/06/124719", "tag": "Bioinformatics", "abstract": "Background: We present the Europe PMC literature component of Open Targets - a target validation platform that integrates various evidence to aid drug target identification and validation. The component identifies target-disease associations in documents and ranks the documents based on their confidence from the Europe PMC literature database, by using rules utilising expert-provided heuristic information and serves the platform regularly with the up-to-date data since December, 2015. Results: Currently, there are a total number of 1168365 distinct target-disease associations text mined from >26 million PubMed abstracts and >1.2 million Open Access full text articles. Our comparative analyses on the current available evidence data in the platform revealed that 850179 of these associations are exclusively identified by literature mining. Conclusion: This component helps the platform's users by providing the most relevant literature hits for a given target and disease. The text mining evidence along with the other types of evidence can be explored visually through https://www.targetvalidation.org and all the evidence data is available for download in json format from https://www.targetvalidation.org/downloads/data ."}, {"title": "MMTF - an efficient file format for the transmission, visualization, and analysis of macromolecular structures", "url": "https://www.biorxiv.org/content/early/2017/04/05/122689", "tag": "Bioinformatics", "abstract": "Recent advances in experimental techniques have led to a rapid growth in complexity, size, and number of macromolecular structures that are made available through the Protein Data Bank. This creates a challenge for macromolecular visualization and analysis. Macromolecular structure files, such as PDB or PDBx/mmCIF files can be slow to transfer, parse, and hard to incorporate into third-party software tools. Here, we present a new binary and compressed data representation, the MacroMolecular Transmission Format, MMTF, as well as software implementations in several languages that have been developed around it, which address these issues. We describe the new format and its APIs and demonstrate that it is several times faster to parse, and about a quarter of the file size of the current standard format, PDBx/mmCIF. As a consequence of the new data representation, it is now possible to visualize structures with millions of atoms in a web browser, keep the whole PDB archive in memory or parse it within few minutes on average computers, which opens up a new way of thinking how to implement efficient algorithms in structural bioinformatics. The PDB archive is available in MMTF file format through web services and data are updated on a weekly basis."}, {"title": "DIVERSITY In Binding, Regulation, And Evolution Revealed From High-Throughput ChIP", "url": "https://www.biorxiv.org/content/early/2017/04/05/122325", "tag": "Bioinformatics", "abstract": "A high-throughput chromatin immunoprecipitation (ChIP) experiment is like a black-box: it reports all regions that are associated with the profiled protein based on the initial cross-linking step. These regions can be a highly diverse set of DNA sequences, with some making direct contact with the protein, some binding through intermediaries, and some being a result of long-range interactions involving the protein. We present DIVERSITY, a method that identifies the distinct components of such a mixture, leaving no data behind, while at the same time, using no prior motif knowledge. Using the example of the REST protein, we show that these different components give insights into the various complexes that may be forming along the chromatin and their regulatory functions. Webserver at http://diversity.ncl.res.in/. Standalone at https://github.com/NarlikarLab/DIVERSITY"}, {"title": "HadoopCNV: A Dynamic Programming Imputation Algorithm To Detect Copy Number Variants From Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/04/05/124339", "tag": "Bioinformatics", "abstract": "BACKGROUND: Whole-genome sequencing (WGS) data may be used to identify copy number variations (CNVs). Existing CNV detection methods mostly rely on read depth or alignment characteristics (paired-end distance and split reads) to infer gains/losses, while neglecting allelic intensity ratios and cannot quantify copy numbers. Additionally, most CNV callers are not scalable to handle a large number of WGS samples. METHODS: To facilitate large-scale and rapid CNV detection from WGS data, we developed a Dynamic Programming Imputation (DPI) based algorithm called HadoopCNV, which infers copy number changes through both allelic frequency and read depth information. Our implementation is built on the Hadoop framework, enabling multiple compute nodes to work in parallel. RESULTS: Compared to two widely used tools - CNVnator and LUMPY, HadoopCNV has similar or better performance on both simulated data sets and real data on the NA12878 individual. Additionally, analysis on a 10-member pedigree showed that HadoopCNV has a Mendelian precision that is similar or better than other tools. Furthermore, HadoopCNV can accurately infer loss of heterozygosity (LOH), while other tools cannot. HadoopCNV requires only 1.6 hours for a human genome with 30X coverage, on a 32-node cluster, with a linear relationship between speed improvement and the number of nodes. We further developed a method to combine HadoopCNV and LUMPY result, and demonstrated that the combination resulted in better performance than any individual tools. CONCLUSIONS: The combination of high-resolution, allele-specific read depth from WGS data and Hadoop framework can result in efficient and accurate detection of CNVs."}, {"title": "Evidence-based gene models for structural and functional annotations of the oil palm genome", "url": "https://www.biorxiv.org/content/early/2017/04/05/111120", "tag": "Bioinformatics", "abstract": "The advent of rapid and inexpensive DNA sequencing has led to an explosion of data waiting to be transformed into knowledge about genome organization and function. Gene prediction is customarily the starting point for genome analysis. This paper presents a bioinformatics study of the oil palm genome, including comparative genomics analysis, database and tools development, and mining of biological data for genes of interest. We have annotated 26,059 oil palm genes integrated from two independent gene-prediction pipelines, Fgenesh++ and Seqping. This integrated annotation constitutes a significant improvement in comparison to the preliminary annotation published in 2013. We conducted a comprehensive analysis of intronless, resistance and fatty acid biosynthesis genes, and demonstrated that the high quality of the current genome annotation. 3,658 intronless genes were identified in the oil palm genome, an important resource for evolutionary study. Further analysis of the oil palm genes revealed 210 candidate resistance genes involved in pathogen defense. Fatty acids have diverse applications ranging from food to industrial feedstocks, and we identified 42 key genes involved in fatty acid biosynthesis in oil palm. These results provide an important resource for studies of plant genomes and a theoretical foundation for marker-assisted breeding of oil palm and related crops."}, {"title": "PREDICTD: PaRallel Epigenomics Data Imputation With Cloud-based Tensor Decomposition", "url": "https://www.biorxiv.org/content/early/2017/04/04/123927", "tag": "Bioinformatics", "abstract": "The Encyclopedia of DNA Elements (ENCODE) and the Roadmap Epigenomics Project have produced thousands of data sets mapping the epigenome in hundreds of cell types. However, the number of cell types remains too great to comprehensively map given current time and financial constraints. We present a method, PaRallel Epigenomics Data Imputation with Cloud-based Tensor Decomposition (PREDICTD), to address this issue by computationally imputing missing experiments in collections of epigenomics experiments. PREDICTD leverages an intuitive and natural model called \u201ctensor decomposition\u201d to impute many experiments simultaneously. Compared with the current state-of-the-art method, ChromImpute, PREDICTD produces lower overall mean squared error, and combining methods yields further improvement. We show that PREDICTD data can be used to investigate enhancer biology at non-coding human accelerated regions. PREDICTD provides reference imputed data sets and open-source software for investigating new cell types, and demonstrates the utility of tensor decomposition and cloud computing, two technologies increasingly applicable in bioinformatics."}, {"title": "SEARCH_16S: A new algorithm for identifying 16S ribosomal RNA genes in contigs and chromosomes", "url": "https://www.biorxiv.org/content/early/2017/04/04/124131", "tag": "Bioinformatics", "abstract": "SEARCH_16S is a new algorithm that annotates 16S ribosomal RNA genes in microbial genomes and metagenomic sequences. Word counting is used to identify candidate segments, then conserved motifs are used to identify homologous loci close to the gene boundaries. SEARCH_16S has >99% sensitivity to known 16S genes."}, {"title": "SINAPS: Prediction of microbial traits from marker gene sequences", "url": "https://www.biorxiv.org/content/early/2017/04/04/124156", "tag": "Bioinformatics", "abstract": "Microbial communities are often studied by sequencing marker genes such as 16S ribosomal RNA. Marker gene sequences can be used to assess diversity and taxonomy, but do not directly measure functions arising from other genes in the community metagenome. Such functions can be predicted by algorithms that associate marker genes with experimentally determined traits in well-studied species. Typically, such methods use ancestral state reconstruction. Here I describe SINAPS, a new algorithm that predicts traits for marker gene sequences using a fast, simple word-counting algorithm that does not require alignments or trees. A measure of prediction confidence is obtained by bootstrapping. I tested SINAPS predictions from 16S V4 query sequences for traits including energy metabolism, Gram-positive staining, presence of a flagellum, V4 primer mismatches, and 16S copy number. Accuracy was >90% except for copy number, where a large majority of predictions were within +/-2 of the true value."}, {"title": "CellView: Interactive Exploration Of High Dimensional Single Cell RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/04/04/123810", "tag": "Bioinformatics", "abstract": "Advances in high-throughput single cell transcriptomics technologies have revolutionized the study of complex tissues. It is now possible to measure gene expression across thousands of individual cells to define cell types and states. While powerful computational and statistical frameworks are emerging to analyze these complex datasets, a gap exists between this data and a biologist's insight. The CellView web application fills this gap by providing easy and intuitive exploration of single cell transcriptome data."}, {"title": "UNBIAS: An attempt to correct abundance bias in 16S sequencing, with limited success", "url": "https://www.biorxiv.org/content/early/2017/04/04/124149", "tag": "Bioinformatics", "abstract": "Next-generation amplicon sequencing of 16S ribosomal RNA is widely used to survey microbial communities. Alpha and beta diversities of these communities are often quantified on the basis of OTU frequencies in the reads. Read abundances are biased by factors including 16S copy number and PCR primer mismatches which can cause the read abundance distribution to diverge substantially from the species abundance distribution. Using mock community tests with species abundances determined independently by shotgun sequencing, I find that 16S amplicon read frequencies have no meaningful correlation with species frequencies (Pearson coefficient r close to zero). In addition, I show that that the Jaccard distance between the abundance distributions for reads of replicate samples, which ideally would be zero, is typically ~0.15 with values up to 0.71 for replicates sequenced in different runs. Using simulated communities, I estimate that the average rank of a dominant species in the reads is 3. I describe UNBIAS, a method that attempts to correct for abundance bias due to gene copy number and primer mismatches. I show that UNBIAS can achieve informative, but still poor, correlations (r ~0.6) between estimated and true abundances in the idealized case of mock samples where species are well known. However, r falls to ~0.4 when the closest reference species have 97% identity and to ~0.2 at 95% identity. This degradation is mostly explained by the increased difficulty in predicting 16S copy number when OTUs have lower similarity with the reference database, as will typically be the case in practice. 16S abundance bias therefore remains an unsolved problem, calling into question the naive use of alpha and beta diversity metrics based on frequency distributions."}, {"title": "Interactive Visual Exploration And Refinement Of Cluster Assignments", "url": "https://www.biorxiv.org/content/early/2017/04/04/123844", "tag": "Bioinformatics", "abstract": "Background: With ever-increasing amounts of data produced in biology research, scientists are in need of efficient data analysis methods. Cluster analysis, combined with visualization of the results, is one such method that can be used to make sense of large data volumes. At the same time, cluster analysis is known to be imperfect and depends on the choice of algorithms, parameters, and distance measures. Most clustering algorithms don't properly account for ambiguity in the source data, as records are often assigned to discrete clusters, even if an assignment is unclear. While there are metrics and visualization techniques that allow analysts to compare clusterings or to judge cluster quality, there is no comprehensive method that allows analysts to evaluate, compare, and refine cluster assignments based on the source data, derived scores, and contextual data. Results: In this paper, we introduce a method that explicitly visualizes the quality of cluster assignments, allows comparisons of clustering results and enables analysts to manually curate and refine cluster assignments. Our methods are applicable to matrix data clustered with partitional, hierarchical, and fuzzy clustering algorithms. Furthermore, we enable analysts to explore clustering results in context of other data, for example, to observe whether a clustering of genomic data results in a meaningful differentiation in phenotypes. Conclusions: Our methods are integrated into Caleydo StratomeX, a popular, web-based, disease subtype analysis tool. We show in a usage scenario that our approach can reveal ambiguities in cluster assignments and produce improved clusterings that better differentiate genotypes and phenotypes."}, {"title": "GenomicDataCommons: a Bioconductor Interface to the NCI Genomic Data Commons", "url": "https://www.biorxiv.org/content/early/2017/04/04/117200", "tag": "Bioinformatics", "abstract": "The National Cancer Institute (NCI) Genomic Data Commons (Grossman et al. 2016, https://gdc. cancer.gov/) provides the cancer research community with an open and unified repository for sharing and accessing data across numerous cancer studies and projects via a high-performance data transfer and query infrastructure. The Bioconductor project (Huber et al. 2015) is an open source and open development software project built on the R statistical programming environment (R Core Team 2016). A major goal of the Bioconductor project is to facilitate the use, analysis, and comprehension of genomic data. The GenomicDataCommons Bioconductor package provides basic infrastructure for querying, accessing, and mining genomic datasets available from the GDC. We expect that Bioconductor developer and bioinformatics community will build on the GenomicDataCommons package to add higher-level functionality and expose cancer genomics data to many state-of-the-art bioinformatics methods available in Bioconductor. Availability: https://bioconductor.org/packages/GenomicDataCommons & https://github.com/Bioconductor/GenomicDataCommons"}, {"title": "Scallop Enables Accurate Assembly Of Transcripts Through Phasing-Preserving Graph Decomposition", "url": "https://www.biorxiv.org/content/early/2017/04/03/123612", "tag": "Bioinformatics", "abstract": "We introduce Scallop, an accurate, reference-based transcript assembler for RNA-seq data. Scallop significantly improves reconstruction of multi-exon and lowly expressed transcripts. On 10 human samples aligned with STAR, Scallop produces (on average) 35.7% and 37.5% more correct multi-exon transcripts than two leading transcript assemblers, StringTie and TransComb, respectively. For transcripts expressed at low levels in the same samples, Scallop assembles 65.2% and 50.2% more correct multi-exon transcripts than StringTie and TransComb, respectively. Scallop obtains this improvement through a novel algorithm that we prove preserves all phasing paths from reads (including paired-end reads), while also producing a parsimonious set of transcripts and minimizing coverage deviation."}, {"title": "CiliaCarta: An Integrated And Validated Compendium Of Ciliary Genes", "url": "https://www.biorxiv.org/content/early/2017/04/03/123455", "tag": "Bioinformatics", "abstract": "The cilium is an essential organelle at the surface of most mammalian cells whose dysfunction causes a wide range of genetic diseases collectively called ciliopathies. The current rate at which new ciliopathy genes are identified suggests that many ciliary components remain undiscovered. We generated and rigorously analyzed genomic, proteomic, transcriptomic and evolutionary data and systematically integrated these using Bayesian statistics into a predictive score for ciliary function. This resulted in 285 candidate ciliary genes. We found experimental evidence of ciliary associations for 24 out of 36 analyzed candidate proteins. In addition, we show that OSCP1, which has previously been implicated in two distinct non-ciliary functions, causes a cilium dysfunction phenotype when depleted in zebrafish. The candidate list forms the basis of CiliaCarta, a comprehensive ciliary compendium covering 836 genes. The resource can be used to objectively prioritize candidate genes in whole exome or genome sequencing of ciliopathy patients and can be accessed at http://bioinformatics.bio.uu.nl/john/syscilia/ciliacarta/."}, {"title": "blkbox: Integration Of Multiple Machine Learning Approaches To Identify Disease Biomarkers", "url": "https://www.biorxiv.org/content/early/2017/04/03/123430", "tag": "Bioinformatics", "abstract": "Motivation: Machine learning (ML) is a powerful tool to create supervised models that can distinguish between classes and facilitate biomarker selection in high-dimensional datasets, including RNA Sequencing (RNA-Seq). However, it is variable as to which is the best performing ML algorithm(s) for a specific dataset, and identifying the optimal match is time consuming. blkbox is a software package including a shiny frontend, that integrates nine ML algorithms to select the best performing classifier for a specific dataset. blkbox accepts a simple abundance matrix as input, includes extensive visualization, and also provides an easy to use feature selection step to enable convenient and rapid potential biomarker selection, all without requiring parameter optimization. Results: Feature selection makes blkbox computationally inexpensive while multi-functionality, including nested cross-fold validation (NCV), ensures robust results. blkbox identified algorithms that outperformed prior published ML results. Applying NCV identifies features, which are utilized to gain high accuracy. Availability: The software is available as a CRAN R package and as a developer version with extended functionality on github (https://github.com/gboris/blkbox). Contact: b.guennewig@garvan.org.au"}, {"title": "StarBEAST2 brings faster species tree inference and accurate estimates of substitution rates", "url": "https://www.biorxiv.org/content/early/2017/04/02/070169", "tag": "Bioinformatics", "abstract": "Fully Bayesian multispecies coalescent (MSC) methods like *BEAST estimate species trees from multiple sequence alignments. Today thousands of genes can be sequenced for a given study, but using that many genes with *BEAST is intractably slow. An alternative is to use heuristic methods which compromise accuracy or completeness in return for speed. A common heuristic is concatenation, which assumes that the evolutionary history of each gene tree is identical to the species tree. This is an inconsistent estimator of species tree topology, a worse estimator of divergence times, and induces spurious substitution rate variation when incomplete lineage sorting is present. Another class of heuristics directly motivated by the MSC avoids many of the pitfalls of concatenation but cannot be used to estimate divergence times. To enable fuller use of available data and more accurate inference of species tree topologies, divergence times, and substitution rates, we have developed a new version of *BEAST called StarBEAST2. To improve convergence rates we add analytical integration of population sizes, novel MCMC operators and other optimisations. Computational performance improved by 13.5\u00d7 to 13.8\u00d7 when analysing empirical data sets, and an average of 33.1\u00d7 across 30 simulated data sets. To enable accurate estimates of perspecies substitution rates we introduce species tree relaxed clocks, and show that StarBEAST2 is a more powerful and robust estimator of rate variation than concatenation. StarBEAST2 is available through the BEAUTi package manager in BEAST 2.4 and above."}, {"title": "Gene ORGANizer: Linking Genes to the Organs They Affect", "url": "https://www.biorxiv.org/content/early/2017/04/02/106948", "tag": "Bioinformatics", "abstract": "One of the biggest challenges in studying how genes work is understanding their effect on the physiology and anatomy of the body. Existing tools try to address this using indirect features, such as expression levels and biochemical pathways. Here, we present Gene ORGANizer (geneorganizer.huji.ac.il), a phenotype-based tool that directly links human genes to the body parts they affect. It is built upon an exhaustive curated database that links more than 7,000 genes to ~150 anatomical parts using >150,000 gene-organ associations. The tool offers user-friendly platforms to analyze the anatomical effects of individual genes, and identify trends within groups of genes. We demonstrate how Gene ORGANizer can be used to make new discoveries, showing that chromosome X is enriched with genes affecting facial features, that positive selection targets genes with more constrained phenotypic effects, and more. We expect Gene ORGANizer to be useful in a variety of evolutionary, medical and molecular studies aimed at understanding the phenotypic effects of genes."}, {"title": "Intra-protein binding peptide fragments have specific and intrinsic sequence patterns", "url": "https://www.biorxiv.org/content/early/2017/03/31/122978", "tag": "Bioinformatics", "abstract": "The key finding in the DNA double helix model is the specific pairing or binding between nucleotides A-T and C-G, and the pairing rules are the molecule basis of genetic code. Unfortunately, no such rules have been discovered for proteins. Here we show that similar rules and intrinsic sequence patterns between intra-protein binding peptide fragments do exist, and they can be extracted using a deep learning algorithm. Multi-millions of binding and non-binding peptide fragments from currently available protein X-ray structures are classified with an accuracy of up to 93%. This discovery has the potential in helping solve protein folding and protein-protein interaction problems, two open and fundamental problems in molecular biology."}, {"title": "karyoploteR: An R/Bioconductor Package To Plot Customizable Linear Genomes Displaying Arbitrary Data", "url": "https://www.biorxiv.org/content/early/2017/03/31/122838", "tag": "Bioinformatics", "abstract": "Motivation: Data visualization is a crucial tool for data exploration, analysis and interpretation. For the visualization of genomic data there lacks a tool to create customizable non-circular plots of whole genomes from any species. Results: We have developed karyoploteR, an R/Bioconductor package to create linear chromosomal representations of any genome with genomic annotations and experimental data plotted along them. Plot creation process is inspired in R base graphics, with a main function creating karyoplots with no data and multiple additional functions, including custom functions written by the end-user, adding data and other graphical elements. This approach allows the creation of highly customizable plots from arbitrary data with complete freedom on data positioning and representation. Availability: karyoploteR is released under Artistic-2.0 License. Source code and documentation are freely available through Bioconductor at http://www.bioconductor.org/packages/karyoploteR"}, {"title": "Predicting Causal Relationships from Biological Data: Applying Automated Casual Discovery on Mass Cytometry Data of Human Immune Cells", "url": "https://www.biorxiv.org/content/early/2017/03/30/122572", "tag": "Bioinformatics", "abstract": "Learning the causal relationships that define a molecular system allows us to predict how the system will respond to different interventions. Distinguishing causality from mere association typically requires randomized experiments. Methods for automated causal discovery from limited experiments exist, but have so far rarely been tested in systems biology applications. In this work, we apply state-of-the art causal discovery methods on a large collection of public mass cytometry data sets, measuring intra-cellular signaling proteins of the human immune system and their response to several perturbations. We show how different experimental conditions can be used to facilitate causal discovery, and apply two fundamental methods that produce context-specific causal predictions. Causal predictions were reproducible across independent data sets from two different studies, but often disagree with the KEGG pathway databases. Within this context, we discuss the caveats we need to overcome for automated causal discovery to become a part of the routine data analysis in systems biology."}, {"title": "xMWAS: an R package for data-driven integration and differential network analysis", "url": "https://www.biorxiv.org/content/early/2017/03/30/122432", "tag": "Bioinformatics", "abstract": "Summary: Integrative omics is a central component of most systems biology studies. Computational methods are required for extracting meaningful relationships across different omics layers. Various tools have been developed to facilitate integration of paired heterogenous omics data; however most existing tools allow integration of only two omics datasets. Furthermore, existing data integration tools do not incorporate additional steps of identifying sub-networks or communities of highly connected entities and evaluating the topology of the integrative network under different conditions. Here we present xMWAS, an R package for data integration, network visualization, clustering, differential network analysis of data from biochemical and phenotypic assays, and two or more omics platforms. Availability: https://sourceforge.net/projects/xmwas/ Contact: kuppal2@emory.edu"}, {"title": "CoGe LoadExp+: A web-based suite that integrates next-gen sequencing data analysis workflows and visualization", "url": "https://www.biorxiv.org/content/early/2017/03/30/118802", "tag": "Bioinformatics", "abstract": "To make genomic and epigenomic analyses more widely available to the biological research community, we have created LoadExp+, a suite of bioinformatics workflows integrated with the web-based comparative genomics platform, CoGe. LoadExp+ allows users to perform transcriptomic (RNA-seq), epigenomic (BS-seq), chromatin-binding (ChIP-seq), variant identification (SNPs), and population genetics analyses against any genome in CoGe, including genomes integrated by users themselves. Through LoadExp+'s integration with CoGe's existing features, all analyses are available for visualization and additional downstream processing, and are available for export to CyVerse's data management and analysis platforms. LoadExp+ provides easy-to-use functionality to manage genomics and epigenomics data throughout its entire lifecycle and facilitates greater accessibility of genomics analyses to researchers of all skill levels. LoadExp+ can be accessed at https://genomevolution.org."}, {"title": "The Image Data Resource: A Scalable Platform for Biological Image Data Access, Integration, and Dissemination", "url": "https://www.biorxiv.org/content/early/2017/03/30/089359", "tag": "Bioinformatics", "abstract": "Access to primary research data is vital for the advancement of science. To extend the data types supported by community repositories, we built a prototype Image Data Resource (IDR) that collects and integrates imaging data acquired across many different imaging modalities. IDR links high-content screening, super-resolution microscopy, time-lapse and digital pathology imaging experiments to public genetic or chemical databases, and to cell and tissue phenotypes expressed using controlled ontologies. Using this integration, IDR facilitates the analysis of gene networks and reveals functional interactions that are inaccessible to individual studies. To enable re-analysis, we also established a computational resource based on IPython notebooks that allows remote access to the entire IDR. IDR is also an open source platform that others can use to publish their own image data. Thus IDR provides both a novel on-line resource and a software infrastructure that promotes and extends publication and re-analysis of scientific image data."}, {"title": "Multiple Kernel Learning Approach For Medical Image Analysis", "url": "https://www.biorxiv.org/content/early/2017/03/29/121509", "tag": "Bioinformatics", "abstract": "Computer aided diagnosis is gradually making its way into the domain of medical research and clinical diagnosis. With Field of radiology and diagnostic imaging producing petabytes of image data. Machine learning tools, particularly kernel based algorithms seem to be an obvious choice to process and analyze this high dimensional and heterogeneous data. In this chapter, after presenting a brief description about nature of medical images, image features and basics in machine learning and kernel methods, we present the application of multiple kernel learning algorithms for medical image analysis."}, {"title": "Squeakr: An Exact and Approximate k-mer Counting System", "url": "https://www.biorxiv.org/content/early/2017/03/29/122077", "tag": "Bioinformatics", "abstract": "Motivation: k-mer-based algorithms have become increasingly popular in the processing of high-throughput sequencing (HTS) data. These algorithms span the gamut of the analysis pipeline from k-mer counting (e.g., for estimating assembly parameters), to error correction, genome and transcriptome assembly, and even transcript quantification. Yet, these tasks often use very different k-mer representations and data structures. In this paper, we set forth the fundamental operations for maintaining multisets of k-mers and classify existing systems from a data-structural perspective. We then show how to build a k-mer-counting and multiset-representation system using the counting quotient filter (CQF), a feature-rich approximate membership query (AMQ) data structure. We introduce the k-mer-counting/querying system Squeakr (Simple Quotient filter-based Exact and Approximate Kmer Representation), which is based on the CQF. This off-the shelf data structure turns out to be an efficient (approximate or exact) representation for sets or multisets of k-mers. Results: Squeakr takes 2X-4.3X less time than the state-of-the-art to count and perform a random-point-query workload. Squeakr is memory-efficient, consuming 1.5X-4.3X less memory than the state-of-the-art. It offers competitive counting performance, and answers point queries (i.e. queries for the abundance of a particular k-mer) over an order-of-magnitude faster than other systems. The Squeakr representation of the k-mer multiset turns out to be immediately useful for downstream processing (e.g., de Bruijn graph traversal) because it supports fast queries and dynamic k-mer insertion, deletion, and modification. Availability: https://github.com/splatlab/squeakr"}, {"title": "SUPERmerge: ChIP-Seq Coverage Island Analysis Algorithm For Broad Histone Marks", "url": "https://www.biorxiv.org/content/early/2017/03/29/121897", "tag": "Bioinformatics", "abstract": "SUPERmerge is a ChIP-seq read pileup analysis and annotation algorithm for investigating alignment (BAM) files of diffuse histone modification ChIP-seq datasets with broad chromatin domains at a single base pair resolution level. SUPERmerge allows flexible regulation of a variety of read pileup parameters, thereby revealing how read islands aggregate into areas of coverage across the genome and what annotation features they map to within individual biological replicates. SUPERmerge is especially useful for investigating low sample size ChIP-seq experiments in which epigenetic histone modifications (e.g., H3K9me1, H3K27me3) result in inherently broad peaks with a diffuse range of signal enrichment spanning multiple consecutive genomic loci and annotated features."}, {"title": "Enhancing Sensitivity And Controlling False Discovery Rate In Somatic Indel Discovery Using A Latent Variable Model", "url": "https://www.biorxiv.org/content/early/2017/03/29/121954", "tag": "Bioinformatics", "abstract": "Cancer is a genetic disorder in the first place. Therefore, next-generation sequencing (NGS) based discovery of somatically acquired genetic variants has gained widespread attention. Computational prediction of somatic variants, however, is affected by a variety of confounding factors. In addition to the uncertainties that one commonly encounters also in germline variation prediction, such as misplaced and/or inaccurate read alignments, cancer heterogeneity and impure samples significantly add to the issues. Overall, this hampers state-of-the-art indel discovery tools to discover somatic indels at operable performance rates, although they perform excellently when calling germline indels. While affecting all size ranges, both common and cancer-specific problems interfere in particularly unfavorable ways in the prediction of somatic midsize (30-150 bp) insertions and deletions. Here, we present a latent variable model that can take the major confounding factors and uncertainties into a unifying account. Using this modeling framework, we first demonstrate how to efficiently compute the probability for a (putative) indel to be somatic, thereby resolving a principled computational runtime bottleneck in Bayesian uncertainty quantification. Second, we show how to reliably estimate the allele frequencies for a given list of indels. Third, we also present an intuitive and effective way to control the false discovery rate, an issue in genetic variant discovery that has been found notoriously hard to deal with. As a tool that implements all methodology developed, we present PROSIC (PROcessing Somatic Indel Calls). PROSIC achieves significant improvements in particular in terms of recall when applied to deletion call sheets, as provided by prevalent state-of-the-art tools, in comparison to their integrated somatic indel calling routines."}, {"title": "Plasmid Profiler: Comparative Analysis of Plasmid Content in WGS Data", "url": "https://www.biorxiv.org/content/early/2017/03/28/121350.1", "tag": "Bioinformatics", "abstract": "Summary: Comparative analysis of bacterial plasmids from whole genome sequence (WGS) data generated from short read sequencing is challenging. This is due to the difficulty in identifying contigs harbouring plasmid sequence data, and further difficulty in assembling such contigs into a full plasmid. As such, few software programs and bioinformatics pipelines exist to perform comprehensive comparative analyses of plasmids within and amongst sequenced isolates. To address this gap, we have developed Plasmid Profiler, a pipeline to perform comparative plasmid content analysis without the need for de novo assembly. The pipeline is designed to rapidly identify plasmid sequences by mapping reads to a plasmid reference sequence database. Predicted plasmid sequences are then annotated with their incompatibility group, if known. The pipeline allows users to query plasmids for genes or regions of interest and visualize results as an interactive heat map. Availability and Implementation: Plasmid Profiler is freely available software released under the Apache 2.0 open source software license. A stand-alone version of the entire Plasmid Profiler pipeline is available as a Docker container at https://hub.docker.com/r/phacnml/plasmidprofiler_0_1_6/. The conda recipe for the Plasmid R package is available at: https://anaconda.org/bioconda/r-plasmidprofiler. The custom Plasmid Profiler R package is also available as a CRAN package at https://cran.r-project.org/web/packages/Plasmidprofiler/index.html. Galaxy tools associated with the pipeline are available as a Galaxy tool suite at https://toolshed.g2.bx.psu.edu/repository?repository_id=55e082200d16a504. The source code is available at https://github.com/phac-nml/plasmidprofiler; the Galaxy implementation is available at https://github.com/phac-nml/plasmidprofiler-galaxy."}, {"title": "Correcting For Cell-Type Heterogeneity In Epigenome-Wide Association Studies: Premature Analyses And Conclusions", "url": "https://www.biorxiv.org/content/early/2017/03/28/121533", "tag": "Bioinformatics", "abstract": "Recently, a study claimed that a reference-free cell-type deconvolution method, called ReFACTor, leads to improved power and improved estimates of cell-type composition compared to competing reference-free and reference-based methods in the context of Epigenome-Wide Association Studies (EWAS). However, we identified many critical flaws (both conceptual and statistical in nature), which seriously question the validity of the claims. We outlined constructive criticism in a recent correspondence letter. The purpose of this letter is two-fold. First, to present additional analyses, which demonstrate that our original criticism is statistically sound. Second, to highlight additional serious concerns, which have not yet been addressed. In summary, we find that ReFACTor has not been demonstrated to outperform state-of-the-art reference-free methods such as SVA or RefFreeEWAS, nor state-of-the-art reference-based methods. Thus, the claim that ReFACTor represents an advance over the state-of-the-art is not supported by an objective and rigorous statistical analysis of the data."}, {"title": "Scalable Multi-Sample Single-Cell Data Analysis by Partition-Assisted Clustering and Multiple Alignments of Networks", "url": "https://www.biorxiv.org/content/early/2017/03/28/116566", "tag": "Bioinformatics", "abstract": "Mass cytometry (CyTOF) has greatly expanded the capability of cytometry. It is now easy to generate multiple CyTOF samples in a single study, with each sample containing single-cell measurement on 50 markers for more than hundreds of thousands of cells. Current methods do not adequately address the issues concerning combining multiple samples for subpopulation discovery, and these issues can be quickly and dramatically amplified with increasing number of samples. To overcome this limitation, we developed Partition-Assisted Clustering and Multiple Alignments of Networks (PAC-MAN) for the fast automatic identification of cell populations in CyTOF data closely matching that of expert manual-discovery, and for alignments between subpopulations across samples to define dataset-level cellular states. PAC-MAN is computationally efficient, allowing the management of very large CyTOF datasets, which are increasingly common in clinical studies and cancer studies that monitor various tissue samples for each subject."}, {"title": "ClusterEnG: An interactive educational web resource for clustering big data", "url": "https://www.biorxiv.org/content/early/2017/03/27/120915", "tag": "Bioinformatics", "abstract": "Clustering is one of the most common techniques used in data analysis to discover hidden structures by grouping together data points that are similar in some measure into clusters. Although there are many programs available for performing clustering, a single web resource that provides both state-of-the-art clustering methods and interactive visualizations is lacking. ClusterEnG (acronym for Clustering Engine for Genomics) provides an interface for clustering big data and interactive visualizations including 3D views, cluster selection and zoom features. ClusterEnG also aims at educating the user about the similarities and differences between various clustering algorithms and provides clustering tutorials that demonstrate potential pitfalls of each algorithm. The web resource will be particularly useful to scientists who are not conversant with computing but want to understand the structure of their data in an intuitive manner. ClusterEnG is part of a bigger project called KnowEnG (Knowledge Engine for Genomics) and is available at http://education.knoweng.org/clustereng."}, {"title": "Dissecting the genomic heterogeneity of cancer hallmarks' acquisition with SLAPenrich", "url": "https://www.biorxiv.org/content/early/2017/03/27/077701", "tag": "Bioinformatics", "abstract": "We present a computational method, implemented in an open source R package, to explore how cancers from different tissue types might have acquired the same cancer hallmark (evolutionary successful trait) via preferential genomically altering different biological pathways. To this aim, we have curated a collection of 374 orthogonal pathway gene-sets encompassing thousands of genes (from public available resources) mapped to 10 canonical cancer hallmarks. Using this curated data resource, we have characterised the landscape of pathway alterations putatively contributing to the acquisition of different cancer hallmarks via systematic analysis of somatic mutations in large cohorts of patients across 10 cancer types, from the Cancer Genome Atlas. We assume that the heterogeneity of each hallmark in terms of number of corresponding enriched pathway alterations is reflective of its evolutionary fitness to the cancer type under consideration. A systematic evaluation of this heterogeneity across hallmarks and cancer types has resulted into a set of cancer hallmark heterogeneity signatures. These signatures quantitatively confirm the established predominance of certain hallmarks in determined cancer types and their clinical relevance, and they allow an easy data-driven comparison of cancer hallmark heterogeneity across different lineages. We have found, as expected, that most of the pathway alteration enrichments and large hallmark heterogeneities are guided by somatic mutations in established, and highly frequently mutated, high-confidence cancer driver genes. However and most importantly, when excluding these variants from the analyses, we observe that the hallmark heterogeneity signatures, thus the level of predominance of the considered hallmarks, are strikingly preserved across cancer types. Therefore we propose to use the hallmark heterogeneity signatures as a ground truth to characterise long tails of infrequent genomic alterations, across cancer types, and we highlight a number of potential novel cancer driver genes and networks."}, {"title": "Genome Build Information Is An Essential Part Of Genomic Track Files", "url": "https://www.biorxiv.org/content/early/2017/03/27/120923", "tag": "Bioinformatics", "abstract": "Genomic locations are represented as coordinates on a specific genome build version, but the build information is frequently missing when coordinates are provided. It is essential to correctly interpret and analyse the genomic intervals contained in genomic track files. Here, we demonstrate that this crucial metadatum (or rather datum) is often isolated from the genomic track files in public repositories and journal articles, which could be a major time thief. We propose best practices to ensure that genome build version is always carried along with genomic track files. Although not a substitute to the best practices, we also provide a tool to predict the genome build version of genomic track files."}, {"title": "Disorder Atlas: web-based software for the proteome-based interpretation of intrinsic disorder predictions", "url": "https://www.biorxiv.org/content/early/2017/03/26/060699", "tag": "Bioinformatics", "abstract": "Background: Intrinsically disordered proteins lack a stable three-dimensional structure under physiological conditions. While this property has gained considerable interest within the past two decades, disorder poses substantial challenges to experimental characterization efforts. In effect, numerous computational tools have been developed to predict disorder from primary sequences, however, interpreting the output of these algorithms remains a challenge. Results: Here, we present Disorder Atlas, web-based software that facilitates the interpretation of intrinsic disorder predictions using proteome-based descriptive statistics. This service is also equipped to facilitate large-scale systematic exploratory searches for proteins encompassing disorder features of interest, and further allows users to browse the prevalence of multiple disorder features at the proteome level. Disorder Atlas is freely available for non-commercial users at http://www.disorderatlas.org. Conclusion: Disorder Atlas enables a standardized interpretation of intrinsic disorder predictions, and provides researchers with a tool to assess disorder on multiple scales."}, {"title": "Machine Learning-Based State-Of-The-Art Methods For The Classification Of RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2017/03/26/120592", "tag": "Bioinformatics", "abstract": "RNA-Seq measures expression levels of several transcripts simultaneously. The identified reads can be gene, exon, or other region of interest. Various computational tools have been developed for studying pathogen or virus from RNA-Seq data by classifying them according to the attributes in several pre-defined classes, but still computational tools and approaches to analyze complex datasets are still lacking. The development of classification models is highly recommended for disease diagnosis and classification, disease monitoring at molecular level as well as researching for potential disease biomarkers. In this chapter, we are going to discuss various machine learning approaches for RNA-Seq data classification and their implementation. Advancements in bioinformatics, along with developments in machine learning based classification, would provide powerful toolboxes for classifying transcriptome information available through RNA-Seq data."}, {"title": "Intervene: a tool for intersection and visualization of multiple gene or genomic region sets", "url": "https://www.biorxiv.org/content/early/2017/03/25/109728", "tag": "Bioinformatics", "abstract": "Background: A common task for scientists relies on comparing lists of genes or genomic regions derived from high-throughput sequencing experiments. While several tools exist to intersect and visualize sets of genes, similar tools dedicated to the visualization of genomic region sets are currently limited. Results: To address this gap, we have developed the Intervene tool, which provides an easy and automated interface for the effective intersection and visualization of genomic region or list sets, thus facilitating their analysis and interpretation. Intervene contains three modules: venn to generate Venn diagrams of up to six sets, upset to generate UpSet plots of multiple sets, and pairwise to compute and visualize intersections of multiple sets as clustered heat maps. Intervene, and its interactive web ShinyApp companion, generate publication-quality figures for the interpretation of genomic region and list sets. Conclusions: Intervene and its web application companion provide an easy command line, and an interactive web interface to compute intersections of multiple genomic and list sets. They also have the capacity to plot intersections using easy-to-interpret visual approaches. Intervene is developed and designed to meet the needs of both computer scientists and biologists. The source code is freely available at https://bitbucket.org/CBGR/intervene, with the web application available at https://asntech.shinyapps.io/intervene."}, {"title": "Evolutionarily Conserved Alternative Splicing Across Monocots", "url": "https://www.biorxiv.org/content/early/2017/03/25/120469", "tag": "Bioinformatics", "abstract": "One difficulty when identifying and analyzing alternative splicing (AS) events in plants is distinguishing functional AS from splicing noise. One way to add confidence to the validity of a splice isoform is to observe that it is conserved across evolutionarily related species. We use a high throughput method to identify junction based conserved AS events from RNA-Seq data across nine plant species including: five grass monocots (maize, sorghum, rice, Brachpodium and foxtail millet), plus two non-grass monocots (bananan and African oil palm), the eudicot Arabidopsis and the basal angiosperm Amborella. In total, 9,804 conserved AS events within 19,235 genes were identified conserved between 2 or more species studied. In grasses containing large regions of conserved synteny, the frequency of conserved AS events is twice that observed for genes outside of conserved synteny blocks. In plant-specific RS and RS2Z subfamilies, we observe both conservation and divergence of AS events after the whole genome duplication in maize. In addition, plant-specific RS and RS2Z subfamilies are highly connected with R2R3-MYB in splicing networks. Furthermore, we discovered that the network based on genes harboring conserved AS events is enriched for phosphatases, kinases and ubiquitylation genes, which suggests that AS may participate in regulating signaling pathways. These data lay the foundation for identifying and studying conserved AS events in the monocots, particularly across grass species, and this conserved AS resource identifies an additional layer between genotype to phenotype that may impact future crop improvement efforts."}, {"title": "UpSetR: An R Package For The Visualization Of Intersecting Sets And Their Properties", "url": "https://www.biorxiv.org/content/early/2017/03/25/120600", "tag": "Bioinformatics", "abstract": "Venn and Euler diagrams are a popular yet inadequate solution for quantitative visualization of set intersections. A scalable alternative to Venn and Euler diagrams for visualizing intersecting sets and their properties is needed. We developed UpSetR, an open source R package that employs a scalable matrix-based visualization to show intersections of sets, their size, and other properties. UpSetR is available at https://cran.r-project.org/package=UpSetR and released under the MIT License. A Shiny app is available at https://gehlenborglab.shinyapps.io/upsetr."}, {"title": "Fast functional annotation of metagenomic shotgun data by DNA alignment to a microbial gene catalog", "url": "https://www.biorxiv.org/content/early/2017/03/25/120402", "tag": "Bioinformatics", "abstract": "Background: Metagenomic shotgun sequencing is becoming increasingly popular to study microbes associated with the human body and in environmental samples. A key goal of shotgun metagenomic sequencing is to identify gene functions and metabolic pathways that differ between samples or conditions. However, current methods to identify function in the large number of reads in a highthroughput sequence data file rely on the computationally intensive and low stringency approach of mapping each read to a generic database of proteins or reference microbial genomes. Results: We have developed an alternative analysis approach for shotgun metagenomic sequence data utilizing Bowtie2 DNA-DNA alignment of the reads to a database of well annotated genes compiled from human microbiome data. This method is rapid, and provides high stringency matches (>90% DNA sequence identity) of shotgun metagenomics reads to genes with annotated functions. We demonstrate the use of this method with synthetic data, Human Microbiome Project shotgun metagenomic data sets, and data from a study of liver disease. Differentially abundant KEGG gene functions can be detected in these experiments. Conclusions: Functional annotation of metagenomic shotgun sequence reads can be accomplished by rapid DNA-DNA matching to a custom database of microbial sequences using the Bowtie2 sequence alignment tool. This method can be used for a variety of microbiome studies and allows functional analysis which is otherwise computationally demanding. This rapid annotation method is freely available as a Galaxy workflow within a Docker image."}, {"title": "STAG-CNS: An Order-Aware Conserved Non-coding Sequences Discovery Tool For Arbitrary Numbers of Species", "url": "https://www.biorxiv.org/content/early/2017/03/24/120428", "tag": "Bioinformatics", "abstract": "One method for identifying noncoding regulatory regions of a genome is to quantify rates of divergence between related species, as functional sequence will generally diverge more slowly. Most approaches to identifying these conserved noncoding sequences (CNS) based on alignment have had relatively large minimum sequence lengths (15 base pair) compared to the average length of known transcription factor binding sites. To circumvent this constraint, STAG-CNS integrates data from the promoters of conserved orthologous genes in three or more species simultaneously. Using data from up to six grass species made it possible to identify conserved sequences as short at 9 base pairs with FDP < 0.05. These CNS exhibit greater overlap with open chromatin regions identified using DNase I hypersensitivity, and are enriched in the promoters of genes involved in transcriptional regulation. STAG-CNS was further employed to characterize loss of conserved noncoding sequences associated with retained duplicate genes from the ancient maize polyploidy. Genes with fewer retained CNS show lower overall expression, although this bias is more apparent in samples of complex organ systems containing many cell types, suggesting CNS loss may correspond to a reduced number of expression contexts rather than lower expression levels across the entire ancestral expression domain."}, {"title": "STAR-Fusion: Fast and Accurate Fusion Transcript Detection from RNA-Seq", "url": "https://www.biorxiv.org/content/early/2017/03/24/120295", "tag": "Bioinformatics", "abstract": "Motivation: Fusion genes created by genomic rearrangements can be potent drivers of tumorigenesis. However, accurate identification of functionally fusion genes from genomic sequencing requires whole genome sequencing, since exonic sequencing alone is often insufficient. Transcriptome sequencing provides a direct, highly effective alternative for capturing molecular evidence of expressed fusions in the precision medicine pipeline, but current methods tend to be inefficient or insufficiently accurate, lacking in sensitivity or predicting large numbers of false positives. Here, we describe STAR-Fusion, a method that is both fast and accurate in identifying fusion transcripts from RNA-Seq data. Results: We benchmarked STAR-Fusion's fusion detection accuracy using both simulated and genuine Illumina paired-end RNA-Seq data, and show that it has superior performance compared to popular alternative fusion detection methods. Availability and implementation: STAR-Fusion is implemented in Perl, freely available as open source software at http://star-fusion.github.io, and supported on Linux."}, {"title": "A fast approximate algorithm for mapping long reads to large reference databases", "url": "https://www.biorxiv.org/content/early/2017/03/24/103812", "tag": "Bioinformatics", "abstract": "Emerging single-molecule sequencing technologies from Pacific Biosciences and Oxford Nanopore have revived interest in long read mapping algorithms. Alignment-based seed-and-extend methods demonstrate good accuracy, but face limited scalability, while faster alignment-free methods typically trade decreased precision for efficiency. In this paper, we combine a fast approximate read mapping algorithm based on minimizers with a novel MinHash identity estimation technique to achieve both scalability and precision. In contrast to prior methods, we develop a mathematical framework that defines the types of mapping targets we uncover, establish probabilistic estimates of p-value and sensitivity, and demonstrate tolerance for alignment error rates up to 20%. With this framework, our algorithm automatically adapts to different minimum length and identity requirements and provides both positional and identity estimates for each mapping reported. For mapping human PacBio reads to the hg38 reference, our method is 290x faster than BWA-MEM with a lower memory footprint and recall rate of 96%. We further demonstrate the scalability of our method by mapping noisy PacBio reads (each \u2265 5 kbp in length) to the complete NCBI RefSeq database containing 838 Gbp of sequence and > 60,000 genomes."}, {"title": "ATLAS: Analysis Tools for Low-depth and Ancient Samples", "url": "https://www.biorxiv.org/content/early/2017/03/24/105346", "tag": "Bioinformatics", "abstract": "Summary: Post-mortem damage (PMD) obstructs the proper analysis of ancient DNA samples and can currently only be addressed by removing or down-weighting potentially damaged data. Here we present ATLAS, a suite of methods to accurately genotype and estimate genetic diversity from ancient samples, while accounting for PMD. It works directly from raw BAM files and enables the building of complete and customized pipelines for the analysis of ancient and other low-depth samples in a very user-friendly way. Based on simulations we show that, in the presence of PMD, a dedicated pipeline of ATLAS calls genotypes more accurately than the state-of-the-art pipeline of GATK combined with mapDamage 2.0. Availability: ATLAS is an open-source C++ program freely available at https://bitbucket.org/phaentu/atlas."}, {"title": "Network analysis of patient flow in two UK acute care hospitals identifies key sub-networks for A&E performance", "url": "https://www.biorxiv.org/content/early/2017/03/24/120188.1", "tag": "Bioinformatics", "abstract": "The topology of the patient flow network in a hospital is complex, comprising hundreds of overlapping patient journeys, and is a determinant of operational efficiency. To understand the network architecture of patient flow, we performed a data-driven network analysis of patient flow through two acute hospital sites of King's College Hospital NHS Foundation Trust. Administration databases were queried for all intra-hospital patient transfers in an 18-month period and modelled as a dynamic weighted directed graph. A 'core' subnetwork containing only 13-17% of all edges channelled 83-90% of the patient flow, while an 'ephemeral' network constituted the remainder. Unsupervised cluster analysis and differential network analysis identified sub-networks where traffic is most associated with A&E performance the following day. Increased flow to clinical decision units was associated with the best A&E performance in both sites. The component analysis also detected a weekend effect on patient transfers which was not associated with performance. We have performed the first data-driven hypothesis-free analysis of patient flow which can enhance understanding of whole healthcare systems. Such analysis can drive transformation in healthcare as it has in industries such as manufacturing."}, {"title": "JfxNgs : A BAM/VCF viewer with javascript-based filtering/reformatting functionalities", "url": "https://www.biorxiv.org/content/early/2017/03/24/120196", "tag": "Bioinformatics", "abstract": "Visualizing BAM and VCF files is a common task for biologists, but they're missing a way to filter and to explore the details of each short-read or variation. In that context, we wrote an interactive java-based interface named JfxNgs that uses javascript snippets to filter and reformat BAM and VCF files."}, {"title": "BRANE Clust: Cluster-Assisted Gene Regulatory Network Inference Refinement", "url": "https://www.biorxiv.org/content/early/2017/03/24/114769", "tag": "Bioinformatics", "abstract": "Discovering meaningful gene interactions is crucial for the identification of novel regulatory processes in cells. Building accurately the related graphs remains challenging due to the large number of possible solutions from available data. Nonetheless, enforcing a priori on the graph structure, such as modularity, may reduce network indeterminacy issues. BRANE Clust (Biologically-Related A priori Network Enhancement with Clustering) refines gene regulatory network (GRN) inference thanks to cluster information. It works as a post-processing tool for inference methods (i.e. CLR, GENIE3). In BRANE Clust, the clustering is based on the inversion of a system of linear equations involving a graph-Laplacian matrix promoting a modular structure. Our approach is validated on DREAM4 and DREAM5 datasets with objective measures, showing significant comparative improvements. We provide additional insights on the discovery of novel regulatory or co-expressed links in the inferred Escherichia coli network evaluated using the STRING database. The comparative pertinence of clustering is discussed computationally (SIMoNe, WGCNA, X-means) and biologically (RegulonDB). BRANE Clust software is available at: http://www-syscom.univ-mlv.fr/~pirayre/Codes-GRN-BRANE-clust.html"}, {"title": "Transcripts evolutionary conservation and structural dynamics give insights into the role of alternative splicing for the JNK family.", "url": "https://www.biorxiv.org/content/early/2017/03/23/119891", "tag": "Bioinformatics", "abstract": "Alternative splicing (AS), by producing several transcript isoforms from the same gene, has the potential to greatly expand the proteome in eukaryotes. Its deregulation has been associated to the development of various diseases, including cancer. Although the AS mechanisms are well described at the genomic level, little is known about the contribution of AS to protein evolution and the impact of AS at the level of the protein structure. Here, we address both issues by reconstructing the evolutionary history of the c-Jun N-terminal kinase (JNK) family, and by describing the tertiary structures and dynamical behavior of several JNK isoforms. JNKs bear a great interest for medicinal research as they are involved in crucial signaling pathways. We reconstruct the phylogenetic forest relating 60 JNK transcripts observed in 7 species. We use it to estimate the evolutionary conservation of transcripts and to identify ASEs likely to be functionally important. We show that ASEs of ancient origin and having significant functional outcome may induce very subtle changes on the protein's structural dynamics. We also propose that phylogenetic reconstruction, combined with structural modeling, can help identify new potential therapeutic targets. Finally, we show that transcripts likely non-functional (i.e. not conserved) display peculiar sequence and structural properties. Our approach is implemented in PhyloSofS (Phylogenies of Splicing Isoforms Structures), a fully automated computational tool that infers plausible evolutionary scenarios explaining a set of transcripts observed in several species and models the three-dimensional structures of the protein isoforms. PhyloSofS has broad applicability and can be used, for example, to study transcripts diversity between different individuals (e.g. patients affected by a particular disease). It is freely available at www.lcqb.upmc.fr/PhyloSofS."}, {"title": "Genomic analysis of human polymorphisms affecting drug-protein interactions", "url": "https://www.biorxiv.org/content/early/2017/03/23/119933", "tag": "Bioinformatics", "abstract": "Human genetic variability is thought to account for a substantial fraction of individual biochemical characteristics \u2014 in biomedical sense, of individual drug response. However, only a handful of human genetic variants have been linked to medication outcomes. Here, we combine data on drug-protein interactions and human genome sequences to assess the impact of human variation on their binding affinity. Using data from the complexes of FDA-drugs and drug-like compounds, we predict SNPs substantially affecting the protein-ligand binding affinities. We estimate that an average individual carries ~6 SNPs affecting ~5 different FDA-approved drugs from among all of the approved compounds. SNPs affecting drug-protein binding affinity have low frequency in the population indicating that the genetic component for many ADEs may be highly personalized with each individual carrying a unique set of relevant SNPs. The reduction of ADEs, therefore, may primarily rely on the application of computational genome analysis in the clinic rather than the experimental study of common SNPs."}, {"title": "Integrated Structure-Transcription analysis of small molecules reveals widespread noise in drug-induced transcriptional responses and a transcriptional signature for drug-induced phospholipidosis", "url": "https://www.biorxiv.org/content/early/2017/03/23/119990", "tag": "Bioinformatics", "abstract": "We performed an integrated analysis of drug chemical structures and drug-induced transcriptional responses. We demonstrated that a network representing 3D structural similarities among 5,452 compounds can be used to automatically group together drugs with similar scaffolds and mode-of-action. We then compared the structural network to a network representing transcriptional similarities among a subset of 1,309 drugs for which transcriptional response were available in the Connectivity Map dataset. Analysis of structurally similar, but transcriptionally different, drugs sharing the same mode of action (MOA) enabled us to detect and remove weak and noisy transcriptional responses, greatly enhancing the reliability and usefulness of transcription-based approaches to drug discovery and drug repositioning. Analysis of transcriptionally similar, but structurally different drugs with unrelated MOA, led us to the identification of a toxic transcriptional signature indicative of lysosomal stress (lysosomotropism) and lipid accumulation (phospholipidosis) partially masking the target-specific transcriptional effects of these drugs. We further demonstrated by High Content Screening that this transcriptional signature is caused by the activation of the transcription factor TFEB, a master regulator of lysosomal biogenesis and autophagy. Our results show that chemical structures and transcriptional profiles provide complementary information and that combined analysis can lead to new insights on on- and off-target effects of small molecules."}, {"title": "AllSome Sequence Bloom Trees", "url": "https://www.biorxiv.org/content/early/2017/03/23/090464", "tag": "Bioinformatics", "abstract": "The ubiquity of next generation sequencing has transformed the size and nature of many databases, pushing the boundaries of current indexing and searching methods. One particular example is a database of 2,652 human RNA-seq experiments uploaded to the Sequence Read Archive. Recently, Solomon and Kingsford proposed the Sequence Bloom Tree data structure and demonstrated how it can be used to accurately identify SRA samples that have a transcript of interest potentially expressed. In this paper, we propose an improvement called the AllSome Sequence Bloom Tree. Results show that our new data structure significantly improves performance, reducing the tree construction time by 52.7% and query time by 39 - 85%. Notably, it can query a batch of 198,074 queries in under 8 hours(compared to around two days previously) and a whole set of k-mers from a sequencing experiment(about 27 mil k-mers) in under 11 minutes."}, {"title": "Assessing The Reliability Of Spike-In Normalization For Analyses Of Single-Cell RNA Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/03/23/119784", "tag": "Bioinformatics", "abstract": "By profiling the transcriptomes of individual cells, single-cell RNA sequencing provides unparalleled resolution to study cellular heterogeneity. However, this comes at the cost of high technical noise, including cell-specific biases in capture efficiency and library generation. One strategy for removing these biases is to add a constant amount of spike-in RNA to each cell, and to scale the observed expression values so that the coverage of spike-in RNA is constant across cells. This approach has previously been criticized as its accuracy depends on the precise addition of spike-in RNA to each sample, and on similarities in behaviour (e.g., capture efficiency) between the spike-in and endogenous transcripts. Here, we perform mixture experiments using two different sets of spike-in RNA to quantify the variance in the amount of spike-in RNA added to each well in a plate-based protocol. We also obtain an upper bound on the variance due to differences in behaviour between the two spike-in sets. We demonstrate that both factors are small contributors to the total technical variance and have only minor effects on downstream analyses such as detection of highly variable genes and clustering. Our results suggest that spike-in normalization is reliable enough for routine use in single-cell RNA sequencing data analyses."}, {"title": "Phandango: an interactive viewer for bacterial population genomics.", "url": "https://www.biorxiv.org/content/early/2017/03/22/119545", "tag": "Bioinformatics", "abstract": "Fully exploiting the wealth of data in current bacterial population genomics datasets requires synthesising and integrating different types of analysis across millions of base pairs in hundreds or thousands of isolates. Current approaches often use static representations of phylogenetic, epidemiological, statistical and evolutionary analysis results that are difficult to relate to one another. Phandango is an interactive application running in a web browser allowing fast exploration of large-scale population genomics datasets combining the output from multiple genomic analysis methods in an intuitive and interactive manner. Phandango is a web application freely available for use at https://jameshadfield.github.io/phandango and includes a diverse collection of datasets as examples. Source code together with a detailed wiki page is available on GitHub at https://github.com/jameshadfield/phandango"}, {"title": "LncATLAS database for subcellular localisation of long noncoding RNAs", "url": "https://www.biorxiv.org/content/early/2017/03/22/116335", "tag": "Bioinformatics", "abstract": "Background: The subcellular localisation of long noncoding RNAs (lncRNAs) holds valuable clues to their molecular function. However, measuring localisation of newly-discovered lncRNAs involves time-consuming and costly experimental methods. Results: We have created \"LncATLAS\", a comprehensive resource of lncRNA localisation in human cells based on RNA-sequencing datasets. Altogether, 6768 GENCODE-annotated lncRNAs are represented across various compartments of 15 cell lines. We introduce \"Relative concentration index\" (RCI) as a useful measure of localisation derived from ensemble RNAseq measurements. LncATLAS is accessible through an intuitive and informative webserver, from which lncRNAs of interest are accessed using identifiers or names. Localisation is presented across cell types and organelles, and may be compared to the distribution of all other genes. Publication-quality figures and raw data tables are automatically generated with each query, and the entire dataset is also available to download. Conclusions: LncATLAS makes lncRNA subcellular localisation data available to the widest possible number of researchers. It is available at lncATLAS.crg.eu."}, {"title": "Proteus: A Random Forest Classifier to Predict Disorder-to-Order Transitioning Binding Regions in Intrinsically Disordered Proteins.", "url": "https://www.biorxiv.org/content/early/2017/03/22/080788", "tag": "Bioinformatics", "abstract": "The focus of the computational structural biology community has taken a dramatic shift over the past one-and-a-half decades from the classical protein structure prediction problem to the possible understanding of intrinsically disordered proteins (IDP) or proteins containing regions of disorder (IDPR). The current interest lies in the unraveling of a disorder-to-order transitioning code embedded in the amino acid sequences of IDPs / IDPRs. Disordered proteins are characterized by an enormous amount of structural plasticity which makes them promiscuous in binding to different partners, multi-functional in cellular activity and atypical in folding energy landscapes resembling partially folded molten globules. Also, their involvement in several deadly human diseases (e.g. cancer, cardiovascular and neurodegenerative diseases) makes them attractive drug targets, and important for a biochemical understanding of the disease(s). The study of the structural ensemble of IDPs is rather difficult, in particular for transient interactions. When bound to a structured partner, an IDPR adapts an ordered conformation in the complex. The residues that undergo this disorder-to-order transition are called protean residues, generally found in short contiguous stretches and the first step in understanding the modus operandi of an IDP / IDPR would be to predict these residues. There are a few available methods which predict these protean segments from their amino acid sequences; however, their performance reported in the literature leaves clear room for improvement. With this background, the current study presents 'Proteus', a random forest classifier that predicts the likelihood of a residue undergoing a disorder-to-order transition upon binding to a potential partner protein. The prediction is based on features that can be calculated using the amino acid sequence alone. Proteus compares favorably with existing methods predicting twice as many true positives as the second best method (55% vs. 27%) with a much higher precision on an independent data set. The current study also sheds some light on a possible 'disorder-to-order' transitioning consensus, untangled, yet embedded in the amino acid sequence of IDPs. Some guidelines have also been suggested for proceeding with a real-life structural modeling involving an IDPR using Proteus."}, {"title": "CRISPulator: A Discrete Simulation Tool For Pooled Genetic Screens", "url": "https://www.biorxiv.org/content/early/2017/03/22/119131", "tag": "Bioinformatics", "abstract": "The rapid adoption of CRISPR technology has enabled biomedical researchers to conduct CRISPR-based genetic screens in a pooled format. The quality of results from such screens is heavily dependent on optimal screen design, which also affects cost and scalability. We present CRISPulator, a computational tool that simulates the impact of screen parameters on the robustness of screen results, thereby enabling users to build intuition and insights that will inform their experimental strategy. We illustrate its power by deriving non-obvious rules for optimal screen design."}, {"title": "Assembly Of Whole-Chromosome Pseudomolecules For Polyploid Plant Genomes Using Outcrossed Mapping Populations", "url": "https://www.biorxiv.org/content/early/2017/03/22/119271", "tag": "Bioinformatics", "abstract": "The assembly of whole-chromosome pseudomolecules for plant genomes remains challenging due to polyploidy and high repeat content. We developed an approach for constructing complete pseudomolecules for polyploid species using genotyping-by-sequencing data from outcrossing mapping populations coupled with high coverage whole genome sequence data of a reference genome. Our approach combines de novo assembly with linkage mapping to arrange scaffolds into pseudomolecules. We show that the method is able to reconstruct simulated chromosomes for both diploid and tetraploid genomes. Comparisons to three existing genetic mapping tools show that our method outperforms the other methods in accuracy on both grouping and ordering, and is robust to the presence of substantial amounts of missing data and genotyping errors. We applied our method to three real datasets including a diploid Ipomoea trifida and two tetraploid potato mapping populations. The linkage maps show significant concordance with the reference chromosomes. We resolved seven assembly errors for the published Ipomoea trifida genome assembly as well as anchored an unplaced scaffold in the published potato genome."}, {"title": "UBiT2: a client-side web-application for gene expression data analysis", "url": "https://www.biorxiv.org/content/early/2017/03/22/118992", "tag": "Bioinformatics", "abstract": "We present a purely client-side web-application, UBiT2 (User-friendly BioInformatics Tools), that provides installation-free, offline alignment, analysis, and visualization of RNA-sequencing as well as qPCR data. Analysis modules were designed with single cell transcriptomic analysis in mind. Using just a browser, users can perform standard analyses such as quality control, filtering, hierarchical clustering, principal component analysis, differential expression analysis, gene set enrichment testing, and more, all with interactive visualizations and exportable publication-quality figures. We apply UBiT2 to recapitulate findings from single cell RNA-seq and Fluidigm BiomarkTM multiplex RT-qPCR gene expression datasets. UBiT2 is available at http://pklab.med.harvard.edu/jean/ubit2/index.html with open-source code available at https://github.com/JEFworks/ubit2."}, {"title": "Gene Length And Detection Bias In Single Cell RNA Sequencing Protocols", "url": "https://www.biorxiv.org/content/early/2017/03/22/119222", "tag": "Bioinformatics", "abstract": "Single cell RNA sequencing (scRNA-seq) has rapidly gained popularity for profiling transcriptomes of hundreds to thousands of single cells. This technology has led to the discovery of novel cell types and revealed insights into the development of complex tissues. However, many technical challenges need to be overcome during data generation. Due to minute amounts of starting material, samples undergo extensive amplification, increasing technical variability. A solution for mitigating amplification biases is to include Unique Molecular Identifiers (UMIs), which tag individual molecules. Transcript abundances are then estimated from the number of unique UMIs aligning to a specific gene and PCR duplicates resulting in copies of the UMI are not included in expression estimates. Here we investigate the effect of gene length bias in scRNA-Seq across a variety of datasets differing in terms of capture technology, library preparation, cell types and species. We find that scRNA-seq datasets that have been sequenced using a full-length transcript protocol exhibit gene length bias akin to bulk RNA-seq data. Specifically, shorter genes tend to have lower counts and a higher rate of dropout. In contrast, protocols that include UMIs do not exhibit gene length bias, and have a mostly uniform rate of dropout across genes of varying length. Across four different scRNA-Seq datasets profiling mouse embryonic stem cells (mESCs), we found the subset of genes that are only detected in the UMI datasets tended to be shorter, while the subset of genes detected only in the full-length datasets tended to be longer. We briefly discuss the role of these genes in the context of differential expression testing and GO analysis. In addition, despite clear differences between UMI and full-length transcript data, we illustrate that full-length and UMI data can be combined to reveal underlying biology influencing expression of mESCs."}, {"title": "Cross-Platform Normalization Enables Machine Learning Model Training On Microarray And RNA-Seq Data Simultaneously", "url": "https://www.biorxiv.org/content/early/2017/03/21/118349", "tag": "Bioinformatics", "abstract": "Motivation: Large compendia of gene expression data have proven valuable for the discovery of novel biological relationships. The majority of available RNA assays are run on microarray, while RNA-seq is becoming the platform of choice for new experiments. The data structure and distributions between the platforms differ, making it challenging to combine them. We performed supervised and unsupervised machine learning evaluations, as well as differential expression analyses, to assess which normalization methods are best suited for combining microarray and RNA-seq data. Results: We find that quantile and Training Distribution Matching normalization allow for supervised and unsupervised model training on microarray and RNA-seq data simultaneously. Nonparanormal normalization and z-scores are also appropriate for some applications, including differential expression analysis. Availability and Implementation: These analyses were performed in R and are available at https://www.github.com/greenelab/RNAseq_titration_results under a BSD-3 clause license."}, {"title": "SIMLR: A Tool For Large-Scale Single-Cell Analysis By Multi-Kernel Learning", "url": "https://www.biorxiv.org/content/early/2017/03/21/118901", "tag": "Bioinformatics", "abstract": "Motivation: We here present SIMLR (Single-cell Interpretation via Multi-kernel LeaRning), an open-source tool that implements a novel framework to learn a cell-to-cell similarity measure from single-cell RNA-seq data. SIMLR can be effectively used to perform tasks such as dimen- sion reduction, clustering, and visualization of heterogeneous populations of cells. SIMLR was benchmarked against state-of-the-art methods for these three tasks on several public datasets, showing it to be scalable and capable of greatly improving clustering performance, as well as providing valuable insights by making the data more interpretable via better a visualization. Availability and Implementation: SIMLR is available on GitHub in both R and MATLAB implementations. Furthermore, it is also available as an R package on bioconductor.org. Contact: bowang87@stanford.edu or daniele.ramazzotti@stanford.edu Supplementary Information: Supplementary data are available at Bioinformatics online."}, {"title": "SmartScope2: Simultaneous Imaging and Reconstruction of Neuronal Morphology", "url": "https://www.biorxiv.org/content/early/2017/03/21/118927", "tag": "Bioinformatics", "abstract": "Quantitative analysis of neuronal morphology is critical in cell type classification and for deciphering how structure gives rise to function in the brain. Most current approaches to imaging and tracing neuronal 3D morphology are data intensive. We introduce SmartScope2, the first open source, automated neuron reconstruction machine integrating online image analysis with automated multiphoton imaging. SmartScope2 takes advantage of a neuron's sparse morphology to improve imaging speed and reduce image data stored, transferred and analyzed. We show that SmartScope2 is able to produce the complex 3D morphology of human and mouse cortical neurons with six-fold reduction in image data requirements and three times the imaging speed compared to conventional methods."}, {"title": "Correcting for Dependent P-values in Rhythm Detection", "url": "https://www.biorxiv.org/content/early/2017/03/20/118547", "tag": "Bioinformatics", "abstract": "There is much interest in using genome-wide expression time series to identify circadian genes. Several methods have been developed to test for rhythmicity in sparsely sampled time series typical of such measurements. Because these methods are statistical in nature, they rely on estimating the probabilities that patterns arise by chance (i.e., p-values). Here we show that leading methods implicitly make inappropriate assumptions of independence when estimating p-values. We show how to correct for the dependence to obtain accurate estimates for statistical significance during rhythm detection."}, {"title": "GAMtools: an automated pipeline for analysis of Genome Architecture Mapping data", "url": "https://www.biorxiv.org/content/early/2017/03/20/114710", "tag": "Bioinformatics", "abstract": "Genome Architecture Mapping (GAM) is a recently developed method for mapping chromatin interactions genome-wide. GAM is based on sequencing genomic DNA extracted from thin cryosections of cell nuclei. As a new approach, GAM datasets require specialized analytical tools and approaches. Here we present GAMtools, a pipeline for analysing GAM datasets. GAMtools covers the automated mapping of raw next-generation sequencing data generated by GAM, detection of genomic regions present in each nuclear slice, calculation of quality control metrics, generation of inferred proximity matrices, plotting of heatmaps and detection of genomic features for which chromatin interactions are enriched/depleted."}, {"title": "RegCyanoDB: a database of cyanobacterial regulatory interactions", "url": "https://www.biorxiv.org/content/early/2017/03/20/117127", "tag": "Bioinformatics", "abstract": "Background: Cyanobacteria are photoautotrophic organisms with environmental, evolutionary, and industrial importance. Knowledge of its regulatory interactions are important to predict, optimise, and engineer their characteristics. However, at present, very few of their regulatory interactions are known. The regulatory interactions are known only for a few model organisms such as Escherichia coli due to technical and economical constraints, which are unlikely to change soon. Thus, mapping of regulatory interactions from well-studied organisms to less-studied organisms by using computational techniques is widely used. Reverse Best Hit (RBH), with appropriate algorithm parameters, is a simple and efficient method for detecting functional homologs Description: We predict the regulatory interactions in 30 strains of cyanobacteria using the known regulatory interactions from the best-studied organism, E. coli. RBH method with appropriate parameters is used to identify the functional homologs. An interaction is mapped to a cyanobacterial strain if functional homologs exist for a known transcription factor and its target gene. The confidence of the detected homologs and interactions are also provided. Since RBH is a conservative method, homolog-grouping is performed to recover lost putative interactions. A database of the predicted interactions from all the 30 strains of cyanobacteria is constructed. Conclusion: RegcyanoDB contains 20,280 interactions with confidence levels for 30 cyanobacterial strains. The predicted regulatory interactions exhibit a scale free network topology as observed in model organisms. The interacting genes in E. coli and cyanobacteria are mostly found to have the same gene annotation. This database can be used for posing novel hypotheses and validation studies in wet-lab and computational domains. The database is available at http://www.che.iitb.ac.in/grn/RegCyanoDB/"}, {"title": "Bootstrapping and Empirical Bayes Methods Improve Rhythm Detection in Sparsely Sampled Data", "url": "https://www.biorxiv.org/content/early/2017/03/20/118521", "tag": "Bioinformatics", "abstract": "Motivation: There is much interest in using genome-wide expression time series to identify circadian genes. However, the cost and effort of such measurements often limits data collection. Consequently, it is difficult to assess the experimental uncertainty in the measurements and, in turn, to detect periodic patterns with statistical confidence. Results: We show that parametric bootstrapping and empirical Bayes methods for variance shrinkage can improve rhythm detection in genome-wide expression time series. We demonstrate these approaches by building on the empirical JTK CYCLE method (eJTK) to formulate a method that we term BooteJTK. Our procedure rapidly and accurately detects cycling time series by combining information about measurement uncertainty with information about the rank order of the time series values. We exploit a publicly available genome-wide dataset with high time resolution to show that BooteJTK provides more consistent rhythm detection than existing methods at typical sampling frequencies. Then, we apply BooteJTK to genome-wide expression time series from multiple tissues and show that it reveals biologically sensible tissue relationships that eJTK misses. Availability: Bootstrap eJTK (BooteJTK) is implemented in Python and is freely available on GitHub at https://github.com/alanlhutchison/BooteJTK ."}, {"title": "Identifiers for the 21st century: How to design, provision, and reuse persistent identifiers to maximize utility and impact of life science data", "url": "https://www.biorxiv.org/content/early/2017/03/20/117812", "tag": "Bioinformatics", "abstract": "In many disciplines, data is highly decentralized across thousands of online databases (repositories, registries, and knowledgebases). Wringing value from such databases depends on the discipline of data science and on the humble bricks and mortar that make integration possible; identifiers are a core component of this integration infrastructure. Drawing on our experience and on work by other groups, we outline ten lessons we have learned about the identifier qualities and best practices that facilitate large-scale data integration. Specifically, we propose actions that identifier practitioners (database providers) should take in the design, provision and reuse of identifiers; we also outline important considerations for those referencing identifiers in various circumstances, including by authors and data generators. While the importance and relevance of each lesson will vary by context, there is a need for increased awareness about how to avoid and manage common identifier problems, especially those related to persistence and web-accessibility/resolvability. We focus strongly on web-based identifiers in the life sciences; however, the principles are broadly relevant to other disciplines."}, {"title": "MELODI - Mining Enriched Literature Objects to Derive Intermediates", "url": "https://www.biorxiv.org/content/early/2017/03/20/118513.1", "tag": "Bioinformatics", "abstract": "Motivation: The scientific literature contains a wealth of information from different fields on potential disease mechanisms. However, prioritising mechanisms for further analytical evaluation presents enormous challenges in terms of the quantity and diversity of published research. The application of data mining approaches to the literature offers the potential to identify and prioritise mechanisms for more focused and detailed analysis. Results: Here we present MELODI, a literature mining platform that can identify mechanistic pathways between any two biomedical concepts. Two case studies demonstrate the potential uses of MELODI and how it can generate hypotheses for further investigation. Firstly, an analysis of ERG and prostate cancer derives the intermediate transcription factor SP1, recently confirmed to be physically interacting with ERG. Secondly, examining the relationship between a new potential risk factor for pancreatic cancer identifies possible mechanistic insights which can be studied in vitro. Availability: MELODI has been implemented as a Python/Django web application, and is freely available to use at www.melodi.biocompute.org.uk"}, {"title": "A Network Integration Approach for Drug-Target Interaction Prediction and Computational Drug Repositioning from Heterogeneous Information", "url": "https://www.biorxiv.org/content/early/2017/03/20/100305", "tag": "Bioinformatics", "abstract": "The emergence of large-scale genomic, chemical and pharmacological data provides new opportunities for drug discovery and repositioning. Systematic integration of these heterogeneous data not only serves as a promising tool for identifying new drug-target interactions (DTIs), which is an important step in drug development, but also provides a more complete understanding of the molecular mechanisms of drug action. In this work, we integrate diverse drug-related information, including drugs, proteins, diseases and side-effects, together with their interactions, associations or similarities, to construct a heterogeneous network with 12,015 nodes and 1,895,445 edges. We then develop a new computational pipeline, called DTINet, to predict novel drug-target interactions from the constructed heterogeneous network. Specifically, DTINet focuses on learning a low-dimensional vector representation of features for each node, which accurately explains the topological properties of individual nodes in the heterogeneous network, and then predicts the likelihood of a new DTI based on these representations via a vector space projection scheme. DTINet achieves substantial performance improvement over other state-of-the-art methods for DTI prediction. Moreover, we have experimentally validated the novel interactions between three drugs and the cyclooxygenase (COX) protein family predicted by DTINet, and demonstrated the new potential applications of these identified COX inhibitors in preventing inflammatory diseases. These results inculcate that DTINet can provide a practically useful tool for integrating heterogeneous information to predict new drug-target interactions and repurpose existing drugs. The source code of DTINet and the input heterogeneous network data can be downloaded from http://github.com/luoyunan/DTINet."}, {"title": "Title Survey on Prevalence of Canine Cutaneous Myiasis in Some Selected Kebeles of Dire Dawa City Administration", "url": "https://www.biorxiv.org/content/early/2017/03/20/110502", "tag": "Bioinformatics", "abstract": "ABSTRACT A cross sectional study of canine cutaneous myiasis was conducted in five randomly selected kebeles of Dire Dawa Administrative council from December 2009 up to April 2010 to determine the prevalence of canine cutaneous myiasis and to assess factors that determine the occurrence of the disease specifically in dog. Active questionnaire survey among 60 households were used for which 384 dogs were sampled. From a total of 384 dogs, 162 (42.19%) were found harboring the disease cutaneous myiasis among this 120 (74.07%) were infested with the 3rd and 2nd instar larvae of Cordylobiaantropophaga. whereas the remaining 42 (25.93%) observed dogs were found infested with cutaneous myiasis. The larvae were identified in Dire Dawa regional diagnostic veterinary parasitology laboratory. Analysis of active questionnaire survey showed that there is no statistically significance difference in the prevalence of disease among different breeds and sexes (P >0.05). In this study, an overall prevalence rate of 162 (42.19%) was found with a statistically significant association among different age groups, housing system and living area (kebele) (P<0.05). Higher prevalence was recorded at 02 kebele (Sabian area) 59 (54.65%), 03 Kebele (Depo and number-one), 43 (44.33%), 04 Keble (Gende kore and Greek camp) 33 (37.50%), Addis Ketema. 27 (51.92%) and 05 Keeble (Dechatu) 0 (0%). There was 121 (49.59%) confined dogs and 41 (29.29%) were stray dogs which let out without any control, and puppies of age less than 6 month old (71.56 %), and dogs of age range between 6 months and 18 months (79.03%) while those of greater than 18 months (16.43%), were least affected. Key words: Canine; Cordylobiaantropophaga; Dire Dawa; Myiasis; Prevalence; tumbu fly"}, {"title": "ChIPSeqFPro, a pipeline for sequential processing of ChIP-Seq fastq to bigwig files", "url": "https://www.biorxiv.org/content/early/2017/03/18/118281", "tag": "Bioinformatics", "abstract": "ChIPSeqFPro is a pipeline that automates processing of a collection of ChIPSeq or ATAC-Seq data starting from the gzip compressed fastq files. It performs the quality control using FastQC, mapping to the human genome hg19 or mouse mm10 using BWA mapper, for both single read or paired end sequencing fastq files, followed with sam to bam conversion using samtools view, creates statistics on bam files using samtools flagstat, peak calling with MACS, and finally creates high resolution bigwig files from bam files using a custom script bam2bigwig that invokes bedtools bamtobed and UCSC scripts, bedItemOverlapCount, bedGraphToBigWig and fetchChromSizes."}, {"title": "SPRING: a kinetic interface for visualizing high dimensional single-cell expression data", "url": "https://www.biorxiv.org/content/early/2017/03/18/090332", "tag": "Bioinformatics", "abstract": "Motivation: Single-cell gene expression profiling technologies can map the cell states in a tissue or organism. As these technologies become more common, there is a need for computational tools to explore the data they produce. In particular, existing data visualization approaches are imperfect for studying continuous gene expression topologies. Results: Force-directed layouts of k-nearest-neighbor graphs can visualize continuous gene expression topologies in a manner that preserves high-dimensional relationships and allows manually exploration of different stable two- dimensional representations of the same data. We implemented an interactive web-tool to visualize single-cell data using force-directed graph layouts, called SPRING. SPRING reveals more detailed biological relationships than existing approaches when applied to branching gene expression trajectories from hematopoietic progenitor cells. Visualizations from SPRING are also more reproducible than those of stochastic visualization methods such as tSNE, a state-of-the-art tool. Availability: https://kleintools.hms.harvard.edu/tools/spring.html, https://github.com/AllonKleinLab/SPRING/"}, {"title": "deStruct: Accurate Rearrangement Detection using Breakpoint Specific Realignment", "url": "https://www.biorxiv.org/content/early/2017/03/18/117523", "tag": "Bioinformatics", "abstract": "We propose that a breakpoint specific alignment procedure would improve breakpoint prediction. Our method, deStruct, uses multiple stages of realignment and clustering to progressively refine breakpoint prediction quality and accuracy. We show using simulated data that deStruct predicts breakpoints with higher sensitivity and specificity than existing breakpoint prediction tools."}, {"title": "shinyheatmap: ultra fast low memory heatmap software for big data genomics", "url": "https://www.biorxiv.org/content/early/2017/03/17/076463", "tag": "Bioinformatics", "abstract": "Transcriptomics, metabolomics, metagenomics, and other various next-generation sequencing (-omics) fields are known for their production of large datasets, especially across single-cell sequencing studies. Visualizing such big data has posed technical challenges in biology, both in terms of available computational resources as well as programming acumen. Since heatmaps are used to depict high-dimensional numerical data as a colored grid of cells, efficiency and speed have often proven to be critical considerations in the process of successfully converting data into graphics. For example, rendering interactive heatmaps from large input datasets (e.g., 100k+ rows) has been computationally infeasible on both desktop computers and web browsers. In addition to memory requirements, programming skills and knowledge have frequently been barriers-to-entry for creating highly customizable heatmaps. We propose shinyheatmap: an advanced user-friendly heatmap software suite capable of efficiently creating highly customizable static and interactive biological heatmaps in a web browser. shinyheatmap is a low memory footprint program, making it particularly well-suited for the interactive visualization of extremely large datasets that cannot typically be computed in-memory due to size restrictions. Also, shinyheatmap features a built-in high performance web plug-in, fastheatmap, for rapidly plotting interactive heatmaps of datasets as large as 105 \u2013 107 rows within seconds, effectively shattering previous performance benchmarks of heatmap rendering speed. shinyheatmap is hosted online as a freely available web server with an intuitive graphical user interface: http://shinyheatmap.com. The methods are implemented in R, and are available as part of the shinyheatmap project at: https://github.com/Bohdan-Khomtchouk/shinyheatmap. Users can access fastheatmap directly from within the shinyheatmap web interface, and all source code has been made publicly available on Github: https://github.com/Bohdan-Khomtchouk/fastheatmap."}, {"title": "Accounting For Technical Noise In Single-Cell RNA Sequencing Analysis", "url": "https://www.biorxiv.org/content/early/2017/03/17/116939", "tag": "Bioinformatics", "abstract": "Recent technological breakthroughs have made it possible to measure RNA expression at the single-cell level, thus paving the way for exploring expression heterogeneity among individual cells. Current single-cell RNA sequencing (scRNA-seq) protocols are complex and introduce technical biases that vary across cells, which can bias downstream analysis without proper adjustment. To account for cell-to-cell technical differences, we propose a statistical framework, TASC (Toolkit for Analysis of Single Cell RNA-seq), an empirical Bayes approach to reliably model the cell-specific dropout rates and amplification bias by use of external RNA spike-ins. TASC incorporates the technical parameters, which reflect cell-to-cell batch effects, into a hierarchical mixture model to estimate the biological variance of a gene and detect differentially expressed genes. More importantly, TASC is able to adjust for covariates to further eliminate confounding that may originate from cell size and cell cycle differences. In simulation and real scRNA-seq data, TASC achieves accurate Type I error control and displays competitive sensitivity and improved robustness to batch effects in differential expression analysis, compared to existing methods. TASC is programmed to be computationally efficient, taking advantage of multi-threaded parallelization. We believe that TASC will provide a robust platform for researchers to leverage the power of scRNA-seq."}, {"title": "A Novel And Efficient Algorithm For De Novo Discovery Of Mutated Driver Pathways In Cancer", "url": "https://www.biorxiv.org/content/early/2017/03/17/117473", "tag": "Bioinformatics", "abstract": "Next-generation sequencing studies on cancer somatic mutations have discovered that driver mutations tend to appear in most tumor samples, but they barely overlap in any single tumor sample, presumably because a single driver mutation can perturb the whole pathway. Based on the corresponding new concepts of coverage and mutual exclusivity, new methods can be designed for de novo discovery of mutated driver pathways in cancer. Since the computational problem is a combinatorial optimization with an objective function involving a discontinuous indicator function in high dimension, many existing optimization algorithms, such as a brute force enumeration, gradient descent and Newton's methods, are practically infeasible or directly inapplicable. We develop a new algorithm based on a novel formulation of the problem as non-convex programming and non-convex regularization. The method is computationally more efficient, effective and scalable than existing Monte Carlo searching and several other algorithms, which have been applied to The Cancer Genome Atlas (TCGA) project. We also extend the new method for integrative analysis of both mutation and gene expression data. We demonstrate the promising performance of the new methods with applications to three cancer datasets to discover de novo mutated driver pathways."}, {"title": "CpG traffic lights are markers of regulatory regions in humans", "url": "https://www.biorxiv.org/content/early/2017/03/17/095968", "tag": "Bioinformatics", "abstract": "DNA methylation is involved in regulation of gene expression. Although modern methods profile DNA methylation at single CpG sites, methylation levels are usually averaged over genomic regions in the downstream analyses. In this study we demonstrate that single CpG methylation can serve as a more accurate predictor of gene expression compared to average promoter / gene body methylation. CpG positions with significant correlation between methylation and expression of a gene nearby (named CpG traffic lights) are evolutionary conserved and enriched for exact TSS positions and active enhancers. Among all promoter types, CpG traffic lights are especially enriched in poised promoters. Genes that harbor CpG traffic lights are associated with development and signal transduction. Methylation levels of individual CpG traffic lights vary between cell types dramatically with the increased frequency of intermediate methylation levels, indicating cell population heterogeneity in CpG methylation levels. Being in line with the concept of the inherited stochastic epigenetic variation, methylation of such CpG positions might contribute to transcriptional regulation. Alternatively, one can hypothesize that traffic lights are markers of absent gene expression resulting from inactivation of their regulatory elements. The CpG traffic lights provide a promising insight into mechanisms of enhancer activity and gene regulation linking methylation of single CpG to expression."}, {"title": "Probabilistic Recovery Of Cryptic Haplotypes From Metagenomic Data", "url": "https://www.biorxiv.org/content/early/2017/03/17/117838", "tag": "Bioinformatics", "abstract": "The cryptic diversity of microbial communities represent an untapped biotechnological resource for biomining, biorefining and synthetic biology. Revealing this information requires the recovery of the exact sequence of DNA bases (or \u201chaplotype\u201d) that constitutes the genes and genomes of every individual present. This is a computationally difficult problem complicated by the requirement for environmental sequencing approaches (metagenomics) due to the resistance of the constituent organisms to culturing in vitro. Haplotypes are identified by their unique combination of DNA variants. However, standard approaches for working with metagenomic data require simplifications that violate assumptions in the process of identifying such variation. Furthermore, current haplotyping methods lack objective mechanisms for choosing between alternative haplotype reconstructions from microbial communities. To address this, we have developed a novel probabilistic approach for reconstructing haplotypes from complex microbial communities and propose the \u201cmetahaplome\u201d as a definition for the set of haplotypes for any particular genomic region of interest within a metagenomic dataset. Implemented in the twin software tools Hansel and Gretel, the algorithm performs incremental probabilistic haplotype recovery using Naive Bayes - an efficient and effective technique. Our approach is capable of reconstructing the haplotypes with the highest likelihoods from metagenomic datasets without a priori knowledge or making assumptions of the distribution or number of variants. Additionally, the algorithm is robust to sequencing and alignment error without altering or discarding observed variation and uses all available evidence from aligned reads. We validate our approach using synthetic metahaplomes constructed from sets of real genes, and demonstrate its capability using metagenomic data from a complex HIV-1 strain mix. The results show that the likelihood framework can allow recovery from microbial communities of cryptic functional isoforms of genes with 100% accuracy."}, {"title": "Comparison Of Multi-locus Sequence Typing Software For Next Generation Sequencing Data", "url": "https://www.biorxiv.org/content/early/2017/03/17/117770", "tag": "Bioinformatics", "abstract": "Multi-locus sequence typing (MLST) is a widely used method for categorising bacteria. Increasingly MLST is being performed using next generation sequencing data by reference labs and for clinical diagnostics. Many software applications have been developed to calculate sequence types from NGS data; however, there has been no comprehensive review to date on these methods. We have compared six of these applications against real and simulated data and present results on: 1. the accuracy of each method against traditional typing methods, 2. the performance on real outbreak datasets, 3. in the impact of contamination and varying depth of coverage, and 4. the computational resource requirements."}, {"title": "Classification of RNA-Seq Data via Gaussian Copulas", "url": "https://www.biorxiv.org/content/early/2017/03/17/116046", "tag": "Bioinformatics", "abstract": "RNA-sequencing (RNA-Seq) has become a preferred option to quantify gene expression, because it is more accurate and reliable than microarrays. In RNA-Seq experiments, the expression level of a gene is measured by the count of short reads that are mapped to the gene region. Although some normal-based statistical methods may also be applied to log-transformed read counts, they are not ideal for directly modeling RNA-Seq data. Two discrete distributions, Poisson distribution and negative binomial distribution, have been commonly used in the literature to model RNA-Seq data, where the latter is a natural extension of the former with allowance of overdispersion. Due to the technical difficulty in modeling correlated counts, most existing classifiers based on discrete distributions assume that genes are independent of each other. However, as we show in this paper, the independence assumption may cause non-ignorable bias in estimating the discriminant score, making the classification inaccurate. To this end, we drop the independence assumption and explicitly model the dependence between genes using Gaussian copula. We apply a Bayesian approach to estimate covariance matrix and the overdispersion parameter in negative binomial distribution. Both synthetic data and real data are used to demonstrate the advantages of our model."}, {"title": "MEACA: efficient gene-set interpretation of expression data using mixed models", "url": "https://www.biorxiv.org/content/early/2017/03/16/106781", "tag": "Bioinformatics", "abstract": "Competitive gene-set analysis, also called enrichment analysis, is a widely used tool for functional interpretation of high-throughput biological data such as gene expression data. It aims at testing a known category (e.g. a pathway) of genes for enriched differential expression (DE) signals compared to genes not in the category. Most enrichment testing methods ignore the widespread correlations among genes, which has been shown to result in excessive false positives. We show, both theoretically and empirically, that existing methods to account for correlations, such as GSEA and CAMERA, can result in severely mis-calibrated type 1 error and/or considerable power loss due to the failure to properly accommodate the DE heterogeneity across genes. We propose MEACA, a new gene-set testing framework based on a mixed effects model. Our method flexibly incorporates the unknown distribution of DE effects, effectively adjusts for completely unknown, unstructured correlations among genes, and does not rely on time-consuming permutations. Compared to existing methods, MEACA enjoys robust type 1 error control in widely ranging scenarios and substantially improves power. Applications of MEACA to a Huntington's disease study and a lymphoblastoid cell line data set demonstrate its ability to recover biologically meaningful relationships. MEACA is available as an R package."}, {"title": "Genoppi: a web application for interactive integration of experimental proteomics results with genetic datasets", "url": "https://www.biorxiv.org/content/early/2017/03/16/115576", "tag": "Bioinformatics", "abstract": "Summary: Integrating protein-protein interaction experiments and genetic datasets can lead to new insight into the cellular processes implicated in diseases, but this integration is technically challenging. Here, we present Genoppi, a web application that integrates quantitative interaction proteomics data and results from genome-wide association studies or exome sequencing projects, to highlight biological relationships that might otherwise be difficult to discern. Written in R, Python and Bash script, Genoppi is a user-friendly framework easily deployed across Mac OS and Linux distributions. Availability: Genoppi is open source and available at https://github.com/lagelab/Genoppi Contact: aprilkim@broadinstitute.org and lage.kasper@mgh.harvard.edu"}, {"title": "Gene nucleotide composition accurately predicts expression and is linked to topological chromatin domains", "url": "https://www.biorxiv.org/content/early/2017/03/16/117499", "tag": "Bioinformatics", "abstract": "Gene expression is orchestrated by distinct regulatory regions (e.g. promoters, enhancers, UTRs) to ensure a wide variety of cell types and functions. A challenge is to identify which regulatory regions are active, what are their associated features and how they work together in each cell type. Several approaches have tackled this problem by modeling gene expression based on epigenetic marks (e.g. ChIP-seq, methylation, DNase hypersensitivity), with the ultimate goal of identifying driving genomic regions and mutations that are clinically relevant in particular in precision medicine. However, these models rely on experimental data, which are limited to specific samples (even often to cell lines) and cannot be generated for all regulators and all patients. In addition, we show here that, although these approaches are accurate in predicting gene expression, their biological interpretation can be misleading. Finally these methods are not designed to capture potential regulation instructions present at the sequence level, before the binding of regulators or the opening of the chromatin. We develop here a method for predicting mRNA levels based solely on sequence features collected from distinct regulatory regions, which is as accurate as methods based on experimental data. Our approach confirms the importance of nucleotide composition in predicting gene expression and ranks regulatory regions according to their contribution. It also unveils strong influence of gene body sequence, in particular introns. We further provide evidence that the contribution of nucleotide content can be linked to co-regulations associated with genome 3D architecture and to associations of genes within topologically associated domains."}, {"title": "HINGE: Long-Read Assembly Achieves Optimal Repeat Resolution", "url": "https://www.biorxiv.org/content/early/2017/03/16/062117", "tag": "Bioinformatics", "abstract": "Long-read sequencing technologies have the potential to produce gold-standard de novo genome assemblies, but fully exploiting error-prone reads to resolve repeats remains a challenge. Aggressive approaches to repeat resolution often produce mis-assemblies, and conservative approaches lead to unnecessary fragmentation. We present HINGE, an assembler that seeks to achieve optimal repeat resolution by distinguishing repeats that can be resolved given the data from those that cannot. This is accomplished by adding \"hinges\" to reads for constructing an overlap graph where only unresolvable repeats are merged. As a result, HINGE combines the error resilience of overlap-based assemblers with repeat-resolution capabilities of de Bruijn graph assemblers. HINGE was evaluated on the long-read bacterial datasets from the NCTC project. HINGE produces more finished assemblies than Miniasm and the manual pipeline of NCTC based on the HGAP assembler and Circlator. HINGE also allows us to identify 40 datasets where unresolvable repeats prevent the reliable construction of a unique finished assembly. In these cases, HINGE outputs a visually interpretable assembly graph that encodes all possible finished assemblies consistent with the reads, while other approaches such as the NCTC pipeline and FALCON either fragment the assembly or resolve the ambiguity arbitrarily."}, {"title": "Bio-Docklets: Virtualization Containers for Single-Step Execution of NGS Pipelines.", "url": "https://www.biorxiv.org/content/early/2017/03/15/116962", "tag": "Bioinformatics", "abstract": "Background: Processing of Next-Generation Sequencing (NGS) data requires significant technical skills, involving installation, configuration, and execution of bioinformatics data pipelines, in addition to specialized post-analysis visualization and data mining software. In order to address some of these challenges, developers have leveraged virtualization containers, towards seamless deployment of preconfigured bioinformatics software and pipelines on any computational platform. Findings: We present an approach for abstracting the complex data operations of multi-step, bioinformatics pipelines for NGS data analysis. As examples, we have deployed two pipelines for RNAseq and CHIPseq, pre-configured within Docker virtualization containers we call Bio-Docklets. Each Bio-Docklet exposes a single data input and output endpoint and from a user perspective, running the pipelines is as simple as running a single bioinformatics tool. This is achieved through a \u2018meta-script\u2019 that automatically starts the Bio-Docklets, and controls the pipeline execution through the BioBlend software library and the Galaxy Application Programming Interface (API). The pipelne output is postprocessed using the Visual Omics Explorer (VOE) framework, providing interactive data visualizations that users can access through a web browser. Conclusions: The goal of our approach is to enable easy access to NGS data analysis pipelines for nonbioinformatics experts, on any computing environment whether a laboratory workstation, university computer cluster, or a cloud service provider. Besides end-users, the Bio-Docklets also enables developers to programmatically deploy and run a large number of pipeline instances for concurrent analysis of multiple datasets."}, {"title": "Reconstructing Promoter Activity From Lux Bioluminescent Reporters", "url": "https://www.biorxiv.org/content/early/2017/03/15/117093", "tag": "Bioinformatics", "abstract": "The bacterial Lux system is used as a gene expression reporter. It is fast, sensitive and non-destructive, enabling high frequency measurements. Originally developed for bacterial cells, it has been adapted for eukaryotic cells, and can be used for whole cell biosensors, or in real time with live animals without the need for slaughter. However, correct interpretation of bioluminescent data is limited: the bioluminescence is different from gene expression because of nonlinear molecular and enzyme dynamics of the Lux system. We have developed a modelling approach that, for the first time, allows users of Lux assays to infer gene transcription levels from the light output. We show examples where a decrease in bioluminescence would be better interpreted as a switching off of the promoter, or where an increase in bioluminescence would be better interpreted as a longer period of gene expression. This approach could benefit all users of Lux technology."}, {"title": "The Junction Usage Model (JUM): A method for comprehensive annotation-free differential analysis of tissue-specific global alternative pre-mRNA splicing patterns", "url": "https://www.biorxiv.org/content/early/2017/03/14/116863", "tag": "Bioinformatics", "abstract": "Alternative pre-mRNA splicing (AS) generates exceptionally diverse transcriptome and proteome profiles that critically affect eukaryotic gene expression in different tissues, developmental stages and disease. However, current efforts to evaluate tissue-specific AS patterns rely completely or partially on an annotated libraries of known gene transcripts, which hinders the analysis of AS patterns that are novel or specific to the cell/tissue or for non- or poorly annotated genomes. To tackle this problem, we describe a method called the Junction Usage Model (JUM) that offers a de novo approach to analyze tissue-specific AS profiles without any prior knowledge of the transcriptome. JUM exclusively uses RNA-seq reads mapped to splice junctions to construct statistical models and to accurately quantify AS changes, and then faithfully reconstructs the detected splice junctions into AS patterns based on their unique topological features. Compared to other recent methods, we found that JUM consistently identified true novel tissue-specific AS events that could not be identified by other methods, and further rejected false positive and/or misclassified AS events. In summary, JUM provides a new framework and software that enables the thorough investigation of the dynamic and tissue-specific AS regulation in a wide range of cells, tissues and organisms."}, {"title": "Genome Graphs and the Evolution of Genome Inference", "url": "https://www.biorxiv.org/content/early/2017/03/14/101816", "tag": "Bioinformatics", "abstract": "The human reference genome is part of the foundation of modern human biology, and a monumental scientific achievement. However, because it excludes a great deal of common human variation, it introduces a pervasive reference bias into the field of human genomics. To reduce this bias, it makes sense to draw on representative collections of human genomes, brought together into reference cohorts. There are a number of techniques to represent and organize data gleaned from these cohorts, many using ideas implicitly or explicitly borrowed from graph based models. Here, we survey various projects underway to build and apply these graph based structures\u2014which we collectively refer to as genome graphs\u2014and discuss the improvements in read mapping, variant calling, and haplotype determination that genome graphs are expected to produce."}, {"title": "MRCZ - A proposed fast compressed MRC file format and direct detector normalization strategies", "url": "https://www.biorxiv.org/content/early/2017/03/13/116533", "tag": "Bioinformatics", "abstract": "The introduction of high-speed CMOS detectors is fast marching the field of transmission electron microscopy into an intersection with the computer science field of big data. Automated data pipelines to control the instrument and the initial processing steps are imposing more and more onerous requirements on data transfer and archiving. We present a proposal for expansion of the venerable MRC file format to combine integer decimation and lossless compression to reduce storage requirements and improve file read/write times by >1000 % compared to uncompressed floating-point data. The integer decimation of data necessitates application of the gain normalization and outlier pixel removal at the data destination, rather than the source. With direct electron detectors, the normalization step is typically provided by the vendor and is not open-source. We provide robustly tested normalization algorithms that perform at-least as well as vendor software. We show that the generation of hot pixels is a highly dynamic process in direct electron detectors, and that outlier pixels must be detected on a stack-by-stack basis. In comparison, the low-frequency bias features of the detectors induced by the electronics on-top of the active layer, are extremely stable with time. Therefore we introduce a stochastic-based approach to identify outlier pixels and smoothly filter them, such that the degree of correlated noise in micrograph stacks is reduced. Both a priori and a posteriori gain normalization approaches that are compatible with pipeline image processing are discussed. The a priori approach adds a gamma-correction to the gain reference, and the a posteriori approach normalized by a moving average of time-adjacent stacks, with the current stack being knocked-out, known as the KOMA (knock-out moving average) filter. The combination of outlier filter and KOMA normalization over ~25 frames can reduce the correlated noise in movies to nearly zero. Sample libraries and a command-line utility are hosted at github.com/em-MRCZ and released under the BSD license."}, {"title": "Automated assembly of a reference taxonomy for phylogenetic data synthesis", "url": "https://www.biorxiv.org/content/early/2017/03/13/116418", "tag": "Bioinformatics", "abstract": "Taxonomy and nomenclature data are critical for any project that synthesizes biodiversity data, as most biodiversity data sets use taxonomic names to identify taxa. Open Tree of Life is one such project, synthesizing sets of published phylogenetic trees into comprehensive supertrees. No single published taxonomy met the taxonomic and nomenclatural needs of the project. Here we describe a system for reproducibly combining several source taxonomies into a synthetic taxonomy, and we discuss the challenges of taxonomic and nomenclatural synthesis for downstream biodiversity projects."}, {"title": "DEVELOPING GENE-SPECIFIC META-PREDICTOR OF VARIANT PATHOGENICITY", "url": "https://www.biorxiv.org/content/early/2017/03/10/115956", "tag": "Bioinformatics", "abstract": "Rapid, accurate, and inexpensive genome sequencing promises to transform medical care. However, a critical hurdle to enabling personalized genomic medicine is predicting the functional impact of novel genomic variation. Various methods of missense variants pathogenicity prediction have been developed by now. Here we present a new strategy for developing a pathogenicity predictor of improved accuracy by applying and training a supervised machine learning model in a gene-specific manner. Our meta-predictor combines outputs of various existing predictors, supplements them with an extended set of stability and structural features of the protein, as well as its physicochemical properties, and adds information about allele frequency from various datasets. We used such a supervised gene- specific meta-predictor approach to train the model on the CFTR gene, and to predict pathogenicity of about 1,000 variants of unknown significance that we collected from various publicly available and internal resources. Our CFTR-specific meta-predictor based on the Random Forest model performs better than other machine learning algorithms that we tested, and also outperforms other available tools, such as CADD, MutPred, SIFT, and PolyPhen-2. Our predicted pathogenicity probability correlates well with clinical measures of Cystic Fibrosis patients and experimental functional measures of mutated CFTR proteins. Training the model on one gene, in contrast to taking a genome wide approach, allows taking into account structural features specific for a particular protein, thus increasing the overall accuracy of the predictor. Collecting data from several separate resources, on the other hand, allows to accumulate allele frequency information, estimated as the most important feature by our approach, for a larger set of variants. Finally, our predictor will be hosted on the ClinGen Consortium database to make it available to CF researchers and to serve as a feasibility pilot study for other Mendelian diseases."}, {"title": "Identification and characterization of m6A circular RNA epitranscriptomes", "url": "https://www.biorxiv.org/content/early/2017/03/10/115899", "tag": "Bioinformatics", "abstract": "This study brings together the expanding fields of RNA modifications and circular (circ) RNAs. We find that cells express thousands of m6A methylated circRNAs, with cell-type specificity observed between human embryonic stem cells and HeLa cells. m6A-circRNAs were identified by RNA sequencing of total RNA following ribosome depletion and m6A immunoprecipitation. The presence of m6A-circRNAs is corroborated by the identification of complexes between circRNAs and YTHDF1 and YTHDF2, proteins that \u201cread\u201d m6A sites in mRNAs. Furthermore, m6A modifications on non-linear RNAs depend on METTL3 and METTL14, the known m6A methyltransferase \u201cwriter\u201d complex components, suggesting that circRNAs are methylated by the same complexes responsible for m6A modification of linear RNAs. Despite sharing m6A readers and writers, m6A-circRNAs are frequently derived from exons not methylated in mRNAs. Nevertheless, m6A-mRNAs that are methylated on the same exons as those composing m6A-circRNAs exhibit less stability than other m6A-mRNA, and this circRNA-mRNA cross-talk is regulated by YTHDF2. Thus, our results expand the m6A regulatory code through identification of the first circRNA epitranscriptome."}, {"title": "The Co-regulation Data Harvester for Tetrahymena thermophila: automated high-throughput gene annotation and functional inference in a microbial eukaryote", "url": "https://www.biorxiv.org/content/early/2017/03/10/115816", "tag": "Bioinformatics", "abstract": "Identifying co-regulated genes can provide a useful approach for defining pathway-specific machinery in an organism. To be efficient, this approach relies on thorough genome annotation, which is not available for most organisms with sequenced genomes. Studies in Tetrahymena thermophila, the most experimentally accessible ciliate, have generated a rich transcriptomic database covering many well-defined physiological states. Genes that are involved in the same pathway show significant co-regulation, and screens based on gene co-regulation have identified novel factors in specific pathways, for example in membrane trafficking. However, a limitation has been the relatively sparse annotation of the Tetrahymena genome, making it impractical to approach genome-wide analyses. We have therefore developed an efficient approach to analyze both co-regulation and gene annotation, called the Co-regulation Data Harvester (CDH). The CDH automates identification of co-regulated genes by accessing the Tetrahymena transcriptome database, determines their orthologs in other organisms via reciprocal BLAST searches, and collates the annotations of those orthologs' functions. Inferences drawn from the CDH reproduce and expand upon experimental findings in Tetrahymena. The CDH, which is freely available, represents a powerful new tool for analyzing cell biological pathways in Tetrahymena. Moreover, to the extent that genes and pathways are conserved between organisms, the inferences obtained via the CDH should be relevant, and can be explored, in many other systems."}, {"title": "OME Files - An open source reference library for the OME-XML metadata model and the OME-TIFF file format", "url": "https://www.biorxiv.org/content/early/2017/03/09/088740", "tag": "Bioinformatics", "abstract": "We describe OME Files, software for reading and writing an open imaging file format, OME-TIFF."}, {"title": "Improving pairwise comparison of protein sequences with domain co-occurrence", "url": "https://www.biorxiv.org/content/early/2017/03/09/115543", "tag": "Bioinformatics", "abstract": "Motivations: Comparing and aligning protein sequences is an essential task in bioinformatics. More specifically, local alignment tools like BLAST are widely used for identifying conserved protein sub-sequences, which likely correspond to protein domains or functional motifs. However, to limit the number of false positives, these tools are used with stringent sequence-similarity thresholds and hence can miss several hits, especially for species that are phylogenetically distant from reference organisms. A solution to this problem is then to integrate additional contextual information to the procedure. Results: Here, we propose to use domain co-occurrence to increase the sensitivity of pairwise sequence comparisons. Domain co-occurrence is a strong feature of proteins, since most protein domains tend to appear with a limited number of other domains on the same protein. We propose a method to take this information into account in a typical BLAST analysis and to construct new domain families on the basis of these results. We used Plasmodium falciparum as a case study to evaluate our method. The experimental findings showed an increase of 16% of the number of significant BLAST hits and an increase of 28% of the proteome area that can be covered with a domain. Our method identified 2473 new domains for which, in most cases, no model of the Pfam database could be linked. Moreover, our study of the quality of the new domains in terms of alignment and physicochemical properties show that they are close to that of standard Pfam domains. Availability: Software implementing the proposed approach and the Supplementary Data are available at: https://gite.lirmm.fr/menichelli/pairwise-comparison-with-cooccurrence"}, {"title": "A modified GC-specific MAKER gene annotation method reveals improved and novel gene predictions of high and low GC content in Oryza sativa", "url": "https://www.biorxiv.org/content/early/2017/03/09/115345", "tag": "Bioinformatics", "abstract": "Accurate structural annotation depends on well-trained gene prediction programs. Training data for gene prediction programs are often chosen randomly from a subset of high-quality genes that ideally represent the variation found within a genome. One aspect of gene variation is GC content, which differs across species and is bimodal in grass genomes. We find that gene prediction programs trained on genes with random GC content do not completely predict all grass genes with extreme GC content. We present a new GC-specific MAKER annotation protocol to predict new and improved gene models and assess the biological significance of this method in Oryza sativa."}, {"title": "Combining Bayesian Approaches and Evolutionary Techniques for the Inference of Breast Cancer Networks", "url": "https://www.biorxiv.org/content/early/2017/03/09/115261", "tag": "Bioinformatics", "abstract": "Gene and protein networks are very important to model complex large-scale systems in molecular biology. Inferring or reverseengineering such networks can be defined as the process of identifying gene/protein interactions from experimental data through computational analysis. However, this task is typically complicated by the enormously large scale of the unknowns in a rather small sample size. Furthermore, when the goal is to study causal relationships within the network, tools capable of overcoming the limitations of correlation networks are required. In this work, we make use of Bayesian Graphical Models to attach this problem and, specifically, we perform a comparative study of different state-of-the-art heuristics, analyzing their performance in inferring the structure of the Bayesian Network from breast cancer data."}, {"title": "OncoScore: a novel, Internet-based tool to assess the oncogenic potential of genes", "url": "https://www.biorxiv.org/content/early/2017/03/09/115329", "tag": "Bioinformatics", "abstract": "The complicated, evolving landscape of cancer mutations poses a formidable challenge to identify cancer genes among the large lists of mutations typically generated in NGS experiments. The ability to prioritize these variants is therefore of paramount importance. To address this issue we developed OncoScore, a text-mining tool that ranks genes according to their association with cancer, based on available biomedical literature. Receiver operating characteristic curve and the area under the curve (AUC) metrics on manually curated datasets confirmed the excellent discriminating capability of OncoScore (OncoScore cut-off threshold = 21.09; AUC = 90.3%, 95% CI: 88.1-92.5%), indicating that OncoScore provides useful results in cases where an efficient prioritization of cancer-associated genes is needed."}, {"title": "Modeling cumulative biological phenomena with Suppes-Bayes Causal Networks", "url": "https://www.biorxiv.org/content/early/2017/03/08/041343", "tag": "Bioinformatics", "abstract": "Several diseases related to cell proliferation are characterized by the accumulation of somatic DNA changes, with respect to wildtype conditions. Cancer and HIV are two common examples of such diseases, where the mutational load in the cancerous/viral population increases over time. In these cases, selective pressures are often observed along with competition, cooperation and parasitism among distinct cellular clones. Recently, we presented a mathematical framework to model these phenomena, based on a combination of Bayesian inference and Suppes' theory of probabilistic causation, depicted in graphical structures dubbed Suppes-Bayes Causal Networks (SBCNs). SBCNs are generative probabilistic graphical models that recapitulate the potential ordering of accumulation of such DNA changes during the progression of the disease. Such models can be inferred from data by exploiting likelihood-based model-selection strategies with regularization. In this paper we discuss the theoretical foundations of our approach and we investigate in depth the influence on the model-selection task of: (i) the poset based on Suppes' theory and (ii) different regularization strategies. Furthermore, we provide an example of application of our framework to HIV genetic data highlighting the valuable insights provided by the inferred."}, {"title": "Tagger: BeCalm API for rapid named entity recognition", "url": "https://www.biorxiv.org/content/early/2017/03/08/115022", "tag": "Bioinformatics", "abstract": "Most BioCreative tasks to date have focused on assessing the quality of text-mining annotations in terms of precision of recall. Interoperability, speed, and stability are, however, other important factors to consider for practical applications of text mining. The new BioCreative/BeCalm TIPS task focuses purely on these. To participate in this task, I implemented a BeCalm API within the real-time tagging server also used by the Reflect and EXTRACT tools. In addition to retrieval of patent abstracts, PubMed abstracts, and PubMed Central open-access articles as required in the TIPS task, the BeCalm API implementation facilitates retrieval of documents from other sources specified as custom request parameters. As in earlier tests, the tagger proved to be both highly efficient and stable, being able to consistently process requests of 5000 abstracts in less than half a minute including retrieval of the document text."}, {"title": "A study of the structural properties of sites modified by the O-linked 6-N-acetylglucosamine transferase", "url": "https://www.biorxiv.org/content/early/2017/03/08/115121", "tag": "Bioinformatics", "abstract": "Protein O-GlcNAcylation (O-GlcNAc) is an essential post-translational modification (PTM) in higher eukaryotes. The O-linked \u03b2-N-acetylglucosamine transferase (OGT), targets specific Serines and Threonines (S/T) in intracellular proteins. However, unlike phosphorylation, fewer than 25% of known O-GlcNAc sites match a clear sequence pattern. Accordingly, the three-dimensional structures of O-GlcNAc sites were characterised to investigate the role of structure in molecular recognition. Of the 143/1,584 O-GlcNAc sites in 620 proteins were mapped to protein X-ray structures. The modified S/T were 1.7x more likely to be annotated in the REM465 field which defines missing residues in a protein structure, while 7 O-GlcNAc sites were solvent inaccessible and unlikely to be targeted by OGT. The 132/143 sites with complete backbone atoms clustered into 10 groups, but these were indistinguishable from clusters from unmodified S/T. This suggests there is no prevalent three-dimensional motif for OGT recognition. Predicted features from the 620 proteins were compared to unmodified S/T in O-GlcNAcylated proteins and globular proteins. The Jpred4 predicted secondary structure shows that modified S/T were more likely to be coils. 5/6 methods to predict intrinsic disorder indicated O-GlcNAcylated S/T to be significantly more disordered than unmodified S/T. Although the analysis did not find a pattern in the site three-dimensional structure, it revealed the residues around the modification site are likely to be disordered and suggests a potential role of secondary structure elements in OGT site recognition."}, {"title": "A novel method for large-scale identification of polymorphic microsatellites through comparative transcriptome analysis", "url": "https://www.biorxiv.org/content/early/2017/03/07/114645", "tag": "Bioinformatics", "abstract": "Microsatellite (SSR) is one of the most popular markers for applied genetic research, but generally the current methods to develop SSRs are relatively time-consuming and expensive. Although high-throughput sequencing (HTS) approach has become a practical and relatively inexpensive option so far, only a small percentage of SSR markers turn out to be polymorphic. Here, we designed a new method to enrich polymorphic SSRs through the comparative transcriptome analysis. This program contains five main steps: 1) transcriptome data downloading or RNA-seq; 2) sequence assembly; 3) SSR mining and enrichment of sequences containing SSRs; 4) sequence alignment; 5) enrichment of sequences containing polymorphic SSRs. A validation experiment was performed and the results showed almost all markers (> 90%) that were indicated as putatively polymorphic by this method were indeed polymorphic. The frequency of polymorphic SSRs was significantly higher (P < 0.05) but the cost and running time were much lower than those of traditional and HTS approaches. The method has a practical value for polymorphic SSRs development and might be widely used for genetic analyses in any species."}, {"title": "WTFgenes: What's The Function of these genes? Static sites for model-based gene set analysis", "url": "https://www.biorxiv.org/content/early/2017/03/07/114785", "tag": "Bioinformatics", "abstract": "A common technique for interpreting experimentally-identified lists of genes is to look for enrichment of genes associated to particular ontology terms. The most common technique uses the hypergeometric distribution; more recently, a model-based approach was proposed. These approaches must typically be run using downloaded software, or on a server. We develop a collapsed likelihood for model-based gene set analysis and present WTFgenes, an implementation of both hypergeometric and model-based approaches, that can be published as a static site with computation run in JavaScript on the user's web browser client. Apart from hosting files, zero server resources are required: the site can (for example) be served directly from Amazon S3 or GitHub Pages. A C++11 implementation yielding identical results runs roughly twice as fast as the JavaScript version. WTFgenes is available from https://github.com/evoldoers/wtfgenes under the BSD3 license. A demonstration for the Gene Ontology is usable at https://evoldoers.github.io/wtfgo. Contact: Ian Holmes ihholmes+wtfgenes@gmail.com."}, {"title": "MultiDCoX: Multi-factor Analysis of Differential Co-expression", "url": "https://www.biorxiv.org/content/early/2017/03/06/114397", "tag": "Bioinformatics", "abstract": "Background: Differential co-expression, complementary to differential expression, signifies change in degree of co-expression of a set of genes between different biological conditions. It has been used to identify differential co-expression networks or interactomes. Many algorithms or methodologies have been developed for single-factor differential co-expression analysis and applied in a variety of studies. However, in many studies, the samples are characterized by multiple factors such as genetic markers, clinical variables and treatments. No algorithm or methodology is available for multi-factor analysis of differential co-expression. Results: We developed a novel formulation and a computationally efficient greedy search algorithm called MultiDCoX to perform multi-factor differential co-expression analysis using genome-wide gene expression data. Simulated data analysis demonstrates that the algorithm can effectively elicit differentially co-expressed (DCX) gene sets and quantify the influence of each factor on co-expression. MultiDCoX analysis of a breast cancer dataset identified interesting biologically meaningful differentially co-expressed (DCX) gene sets along with genetic and clinical factors that influenced the respective differential co-expression. Conclusions: Similar to differential expression, differential co-expression also needs to be analyzed in the context of multiple genetic and clinical factors. MultiDCoX is a space and time efficient procedure to identify differentially co-expressed gene sets and successfully identify the influence of individual factors on differential co-expression."}, {"title": "ARSDA: A new approach for storing, transmitting and analyzing high-throughput sequencing data", "url": "https://www.biorxiv.org/content/early/2017/03/06/114470", "tag": "Bioinformatics", "abstract": "Two major stumbling blocks exist in high-throughput sequencing (HTS) data analysis. The first is the sheer file size typically in gigabytes when uncompressed, causing problems in storage, transmission and analysis. However, these files do not need to be so large and can be reduced without loss of information. Each HTS file, either in compressed .SRA or plain text .fastq format, contains numerous identical reads stored as separate entries. For example, among 44603541 forward reads in the SRR4011234.sra file (from a Bacillus subtilis transcriptomic study) deposited at NCBI??s SRA database, one read has 497027 identical copies. Instead of storing them as separate entries, one can and should store them as a single entry with the SeqID_NumCopy format (which I dub as FASTA+ format). The second is the proper allocation reads that map equally well to paralogous genes. I illustrate in detail a new method for such allocation. I have developed ARSDA software that implement these new approaches. A number of HTS files for model species are in the process of being processed and deposited at http://coevol.rdc.uottawa.ca to demonstrate that this approach not only saves a huge amount of storage space and transmission bandwidth, but also dramatically reduces time in downstream data analysis. Instead of matching the 497027 identical reads separately against the Bacillus subtilis genome, one only needs to match it once. ARSDA includes functions to take advantage of HTS data in the new sequence format for downstream data analysis such as gene expression characterization. ARSDA can be run on Windows, Linux and Macintosh computers and is freely available at http://dambe.bio.uottawa.ca/ARSDA/ARSDA.aspx."}, {"title": "QuPath: Open source software for digital pathology image analysis", "url": "https://www.biorxiv.org/content/early/2017/03/06/099796", "tag": "Bioinformatics", "abstract": "QuPath is new bioimage analysis software designed to meet the growing need for a user-friendly, extensible, open-source solution for digital pathology and whole slide image analysis. In addition to offering a comprehensive panel of tumor identification and high-throughput biomarker evaluation tools, QuPath provides researchers with powerful batch-processing and scripting functionality, and an extensible platform with which to develop and share new algorithms to analyze complex tissue images. Furthermore, QuPath's flexible design makes it suitable for a wide range of additional image analysis applications across biomedical research."}, {"title": "Metaviz: interactive statistical and visual analysis of metagenomic data", "url": "https://www.biorxiv.org/content/early/2017/03/06/105205", "tag": "Bioinformatics", "abstract": "Along with the survey techniques of 16S rRNA amplicon and whole-metagenome shotgun sequencing, an array of tools exists for clustering, taxonomic annotation, normalization, and statistical analysis of microbiome sequencing results. Integrative and interactive visualization that enables researchers to perform exploratory analysis in this feature rich hierarchical data is an area of need. In this work, we present Metaviz, a web browser-based tool for interactive exploratory metagenomic data analysis. Metaviz can visualize abundance data served from an R session or a Python web service that queries a graph database. As metagenomic sequencing features have a hierarchy, we designed a novel navigation mechanism to explore this feature space. We visualize abundance counts with heatmaps and stacked bar plots that are dynamically updated as a user selects taxonomic features to inspect. Metaviz also supports common data exploration techniques, including PCA scatter plots to interpret variability in the dataset and alpha diversity boxplots for examining ecological community composition. The Metaviz application and documentation is hosted at http://www.metaviz.org."}, {"title": "Mindcontrol: A Web Application for Brain Segmentation Quality Control", "url": "https://www.biorxiv.org/content/early/2017/03/05/090431", "tag": "Bioinformatics", "abstract": "Tissue classification plays a crucial role in the investigation of normal neural development, brain-behavior relationships, and the disease mechanisms of many psychiatric and neurological illnesses. Ensuring the accuracy of tissue classification is important for quality research and, in particular, the translation of imaging biomarkers to clinical practice. Assessment with the human eye is vital to correct various errors inherent to all currently available segmentation algorithms. Manual quality assurance becomes methodologically difficult at a large scale - a problem of increasing importance as the number of data sets is on the rise. To make this process more efficient, we have developed Mindcontrol, an open-source web application for the collaborative quality control of neuroimaging processing outputs. The Mindcontrol platform consists of a dashboard to organize data, descriptive visualizations to explore the data, an imaging viewer, and an in-browser annotation and editing toolbox for data curation and quality control. Mindcontrol is flexible and can be configured for the outputs of any software package in any data organization structure. Example configurations for three large, open-source datasets are presented: the 1000 Functional Connectomes Project (FCP), the Consortium for Reliability and Reproducibility (CoRR), and the Autism Brain Imaging Data Exchange (ABIDE) Collection. These demo applications link descriptive quality control metrics, regional brain volumes, and thickness scalars to a 3D imaging viewer and editing module, resulting in an easy-to-implement quality control protocol that can be scaled for any size and complexity of study."}, {"title": "Impulse model-based differential expression analysis of time course sequencing data", "url": "https://www.biorxiv.org/content/early/2017/03/05/113548", "tag": "Bioinformatics", "abstract": "The global gene expression trajectories of cellular systems in response to developmental or environmental stimuli often follow the prototypic single-pulse or state-transition patterns which can be modeled with the impulse model. Here we combine the continuous impulse expression model with a sequencing data noise model in ImpulseDE2, a differential expression algorithm for time course sequencing experiments such as RNA-seq, ATAC-seq and ChIP-seq. We show that ImpulseDE2 outperforms currently used differential expression algorithms on data sets with sufficiently many sampled time points. ImpulseDE2 is capable of differentiating between transiently and monotonously changing expression trajectories. This classification separates genes which are responsible for the initial and final cell state phenotypes from genes which drive or are driven by the cell state transition and identifies down-regulation of oxidative-phosphorylation as a molecular signature which can drive human embryonic stem cell differentiation."}, {"title": "Multi-scale Bayesian modeling of cryo-electron microscopy density maps", "url": "https://www.biorxiv.org/content/early/2017/03/04/113951", "tag": "Bioinformatics", "abstract": "Cryo-electron microscopy has become a mainstream structural biology technique by enabling the characterization of biological architectures that for many years have eluded traditional methods like X-ray crystallography and Nuclear Magnetic Resonance (NMR) spectroscopy. However, the translation of cryo-electron microscopy data into accurate structural models is hampered by the presence of random and systematic errors in the data, sample heterogeneity, data correlation, and noise correlation. As a consequence, in integrative biology approaches, it has been difficult to objectively weigh EM-derived restraints with respect to other sources of information. To address these challenges, here we introduce a Bayesian approach that allows efficient and accurate structural modeling of cryo-electron microscopy density maps at multiple scales, from coarse-grained to atomistic resolution. The accuracy of the method is benchmarked using a set of structures of macromolecular assemblies. The approach is implemented in the open-source Integrative Modeling Platform package (http://integrativemodeling.org) in order to enable structural determination by combining cryo-electron microscopy with other information, such as chemical cross-linking/mass spectrometry, NMR, and small angle X-ray scattering data."}, {"title": "Salt-bridge Networks within Globular and Disordered Proteins - Characterizing Trends for Designable Interactions", "url": "https://www.biorxiv.org/content/early/2017/03/04/113621", "tag": "Bioinformatics", "abstract": "There has been fare amount of debate regarding the contribution of salt-bridges in the stabilization of protein folds. However, their participation in crucial protein functions are well established. The current study analyzes their modes of association, in terms of networks, both within globular proteins and also at protein-protein interfaces. Apart from the most common and trivial case of isolated salt-bridges, bifurcated salt-bridges appear to be a special salt-bridge motif both in terms of its topology and geometry and found ubiquitously in proteins and inter-protein complexes. Interesting and attractive examples presenting different interaction-modes have been highlighted. Bifurcated salt-bridges appear to function as molecular clips instrumental in stitching large surface contours of interacting protein-protein interfaces. The work also emphasizes the key role of salt-bridge mediated interactions in the partial folding of proteins containing large amount of disordered regions. Salt-bridge mediated interactions seem pivotal in promoting \u2018disorder-to-order\u2019 transitions for small disordered protein fragments and their stabilization upon binding. The results should guide to elucidate the modus operandi of these partially disordered proteins and also should be helpful to conceptualize how these proteins manage to keep necessary amount of disorder even in their functionally active bound forms, encouraging future studies. It should also be potentially beneficial towards the proposed notion of geometrically specific designable interactions involving salt-bridges."}, {"title": "Representing Genetic Determinants in Bacterial GWAS with Compacted De Bruijn Graphs", "url": "https://www.biorxiv.org/content/early/2017/03/03/113563", "tag": "Bioinformatics", "abstract": "Motivation: Antimicrobial resistance has become a major worldwide public health concern, calling for a better characterization of existing and novel resistance mechanisms. GWAS methods applied to bacterial genomes have shown encouraging results for new genetic marker discovery. Most existing approaches either look at SNPs obtained by sequence alignment or consider sets of kmers, whose presence in the genome is associated with the phenotype of interest. While the former approach can only be performed when genomes are similar enough for an alignment to make sense, the latter can lead to redundant descriptions and to results which are hard to interpret. Results: We propose an alignment-free GWAS method detecting haplotypes of variable length associated to resistance, using compacted De Bruijn graphs. Our representation is flexible enough to deal with very plastic genomes subject to gene transfers while drastically reducing the number of features to explore compared to kmers, without loss of information. It accomodates polymorphisms in core genes, accessory genes and noncoding regions. Using our representation in a GWAS leads to the selection of a small number of entities which are easier to visualize and interpret than fixed-length kmers. We illustrate the benefit of our approach by describing known as well as potential novel determinants of antimicrobial resistance in P. aeruginosa, a pathogenic bacteria with a highly plastic genome. Availability and implementation: The code and data used in the experiments will be made available upon acceptance of this manuscript. Contact: magali.dancette@biomerieux.com"}, {"title": "InterPred: A pipeline to identify and model protein-protein interactions", "url": "https://www.biorxiv.org/content/early/2017/03/03/080754", "tag": "Bioinformatics", "abstract": "Protein-protein interactions (PPI) are crucial for protein function. There exist many techniques to identify PPIs experimentally, but to determine the interactions in molecular detail is still difficult and very time-consuming. The fact that the number of PPIs is vastly larger than the number of individual proteins makes it practically impossible to characterize all interactions experimentally. Computational approaches that can bridge this gap and predict PPIs and model the interactions in molecular detail are greatly needed. Here we present InterPred, a fully automated pipeline that predicts and model PPIs from sequence using structural modelling combined with massive structural comparisons and molecular docking. A key component of the method is the use of a novel random forest classifier that integrate several structural features to distinguish correct from incorrect protein-protein interaction models. We show that InterPred represents a major improvement in protein-protein interaction detection with a performance comparable or better than experimental high-throughput techniques. We also show that our full-atom protein-protein complex modelling pipeline performs better than state of the art protein docking methods on a standard benchmark set. In addition, InterPred was also one of the top predictors in the latest CAPRI37 experiment. InterPred source code can be downloaded from http://wallnerlab.org/InterPred"}, {"title": "RecBlast: Cloud-Based Large Scale Orthology Detection", "url": "https://www.biorxiv.org/content/early/2017/03/02/112946", "tag": "Bioinformatics", "abstract": "Background: The effective detection and comparison of orthologues is crucial for answering many questions in comparative genomics, phylogenetics and evolutionary biology. One of the most common methods for discovering orthologues is widely known as 'Reciprocal Blast'. While this method is simple when comparing only two genomes, performing a large-scale comparison of Multiple Genes across Multiple Taxa becomes a labor-intensive and inefficient task. The low efficiency of this complicated process limits the scope and breadth of questions that would otherwise benefit from this powerful method. Findings: Here we present RecBlast, an intuitive and easy-to-use pipeline that enables fast and easy discovery of orthologues along and across the evolutionary tree. RecBlast is capable of running heavy, large-scale and complex Reciprocal Blast comparisons across multiple genes and multiple taxa, in a completely automatic way. RecBlast is available as a cloud-based web server, which includes an easy-to-use user interface, implemented using cloud computing and an elastic and scalable server architecture. RecBlast is also available as a powerful standalone software supporting multi-processing for large datasets, and a cloud image which can be easily deployed on Amazon Web Services cloud. We also include sample results spanning 448 human genes, which illustrate the potential of RecBlast in detecting orthologues and in highlighting patterns and trends across multiple taxa. Conclusions: RecBlast provides a fast, inexpensive and valuable insight into trends and phenomena across distance phyla, and provides data, visualizations and directions for downstream analysis. RecBlast's fully automatic pipeline provides a new and intuitive discovery platform for researchers from any domain in biology who are interested in evolution, comparative genomics and phylogenetics, regardless of their computational skills."}, {"title": "Genome-wide association study of asthma in individuals of African ancestry reveals novel asthma susceptibility loci", "url": "https://www.biorxiv.org/content/early/2017/03/02/112953", "tag": "Bioinformatics", "abstract": "BACKGROUND: Asthma is a complex disease with striking disparities across racial and ethnic groups, which may be partly attributable to genetic factors. One of the main goals of the Consortium on Asthma among African-ancestry Populations in the Americas (CAAPA) is to discover genes conferring risk to asthma in populations of African descent. METHODS: We performed a genome-wide meta-analysis of asthma across 11 CAAPA datasets (4,827 asthma cases and 5,397 controls), genotyped on the African Diaspora Power Chip (ADPC) and including existing GWAS array data. The genotype data were imputed up to a whole genome sequence reference panel from n=880 African ancestry individuals for a total of 61,904,576 SNPs. Statistical models appropriate to each study design were used to test for association, and results were combined using the weighted Z-score method. We also used admixture mapping as a complementary approach to identify loci involved in asthma pathogenesis in subjects of African ancestry. RESULTS: SNPs rs787160 and rs17834780 on chromosome 2q22\u00b73 were significantly associated with asthma (p=6 \u00b757\u00d710\u22129 and 2\u00b797 \u00d7 10\u22128 respectively). These SNPs lie in the intergenic region between the Rho GTPase Activating Protein 15 (ARHGAP15) and Glycosyltransferase Like Domain Containing 1 (GTDC1) genes. Four low frequency variants on chromosome 1q21.3, which may be involved in the \"atopic march\" and which are not polymorphic in Europeans, also showed evidence for association with asthma (1\u00b718 \u00d7 10\u22126 \u2264p\u22643\u00b706 \u00d710 \u22126). SNP rs11264909 on chromosome 1q23\u00b71, close to a region previously identified by the EVE asthma meta-analysis as having a putative African ancestry specific effect, only showed differences in counts in subjects homozygous for alleles of African ancestry. Admixture mapping also identified a significantly associated region on chromosome 6q23\u00b72, which includes the Transcription Factor 21 (TCF21) gene, previously shown to be differentially expressed in bronchial tissues of asthmatics and non-asthmatics. CONCLUSIONS: We have identified a number of novel asthma association signals warranting further investigation."}, {"title": "Using Single Nucleotide Variations in Single-Cell RNA-Seq to Identify Tumor Subpopulations and Genotype-phenotype Linkage", "url": "https://www.biorxiv.org/content/early/2017/03/01/095810", "tag": "Bioinformatics", "abstract": "Despite its popularity, characterization of subpopulations with transcript abundance is subject to significant amount of noise. We propose to use effective and expressed nucleotide variations (eeSNVs) from scRNA-seq as alternative features for tumor subpopulation identification. We developed a linear modeling framework SSrGE to link eeSNVs associated with gene expression. In all the cancer datasets tested, eeSNVs achieve better accuracies and more complexity than gene expression for identifying subpopulations. Previously validated cancer relevant genes are also highly ranked, confirming the significance of the method. Moreover, SSrGE is capable of analyzing coupled DNA-seq and RNA-seq data from the same single cells, demonstrating its power over the cutting-edge single-cell genomics techniques. In summary, SNV features from scRNA-seq data have merits for both subpopulation identification and linkage of genotype-phenotype relationship. SSrGE method is available at https://github.com/lanagarmire/SSrGE."}, {"title": "HiCPlus: Resolution Enhancement of Hi-C interaction heatmap", "url": "https://www.biorxiv.org/content/early/2017/03/01/112631", "tag": "Bioinformatics", "abstract": "Motivation: The Hi-C technology has become an efficient tool to measure the spatial organization of the genome. With the recent advance of 1Kb resolution Hi-C experiment, some of the essential regulatory features have been uncovered. However, most available Hi-C datasets are in coarse-resolution due to the extremely high cost for generating high-resolution data. Therefore, a computational method to maximum the usage of the current available Hi-C data is urgently desired. Results: Inspired by the super-resolution image technique, we develop a computational approach to impute the high-resolution Hi-C data from low-resolution Hi-C data using the deep convolutional neural network. We hypothesize that the Hi-C interaction heatmap contains the repeating features, and develop an end-to-end framework to map these features from low-resolution Hi-C heatmap to high-resolution Hi-C heatmap at the feature level. Our approach successfully reconstructs the high-resolution Hi-C interaction map from the low-resolution counterpart, which also proves that the Hi-C interaction matrix is a combination of the regional features. Besides, our approach is highly expandable, and we can also increase prediction accuracy by incorporating ChIA-PET data. Availability: Source code is publicly available at https://github.com/zhangyan32/HiCPlus"}, {"title": "GenomeScope: Fast reference-free genome profiling from short reads", "url": "https://www.biorxiv.org/content/early/2017/02/28/075978", "tag": "Bioinformatics", "abstract": "Summary: GenomeScope is an open-source web tool to rapidly estimate the overall characteristics of a genome, including genome size, heterozygosity rate, and repeat content from unprocessed short reads. These features are essential for studying genome evolution, and help to choose parameters for downstream analysis. We demonstrate its accuracy on 324 simulated and 16 real datasets with a wide range in genome sizes, heterozygosity levels, and error rates. Availability and Implementation: http://genomescope.org, https://github.com/schatzlab/genomescope.git"}, {"title": "Visualization and analysis of single-cell RNA-seq data by kernel-based similarity learning", "url": "https://www.biorxiv.org/content/early/2017/02/28/052225", "tag": "Bioinformatics", "abstract": "Single-cell RNA-seq technologies enable high throughput gene expression measurement of individual cells, and allow the discovery of heterogeneity within cell populations. Measurement of cell-to-cell gene expression similarity is critical to identification, visualization and analysis of cell populations. However, single-cell data introduce challenges to conventional measures of gene expression similarity because of the high level of noise, outliers and dropouts. Here, we propose a novel similarity-learning framework, SIMLR (single-cell interpretation via multi-kernel learning), which learns an appropriate distance metric from the data for dimension reduction, clustering and visualization applications. Benchmarking against state-of-the-art methods for these applications, we used SIMLR to re-analyse seven representative single-cell data sets, including high-throughput droplet-based data sets with tens of thousands of cells. We show that SIMLR greatly improves clustering sensitivity and accuracy, as well as the visualization and interpretability of the data."}, {"title": "GMPR: A novel normalization method for microbiome sequencing data", "url": "https://www.biorxiv.org/content/early/2017/02/28/112565", "tag": "Bioinformatics", "abstract": "Normalization is the first and a critical step in microbiome sequencing (microbiome-Seq) data analysis to account for variable library sizes. Though RNA-Seq based normalization methods have been adapted for microbiome-Seq data, they fail to consider the unique characteristics of microbiome-Seq data, which contain a vast number of zeros due to the physical absence or undersampling of the microbes. Normalization methods that specifically address the zeroinflation remain largely undeveloped. Here we propose GMPR - a simple but effective normalization method - for zeroinflated sequencing data such as microbiome-Seq data. Simulation studies and analyses of 38 real gut microbiome datasets from 16S rRNA gene amplicon sequencing demonstrated the superior performance of the proposed method."}, {"title": "A Bayesian Framework for Estimating Cell Type Composition from DNA Methylation Without the Need for Methylation Reference", "url": "https://www.biorxiv.org/content/early/2017/02/28/112417", "tag": "Bioinformatics", "abstract": "Genome-wide DNA methylation levels measured from a target tissue across a population have become ubiquitous over the last few years, as methylation status is suggested to hold great potential for better understanding the role of epigenetics. Different cell types are known to have different methylation profiles. Therefore, in the common scenario where methylation levels are collected from heterogeneous sources such as blood, convoluted signals are formed according to the cell type composition of the samples. Knowledge of the cell type proportions is important for statistical analysis, and it may provide novel biological insights and contribute to our understanding of disease biology. Since high resolution cell counting is costly and often logistically impractical to obtain in large studies, targeted methods that are inexpensive and practical for estimating cell proportions are needed. Although a supervised approach has been shown to provide reasonable estimates of cell proportions, this approach leverages scarce reference methylation data from sorted cells which are not available for most tissues and are not appropriate for any target population. Here, we introduce BayesCCE, a Bayesian semi-supervised method that leverages prior knowledge on the cell type composition distribution in the studied tissue. As we demonstrate, such prior information is substantially easier to obtain compared to appropriate reference methylation levels from sorted cells. Using real and simulated data, we show that our proposed method is able to construct a set of components, each corresponding to a single cell type, and together providing up to 50% improvement in correlation when compared with existing reference-free methods. We further make a design suggestion for future data collection efforts by showing that results can be further improved using cell count measurements for a small subset of individuals in the study sample or by incorporating external data of individuals with measured cell counts. Our approach provides a new opportunity to investigate cell compositions in genomic studies of tissues for which it was not possible before."}, {"title": "The 3D Genome Browser: a web-based browser for visualizing 3D genome organization and long-range chromatin interactions", "url": "https://www.biorxiv.org/content/early/2017/02/27/112268", "tag": "Bioinformatics", "abstract": "Recent advent of 3C-based technologies such as Hi-C and ChIA-PET provides us an opportunity to explore chromatin interactions and 3D genome organization in an unprecedented scale and resolution. However, it remains a challenge to visualize chromatin interaction data due to its size and complexity. Here, we introduce the 3D Genome Browser (http://3dgenome.org), which allows users to conveniently explore both publicly available and their own chromatin interaction data. Users can also seamlessly integrate other \u201comics\u201d data sets, such as ChIP-Seq and RNA-Seq for the same genomic region, to gain a complete view of both regulatory landscape and 3D genome structure for any given gene. Finally, our browser provides multiple methods to link distal cis-regulatory elements with their potential target genes, including virtual 4C, ChIA-PET, Capture Hi-C and cross-cell-type correlation of proximal and distal DNA hypersensitive sites, and therefore represents a valuable resource for the study of gene regulation in mammalian genomes."}, {"title": "Exhaustively Identifying Cross-Linked Peptides with a Linear Computational Complexity", "url": "https://www.biorxiv.org/content/early/2017/02/27/097089", "tag": "Bioinformatics", "abstract": "Chemical cross-linking coupled with mass spectrometry is a powerful tool to study protein-protein interactions and protein conformations. Two linked peptides are ionized and fragmented to produce a tandem mass spectrum. In such an experiment, a tandem mass spectrum contains ions from two peptides. The peptide identification problem becomes a peptide-peptide pair identification problem. Currently, most existing tools don't search all possible pairs due to the quadratic time complexity. Consequently, a significant percentage of linked peptides are missed. In our earlier work, we developed a tool named ECL to search all pairs of peptides exhaustively. While ECL does not miss any linked peptides, it is very slow due to the quadratic computational complexity, especially when the database is large. Furthermore, ECL uses a score function without statistical calibration, while researchers have demonstrated that using a statistical calibrated score function can achieve a higher sensitivity than using an uncalibrated one. Here, we propose an advanced version of ECL, named ECL 2.0. It achieves a linear time and space complexity by taking advantage of the additive property of a score function. It can analyze a typical data set containing tens of thousands of spectra using a large-scale database containing thousands of proteins in a few hours. Comparison with other five state-of-the-art tools shows that ECL 2.0 is much faster than pLink, StavroX, ProteinProspector, and ECL. Kojak is the only one tool that is faster than ECL 2.0. But Kojak does not exhaustively search all possible peptide pairs. We also adopt an e-value estimation method to calibrate the original score. Comparison shows that ECL 2.0 has the highest sensitivity among the state-of-the-art tools. The experiment using a large-scale in vivo cross-linking data set demonstrates that ECL 2.0 is the only tool that can find PSMs passing the false discovery rate threshold. The result illustrates that exhaustive search and well calibrated score function are useful to find PSMs from a huge search space."}, {"title": "Post-selection Inference Following Aggregate Level Hypothesis Testing in Large Scale Genomic Data", "url": "https://www.biorxiv.org/content/early/2017/02/27/058404", "tag": "Bioinformatics", "abstract": "In many genomic applications, hypotheses tests are performed by aggregating test-statistics across units within naturally defined classes for powerful identification of signals. Following class-level testing, it is naturally of interest to identify the lower level units which contain true signals. Testing the individual units within a class without taking into account the fact that the class was selected using an aggregate-level test-statistic, will produce biased inference. We develop a hypothesis testing framework that guarantees control for false positive rates conditional on the fact that the class was selected. Specifically, we develop procedures for calculating unit level p-values that allows rejection of null hypotheses controlling for two types of conditional error rates, one relating to family wise rate and the other relating to false discovery rate. We use simulation studies to illustrate validity and power of the proposed procedure in comparison to several possible alternatives. We illustrate the power of the method in a natural application involving whole-genome expression quantitative trait loci (eQTL) analysis across 17 tissue types using data from The Cancer Genome Atlas (TCGA) Project."}, {"title": "Deconvolution of multiple infections in Plasmodium falciparum from high throughput sequencing data", "url": "https://www.biorxiv.org/content/early/2017/02/25/099499", "tag": "Bioinformatics", "abstract": "Motivation: The presence of multiple infecting strains of the malarial parasite Plasmodium falciparum affects key phenotypic traits, including drug resistance and risk of severe disease. Advances in protocols and sequencing technology have made it possible to obtain high-coverage genome-wide sequencing data from blood samples and blood spots taken in the field. However, analysing and interpreting such data is challenging because of the high rate of multiple infections present. Results: We have developed a statistical method and implementation for deconvolving multiple genome sequences present in an individual with mixed infections. The software package DEploid uses haplotype structure within a reference panel of clonal isolates as a prior for haplotypes present in a given sample. It estimates the number of strains, their relative proportions and the haplotypes presented in a sample, allowing researchers to study multiple infection in malaria with an unprecedented level of detail. Availability and implementation: The open source implementation DEploid is freely available at https://github.com/mcveanlab/DEploid under the conditions of the GPLv3 license. An R version is available at https://github.com/mcveanlab/DEploid-r. Contact: joe.zhu@well.ox.ac.uk or mcvean@well.ox.ac.uk"}, {"title": "MAGIC: A diffusion-based imputation method reveals gene-gene interactions in single-cell RNA-sequencing data", "url": "https://www.biorxiv.org/content/early/2017/02/25/111591", "tag": "Bioinformatics", "abstract": "Single-cell RNA-sequencing is fast becoming a major technology that is revolutionizing biological discovery in fields such as development, immunology and cancer. The ability to simultaneously measure thousands of genes at single cell resolution allows, among other prospects, for the possibility of learning gene regulatory networks at large scales. However, scRNA-seq technologies suffer from many sources of significant technical noise, the most prominent of which is dropout due to inefficient mRNA capture. This results in data that has a high degree of sparsity, with typically only 10% non-zero values. To address this, we developed MAGIC (Markov Affinity-based Graph Imputation of Cells), a method for imputing missing values, and restoring the structure of the data. After MAGIC, we find that two- and three-dimensional gene interactions are restored and that MAGIC is able to impute complex and non-linear shapes of interactions. MAGIC also retains cluster structure, enhances cluster-specific gene interactions and restores trajectories, as demonstrated in mouse retinal bipolar cells, hematopoiesis, and our newly generated epithelial-to-mesenchymal transition dataset."}, {"title": "A recurrence based approach for validating structural variation using long-read sequencing technology.", "url": "https://www.biorxiv.org/content/early/2017/02/24/105817", "tag": "Bioinformatics", "abstract": "Although there are numerous algorithms that have been developed to identify structural variation (SVs) in genomic sequences, there is a dearth of approaches that can be used to evaluate their results. The emergence of new sequencing technologies that generate longer sequence reads can, in theory, provide direct evidence for all types of SVs regardless of the length of region through which it spans. However, current efforts to use these data in this manner require the use of large computational resources to assemble these sequences as well as manual inspection of each region. Here, we present VaPoR, a highly efficient algorithm that autonomously validates large SV sets using long read sequencing data. We assess of the performance of VaPoR on both simulated and real SVs and report a high-fidelity rate for various features including overall accuracy, sensitivity of breakpoint precision, and predicted genotype. Availability: https://github.com/mills-lab/VaPoR"}, {"title": "16GT: a fast and sensitive variant caller using a 16-genotype probabilistic model", "url": "https://www.biorxiv.org/content/early/2017/02/24/111393", "tag": "Bioinformatics", "abstract": "16GT is a variant caller for Illumina WGS and WES germline data. It uses a new 16- genotype probabilistic model to unify SNP and indel calling in a single variant calling algorithm. In benchmark comparisons with five other widely used variant callers on a modern 36-core server, 16GT ran faster and demonstrated improved sensitivity in calling SNPs, and it provided comparable sensitivity and accuracy in calling indels as compared to the GATK HaplotypeCaller."}, {"title": "Conserved changes in secondary structure and aggregation properties of in vitro evolved proteins for thermo stability", "url": "https://www.biorxiv.org/content/early/2017/02/24/111443", "tag": "Bioinformatics", "abstract": "Most of the screening strategies of directed evolution involved in thermo stability deals with aggregation of proteins either directly or indirectly. Here in this work I investigated what happens in aggregation property and secondary structure of the protein when it improved its thermo stability by incorporating certain amino acid changes in the protein. To study these changes I picked randomly 12 different proteins and I analyzed their 25 different thermo stable mutants.I used open access online Software to get the aggregation propensity values and values for different secondary structure elements propensities of proteins. I compared the aggregation propensity and predicted secondary structure values of thermo stable mutants with their parent Wild type proteins. The stable mutants followed three different conserved patterns to improve their thermo stability."}, {"title": "Improved LC-MS chromatographic alignment increases the accuracy of label-free quantitative proteomics: Comparison of spectral counting versus ion intensity-based proteomic quantification strategies.", "url": "https://www.biorxiv.org/content/early/2017/02/24/111476", "tag": "Bioinformatics", "abstract": "The ability to provide an unbiased qualitative and quantitative description of the global changes to proteins in a cell or an organism would permit the systems-wide study of complex biological systems. Label-free quantitative shotgun proteomic strategies (including LC-MS ion intensity quantification and spectral counting) are attractive because of their relatively low cost, ease of implementation, and the lack of multiplexing restrictions when comparing multiple samples. Owing to improvements in the resolution and sensitivity of mass spectrometers, and the availability of analytical software packages, protein quantification by LC-MS ion intensity has increased in popularity. Here, we have addressed the importance of chromatographic alignment on protein quantification, and then assessed how spectral counting compares to ion intensity-based proteomic quantification. Using a spiked-in protein strategy, we analysed two situations that commonly arise in the application of proteomics to cell biology: (i) samples with a small number of proteins of differential abundance in a larger non-changing background, and (ii) samples with a larger number of proteins of differential abundance. To perform these assessments on biologically relevant samples, we used isolated integrin adhesion complexes (IACs). Technical replicate analysis of isolated IACs resulted in a range of alignment scores using the Progenesis QI software package and demonstrated that higher LC-MS chromatographic alignment scores increased the precision of protein quantification. Furthermore, implementation of a simple sample batch-running strategy enabled good chromatographic alignment for hundreds of samples over multiple batches. Finally, we applied the sample batch-running strategy and compared quantification by LC-MS ion intensity to spectral counting and found that quantification by LC-MS ion intensity was more accurate and precise. In summary, these results demonstrate that chromatographic alignment is important for precise and accurate protein quantification based on LC-MS ion intensity and accordingly we present a simple sample re-ordering strategy to facilitate improved alignment. These findings are not only relevant to label-free quantification using Progenesis QI but may be useful to the wide range of MS-based quantification strategies that rely on chromatographic alignment."}, {"title": "The E. coli molecular phenotype under different growth conditions", "url": "https://www.biorxiv.org/content/early/2017/02/23/082032", "tag": "Bioinformatics", "abstract": "Modern systems biology requires extensive, carefully curated measurements of cellular components in response to different environmental conditions. While high-throughput methods have made transcriptomics and proteomics datasets widely accessible and relatively economical to generate, systematic measurements of both mRNA and protein abundances under a wide range of different conditions are still relatively rare. Here we present a detailed, genome-wide transcriptomics and proteomics dataset of E. coli grown under 34 different conditions. Additionally, we provide measurements of doubling times and in-vivo metabolic fluxes through the central carbon metabolism. We manipulate concentrations of sodium and magnesium in the growth media, and we consider four different carbon sources glucose, gluconate, lactate, and glycerol. Moreover, samples are taken both in exponential and stationary phase, and we include two extensive time-courses, with multiple samples taken between 3 hours and 2 weeks. We find that exponential-phase samples systematically differ from stationary-phase samples, in particular at the level of mRNA. Regulatory responses to different carbon sources or salt stresses are more moderate, but we find numerous differentially expressed genes for growth on gluconate and under salt and magnesium stress. Our data set provides a rich resource for future computational modeling of E. coli gene regulation, transcription, and translation."}, {"title": "TeachEnG: a Teaching Engine for Genomics", "url": "https://www.biorxiv.org/content/early/2017/02/23/111054.1", "tag": "Bioinformatics", "abstract": "Bioinformatics is a rapidly growing field that has emerged from the synergy of computer science, statistics, and biology. Given the interdisciplinary nature of bioinformatics, many students from diverse fields struggle with grasping bioinformatic concepts only from classroom lectures. Interactive tools for helping students reinforce their learning would be thus desirable. Here, we present an interactive online educational tool called TeachEnG (acronym for Teaching Engine for Genomics) for reinforcing key concepts in sequence alignment and phylogenetic tree reconstruction. Our instructional games allow students to align sequences by hand, fill out the dynamic programming matrix in the Needleman-Wunsch global sequence alignment algorithm, and reconstruct phylogenetic trees via the maximum parsimony and Unweighted Pair Group Method with Arithmetic mean (UPGMA) algorithms. With an easily accessible interface and instant visual feedback, TeachEnG will help promote active learning in bioinformatics. TeachEnG is freely available at http://song.igb.illinois.edu/TeachEnG/. It is written in JavaScript and compatible with Firefox, Safari, Chrome, and Microsoft Edge."}, {"title": "Literature Consistency of Bioinformatics Sequence Databases is Effective for Assessing Record Quality", "url": "https://www.biorxiv.org/content/early/2017/02/23/101873", "tag": "Bioinformatics", "abstract": "Bioinformatics sequence databases such as Genbank or UniProt contain hundreds of millions of records of genomic data. These records are derived from direct submissions from individual laboratories, as well as from bulk submissions from large-scale sequencing centres; their diversity and scale means that they suffer from a range of data quality issues including errors, discrepancies, redundancies, ambiguities, incompleteness, and inconsistencies with the published literature. In this work, we seek to investigate and analyze the data quality of sequence databases from the perspective of a curator, who must detect anomalous and suspicious records. Specifically, we emphasize the detection of inconsistent records with respect to the literature. Focusing on GenBank, we propose a set of 24 quality indicators, which are based on treating a record as a query into the published literature, and then use query quality predictors. We then carry out an analysis that shows that the proposed quality indicators and the quality of the records have a mutual relationship, in which one depends on the other. We propose to represent record-literature consistency as a vector of these quality indicators. By reducing the dimensionality of this representation for visualization purposes using Principal Component Analysis, we show that records which have been reported as inconsistent with the literature fall roughly in the same area, and therefore share similar characteristics. By manually analyzing records not previously known to be erroneous that fall in the same area than records know to be inconsistent, we show that 1 record out of 4 is inconsistent with respect to the literature. This high density of inconsistent record opens the way towards the development of automatic methods for the detection of faulty records. We conclude that literature inconsistency is a meaningful strategy for identifying suspicious records."}, {"title": "EXTRACT 2.0: text-mining-assisted interactive annotation of biomedical named entities and ontology terms", "url": "https://www.biorxiv.org/content/early/2017/02/23/111088", "tag": "Bioinformatics", "abstract": "The original version of EXTRACT was designed to support annotation of metagenomic samples with semantically con-trolled environmental descriptors (Pafilis et al., 2016). For this reason, it focused on named entity recognition of terms from the Environment Ontology (ENVO) and ontologies relevant for describing host organisms, tissues, and disease states. EXTRACT 2.0 expands the scope of the tool in several new directions with the aim to make it more broadly useful."}, {"title": "MetaCherchant - an algorithm for analyzing genomic environment of antibiotic resistance gene in gut microbiota", "url": "https://www.biorxiv.org/content/early/2017/02/23/106161", "tag": "Bioinformatics", "abstract": "Antibiotic resistance is an important global public health problem. Human gut human microbiota is an accumulator of resistance genes potentially providing them to pathogens. It is important to develop tools for identifying the mechanisms of how resistance is transmitted between gut microbial species and pathogens. We developed MetaCherchant - an algorithm for extracting the genomic environment of antibiotic resistance genes from metagenomic data in the form of a graph. The algorithm was validated on simulated datasets and applied to new \"shotgun\" metagenomes of gut microbiota from patients with Helicobacter pylori who underwent antibiotic therapy. Genomic context was reconstructed for several dominant resistance genes; taxonomic annotation of the context showed the species carrying the genes. Application of MetaCherchant in differential mode produced specific graph structures suggesting the evidence of possible resistance gene transmission within a mobile element that occurred as a result of the antibiotic therapy. MetaCherchant is a promising tool giving researchers an opportunity to get an insight into dynamics of resistance transmission in vivo based on metagenomic data."}, {"title": "W2RAP: a pipeline for high quality, robust assemblies of large complex genomes from short read data", "url": "https://www.biorxiv.org/content/early/2017/02/22/110999", "tag": "Bioinformatics", "abstract": "Producing high-quality whole-genome shotgun de novo assemblies from plant and animal species with large and complex genomes using low-cost short read sequencing technologies remains a challenge. But when the right sequencing data, with appropriate quality control, is assembled using approaches focused on robustness of the process rather than maximization of a single metric such as the usual contiguity estimators, good quality assemblies with informative value for comparative analyses can be produced. Here we present a complete method described from data generation and qc all the way up to scaffold of complex genomes using Illumina short reads and its application to data from plants and human datasets. We show how to use the w2rap pipeline following a metric-guided approach to produce cost-effective assemblies. The assemblies are highly accurate, provide good coverage of the genome and show good short range contiguity. Our pipeline has already enabled the rapid, cost-effective generation of de novo genome assemblies from large, polyploid crop species with a focus on comparative genomics."}, {"title": "From Sequence to Function: Coevolving Amino Acids Encode Structural and Functional Domains", "url": "https://www.biorxiv.org/content/early/2017/02/22/109397", "tag": "Bioinformatics", "abstract": "Amino acids interactions within protein families are so optimized that the sole analysis of evolutionary co-mutations can identify pairs of contacting residues. It is also known that evolution conserves functional dynamics, i.e., the concerted motion or displacement of large protein regions or domains. Is it, therefore, possible to use a pure sequence-based analysis to identify these dynamical domains? To address this question, we introduce here a general co-evolutionary coupling analysis strategy and apply it to a curated sequence database of hundreds of protein families. For most families, the sequence-based method partitions amino acids into few clusters. When viewed in the context of the native structure, these clusters have the signature characteristics of viable protein domains: they are spatially separated but individually compact. They have a direct functional bearings too, as shown for various reference cases. We conclude that even large-scale structural and functionally-related properties can be recovered from inference methods applied to evolutionary-related sequences. The method introduced here is available as a software package and web server (http://spectrus.sissa.it/spectrus-evo_webserver)."}, {"title": "Scalable genomics: from raw data to aligned reads on Apache YARN", "url": "https://www.biorxiv.org/content/early/2017/02/22/071092", "tag": "Bioinformatics", "abstract": "The adoption of Big Data technologies can potentially boost the scalability of data-driven biology and health workflows by orders of magnitude. Consider, for instance, that technologies in the Hadoop ecosystem have been successfully used in data-driven industry to scale their processes to levels much larger than any biological- or health-driven work attempted thus far. In this work we demonstrate the scalability of a sequence alignment pipeline based on technologies from the Hadoop ecosystem -- namely, Apache Flink and Hadoop MapReduce, both running on the distributed Apache YARN platform. Unlike previous work, our pipeline starts processing directly from the raw BCL data produced by Illumina sequencers. A Flink-based distributed algorithm reconstructs reads from the Illumina BCL data, and then demultiplexes them -- analogously to the bcl2fastq2 program provided by Illumina. Subsequently, the BWA-MEM-based distributed aligner from the Seal project is used to perform read mapping on the YARN platform. While the standard programs by Illumina and BWA-MEM are limited to shared-memory parallelism (multi-threading), our solution is completely distributed and can scale across a large number of computing nodes. Results show excellent pipeline scalability, linear in the number of nodes. In addition, this approach automatically benefits from the robustness to hardware failure and transient cluster problems provided by the YARN pipeline, as well as the scalability of the Hadoop Distributed File System. Moreover, this YARN-based approach complements the up-and-coming version 4 of the GATK toolkit, which is based on Spark and therefore can run on YARN. Together, they can be used to form a scalable complete YARN-based variant calling pipeline for Illumina data, which will be further improved with the arrival of distributed in-memory filesystem technology such as Apache Arrow, thus removing the need to write intermediate data to disk."}, {"title": "A Mixture Copula Bayesian Network Model for Multimodal Genomic Data", "url": "https://www.biorxiv.org/content/early/2017/02/22/110288", "tag": "Bioinformatics", "abstract": "Gaussian Bayesian networks have become a widely used framework to estimate directed associations between joint Gaussian variables, where the network structure encodes decomposition of multivariate normal density into local terms. However, the resulting estimates can be inaccurate when normality assumption is moderately or severely violated, making it unsuitable to deal with recent genomic data such as the Cancer Genome Atlas data. In the present paper, we propose a mixture copula Bayesian network model which provides great flexibility in modeling non-Gaussian and multimodal data for causal inference. The parameters in mixture copula functions can be efficiently estimated by a routine Expectation-Maximization algorithm. A heuristic search algorithm based on Bayesian information criterion is developed to estimate the network structure, and prediction can be further improved by the best-scoring network out of multiple predictions from random initial values. Our method outperforms Gaussian Bayesian networks and regular copula Bayesian networks in terms of modeling flexibility and prediction accuracy, as demonstrated using a cell signaling dataset. We apply the proposed methods to the Cancer Genome Atlas data to study the genetic and epigenetic pathways that underlie serous ovarian cancer."}, {"title": "Comparative insights to the transportome of Nosema: a genus of parasitic microsporidians", "url": "https://www.biorxiv.org/content/early/2017/02/22/110809", "tag": "Bioinformatics", "abstract": "Nosema, a genus of parasitic microsporidia, causes pebrine disease in arthropods, including economically important silkworms and honeybees. Nosema have gene-poor genomes shaped by loss of the metabolic pathways, as a consequence of continued dependence on host-derived substrates. As an act of counterbalance, they have developed an array of transporter proteins that allow stealing from their hosts. Here, we have identified the core set of twelve transporter families present in Nosema genus, viz. N. apis, N. bombycis, N. ceranae and N. antheraea through in silico pipeline. Transportomes of N. apis, N. bombycis, N. ceranae and N. antheraea have a dominant share of secondary carriers and primary active transporters. The comparatively rich and diverse transportome of N. bombycis indicates the role of transporters in its remarkable capability of host adaptation. The core set of transporter families of Nosema includes ones that have a likely role in osmo-regulation, intra- and extra-cellular pH regulation, energy compensation and self-defence mechanism. This study has also revealed a set of ten species-specific transporter families within the genus. To our knowledge, this is the first ever intra-genus study on microsporidian transporters. Both these datasets constitutes a valuable resource that can aid in development of inhibitor-based Nosema management strategies."}, {"title": "GRIDSS: sensitive and specific genomic rearrangement detection using positional de Bruijn graph assembly", "url": "https://www.biorxiv.org/content/early/2017/02/21/110387", "tag": "Bioinformatics", "abstract": "The identification of genomic rearrangements, particularly in cancers, with high sensitivity and specificity using massively parallel sequencing remains a major challenge. Here, we describe the Genome Rearrangement IDentification Software Suite (GRIDSS), a high-speed structural variant (SV) caller that performs efficient genome-wide break-end assembly prior to variant calling using a novel positional de Bruijn graph assembler. By combining assembly, split read and read pair evidence using a probabilistic scoring, GRIDSS achieves high sensitivity and specificity on simulated, cell line and patient tumour data, recently winning SV sub-challenge #5 of the ICGC-TCGA DREAM Somatic Mutation Calling Challenge. On human cell line data, GRIDSS halves the false discovery rate compared to other recent methods. GRIDSS identifies non-template sequence insertions, micro-homologies and large imperfect homologies, and supports multi-sample analysis. GRIDSS is freely available at https://github.com/PapenfussLab/gridss."}, {"title": "SAVE: A secure cloud-based pipeline for CRISPR pooled screen deconvolution", "url": "https://www.biorxiv.org/content/early/2017/02/21/110262", "tag": "Bioinformatics", "abstract": "We present a user-friendly, cloud-based, data analysis pipeline for the deconvolution of pooled screening data. This tool, termed SAVE for Screening Analysis Visual Explorer, serves a dual purpose of extracting, clustering and analyzing raw next generation sequencing files derived from pooled screening experiments while at the same time presenting them in a user-friendly way on a secure web-based platform. Moreover, SAVE serves as a useful web-based analysis pipeline for reanalysis of pooled CRISPR screening datasets. Taken together, the framework described in this study is expected to accelerate development of web-based bioinformatics tool for handling all studies which include next generation sequencing data. SAVE is available at http://save.nrihub.org."}, {"title": "chromVAR: Inferring transcription factor variation from single-cell epigenomic data", "url": "https://www.biorxiv.org/content/early/2017/02/21/110346", "tag": "Bioinformatics", "abstract": "Single cell ATAC-seq (scATAC) yields sparse data that makes application of conventional computational approaches for data analysis challenging or impossible. We developed chromVAR, an R package for analyzing sparse chromatin accessibility data by estimating the gain or loss of accessibility within sets of peaks sharing the same motif or annotation while controlling for known technical biases. chromVAR enables accurate clustering of scATAC-seq profiles and enables characterization of known, or the de novo identification of novel, sequence motifs associated with variation in chromatin accessibility across single cells or other sparse epigenomic data sets."}, {"title": "IcyTree: Rapid browser-based visualization for phylogenetic trees and networks", "url": "https://www.biorxiv.org/content/early/2017/02/21/110213", "tag": "Bioinformatics", "abstract": "Summary: IcyTree is an easy-to-use application which can be used to visualize a wide variety of phylogenetic trees and networks. While numerous phylogenetic tree viewers exist already, IcyTree distinguishes itself by being a purely online tool, having a responsive user interface, supporting phylogenetic networks (ancestral recombination graphs in particular), and efficiently drawing trees that include information such as ancestral locations or trait values. IcyTree also provides intuitive panning and zooming utilities that make exploring large phylogenetic trees of many thousands of taxa feasible. Availability and Implementation: IcyTree is a web application and can be accessed directly at http://tgvaughan.github.io/icytree. Currently-supported web browsers include Mozilla Firefox and Google Chrome. IcyTree is written entirely in client-side JavaScript (no plugin required) and, once loaded, does not require network access to run. IcyTree is free software, and the source code is made available at http://github.com/tgvaughan/icytree under version 3 of the GNU General Public License."}, {"title": "Mass spectrometrists should search for all peptides, but assess only the ones they care about", "url": "https://www.biorxiv.org/content/early/2017/02/21/094581", "tag": "Bioinformatics", "abstract": "In shotgun proteomics identified mass spectra that are deemed irrelevant to the scientific hypothesis are often discarded. Noble (2015) therefore urged researchers to remove irrelevant peptides from the database prior to searching to improve statistical power. We here however, argue that both the classical as well as Noble's revised method produce suboptimal peptide identifications and have problems in controlling the false discovery rate (FDR). Instead, we show that searching for all expected peptides, and removing irrelevant peptides prior to FDR calculation results in more reliable identifications at controlled FDR level than the classical strategy that discards irrelevant peptides post FDR calculation, or than Noble's strategy that discards irrelevant peptides prior to searching. We provide an implementation of our strategy as a user-friendly web-based tool at http://iomics.ugent.be/saas/."}, {"title": "Similarity identification in gene expression patterns as a new approach in phenotype classification", "url": "https://www.biorxiv.org/content/early/2017/02/20/110130", "tag": "Bioinformatics", "abstract": "Stratifying healthy and malignant phenotypes and identifying their biological states using high-throughput molecular data has been the focus of many computational approaches during the last decade. Using multivariate changes in expression of genes within biological pathways, as fingerprints of complex phenotypes, we developed a new methodology for Similarity Identification in Gene expressioN (SIGN). In this approach, we use centroid classifier to identify phenotype of each biological sample. To obtain similarity of a given biological sample with classes of phenotypes, we defined a new distance measure, transcriptional similarity coefficient (TSC) which captures similarity of gene expression patterns between a biological pathway in two samples or populations. We showed that TSC, as an interpretable and stable distance measure in SIGN, captures all oncogenic hallmarks for breast cancer even with low sample size, by comparing healthy and patient tumor samples in the largest breast cancer dataset. In this study, we demonstrate that SIGN is a flexible, yet robust approach for classification based on transcriptomics data. Comparing early and late relapses within each molecular subtypes of breast cancer, our method enabled subtype-specific stratification of breast cancer patients into groups with significantly different survival. Moreover, we used SIGN to classify with more than 99% specificity the site of extraction of healthy and tumor samples from the Genotype-Tissue Expression (GTEx) and The Cancer Genome Atlas (TCGA) datasets. We showed that SIGN also enables robust identification of hematopoietic stem cell and progenitors within the hematopoietic hierarchy. We further explored chemical perturbation data in the Connectivity Map (CMAP) database and showed that SIGN was able to classify seven classes of drugs based on their mechanism of action. In conclusion, we showed that SIGN can be used to achieve interpretable and robust transcriptomic-based classification of healthy and malignant samples, as well as drugs based on their known mechanism of action, supporting the generalizability and relevance of the method for the analysis of gene expression profiles."}, {"title": "Genome-wide regulatory model from MPRA data predicts functional regions, eQTLs, and GWAS hits", "url": "https://www.biorxiv.org/content/early/2017/02/20/110171", "tag": "Bioinformatics", "abstract": "Massively-parallel reporter assays (MPRA) enable unprecedented opportunities to test for regulatory activity of thousands of regulatory sequences. However, MPRA only assay a subset of the genome thus limiting their applicability for genome-wide functional annotations. To overcome this limitation, we have used existing MPRA datasets to train a machine learning model that uses DNA sequence information, regulatory motif annotations, evolutionary conservation, and epigenomic information to predict genomic regions that show enhancer activity when tested in MPRA assays. We used the resulting model to generate global predictions of regulatory activity at single-nucleotide resolution across 14 million common variants. We find that genetic variants with stronger predicted regulatory activity show significantly lower minor allele frequency, indicative of evolutionary selection within the human population. They also show higher overlap with eQTL annotations across multiple tissues relative to the background SNPs, indicating that their perturbations in vivo more frequently result in changes in gene expression. In addition, they are more frequently associated with trait-associated SNPs from genome-wide association studies (GWAS), enabling us to prioritize genetic variants that are more likely to be causal based on their predicted regulatory activity. Lastly, we use our model to compare MPRA inferences across cell types and platforms and to prioritize the assays most predictive of MPRA assay results, including cell-dependent DNase hypersensitivity sites and transcription factors known to be active in the tested cell types. Our results indicate that high-throughput testing of thousands of putative regions, coupled with regulatory predictions across millions of sites, presents a powerful strategy for systematic annotation of genomic regions and genetic variants."}, {"title": "Evolinc: a comparative transcriptomics and genomics pipeline for quickly identifying sequence conserved lincRNAs for functional analysis.", "url": "https://www.biorxiv.org/content/early/2017/02/20/110148", "tag": "Bioinformatics", "abstract": "Long intergenic non-coding RNAs (lincRNAs) are an abundant and functionally diverse class of eukaryotic transcripts. Reported lincRNA repertoires in mammals vary, but are commonly in the thousands to tens of thousands of transcripts, covering ~90% of the genome. In addition to elucidating function, there is particular interest in understanding the origin and evolution of lincRNAs. Aside from mammals, lincRNA populations have been sparsely sampled, precluding evolutionary analyses focused on lincRNA emergence and persistence. Here we present Evolinc, a two-module pipeline designed to facilitate lincRNA discovery and characterize aspects of lincRNA evolution. The first module (Evolinc-I) is a lincRNA identification workflow that also facilitates downstream differential expression analysis and genome browser visualization of identified lincRNAs. The second module (Evolinc-II) is a genomic and transcriptomic comparative analyses workflow that determines the phylogenetic depth to which a lincRNA locus is conserved within a user-defined group of related species. Evolinc-II builds families of homologous lincRNA loci, aligns constituent sequences, infers gene trees, and then uses gene tree / species tree reconciliation to reconstruct evolutionary processes such as gain, loss, or duplication of the locus. Here we demonstrate that Evolinc-I is agnostic to target organism by validating against previously annotated Arabidopsis and human lincRNA data. Using Evolinc-II, we examine ways in which conservation can rapidly be used to winnow down large lincRNA datasets to a small set of candidates for functional analysis. Finally, we show how Evolinc-II can be used to recover the evolutionary history of a known lincRNA, the human telomerase RNA (TERC). The analyses revealed unexpected duplication events as well as the loss and subsequent acquisition of a novel TERC locus in the lineage leading to mice and rats. The Evolinc pipeline is currently integrated in CyVerse\u2032s Discovery Environment and is free to use by researchers."}, {"title": "FUMA: Functional mapping and annotation of genetic associations", "url": "https://www.biorxiv.org/content/early/2017/02/20/110023", "tag": "Bioinformatics", "abstract": "A main challenge in genome-wide association studies (GWAS) is to prioritize genetic variants and identify potential causal mechanisms of human diseases. Although multiple bioinformatics resources are available for functional annotation and prioritization, a standard, integrative approach is lacking. We developed FUMA: a web-based platform to facilitate functional annotation of GWAS results, prioritization of genes and interactive visualization of annotated results by incorporating information from multiple state-of-the-art biological databases."}, {"title": "CRISPRAnalyzeR: Interactive analysis, annotation and documentation of pooled CRISPR screens", "url": "https://www.biorxiv.org/content/early/2017/02/20/109967", "tag": "Bioinformatics", "abstract": "Pooled CRISPR/Cas9 screens are a powerful and versatile tool for the systematic investigation of cellular processes in a variety of organisms. Such screens generate large amounts of data that present a new challenge to analyze and interpret. Here, we developed a web application to analyze, document and explore pooled CRISR/Cas9 screens using a unified single workflow. The end-to-end analysis pipeline features eight different hit calling strategies based on state-of-the-art methods, including DESeq2, MAGeCK, edgeR, sgRSEA, Z-Ratio, Mann-Whitney test, ScreenBEAM and BAGEL. Results can be compared with interactive visualizations and data tables. CRISPRAnalyzeR integrates meta- information from 26 external data resources, providing a wide array of options for the annotation and documentation of screens. The application was developed with user experience in mind, requiring no previous knowledge in bioinformatics. All modern operating systems are supported. Availability and online documentation: The source code, a pre-configured docker application, sample data and a documentation can be found on our GitHub page (http://www.github.com/boutroslab/CRISPRAnalyzeR). A tutorial video can be found at http://www.crispr-analyzer.org."}, {"title": "Approximate Bayesian bisulphite sequencing analysis (ABBA) for analysis of WGBS in disease", "url": "https://www.biorxiv.org/content/early/2017/02/20/041715", "tag": "Bioinformatics", "abstract": "DNA methylation is a key epigenetic modification involved in gene regulation whose contribution to disease susceptibility remains to be fully understood. Here, we present a novel Bayesian smoothing approach (called ABBA) to detect differentially methylated regions (DMRs) from whole-genome bisulphite sequencing (WGBS). We also show how this approach can be leveraged to identify disease-associated changes in DNA methylation, suggesting mechanisms through which these alterations might affect disease. From a data modeling perspective, ABBA has the distinctive feature of automatically adapting to different correlation structures in CpG methylation levels across the genome whilst taking into account the distance between CpG sites as a covariate. Our simulation study shows that ABBA has greater power to detect DMRs than existing methods, providing an accurate identification of DMRs in the large majority of simulated cases. To empirically demonstrate the method's efficacy in generating biological hypotheses, we performed WGBS of primary macrophages derived from an experimental rat system of glomerulonephritis and used ABBA to identify >1,000 disease-associated DMRs. Investigation of these DMRs revealed differential DNA methylation localized to a 600bp region in the promoter of the Ifitm3 gene. This was confirmed by ChIP-seq and RNA-seq analyses, showing differential transcription factor binding at the Ifitm3 promoter by JunD (an established determinant of glomerulonephritis) and a consistent change in Ifitm3 expression. Our ABBA analysis allowed us to propose a new role for Ifitm3 in the pathogenesis of glomerulonephritis via a mechanism involving promoter hypermethylation that is associated with Ifitm3 repression in the rat strain susceptible to glomerulonephritis."}, {"title": "Emergent community agglomeration from data set geometry", "url": "https://www.biorxiv.org/content/early/2017/02/17/109587", "tag": "Bioinformatics", "abstract": "In the statistical learning language, samples are snapshots of random vectors drawn from some unknown distribution. Such vectors usually reside in a high-dimensional Euclidean space, and thus, the \"curse of dimensionality\" often undermines the power of learning methods, including community detection and clustering algorithms, that rely on Euclidean geometry. This paper presents the idea of effective dissimilarity transformation (EDT) on empirical dissimilarity hyperspheres and studies its effects using synthetic and gene expression data sets. Iterating the EDT turns a static data distribution into a dynamical process purely driven by the empirical data set geometry and adaptively ameliorates the curse of dimensionality, partly through changing the topology of a Euclidean feature space into a compact hypersphere. The EDT often improves the performance of hierarchical clustering via the automatic grouping information emerging from global interactions of data points. The EDT is not restricted to hierarchical clustering, and other learning methods based on pairwise dissimilarity should also benefit from the many desirable properties of EDT."}, {"title": "Significance estimation for large scale untargeted metabolomics annotations", "url": "https://www.biorxiv.org/content/early/2017/02/17/109389", "tag": "Bioinformatics", "abstract": "The annotation of small molecules in untargeted mass spectrometry relies on the matching of fragment spectra to reference library spectra. While various spectrum-spectrum match scores exist, the field lacks statistical methods for estimating the false discovery rates (FDR) of these annotations. We present empirical Bayes and target-decoy based methods to estimate the false discovery rate. Relying on estimations of false discovery rates, we explore the effect of different spectrum-spectrum match criteria on the number and the nature of the molecules annotated. We show that the spectral matching settings needs to be adjusted for each project. By adjusting the scoring parameters and thresholds, the number of annotations rose, on average, by +139% (ranging from -92% up to +5705%) when compared to a default parameter set available at GNPS. The FDR estimation methods presented will enable a user to define the scoring criteria for large scale analysis of untargeted small molecule data that has been essential in the advancement of large scale proteomics, transcriptomics, and genomics science."}, {"title": "High-confidence Coding and Noncoding Transcriptome Maps", "url": "https://www.biorxiv.org/content/early/2017/02/17/109363", "tag": "Bioinformatics", "abstract": "The advent of high-throughput RNA-sequencing (RNA-seq) has led to the discovery of unprecedentedly immense transcriptomes encoded by eukaryotic genomes. However, the transcriptome maps are still incomplete partly because they were mostly reconstructed based on RNA-seq reads that lack their orientations (known as unstranded reads) and certain boundary information. Methods to expand the usability of unstranded RNA-seq data by predetermining the orientation of the reads and precisely determining the boundaries of assembled transcripts could significantly benefit the quality of the resulting transcriptome maps. Here, we present a high-performing transcriptome assembly pipeline, called CAFE, that significantly improves the original assemblies, respectively assembled with stranded and/or unstranded RNA-seq data, by orienting unstranded reads using the maximum likelihood estimation and by integrating information about transcription start sites and cleavage and polyadenylation sites. Applying large-scale transcriptomic data comprising ninety-nine billion RNAs-seq reads from the ENCODE, human BodyMap projects, The Cancer Genome Atlas, and GTEx, CAFE enabled us to predict the directions of about eighty-nine billion unstranded reads, which led to the construction of more accurate transcriptome maps, comparable to the manually curated map, and a comprehensive lncRNA catalogue that includes thousands of novel lncRNAs. Our pipeline should not only help to build comprehensive, precise transcriptome maps from complex genomes but also to expand the universe of non-coding genomes."}, {"title": "plantiSMASH: automated identification, annotation and expression analysis of plant biosynthetic gene clusters", "url": "https://www.biorxiv.org/content/early/2017/02/17/083535", "tag": "Bioinformatics", "abstract": "Plant specialized metabolites are chemically highly diverse, play key roles in host-microbe interactions, have important nutritional value in crops and are frequently applied as medicines. It has recently become clear that plant biosynthetic pathway-encoding genes are sometimes densely clustered in specific genomic loci: biosynthetic gene clusters (BGCs). Here, we introduce plantiSMASH, a versatile online analysis platform that automates the identification of candidate plant BGCs. Moreover, it allows integration of transcriptomic data to prioritize candidate BGCs based on the coexpression patterns of predicted biosynthetic enzyme-coding genes, and facilitates comparative genomic analysis to study the evolutionary conservation of each cluster. Applied on 48 high-quality plant genomes, plantiSMASH identifies a rich diversity of candidate plant BGCs. These results will guide further experimental exploration of the nature and dynamics of gene clustering in plant metabolism. Moreover, spurred by the continuing decrease in costs of plant genome sequencing, they will allow genome mining technologies to be applied to plant natural product discovery. The plantiSMASH web server, precalculated results and source code are freely available from http://plantismash.secondarymetabolites.org."}, {"title": "gene-cocite: a web application for extracting, visualising and assessing the cocitations of a list of genes", "url": "https://www.biorxiv.org/content/early/2017/02/16/109173", "tag": "Bioinformatics", "abstract": "The outcome from the analysis of high through-put genomics experiments is commonly a list of genes. The most basic measure of association is whether the genes in the list have ever been cocited together. The web application gene-cocite accepts a list of genes and returns a list of the papers which cocite any two or more of the genes. The proportion of the genes which are cocited with at least one other gene is given, and the p-value for the probability of this proportion of cocitations occurring by chance from a random list of genes of the same length calculated. An interactive graph with links to papers is displayed, showing how the genes in the list are related to each other by publications. gene-cocite (http://sysbio.mrc-bsu.cam.ac.uk/gene-cocite) is designed to be an easy to use first step for biological researchers investigating the background of their list of genes."}, {"title": "Structural and Functional View of Polypharmacology", "url": "https://www.biorxiv.org/content/early/2017/02/17/044289", "tag": "Bioinformatics", "abstract": "Protein domains mediate drug-protein interactions and this effect can explain drug polypharmacology. In this study, we associate polypharmacological drugs with CATH functional families, a type of protein domain and we use the network properties of these druggable protein families to analyse their relationships with drug side effects. We found druggable CATH functional families enriched in drug targets, whose relatives are structurally coherent, gather together in the protein functional network occupying central positions, and tend to be free of proteins associated with drug side effects. Our results demonstrate that CATH functional families can be used to identify drug-target interactions, opening a new research direction in target identification."}, {"title": "Improved assemblies and comparison of two ancient Yersinia pestis genomes", "url": "https://www.biorxiv.org/content/early/2017/02/17/073445", "tag": "Bioinformatics", "abstract": "Yersinia pestis is the causative agent of the bubonic plague, a disease responsible for several dramatic historical pandemics. Progress in ancient DNA (aDNA) sequencing rendered possible the sequencing of whole genomes of important human pathogens, including the ancient Yersinia pestis strains responsible for important outbreaks of the bubonic plague in London in the 14th century and in Marseille in the 18th century among others. However, aDNA sequencing data are still characterized by short reads and non-uniform coverage, so assembling ancient pathogen genomes remains challenging and prevents in many cases a detailed study of genome rearrangements. It has recently been shown that comparative scaffolding approaches can improve the assembly of ancient Yersinia pestis genomes at a chromosome level. In the present work, we address the last step of genome assembly, the gap- filling stage. We describe an optimization-based method AGapEs (Ancestral Gap Estimation) to fill in inter-contig gaps using a combination of a template obtained from related extant genomes and aDNA reads. We show how this approach can be used to refine comparative scaffolding by selecting contig adjacencies supported by a mix of unassembled aDNA reads and evolutionary parsimony signal. We apply our method to two ancient Yersinia pestis genomes from the London and Marseilles outbreaks of the bubonic plague. We obtain highly improved genome assemblies for both the London strain and Marseille strain genomes, comprised of respectively five and six scaffolds, with 95% of the assemblies supported by ancient reads. We analyze the genome evolution between both ancient genomes in terms of genome rearrangements, and observe a high level of synteny conservation between these two strains."}, {"title": "A general and powerful stage-wise testing procedure for differential expression and differential transcript usage", "url": "https://www.biorxiv.org/content/early/2017/02/16/109082", "tag": "Bioinformatics", "abstract": "Background: Reductions in sequencing cost and innovations in expression quantification have prompted an emergence of RNA-seq studies with complex designs and data analysis at transcript resolution. These applications involve multiple hypotheses per gene, leading to challenging multiple testing problems. Conventional approaches provide separate top-lists for every contrast and false discovery rate (FDR) control at individual hypothesis level. Hence, they fail to establish proper gene-level error control, which compromises downstream validation experiments. Tests that aggregate individual hypotheses are more powerful and provide gene-level FDR control, but in the RNA-seq literature no methods are available for post-hoc analysis of individual hypotheses. Results: We introduce a two-stage procedure that leverages the increased power of aggregated hypothesis tests while maintaining high biological resolution by post-hoc analysis of genes passing the screening hypothesis. Our method is evaluated on simulated and real RNA-seq experiments. It provides gene-level FDR control in studies with complex designs while boosting power for interaction effects without compromising the discovery of main effects. In a differential transcript usage/expression context, stage-wise testing gains power by aggregating hypotheses at the gene level, while providing transcript-level assessment of genes passing the screening stage. Finally, a prostate cancer case study highlights the relevance of combining gene with transcript level results. Conclusion: Stage-wise testing is a general paradigm that can be adopted whenever individual hypotheses can be aggregated. In our context, it achieves an optimal middle ground between biological resolution and statistical power while providing gene-level FDR control, which is beneficial for downstream biological interpretation and validation."}, {"title": "CircularLogo: A lightweight web application to visualize intra-motif dependencies", "url": "https://www.biorxiv.org/content/early/2017/02/16/098327", "tag": "Bioinformatics", "abstract": "Background: The sequence logo has been widely used to represent DNA or RNA motifs for more than three decades. Despite its intelligibility and intuitiveness, the traditional sequence logo is unable to display the intra-motif dependencies and therefore is insufficient to fully characterize nucleotide motifs. Many methods have been developed to quantify the intra-motif dependencies, but fewer tools are available for visualization. Result: We developed CircularLogo, a web-based interactive application, which is able to not only visualize the position-specific nucleotide consensus and diversity but also display the intra-motif dependencies. Applying CircularLogo to HNF6 binding sites and tRNA sequences demonstrated its ability to show intra-motif dependencies and intuitively reveal biomolecular structure. CircularLogo is implemented in JavaScript and Python based on the Django web framework. The program\u2032s source code and user\u2032s manual are freely available at http://circularlogo.sourceforge.net. CircularLogo web server can be accessed from http://bioinformaticstools.mayo.edu/circularlogo/index.html. Conclusion: CircularLogo is an innovative web application that is specifically designed to visualize and interactively explore intra-motif dependencies"}, {"title": "Inference of maternal allele inheritance via Bayesian hierarchical model in noninvasive prenatal diagnosis", "url": "https://www.biorxiv.org/content/early/2017/02/16/051995", "tag": "Bioinformatics", "abstract": "Noninvasive prenatal diagnosis (NIPD) poses a promising solution for detecting genetic alterations in fetus genome. However, the inference of the maternal allele inheritance in monogenic autosomal recessive disease is still challenging. Here the Bayesian hierarchical model is proposed to deduce the allele inheritance basing on haplotype frequency. The Bayesian approach, which does not depend on the knowledge of fetus DNA proportion in maternal plasma, provides accurate estimations on both real and simulated data; moreover, it is most robust than current methods in analyzing noisy or even erroneous data."}, {"title": "GenoGAM: Genome-wide generalized additive models for ChIP-seq analysis", "url": "https://www.biorxiv.org/content/early/2017/02/16/047464", "tag": "Bioinformatics", "abstract": "Chromatin immunoprecipitation followed by deep sequencing (ChIP-Seq) is a widely used approach to study protein-DNA interactions. Often, the quantities of interest are the differential occupancies relative to controls, between genetic backgrounds, treatments, or combinations thereof. Current methods for differential occupancy of ChIP-seq data rely however on binning or sliding window techniques, for which the choice of the window and bin sizes are subjective. Here, we present GenoGAM (Genome-wide Generalized Additive Model), which brings the well-established and flexible generalized additive models framework to genomic applications using a data parallelism strategy. We model ChIP-Seq read count frequencies as products of smooth functions along chromosomes. Smoothing parameters are objectively estimated from the data by cross-validation, eliminating ad-hoc binning and windowing needed by current approaches. GenoGAM provides base-level and region-level significance testing for full factorial designs. Application to a ChIP-Seq dataset in yeast showed increased sensitivity over existing differential occupancy methods while controlling for type I error rate. By analyzing a set of DNA methylation data and illustrating an extension to a peak caller, we further demonstrate the potential of GenoGAM as a generic statistical modeling tool for genome-wide assays."}, {"title": "McClintock: An integrated pipeline for detecting transposable element insertions in whole genome shotgun sequencing data.", "url": "https://www.biorxiv.org/content/early/2017/02/14/095372", "tag": "Bioinformatics", "abstract": "Background: Transposable element (TE) insertions are among the most challenging type of variants to detect in genomic data because of their repetitive nature and complex mechanisms of replication. Nevertheless, the recent availability of large resequencing datasets has spurred the development of many new methods to detect TE insertions in whole genome shotgun sequences. These methods generate output in diverse formats and have a large number of software and data dependencies, making their comparative evaluation challenging for potential users. Results: Here we develop an integrated bioinformatics pipeline for the detection of TE insertions in whole genome shotgun data, called McClintock (https://github.com/bergmanlab/mcclintock), that automatically runs and generates standardized output for multiple TE detection methods. We demonstrate the utility of the McClintock system by performing comparative evaluation of six TE detection methods using simulated and real genome data from the model microbal eukaryote, Saccharomyces cerevisiae. We find substantial variation among McClintock component methods in their ability to detect non-reference insertions in the yeast genome, but show that non-reference TEs at nearly all biologically-realistic locations can be detected in simulated data by combining multiple methods that use split-read and read-pair evidence. In general, our results reveal that split-read methods detect fewer non-reference TE insertions than read-pair methods, but generally have much higher positional accuracy. Analysis of a large sample of real yeast genomes reveals that most, but not all, McClintock component methods can recover known aspects of TE biology in yeast such as the transpositional activity status of families, tRNA gene target preferences, and target site duplication structure, albeit with varying levels of positional accuracy. Conclusions: Our results suggest that no single TE detection method currently provides comprehensive detection of non-reference TEs, even in the context of a simplified model eukaryotic genome like S. cerevisiae. In spite of these limitations, the McClintock system provides a framework for testing, developing and integrating results from multiple TE detection methods to achieve this ultimate aim, as well as useful guidance for yeast researchers to select appropriate TE detection tools."}, {"title": "dRep: A tool for fast and accurate genome de-replication that enables tracking of microbial genotypes and improved genome recovery from metagenomes", "url": "https://www.biorxiv.org/content/early/2017/02/13/108142", "tag": "Bioinformatics", "abstract": "The number of microbial genomes sequenced each year is expanding rapidly, in part due to genome-resolved metagenomic studies that routinely recover hundreds of draft-quality genomes. Rapid algorithms have been developed to comprehensively compare large genome sets, but they are not accurate with draft-quality genomes. Here we present dRep, a program that sequentially applies a fast, inaccurate estimation of genome distance and a slow but accurate measure of average nucleotide identity to reduce the computational time for pair-wise genome set comparisons by orders of magnitude. We demonstrate its use in a study where we separately assembled each metagenome from time series datasets. Groups of essentially identical genomes were identified with dRep, and the best genome from each set was selected. This resulted in recovery of significantly more and higher-quality genomes compared to the set recovered using the typical co-assembly method. Documentation is available at http://drep.readthedocs.io/en/master/ and source code is available at https://github.com/MrOlm/drep ."}, {"title": "Topslam: Waddington Landscape Recovery for Single Cell Experiments", "url": "https://www.biorxiv.org/content/early/2017/02/13/057778", "tag": "Bioinformatics", "abstract": "We present an approach to estimate the nature of the Waddington (or epigenetic) landscape that underlies a population of individual cells. Through exploiting high resolution single cell transcription experiments we show that cells can be located on a landscape that reflects their differentiated nature. Our approach makes use of probabilistic non-linear dimensionality reduction that respects the topology of our estimated epigenetic landscape. In simulation studies and analyses of real data we show that the approach, known as topslam, outperforms previous attempts to understand the differentiation landscape. Hereby, the novelty of our approach lies in the correction of distances before extracting ordering information. This gives the advantage over other attempts, which have to correct for extracted time lines by post processing or additional data."}, {"title": "Functional determinants of protein assembly into homomeric complexes", "url": "https://www.biorxiv.org/content/early/2017/02/13/081745", "tag": "Bioinformatics", "abstract": "Approximately half of proteins with experimentally determined structures can interact with other copies of themselves and assemble into homomeric complexes, the overwhelming majority of which (>96%) are symmetric. Although homomerisation is often assumed to be functionally beneficial and the result of evolutionary selection, there has been little systematic analysis of the relationship between homomer structure and function. Here, utilizing the large numbers of structures and functional annotations now available, we have investigated how proteins that assemble into different types of homomers are associated with different biological functions. We observe that homomers from different symmetry groups are significantly enriched in distinct functions, and can often provide simple physical and geometrical explanations for these associations in regards to substrate recognition or physical environment. One of the strongest associations is the tendency for metabolic enzymes to form dihedral complexes, which we suggest is closely related to allosteric regulation. We provide a physical explanation for why allostery is related to dihedral complexes: it allows for efficient propagation of conformational changes across isologous (i.e. symmetric) interfaces. Overall we demonstrate a clear relationship between protein function and homomer symmetry that has important implications for understanding protein evolution, as well as for predicting protein function and quaternary structure."}, {"title": "HTSvis: A web app for exploratory data analysis and visualization of arrayed high-throughput screens", "url": "https://www.biorxiv.org/content/early/2017/02/11/107821", "tag": "Bioinformatics", "abstract": "The analysis and visualization of arrayed high-throughput screens (HTS), such as cell-based RNAi or small-molecule HTS experiments, requires specialized computational methods. Software packages such as the R/Bioconductor package cellHTS have been developed to support the analysis and are broadly used by the high-throughput screening community. However, exploratory data analysis and integration of screening results remains challenging due to the size of produced data tables in multi-channel experiments and the lack of user-friendly tools to integrate and visualize screening results. Here we present HTSvis, an R/Shiny open-source web application for interactive visualization and exploratory analysis of arrayed high-throughput data. Using a light-weight infrastructure suitable for desktop computers, HTSvis can be used to visualize raw data, perform quality control and interactively visualize screening results from single- to multi-channel measurements, such as image-based, screens. Input data can either be a result file obtained upon analysis with cellHTS or a generic table with raw or analyzed data from, e.g. a high-content microscopy screen. HTSvis can be downloaded from http://github.com/boutroslab/HTSvis."}, {"title": "PhyD3: a phylogenetic tree viewer with extended phyloXML support for functional genomics data visualization", "url": "https://www.biorxiv.org/content/early/2017/02/09/107276", "tag": "Bioinformatics", "abstract": "Motivation: Comparative and evolutionary studies utilise phylogenetic trees to analyse and visualise biological data. Recently, several web-based tools for the display, manipulation, and annotation of phylogenetic trees, such as iTOL and Evolview, have released updates to be compatible with the latest web technologies. While those web tools operate an open server access model with a multitude of registered users, a feature-rich open source solution using current web technologies is not available. Results: Here, we present an extension of the widely used PhyloXML standard with several new options to accommodate functional genomics or annotation datasets for advanced visualization. Furthermore, PhyD3 has been developed as a lightweight tool using the JavaScript library D3.js to achieve a state-of-the-art phylogenetic tree visualisation in the web browser, with support for advanced annotations. The current implementation is open source, easily adaptable and easy to implement in third parties' web sites. Availability: More information about PhyD3 itself, installation procedures, and implementation links are available at http://phyd3.bits.vib.be and at http://github.com/vibbits/phyd3/. Contact: bits@vib.be"}, {"title": "BCFtools/csq: Haplotype-aware variant consequences", "url": "https://www.biorxiv.org/content/early/2017/02/09/090811", "tag": "Bioinformatics", "abstract": "Motivation: Prediction of functional variant consequences is an important part of sequencing pipelines, allowing the categorization and prioritization of genetic variants for follow up analysis. However, current predictors analyze variants as isolated events, which can lead to incorrect predictions when adjacent variants alter the same codon, or when a frame-shifting indel is followed by a frame-restoring indel. Exploiting known haplotype information when making consequence predictions can resolve these issues. Results: BCFtools/csq is a fast program for haplotype-aware consequence calling which can take into account known phase. Consequence predictions are changed for 501 of 5019 compound variants found in the 81.7M variants in the 1000 Genomes Project data, with an average of 139 compound variants per haplotype. Predictions match existing tools when run in localized mode, but the program is an order of magnitude faster and requires an order of magnitude less memory. Availability: The program is freely available for commercial and non-commercial use in the BCFtools package which is available for download from http://samtools.github.io/bcftools"}, {"title": "Integrative analysis and refined design of CRISPR knockout screens", "url": "https://www.biorxiv.org/content/early/2017/02/09/106534", "tag": "Bioinformatics", "abstract": "Genome-wide CRISPR-Cas9 screen has been widely used to interrogate gene functions. However, the analysis remains challenging and rules to design better libraries beg further refinement. Here we present MAGeCK-NEST, which integrates protein-protein interaction (PPI), improves the inference accuracy when fewer guide-RNAs (sgRNAs) are available, and assesses screen qualities using information on PPI. MAGeCK-NEST also adopts a maximum-likelihood approach to remove sgRNA outliers, which are characterized with higher G-nucleotide counts, especially in regions distal from the PAM motif. Using MAGeCK-NEST, we found that choosing non-targeting sgRNAs as negative controls lead to strong bias, which can be mitigated by sgRNAs targeting the 'safe harbor' regions. Custom-designed screens confirmed our findings, and further revealed that 19nt sgRNAs consistently gave the best signal-to-noise separation. Collectively, our method enabled robust calling of CRISPR screen hits and motivated the design of an improved genome-wide CRISPR screen library."}, {"title": "Genome-wide analysis of differential translation and differential translational buffering using anota2seq", "url": "https://www.biorxiv.org/content/early/2017/02/08/106922", "tag": "Bioinformatics", "abstract": "Genome-wide analysis of mRNA translation (translatomics) can improve understanding of biological processes and pathological conditions including cancer. Techniques used to identify the effects of various stimuli on the translatomes such as polysome- and ribosome-profiling necessitate adjustment for changes in total mRNA levels to capture bona fide alterations in translational efficiency. In addition to changes in translational efficiency, which affect protein abundance, translation can also act as a buffering mechanism whereby protein levels remain constant despite changes in mRNA levels (translational buffering). Herein, we present anota2seq, an algorithm for analysis of translatomes, which is applicable to DNA-microarray and RNA sequencing data. In contrast to anota2seq, current methods for analysis of translational efficiency using RNA sequencing data as input identify false positives at high rates and/or fail to distinguish changes in translation efficiency from translational buffering when two conditions (e.g. stimulated and non-stimulated cells) are compared. Moreover, we identify data- acquisition specific thresholds, which are necessary during anota2seq analysis. Finally, we employ anota2seq to show that insulin affects gene expression at multiple levels in a largely mTOR-dependent manner. Thus, the universal anota2seq algorithm allows efficient interrogation of the translatomes, including distinction between the changes in translation efficiency and buffering."}, {"title": "Phenotype and gene ontology enrichment as guides for disease modeling in C. elegans", "url": "https://www.biorxiv.org/content/early/2017/02/07/106369", "tag": "Bioinformatics", "abstract": "Genome-wide experiments have the capacity to generate massive amounts of unbiased data about an organism. In order to interpret this data, dimensionality reduction techniques are required. One approach is to annotate genes using controlled languages and to test experimental datasets for term enrichment using probabilistic methods. Although gene, phenotype and anatomy ontologies exist for C. elegans, no unified software offers enrichment analyses of all the ontologies using the same methodology. Here, we present the WormBase Enrichment Suite, which offers users the ability to test all nematode ontologies simultaneously. We show that the WormBase Enrichment Suite provides valuable insight into different biological problems. Briefly, we show that phenotype enrichment analysis (PEA) can help researchers identify disease phenologs, phenotypes that are homologous across species, which can inform disease modeling in C. elegans. The WormBase Enrichment Suite analysis can also shed light on RNA-seq datasets by showing what molecular functions are enriched, which phenotypes these functions are implicated in and what tissues are overrepresented in the dataset. Finally, we explore the phenotype-anatomy relationship, showing that a small subset of highly specific tissues are disproportionately likely to cause an Egl phenotype, but inferring tissue expression from an Egl phenotype is limited to the largest tissues."}, {"title": "aBayesQR: A Bayesian method for reconstruction of viral populations characterized by low diversity", "url": "https://www.biorxiv.org/content/early/2017/02/06/103630", "tag": "Bioinformatics", "abstract": "RNA viruses replicate with high mutation rates, creating closely related viral populations. The heterogeneous virus populations, referred to as viral quasispecies, rapidly adapt to environmental changes thus adversely affecting efficiency of antiviral drugs and vaccines. Therefore, studying the underlying genetic heterogeneity of viral populations plays a significant role in the development of effective therapeutic treatments. Recent high-throughput sequencing technologies have provided invaluable opportunity for uncovering the structure of quasispecies populations (i.e., reconstruction of viral sequences and discovery of their relative frequencies). However, accurate reconstruction of viral quasispecies remains difficult due to limited read-lengths and presence of sequencing errors. The problem is particularly challenging when the strains in a population are highly similar, i.e., the sequences are characterized by low mutual genetic distances, and further exacerbated if some of those strains are relatively rare; this is the setting where state-of-the-art methods struggle. In this paper, we present a novel viral quasispecies reconstruction algorithm, aBayesQR, that employs a maximum-likelihood framework to infer individual sequences in a mixture from high-throughput sequencing data. The search for the most likely quasispecies is conducted on long contigs that our method constructs from the set of short reads via agglomerative hierarchical clustering; operating on contigs rather than short reads enables identification of close strains in a population and provides computational tractability of the Bayesian method. Results on both simulated and real HIV-1 data demonstrate that the proposed algorithm generally outperforms state-of-the-art methods; aBayesQR particularly stands out when reconstructing a set of closely related viral strains (e.g., quasispecies characterized by low diversity)."}, {"title": "Unsupervised idealization of ion channel recordings by Minimum Description Length: Application to human PIEZO1-channels", "url": "https://www.biorxiv.org/content/early/2017/02/06/106187", "tag": "Bioinformatics", "abstract": "Researchers can investigate the mechanistic and molecular basis of many physiological phenomena in cells by analyzing the fundamental properties of single ion channels. These analyses entail recording single channel currents and measuring current amplitudes and transition rates between conductance states. Since most electrophysiological recordings contain noise, the data analysis can proceed by idealizing the recordings to isolate the true currents from the noise. This de-noising can be accomplished with threshold crossing algorithms and Hidden Markov Models, but such procedures generally depend on inputs and supervision by the user, thus requiring some prior knowledge of underlying processes. Channels with unknown gating and/or functional sub-states and the presence in the recording of currents from uncorrelated background channels present substantial challenges to unsupervised analyses. Here we describe and characterize an idealization algorithm based on Rissanens Minimum Description Length (MDL) Principle. This method uses minimal assumptions and idealizes ion channel recordings without requiring a detailed user input or a priori assumptions about channel conductance and kinetics.. Furthermore, we demonstrate that correlation analysis of conductance steps can resolve properties of single ion channels in recordings contaminated by signals from multiple channels. We first validated our methods on simulated data defined with a range of different signal-to-noise levels, and then showed that our algorithm can recover channel currents and their substates from recordings with multiple channels, even under conditions of high noise. We then tested the MDL algorithm on real experimental data from human PIEZO1 channels and found that our method revealed the presence of substates with alternate conductances."}, {"title": "Non Hybrid Long Read Consensus Using Local De Bruijn Graph Assembly", "url": "https://www.biorxiv.org/content/early/2017/02/06/106252", "tag": "Bioinformatics", "abstract": "While second generation sequencing led to a vast increase in sequenced data, the shorter reads which came with it made assembly a much harder task and for some regions impossible with only short read data. This changed again with the advent of third generation long read sequencers. The length of the long reads allows a much better resolution of repetitive regions, their high error rate however is a major challenge. Using the data successfully requires to remove most of the sequencing errors. The first hybrid correction methods used low noise second generation data to correct third generation data, but this approach has issues when it is unclear where to place the short reads due to repeats and also because second generation sequencers fail to sequence some regions which third generation sequencers work on. Later non hybrid methods appeared. We present a new method for non hybrid long read error correction based on De Bruijn graph assembly of short windows of long reads with subsequent combination of these correct windows to corrected long reads. Our experiments show that this method yields a better correction than other state of the art non hybrid correction approaches."}, {"title": "Data-driven identification of potential Zika virus vectors", "url": "https://www.biorxiv.org/content/early/2017/02/06/077966", "tag": "Bioinformatics", "abstract": "Zika is an emerging virus whose rapid spread is of great public health concern. Knowledge about transmission remains incomplete, especially concerning potential transmission in geographic areas in which it has not yet been introduced. To identify unknown vectors of Zika, we developed a data-driven model linking vector species and the Zika virus via vector-virus trait combinations that confer a propensity toward associations in an ecological network connecting flaviviruses and their mosquito vectors. Our model predicts that thirty-five species may be able to transmit the virus, seven of which are found in the continental United States, including Culex quinquefasciatus and Cx. pipiens. We suggest that empirical studies prioritize these species to confirm predictions of vector competence, enabling the correct identification of populations at risk for transmission within the United States."}, {"title": "Morphologically Constrained and Data Informed Cell Segmentation of Budding Yeast", "url": "https://www.biorxiv.org/content/early/2017/02/06/105106", "tag": "Bioinformatics", "abstract": "Although high-content image cytometry is becoming increasingly routine, processing the large amount of data acquired during time-lapse experiments remains a challenge. The majority of approaches for automated single-cell segmentation focus on flat, uniform fields of view covered with a single layer of cells. In the increasingly popular microfluidic devices that trap individual cells for long term imaging, these conditions are not met. Consequently, most segmentation techniques perform poorly. Incorporating information about the microfluidic features, media flow and morphology of the cells can substantially improve performance, though it may constrain the generalizability of software."}, {"title": "Normalization of Mass Spectrometry Data (NOMAD)", "url": "https://www.biorxiv.org/content/early/2017/02/03/105783", "tag": "Bioinformatics", "abstract": "Motivation: iTRAQ reagent-based mass spectrometry (MS) is a commonly used technology for identification and quantification of proteins in biological samples. Such studies are often performed over multiple MS runs, potentially resulting in introduction of MS run bias that could affect downstream analysis. iTRAQ MS data have therefore commonly been normalized using a reference sample which is included in each MS run. We show, however, that such normalization does not efficiently remove systematic MS run bias. A linear model approach was previously proposed to improve on the reference normalization approach but does not computationally scale to larger data. Here we describe the NOMAD (normalization of mass spectrometry data) R package which implements a computationally efficient ANOVA normalization approach with protein assembly functionality. Results: NOMAD provides the same advantages as the linear regression solution but is more computationally efficient which allows superior scaling to larger sample sizes. Moreover, NOMAD efficiently removes bias which allows for valid across MS run comparisons. Availability: The NOMAD Bioconductor package: www.bioconductor .org"}, {"title": "An Evaluation of Machine-learning for Predicting Phenotype: studies in yeast and wheat", "url": "https://www.biorxiv.org/content/early/2017/02/03/105528", "tag": "Bioinformatics", "abstract": "In phenotype prediction the physical character of an organism is predicted from knowledge of its genotype and environment. Such studies are of the highest societal importance as they are now of central importance to medicine, crop-breeding, etc. We investigated two phenotype prediction problems: one simple and clean (yeast), the other complex and real-world (wheat). We compared standard machine learning methods (forward stepwise regression, ridge regression, lasso regression, random forest, gradient boosting machines (GBM), and support vector machines (SVM)) with two state-of-the-art classical statistical genetics methods (including genomic BLUP). Additionally, using the yeast data, we investigated how performance varied with the complexity of the biological mechanism, the amount of observational noise, the number of examples, the amount of missing data, population structure, genotype drift, and the use of different data representations. We found that for almost all phenotypes considered standard machine learning methods outperformed the two methods from classical statistical genetics. On the yeast problem the most successful method was GBM, followed by lasso regression, followed by the two statistical genetics methods and SVM; with greater mechanistic complexity GMB was best, whilst in simpler cases lasso was best. When applied to the wheat study the best two methods were SVM and BLUP. The most robust method in the presence of noise, missing data, etc. was random forests. The classical statistical genetics method of genomic BLUP was found to perform well on problems where there was population structure, which suggests one way to improve standard machine learning methods when population structure is present. We conclude that the application of machine learning methods to phenotype prediction problems holds great promise."}, {"title": "Meta-analysis of the variance ratio", "url": "https://www.biorxiv.org/content/early/2017/02/03/104695", "tag": "Bioinformatics", "abstract": "The most commonly used effect size when using meta-analysis to compare a measurement of interest in two different populations is the standardised mean difference. This is the mean difference of the measurement divided by the pooled standard deviation in the two groups. The standard deviation is usually supposed to be the same for both groups, although this assumption is often made without any particular evidence. It is possible, however, that the difference of the measurement in the two populations resides precisely in their standard deviations. This could be the case, for example, if a population of patients exhibited more \"abnormal\" values than a control population \u2013 both large and small \u2013 even if the mean values were the same. Fisher's test of equality of variance is designed to compare standard deviations. A variance ratio is a Fisher's ratio and Fisher distribution can be used to give confidence intervals to the estimate for one study. However, confidence interval for one study can be very wide if the study does not contain enough subjects. Here we present an approach to combine variance ratios of different studies in a meta-analytic way which produces more robust estimates under these circumstances."}, {"title": "BGDMdocker: an workflow base on Docker for analysis and visualization pan-genome and biosynthetic gene clusters of bacterial", "url": "https://www.biorxiv.org/content/early/2017/02/03/098392", "tag": "Bioinformatics", "abstract": "At present Docker technology is increasing level of attention throughout the bi-oinformatics community,but Docker technology implementation details have not been mastered by most biologist and applied in biological research, In order to apply this technology in the popularization and sufficient use plenty of free academic re-sources of bioinformatics tools images(community, official, and private) in Docker Hub Registry and other Docker sources base on Docker, In this article, we intro-duced full and accurate instance of an bioinformatics workflow base on Docker of analysis and visualization pan-genome and biosynthetic geneclusters of Bacterial, provides the solutions for the data mining bioinformatics big data from various free biology databases of Bacterial. we'll guide you step-by-step through the process from dockerfile building your own image and run an container fast creating an workflow."}, {"title": "Cancer Cell Line Profiler (CCLP): a webserver for the prediction of compound activity across the NCI60 panel", "url": "https://www.biorxiv.org/content/early/2017/02/02/105478", "tag": "Bioinformatics", "abstract": "Summary: CCLP (Cancer Cell Line Profiler) is a webserver for the prediction of compound activity across the NCI60 panel. CCLP uses a multi-task Random Forest model trained on 941,831 data-points that integrates structural information from 17,142 compounds and multi-omics data sets from 59 cancer cell lines. In addition, CCLP also implements conformal prediction to provide individual prediction errors at several confidence levels. CCLP computes compound descriptors for a set of input molecules and predicts their activity across the NCI60 panel. The output of running CCLP consists of one barplot per input compound displaying the predicted activities and errors across the NCI60 panel, as well as a text file reporting the predicted activities and errors in prediction. Availability: CCLP is freely available on the web at cclp.marseille.inserm.fr. Corresponding authors: isidrolauscher@gmail.com and pedro.ballester@inserm.fr"}, {"title": "Efficient Detection of Communities in Biological Bipartite Networks", "url": "https://www.biorxiv.org/content/early/2017/02/02/105197", "tag": "Bioinformatics", "abstract": "Methods to efficiently uncover and extract community structures are required in a number of biological applications where networked data and their interactions can be modeled as graphs, and observing tightly-knit groups of vertices (\"communities\") can offer insights into the structural and functional building blocks of the underlying network. Classical applications of community detection have largely focused on unipartite networks-i.e., graphs built out of a single type of objects. However, due to increased availability of biological data from various sources, there is now an increasing need for handling heterogeneous networks which are built out of multiple types of objects. In this paper, we address the problem of identifying communities from biological bipartite networks-i.e., networks where interactions are observed between two different types of objects (e.g., genes and diseases, drugs and protein complexes, plants and pollinators, hosts and pathogens). Toward detecting communities in such bipartite networks, we make the following contributions: i) (metric) we propose a variant of bipartite modularity; ii) (algorithms) we present an efficient algorithm called biLouvain that implements a set of heuristics toward fast and precise community detection in bipartite networks; and iii) (experiments) we present a thorough experimental evaluation of our algorithm including comparison to other state-of-the-art methods to identify communities in bipartite networks. Experimental results show that our biLouvain algorithm identifies communities that have a comparable or better quality (as measured by bipartite modularity) than existing methods, while significantly reducing the time-to-solution between one and four orders of magnitude."}, {"title": "Comparative analysis of protein abundance studies to quantify the Saccharomyces cerevisiae proteome", "url": "https://www.biorxiv.org/content/early/2017/02/02/104919", "tag": "Bioinformatics", "abstract": "Global gene expression and proteomics tools have allowed large-scale analyses of the transcriptome and proteome in eukaryotic cells. These tools have enabled studies of protein abundance changes that occur in cells under stress conditions, providing insight into regulatory programs required for cellular adaptation. While the proteome of yeast has been subjected to the most comprehensive analysis of any eukaryote, each of the existing datasets is separate and reported in different units. A comparison of all the available protein abundance data sets is key towards developing a complete understanding of the yeast proteome. We evaluated 19 quantitative proteomic analyses performed under normal and stress conditions and normalized and converted all measurements of protein abundance into absolute molecules per cell. Our analysis yields an estimate of the cellular abundance of 97% of the proteins in the yeast proteome, as well as an assessment of the variation in each abundance measurement. We evaluate the variance and sensitivity associated with different measurement methods. We find that C-terminal tagging of proteins, and the accompanying alterations to the 3' untranslated regions of the tagged genes, has little effect on protein abundance. Finally, our normalization of diverse datasets facilitates comparisons of protein abundance remodeling of the proteome during cellular stresses."}, {"title": "Accurate prediction of single-cell DNA methylation states using deep learning", "url": "https://www.biorxiv.org/content/early/2017/02/01/055715", "tag": "Bioinformatics", "abstract": "Recent technological advances have enabled assaying DNA methylation at single-cell resolution. Current protocols are limited by incomplete CpG coverage and hence methods to predict missing methylation states are critical to enable genome-wide analyses. Here, we report DeepCpG, a computational approach based on deep neural networks to predict DNA methylation states from DNA sequence and incomplete methylation profiles in single cells. We evaluated DeepCpG on single-cell methylation data from five cell types generated using alternative sequencing protocols, finding that DeepCpG yields substantially more accurate predictions than previous methods. Additionally, we show that the parameters of our model can be interpreted, thereby providing insights into the effect of sequence composition on methylation variability."}, {"title": "Multi-ethnic polygenic risk scores improve risk prediction in diverse populations.", "url": "https://www.biorxiv.org/content/early/2017/02/01/051458", "tag": "Bioinformatics", "abstract": "Methods for genetic risk prediction have been widely investigated in recent years. However, most available training data involves European samples, and it is currently unclear how to accurately predict disease risk in other populations. Previous studies have used either training data from European samples in large sample size or training data from the target population in small sample size, but not both. Here, we introduce a multi-ethnic polygenic risk score that combines training data from European samples and training data from the target population. We applied this approach to predict type 2 diabetes (T2D) in a Latino cohort using both publicly available European summary statistics in large sample size and Latino training data in small sample size. We attained a >70% relative improvement in prediction accuracy (from R2=0.027 to R2=0.047) compared to methods that use only one source of training data, consistent with large relative improvements in simulations. We observed a systematically lower load of T2D risk alleles in Latino individuals with more European ancestry, which could be explained by polygenic selection in ancestral European and/or Native American populations. Application of our approach to predict T2D in a South Asian UK Biobank cohort attained a >70% relative improvement in prediction accuracy, and application to predict height in an African UK Biobank cohort attained a 30% relative improvement. Our work reduces the gap in polygenic risk prediction accuracy between European and non-European target populations."}, {"title": "annotatr: Genomic regions in context", "url": "https://www.biorxiv.org/content/early/2017/02/01/039685", "tag": "Bioinformatics", "abstract": "Motivation: Analysis of next-generation sequencing data often results in a list of genomic regions. These may include differentially methylated CpGs/regions, transcription factor binding sites, interacting chromatin regions, or GWAS-associated SNPs, among others. A common analysis step is to annotate such genomic regions to genomic annotations (promoters, exons, enhancers, etc.). Existing tools are limited by a lack of annotation sources and flexible options, the time it takes to annotate regions, an artificial one-to-one region-to-annotation mapping, a lack of visualization options to easily summarize data, or some combination thereof. Results: We developed the annotatr Bioconductor package to flexibly and quickly summarize and plot annotations of genomic regions. The annotatr package reports all intersections of regions and annotations, giving a better understanding of the genomic context of the regions. A variety of graphics functions are implemented to easily plot numerical or categorical data associated with the regions across the annotations, and across annotation intersections, providing insight into how characteristics of the regions differ across the annotations. We demonstrate that annotatr is up to 27x faster than comparable R packages. Overall, annotatr enables a richer biological interpretation of experiments. Availability: http://bioconductor.org/packages/annotatr/ Contact: rcavalca@umich.edu"}, {"title": "propr: An R-package for Identifying Proportionally Abundant Features Using Compositional Data Analysis", "url": "https://www.biorxiv.org/content/early/2017/02/01/104935", "tag": "Bioinformatics", "abstract": "In the life sciences, many assays measure only the relative abundances of components for each sample. These data, called compositional data, require special handling in order to avoid misleading conclusions. For example, in the case of correlation, treating relative data like absolute data can lead to the discovery of falsely positive associations. Recently, researchers have proposed proportionality as a valid alternative to correlation for calculating pairwise association in relative data. Although the question of how to best measure proportionality remains open, we present here a computationally efficient R package that implements two proposed measures of proportionality. In an effort to advance the understanding and application of proportionality analysis, we review the mathematics behind proportionality, demonstrate its application to genomic data, and discuss some ongoing challenges in the analysis of relative abundance data."}, {"title": "Integrative Deep Models for Alternative Splicing", "url": "https://www.biorxiv.org/content/early/2017/01/31/104869", "tag": "Bioinformatics", "abstract": "Advancements in sequencing technologies have highlighted the role of alternative splicing (AS) in increasing transcriptome complexity. This role of AS, combined with the relation of aberrant splicing to malignant states, motivated two streams of research, experimental and computational. The first involves a myriad of techniques such as RNA-Seq and CLIP-Seq to identify splicing regulators and their putative targets. The second involves probabilistic models, also known as splicing codes, which infer regulatory mechanisms and predict splicing outcome directly from genomic sequence. To date, these models have utilized only expression data. In this work we address two related challenges: Can we improve on previous models for AS outcome prediction and can we integrate additional sources of data to improve predictions for AS regulatory factors. We perform a detailed comparison of two previous modeling approaches, Bayesian and Deep Neural networks, dissecting the confounding effects of datasets and target functions. We then develop a new target function for AS prediction and show that it significantly improves model accuracy. Next, we develop a modeling framework to incorporate CLIP-Seq, knockdown and over-expression experiments, which are inherently noisy and suffer from missing values. Using several datasets involving key splice factors in mouse brain, muscle and heart we demonstrate both the prediction improvements and biological insights offered by our new models. Overall, the framework we propose offers a scalable integrative solution to improve splicing code modeling as vast amounts of relevant genomic data become available. Availability: code and data will be available on Github following publication."}, {"title": "scRNASeqDB: a database for gene expression profiling in human single cell by RNA-seq", "url": "https://www.biorxiv.org/content/early/2017/01/31/104810", "tag": "Bioinformatics", "abstract": "Summary: Single-cell RNA sequencing (scRNA-Seq) is quickly becoming a powerful tool for high-throughput transcriptomic analysis of cell states and dynamics. Both the number and quality of scRNA-Seq datasets have dramatically increased recently. So far, there is no database that comprehensively collects and curates scRNA-Seq data in humans. Here, we present scRNASeqDB, a database that includes almost all the currently available human single cell transcriptome datasets (n= 36) covering 71 human cell lines or types and 8910 samples. Our online web interface allows user to query and visualize expression profiles of the gene(s) of interest, search for genes that are ex-pressed in different cell types or groups, or retrieve differentially expressed genes between cell types or groups. The scRNASeqDB is a valuable resource for single cell transcriptional studies. Availability: The database is available at https://bioinfo.uth.edu/scrnaseqdb/"}, {"title": "Bootstrap Distillation: Non-parametric Internal Validation of GWAS Results by Subgroup Resampling", "url": "https://www.biorxiv.org/content/early/2017/01/31/104497", "tag": "Bioinformatics", "abstract": "Genome-wide Association Studies are carried out on a large number of genetic variants in a large number of people, allowing the detection of small genetic effects that are associated with a trait. Natural variation of genotypes within populations means that any particular sample from the population may not represent the true genotype frequencies within that population. This may lead to the observation of marker-disease associations when no such association exists. A bootstrap population sub-sampling technique can reduce the influence of allele frequency variation in producing false-positive results for particular samplings of the population. In order to utilise bioinformatics in the service of a serious disease, this sub-sampling method has been applied to the Type 1 Diabetes dataset from the Wellcome Trust Case Control Consortium in order to evaluate its effectiveness. While previous literature on Type 1 Diabetes has identified some DNA variants that are associated with the disease, these variants are not informative for distinguishing between disease cases and controls using genetic information alone (AUC=0.7284). Population sub-sampling filtered out noise from genome-wide association data, and increased the chance of finding useful associative signals. Subsequent filtering based on marker linkage and testing of marker sets of different sizes produced a 5-SNP signature set of markers for Type 1 Diabetes. The group-specific markers used in this set, primarily from the HLA region on chromosome 6, are considerably more informative than previously known associated variants for predicting T1D phenotype from genetic data (AUC=0.8395). Given this predictive quality, the signature set may be useful alone as a screening test, and would be particularly useful in combination with other clinical cofactors for Type 1 Diabetes risk."}, {"title": "DepEst: an R package of important dependency estimators for gene network inference algorithms", "url": "https://www.biorxiv.org/content/early/2017/01/31/102871", "tag": "Bioinformatics", "abstract": "Gene network inference algorithms (GNI) are popular in bioinformatics area. In almost all GNI algorithms, the main process is to estimate the dependency (association) scores among the genes of the dataset. We present a bioinformatics tool, DepEst (Dependency Estimators), which is a powerful and flexible R package that includes 11 important dependency score estimators that can be used in almost all GNI Algorithms. DepEst is the first bioinformatics package that includes such a large number of estimators that runs both in parallel and serial. DepEst is currently available at https://github.com/altayg/Depest. Package access link, instructions, various workflows and example data sets are provided in the supplementary file."}, {"title": "Take ACTION to characterize the functional identity of single cells", "url": "https://www.biorxiv.org/content/early/2017/01/31/081273", "tag": "Bioinformatics", "abstract": "Single-cell transcriptomic data has the potential to radically redefine our view of cell type identity. Cells that were previously believed to be homogeneous are now clearly distinguishable in terms of their expression phenotype. Methods for automatically characterizing the functional identity of cells, and their associated properties, can be used to uncover processes involved in lineage differentiation as well as sub-typing cancer cells. They can also be used to suggest personalized therapies based on molecular signatures associated with pathology. We develop a new method, called ACTION, to infer the functional identity of cells from their transcriptional profile, classify them based on their principal functions, and reconstruct regulatory networks that are responsible for mediating their identity. Results from using ACTION to sub-type cancer cells in Melanoma patients reveal novel biomarkers along with their underlying regulatory networks."}, {"title": "Parallel power posterior analyses for fast computation of marginal likelihoods in phylogenetics", "url": "https://www.biorxiv.org/content/early/2017/01/30/104422", "tag": "Bioinformatics", "abstract": "Motivation: In Bayesian phylogenetic inference, marginal likelihoods are estimated using either the path-sampling or stepping-stone-sampling algorithms. Both algorithms are computationally demanding because they require a series of power posterior Markov chain Monte Carlo (MCMC) simulations. Here we introduce a general parallelization strategy that distributes the power posterior MCMC simulations and the likelihood computations over available CPUs. Our parallelization strategy can easily be applied to any statistical model despite our primary focus on molecular substitution models in this study. Results: Using two phylogenetic example datasets, we demonstrate that the runtime of the marginal likelihood estimation can be reduced significantly even if only two CPUs are available (an average performance increase of 1.96x). The performance increase is nearly linear with the number of available CPUs. We record a performance increase of 11.4x for cluster nodes with 16 CPUs, representing a substantial reduction to the runtime of marginal likelihood estimations. Hence, our parallelization strategy enables the estimation of marginal likelihoods to complete in a feasible amount of time which previously needed days, weeks or even months. Availability: The methods described here are implemented in our open-source software RevBayes which is available from http://www.RevBayes.com."}, {"title": "Nucleotide sequence and DNaseI sensitivity are predictive of 3D chromatin architecture", "url": "https://www.biorxiv.org/content/early/2017/01/30/103614", "tag": "Bioinformatics", "abstract": "Recently, Hi-C has been used to probe the 3D chromatin architecture of multiple organisms and cell types. The resulting collections of pairwise contacts across the genome have connected chromatin architecture to many cellular phenomena, including replication timing and gene regulation. However, high resolution (10 kb or finer) contact maps remain scarce due to the expense and time required for collection. A computational method for predicting pairwise contacts without the need to run a Hi-C experiment would be invaluable in understanding the role that 3D chromatin architecture plays in genome biology. We describe Rambutan, a deep convolutional neural network that predicts Hi-C contacts at 1 kb resolution using nucleotide sequence and DNaseI assay signal as inputs. Specifically, Rambutan identifies locus pairs that engage in high confidence contacts according to Fit-Hi-C, a previously described method for assigning statistical confidence estimates to Hi-C contacts. We first demonstrate Rambutan's performance across chromosomes at 1 kb resolution in the GM12878 cell line. Subsequently, we measure Rambutan's performance across six cell types. In this setting, the model achieves an area under the receiver operating characteristic curve between 0.7662 and 0.8246 and an area under the precision-recall curve between 0.3737 and 0.9008. We further demonstrate that the predicted contacts exhibit expected trends relative to histone modification ChIP-seq data, replication timing measurements, and annotations of functional elements such as promoters and enhancers. Finally, we predict Hi-C contacts for 53 human cell types and show that the predictions cluster by cellular function."}, {"title": "Methods for discovering genomic loci exhibiting complex patterns of differential methylation", "url": "https://www.biorxiv.org/content/early/2017/01/30/021436", "tag": "Bioinformatics", "abstract": "Cytosine methylation is widespread in most eukaryotic genomes and is known to play a substantial role in various regulatory pathways. Unmethylated cytosines may be converted to uracil through the addition of sodium bisulphite, allowing genome-wide quantification of cytosine methylation via high-throughput sequencing. The data thus acquired allows the discovery of methylation 'loci'; contiguous regions of methylation consistently methylated across biological replicates. The mapping of these loci allows for associations with other genomic factors to be identified, and for analyses of differential methylation to take place. The segmentSeq R package is extended to identify methylation loci from high-throughput sequencing data from multiple experimental conditions. A statistical model is then developed that accounts for biological replication and variable rates of non-conversion of cytosines in each sample to compute posterior likelihoods of methylation at each locus within an empirical Bayesian framework. The same model is used as a basis for analysis of differential methylation between multiple experimental conditions with the baySeq R package. We demonstrate this method through an analysis of data derived from Dicer-like mutants in Arabidopsis that reveals complex interactions between the different Dicer-like mutants and their methylation pathways. We also show in simulation studies that this approach can be significantly more powerful in the detection of differential methylation than existing methods."}, {"title": "Comparison of Principal Component Analysis and t-Stochastic Neighbor Embedding with Distance Metric Modifications for Single-cell RNA-sequencing Data Analysis", "url": "https://www.biorxiv.org/content/early/2017/01/29/102780", "tag": "Bioinformatics", "abstract": "Recent developments in technological tools such as next generation sequencing along with peaking interest in the study of single cells has enabled single-cell RNA-sequencing, in which whole transcriptomes are analyzed on a single-cell level. Studies, however, have been hindered by the ability to effectively analyze these single cell RNA-seq datasets, due to the high-dimensional nature and intrinsic noise in the data. While many techniques have been introduced to reduce dimensionality of such data for visualization and subpopulation identification, the utility to identify new cellular subtypes in a reliable and robust manner remains unclear. Here, we compare dimensionality reduction visualization methods including principle component analysis and t-stochastic neighbor embedding along with various distance metric modifications to visualize single-cell RNA-seq datasets, and assess their performance in identifying known cellular subtypes. Our results suggest that selecting variable genes prior to analysis on single-cell RNA-seq data is vital to yield reliable classification, and that when variable genes are used, the choice of distance metric modification does not particularly influence the quality of classification. Still, in order to take advantage of all the gene expression information, alternative methods must be used for a reliable classification."}, {"title": "Improving the performance of minimizers and winnowing schemes", "url": "https://www.biorxiv.org/content/early/2017/01/29/104075", "tag": "Bioinformatics", "abstract": "The minimizers scheme is a method for selecting k-mers from sequences. It is used in many bioinformatics software tools to bin comparable sequences or to sample a sequence in a deterministic fashion at approximately regular intervals, in order to reduce memory consumption and processing time. Although very useful, the minimizers selection procedure has undesirable behaviors (e.g., too many k-mers are selected when processing certain sequences). Some of these problems were already known to the authors of the minimizers technique, and the natural lexicographic ordering of k-mers used by minimizers was recognized as their origin. Many software tools using minimizers employ ad hoc variations of the lexicographic order to alleviate those issues. We provide an in-depth analysis of the effect of k-mer ordering on the performance of the minimizers technique. By using small universal hitting sets (a recently defined concept), we show how to significantly improve the performance of minimizers and avoid some of its worse behaviors. Based on these results, we encourage bioinformatics software developers to use an ordering based on a universal hitting set or, if not possible, a randomized ordering, rather than the lexicographic order. This analysis also settles negatively a conjecture (by Schleimer et al.) on the expected density of minimizers in a random sequence."}, {"title": "Genome-wide Search for Zelda-like Chromatin Signatures Identifies GAF as a Pioneer Factor in Early Fly Development", "url": "https://www.biorxiv.org/content/early/2017/01/29/104067", "tag": "Bioinformatics", "abstract": "The protein Zelda was shown to play a key role in early Drosophila development, binding thousands of promoters and enhancers prior to maternal-to-zygotic transition (MZT), and marking them for transcriptional activation. Recently, we showed that Zelda acts through specific chromatin patterns of histone modifications to mark developmental enhancers and active promoters. Intriguingly, some Zelda sites still maintain these chromatin patterns in Drosophila embryos lacking maternal Zelda protein. This suggests that additional Zelda-like pioneer factors may act in early fly embryos. We developed a computational method to analyze and refine the chromatin landscape surrounding early Zelda peaks, using a multi-channel spectral clustering. This allowed us to characterize their chromatin patterns through MZT (mitotic cycles 8-14). Specifically, we focused on H3K4me1, H3K4me3, H3K18ac, H3K27ac, and H3K27me3 and identified three different classes of chromatin signatures, matching \"promoters\", \"enhancers\" and \"transiently bound\" Zelda peaks. We then further scanned the genome using these chromatin patterns and identified additional loci - with no Zelda binding - that show similar chromatin patterns, resulting with hundreds of Zelda- independent putative enhancers. These regions were found to be enriched with GAGA factor (GAF, Trl), and are typically located near early developmental zygotic genes. Overall our analysis suggests that GAF, together with Zelda, plays an important role in activating the zygotic genome. As we show, our computational approach offers an efficient algorithm for characterizing chromatin signatures around some loci of interest, and allows a genome-wide identification of additional loci with similar chromatin patterns."}, {"title": "Deep Recurrent Neural Network for Protein Function Prediction from Sequence", "url": "https://www.biorxiv.org/content/early/2017/01/28/103994", "tag": "Bioinformatics", "abstract": "As high-throughput biological sequencing becomes faster and cheaper, the need to extract useful information from sequencing becomes ever more paramount, often limited by low-throughput experimental characterizations. For proteins, accurate prediction of their functions directly from their primary amino-acid sequences has been a long standing challenge. Here, machine learning using artificial recurrent neural networks (RNN) was applied towards classification of protein function directly from primary sequence without sequence alignment, heuristic scoring or feature engineering. The RNN models containing long-short-term-memory (LSTM) units trained on public, annotated datasets from UniProt achieved high performance for in-class prediction of four important protein functions tested, particularly compared to other machine learning algorithms using sequence-derived protein features. RNN models were used also for out-of-class predictions of phylogenetically distinct protein families with similar functions, including proteins of the CRISPR-associated nuclease, ferritin-like iron storage and cytochrome P450 families. Applying the trained RNN models on the partially unannotated UniRef100 database predicted not only candidates validated by existing annotations but also currently unannotated sequences. Some RNN predictions for the ferritin-like iron sequestering function were experimentally validated, even though their sequences differ significantly from known, characterized proteins and from each other and cannot be easily predicted using popular bioinformatics methods. As sequencing and experimental characterization data increases rapidly, the machine-learning approach based on RNN could be useful for discovery and prediction of homologues for a wide range of protein functions."}, {"title": "Understanding sequence conservation with deep learning", "url": "https://www.biorxiv.org/content/early/2017/01/28/103929", "tag": "Bioinformatics", "abstract": "Motivation: Comparing the human genome to the genomes of closely related mammalian species has been a powerful tool for discovering functional elements in the human genome. Millions of conserved elements have been discovered. However, understanding the functional roles of these elements still remain a challenge, especially in noncoding regions. In particular, it is still unclear why these elements are evolutionarily conserved and what kind of functional elements are encoded within these sequences. Results: We present a deep learning framework, called DeepCons, to uncover potential functional elements within conserved sequences. DeepCons is a convolutional neural net (CNN) that receives a short segment of DNA sequence as input and outputs the probability of the sequence of being evolutionary conserved. DeepCons utilizes hundreds of convolution kernels to detect features within DNA sequences, and automatically learns these kernels after training the CNN model using 887,577 conserved elements and a similar number of nonconserved elements in the human genome. On a balanced test dataset, DeepCons can achieve an accuracy of 75% in determining whether a sequence element is conserved or not, and the area under the ROC curve of 0.83, based on information from the human genome alone. We further investigate the properties of the learned kernels. Some kernels are directly related to well-known regulatory motifs corresponding to transcription factors. Many kernels show positional biases relative to transcriptional start sites or transcription end sites. But most of discovered kernels do not correspond to any known functional element, suggesting that they might represent unknown categories of functional elements. We also utilize DeepCons to annotate how changes at each individual nucleotide might impact the conservation properties of the surrounding sequences. Availability: The source code of DeepCons and all the learned convolution kernels in motif format is publicly available online at https://github.com/uci-cbcl/DeepCons."}, {"title": "Review: Population Structure in Genetic Studies: Confounding Factors and Mixed Models", "url": "https://www.biorxiv.org/content/early/2017/01/28/092106", "tag": "Bioinformatics", "abstract": "A genome-wide association study (GWAS) seeks to identify genetic variants that contribute to the development and progression of a specific disease. Over the past 10 years, new approaches using mixed models have emerged to mitigate the deleterious effects of population structure and relatedness in association studies. However, developing GWAS techniques to effectively test for association while correcting for population structure is a computational and statistical challenge. Our review motivates the problem of population structure in association studies using laboratory mouse strains and how it can cause false positives associations. We then motivate mixed models in the context of unmodeled factors."}, {"title": "Escape Excel: a tool for preventing gene symbol and accession conversion errors", "url": "https://www.biorxiv.org/content/early/2017/01/27/103820", "tag": "Bioinformatics", "abstract": "Background: Microsoft Excel automatically converts certain gene symbols, database accessions, and other alphanumeric text and numbers into dates, scientific notation, and other numerical representations, which may lead to subsequent, irreversible corruption of the imported text. A recent survey of popular genomic literature estimates that one-fifth of all papers with supplementary data containing gene lists in Excel format suffer from this issue. Results: Here, we present an open-source tool, Escape Excel, which prevents these erroneous conversions by generating an escaped text file that can be safely imported into Excel. Escape Excel is available in the Galaxy web environment and can be installed through the Galaxy ToolShed. Escape Excel is also available as a stand-alone, command line Perl script on GitHub (http://www.github.com/pstew/escape_excel). A Galaxy test server implementation is accessible at http://apostl.moffitt.org. Conclusions: Escape Excel detects and escapes a wide variety of problematic text strings so that they are not erroneously converted into other representations upon importation into Excel. Examples of problematic strings include date-like strings, time-like strings, leading zeroes in front of numbers, and long numeric and alpha-numeric identifiers that should not be automatically converted into scientific notation. It is hoped that greater awareness of these potential data-corruption issues, together with diligent escaping of text files prior to importation into Excel, will help to reduce the amount of Excel-corrupted data in scientific analyses and publications."}, {"title": "Health Information Needs and Health Seeking Behavior during the 2014-2016 Ebola Outbreak: A Twitter Content Analysis", "url": "https://www.biorxiv.org/content/early/2017/01/27/103515", "tag": "Bioinformatics", "abstract": "Introduction. For effective public communication during major disease outbreaks like the 2014-2016 Ebola epidemic, health information needs of the population must be adequately assessed. Through content analysis of social media data, like tweets, public health information needs can be effectively assessed and in turn provide appropriate health information to effectively address such needs. The aim of the current study was to assess health information needs about Ebola, at distinct epidemic time points, through longitudinal tracking. Methods. Natural language processing was applied to explore public response to Ebola over time from the beginning of the outbreak (July 2014) to six month post outbreak (March 2015). A total 155,647 tweets (unique 68,736, retweet 86,911) mentioning Ebola were analyzed and visualized with infographics. Results. Public fear, frustration, and health information seeking regarding Ebola-related global priorities were observed across time. Our longitudinal content analysis revealed that due to ongoing health information deficiencies, resulting in fear and frustration, social media was at times an impediment and not a vehicle to support health information needs. Discussion. Content analysis of tweets effectively assessed Ebola information needs. Our study also demonstrates the use of Twitter as a method for capturing real-time data to assess ongoing information needs, fear, and frustration over time."}, {"title": "Denoising genome-wide histone ChIP-seq with convolutional neural networks", "url": "https://www.biorxiv.org/content/early/2017/01/27/052118", "tag": "Bioinformatics", "abstract": "Motivation: Chromatin immunoprecipitation sequencing (ChIP-seq) experiments are commonly used to obtain genome-wide profiles of histone modifications associated with different types of functional genomic elements. However, the quality of histone ChIP-seq data is affected by a myriad of experimental parameters such as the amount of input DNA, antibody specificity, ChIP enrichment, and sequencing depth. Making accurate inferences from chromatin profiling experiments that involve diverse experimental parameters is challenging. Results: We introduce a convolutional denoising algorithm, Coda, that uses convolutional neural networks to learn a mapping from suboptimal to high-quality histone ChIP-seq data. This overcomes various sources of noise and variability, substantially enhancing and recovering signal when applied to low-quality chromatin profiling datasets across individuals, cell types, and species. Our method has the potential to improve data quality at reduced costs. More broadly, this approach -- using a high-dimensional discriminative model to encode a generative noise process -- is generally applicable to other biological domains where it is easy to generate noisy data but difficult to analytically characterize the noise or underlying data distribution. Availability: https://github.com/kundajelab/coda"}, {"title": "Reverse-complement parameter sharing improves deep learning models for genomics", "url": "https://www.biorxiv.org/content/early/2017/01/27/103663", "tag": "Bioinformatics", "abstract": "Deep learning approaches that have produced breakthrough predictive models in computer vision, speech recognition and machine translation are now being successfully applied to problems in regulatory genomics. However, deep learning architectures used thus far in genomics are often directly ported from computer vision and natural language processing applications with few, if any, domain-specific modifications. In double-stranded DNA, the same pattern may appear identically on one strand and its reverse complement due to complementary base pairing. Here, we show that conventional deep learning models that do not explicitly model this property can produce substantially different predictions on forward and reverse-complement versions of the same DNA sequence. We present four new convolutional neural network layers that leverage the reverse-complement property of genomic DNA sequence by sharing parameters between forward and reverse-complement representations in the model. These layers guarantee that forward and reverse-complement sequences produce identical predictions within numerical precision. Using experiments on simulated and in vivo transcription factor binding data, we show that our proposed architectures lead to improved performance, faster learning and cleaner internal representations compared to conventional architectures trained on the same data."}, {"title": "LRSim: a Linked Reads Simulator generating insights for better genome partitioning", "url": "https://www.biorxiv.org/content/early/2017/01/26/103549", "tag": "Bioinformatics", "abstract": "Motivation: Linked reads are a form of DNA sequencing commercialized by 10X Genomics that uses highly multiplexed barcoding within microdroplets to tag short reads to progenitor molecules. The linked reads, spanning tens to hundreds of kilobases, offer an alternative to long-read sequencing for de novo assembly, haplotype phasing and other applications. However, there is no available simulator, making it difficult to measure their capability or develop new informatics tools. Results: Our analysis of 13 real linked read datasets revealed their characteristics of barcodes, molecules and partitions. Based on this, we introduce LRSim that simulates linked reads by emulating the library preparation and sequencing process with fine control of 1) the number of simulated variants; 2) the linked-read characteristics; and 3) the Illumina reads profile. We conclude from the phasing and genome assembly of multiple datasets, recommendations on coverage, fragment length, and partitioning when sequencing human and non-human genome. Availability: LRSIM is under MIT license and is freely available at https://github.com/aquaskyline/LRSIM"}, {"title": "PGS: a dynamic and automated population-based genome structure software", "url": "https://www.biorxiv.org/content/early/2017/01/26/103358", "tag": "Bioinformatics", "abstract": "Hi-C technologies are widely used to investigate the spatial organization of genomes. However, the structural variability of the genome is a great challenge to interpreting ensemble-averaged Hi-C data, particularly for long-range/interchromosomal interactions. We pioneered a probabilistic approach for generating a population of distinct diploid 3D genome structures consistent with all the chromatin-chromatin interaction probabilities from Hi-C experiments. Each structure in the population is a physical model of the genome in 3D. Analysis of these models yields new insights into the causes and the functional properties of the genome's organization in space and time. We provide a user-friendly software package, called PGS, that runs on local machines and high-performance computing platforms. PGS takes a genome-wide Hi-C contact frequency matrix and produces an ensemble of 3D genome structures entirely consistent with the input. The software automatically generates an analysis report, and also provides tools to extract and analyze the 3D coordinates of specific domains."}, {"title": "CIDR: Ultrafast and accurate clustering through imputation for single-cell RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2017/01/26/068775", "tag": "Bioinformatics", "abstract": "Most existing dimensionality reduction and clustering packages for single-cell RNA-Seq (scRNA-Seq) data deal with dropouts by heavy modelling and computational machinery. Here we introduce CIDR (Clustering through Imputation and Dimensionality Reduction), an ultrafast algorithm which uses a novel yet very simple 'implicit imputation' approach to alleviate the impact of dropouts in scRNA-Seq data in a principled manner. Using a range of simulated and real data, we have shown that CIDR improves the standard principal component analysis and outperforms the state-of-the-art methods, namely t-SNE, ZIFA and RaceID, in terms of clustering accuracy. CIDR typically completes within seconds for processing a data set of hundreds of cells, and minutes for a data set of thousands of cells. CIDR can be downloaded at https://github.org/VCCRI/CIDR."}, {"title": "New Techniques for Ancient Proteins: Direct Coupling Analysis Applied on Proteins involved in Iron Sulfur Cluster Biogenesis", "url": "https://www.biorxiv.org/content/early/2017/01/25/103283", "tag": "Bioinformatics", "abstract": "Direct coupling analysis (DCA) is a powerful tool based on protein evolution and introduced to predict protein fold and protein-protein interactions which has been applied also to the prediction of entire interactomes. We have used DCA to analyse three proteins of the iron-sulfur biogenesis machine, an essential metabolic pathway conserved in all organisms. We show that, although based on a relatively small number of sequences due to its distribution in genomes, we can correctly recapitulate all the features of the fold of the CyaY/frataxin family, a protein involved in the human disease Friedreich's ataxia. This result gave us confidence in the use of this tool. Application of DCA to the iron-sulfur cluster scaffold protein IscU, which has been suggested to function both as an ordered and a disordered form, allows us to clearly distinguish evolutionary traces of the structured species, suggesting that, if present in the cell, the disordered form has not left any evolutionary imprinting. We observe instead, for the first time, direct indications of how the protein can dimerize head-to-head and bind 4Fe4S clusters. Analysis of the alternative scaffold protein IscA provides strong support to a coordination of the cluster mediated by a dimeric rather than a tetrameric form as previously suggested. Our analysis also suggests the presence in solution of a mixture of monomeric and dimeric species and guide us to the prevalent one. Finally, we used DCA to analyse protein-protein interactions between some of these proteins and discuss the potentialities and the limitations of the method."}, {"title": "UMI-Reducer: Collapsing duplicate sequencing reads via Unique Molecular Identifiers", "url": "https://www.biorxiv.org/content/early/2017/01/25/103267", "tag": "Bioinformatics", "abstract": "Every sequencing library contains duplicate reads. While many duplicates arise during polymerase chain reaction (PCR), some duplicates derive from multiple identical fragments of mRNA present in the original lysate (termed \"biological duplicates\"). Unique Molecular Identifiers (UMIs) are random oligonucleotide sequences that allow differentiation between technical and biological duplicates. Here we report the development of UMI-Reducer, a new computational tool for processing and differentiating PCR duplicates from biological duplicates. UMI-Reducer uses UMIs and the mapping position of the read to identify and collapse reads that are technical duplicates. Remaining true biological reads are further used for bias-free estimate of mRNA abundance in the original lysate. This strategy is of particular use for libraries made from low amounts of starting material, which typically require additional cycles of PCR and therefore are most prone to PCR duplicate bias."}, {"title": "Interactive analysis of Long-read RNA isoforms with Iso-Seq Browser", "url": "https://www.biorxiv.org/content/early/2017/01/25/102905", "tag": "Bioinformatics", "abstract": "Background: Long-read RNA sequencing, such as Pacific Biosciences Iso-Seq method, enables generation of sequencing reads that are 10 kilobases or even longer. These reads are ideal for discovering splice junctions and resolving full-length gene transcripts without time-consuming and error-prone techniques such as transcript assembly and junction inference. Results: Iso-Seq Browser is a Web-based visual analytics tool for long-read RNA sequencing data produced by Pacific Biosciences isoform sequencing (Iso-Seq) techniques. Key features of the Iso-Seq Browser are: 1) an exon-only web-based interface with zooming and exon highlighting for exploring reference gene transcripts and novel gene isoforms, 2) automated grouping of transcripts and isoforms by similarity, 3) many customization features for data exploration and creating publication ready figures, and 4) exporting selected isoforms into fasta files for further analysis. Iso-Seq Browser is written in Python using several scientific libraries. The application and analyses described in this paper are freely available to both academic and commercial users at https://github.com/goeckslab/isoseq-browser Conclusions: Iso-Seq Browser enables interactive genome-wide visual analysis of long RNA sequence reads. Through visualization, highlighting, clustering, and filtering of gene isoforms, ISB makes it simple to identify novel isoforms and novel isoform features such as exons, introns and untranslated regions."}, {"title": "DATS: the data tag suite to enable discoverability of datasets", "url": "https://www.biorxiv.org/content/early/2017/01/25/103143", "tag": "Bioinformatics", "abstract": "Today's science increasingly requires effective ways to find and access existing datasets that are distributed across a range of repositories. For researchers in the life sciences, discoverability of datasets may soon become as essential as identifying the latest publications via PubMed. Through an international collaborative effort funded by the National Institutes of Health (NIH)'s Big Data to Knowledge (BD2K) initiative, we have designed and implemented the DAta Tag Suite (DATS) model to support the DataMed data discovery index. DataMed's goal is to be for data what PubMed has been for the scientific literature. Akin to the Journal Article Tag Suite (JATS) used in PubMed, the DATS model enables submission of metadata on datasets to DataMed. DATS has a core set of elements, which are generic and applicable to any type of datasets, and an extended set that can accommodate more specialized data types. DATS is a platform-independent model also available as a Schema.org annotated serialization to be used beyond DataMed, for example, in projects like DataCite."}, {"title": "kpLogo: positional k-mer analysis reveals hidden specificity in biological sequences", "url": "https://www.biorxiv.org/content/early/2017/01/25/102921", "tag": "Bioinformatics", "abstract": "Motifs of only 1-4 letters can play important roles when present at key locations within macromolecules. Because existing motif-discovery tools typically miss these position-specific short motifs, we developed kpLogo, a probability-based logo tool for integrated detection and visualization of position-specific ultra-short motifs. kpLogo also overcomes the limitations of conventional motif-visualization tools in handling positional interdependencies and utilizing ranked or weighted sequences increasingly available from high-throughput assays."}, {"title": "Context-Aware Prediction of Pathogenicity of Missense Mutations Involved in Human Disease", "url": "https://www.biorxiv.org/content/early/2017/01/25/103051", "tag": "Bioinformatics", "abstract": "Amino-acid substitutions are implicated in a wide range of human diseases, many of which are lethal. Distinguishing such mutations from polymorphisms without significant effect on human health is a necessary step in understanding the etiology of such diseases. Computational methods can be used to select interesting mutations within a larger set, to corroborate experimental findings and to elucidate the cause of the deleterious effect. In this work, we show that taking into account the sequence context in which the mutation appears allows to improve the predictive and explanatory power of such methods. We present an unsupervised approach based on the direct-coupling analysis of homologous proteins. We show its capability to quantify mutations where methods without context dependence fail. We highlight cases where the context dependence is interpretable as functional or structural constraints and show that our simple and unsupervised method has an accuracy similar to state-of-the-art methods, including supervised ones."}, {"title": "Implicating candidate genes at GWAS signals by leveraging topologically associating domains", "url": "https://www.biorxiv.org/content/early/2017/01/25/087718", "tag": "Bioinformatics", "abstract": "Genome wide association studies (GWAS) have contributed significantly to the understanding of complex disease genetics. However, GWAS only report associated signals and do not necessarily identify culprit genes. As most signals occur in non-coding regions of the genome, it is often challenging to assign genomic variants to the underlying causal mechanism(s). Topologically associating domains (TADs) are primarily cell-type independent genomic regions that define interactome boundaries and can aid in the designation of limits within which an association most likely impacts gene function. We describe and validate a computational method that uses the genic content of TADs to discover candidate genes. Our method, called \"TAD_Pathways\", performs a Gene Ontology (GO) analysis over genes that reside within TAD boundaries corresponding to GWAS signals for a given trait or disease. We applied our pipeline to the GWAS catalog entries associated with bone mineral density (BMD), identifying 'Skeletal System Development' (Benjamini-Hochberg adjusted p=1.02x10-5) as the top ranked pathway. In many cases, our method implicated a gene other than the nearest gene. Our molecular experiments describe a novel example: ACP2, implicated at the canonical 'ARHGAP1' locus. We found ACP2 to be an important regulator of osteoblast metabolism, whereas ARHGAP1 was not supported. Our results via the example of BMD demonstrate how basic principles of three-dimensional genome organization can define biologically informed association windows."}, {"title": "Designing the database for microarray experiments metadata", "url": "https://www.biorxiv.org/content/early/2017/01/25/101956", "tag": "Bioinformatics", "abstract": "Advancements in both computer science and biotechnology opened way to unprecedented amount and variety of gene expression studies raw data in the open access. It is sometimes worth to rearrange and unite data from several similar gene expression studies into new case-control groups to test new hypothesis using available data. Unfortunately, most popular gene expression databases such as GEO and ArrayExpress were not designed to allow such cross-study procedures. In order to locate comparable samples in different studies numerous steps are required including gathering additional sample metadata and its standardization. Specialized databases are developed by investigators in their own fields of interest to reuse the processed data and create different case-control groups and test multiple hypothesis. Here we present detailed description of the specialized database creation along with its use case which is 32 gene expression cDNA microarray datasets on human placenta under conditions of pre-eclampsia containing expression data on more than 1000 biological samples. Samples contain sufficient metadata for them to be merged into relevant cross-experiment case-control groups for further integrative analysis."}, {"title": "The circadian dynamics of small nucleolar RNA", "url": "https://www.biorxiv.org/content/early/2017/01/23/102533", "tag": "Bioinformatics", "abstract": "The circadian regulation of gene expression allows plants and animals to anticipate predictable environmental changes. While the influence of the circadian clock has recently been shown to extend to ribosome biogenesis, the dynamics and regulation of the many small nucleolar RNA that are required in pre-ribosomal RNA folding and modification are unknown. Using a novel computational method, we show that 18S and 28S pre-rRNA are subject to circadian regulation in a nuclear RNA sequencing time course. A population of snoRNA with circadian expression is identified that is functionally associated with rRNA modification. More generally, we find the abundance of snoRNA known to modify 18S and 28S to be inversely correlated with the abundance of their target. Cyclic patterns in the expression of a number of snoRNA indicate a coordination with rRNA maturation, potentially through an upregulation in their biogenesis, or their release from mature rRNA at the end of the previous cycle of rRNA maturation, in antiphase with the diurnal peak in pre-rRNA. Few cyclic snoRNA have cyclic host genes, indicating the action of regulatory mechanisms in addition to transcriptional activation of the host gene. For highly-expressed independently transcribed snoRNA, we find a characteristic RNA polymerase II and H3K4me3 signature that correlates with mean snoRNA expression over the day."}, {"title": "De novo assembly of viral quasispecies using overlap graphs", "url": "https://www.biorxiv.org/content/early/2017/01/21/080341", "tag": "Bioinformatics", "abstract": "A viral quasispecies, the ensemble of viral strains populating an infected person, can be highly diverse. For optimal assessment of virulence, pathogenesis and therapy selection, determining the haplotypes of the individual strains can play a key role. As many viruses are subject to high mutation and recombination rates, high-quality reference genomes are often not available at the time of a new disease outbreak. We present SAVAGE, a computational tool for reconstructing individual haplotypes of intra-host virus strains without the need for a high-quality reference genome. SAVAGE makes use of either FM-index based data structures or ad-hoc consensus reference sequence for constructing overlap graphs from patient sample data. In this overlap graph, nodes represent reads and/or contigs, while edges reflect that two reads/contigs, based on sound statistical considerations, represent identical haplotypic sequence. Following an iterative scheme, a new overlap assembly algorithm that is based on the enumeration of statistically well-calibrated groups of reads/contigs then efficiently reconstructs the individual haplotypes from this overlap graph. In benchmark experiments on simulated and on real deep coverage data, SAVAGE drastically outperforms generic de novo assemblers as well as the only specialized de novo viral quasispecies assembler available so far. When run on ad-hoc consensus reference sequence, SAVAGE performs very favorably in comparison with state-of-the-art reference genome guided tools. We also apply SAVAGE on two deep coverage samples of patients infected by the Zika and the hepatitis C virus, respectively, which sheds light on the genetic structures of the respective viral quasispecies."}, {"title": "Creation of gene expression database on preeclampsia-affected human placenta", "url": "https://www.biorxiv.org/content/early/2017/01/21/102012", "tag": "Bioinformatics", "abstract": "Publication of gene expression raw data in open access at online resources like NCBI or ArrayExpress made it possible to use these data for cross-experiment integrative analysis and make new insights into biological phenomena. However, most popular of the present online resources are meant to be archives rather than ready for immediate access and interpretation databases. Data uploaded by independent contributors is not standardized and sometimes incomplete and needs further processing before it is ready for the analysis. Hence, the need for a specialized database appears. Given in this article is the description of the database that was created after processing a collection of 33 relevant datasets on pre-eclampsia-affected human placenta. Data processing includes the choice of relevant experiments from ArrayExpress database, the experiment sample attributes standardization according to MeSH term dictionary and Experimental Factor Ontology and the completion of missing data using information from the corresponding articles and authors. A database of more than 1000 samples contains sufficient sample-wise metadata for them to be arranged into relevant case-control groups. Metadata includes information on biological specimen, donor's diagnosis, gestational age, mode of delivery etc. The average size of these groups will be higher than it is in separate experiments. This will reduce experiment bias and enhance statistical accuracy of the subsequent analysis such as search for differentially expressed genes or inferring gene networks. The article concludes with the guidelines for the microarray experiment metadata uploading for future contributors."}, {"title": "aRNApipe: A balanced, efficient and distributed pipeline for processing RNA-seq data in high performance computing environments", "url": "https://www.biorxiv.org/content/early/2017/01/20/060277", "tag": "Bioinformatics", "abstract": "The wide range of RNA-seq applications and their high computational needs require the development of pipelines orchestrating the entire workflow and optimizing usage of available computational resources. We present aRNApipe, a project-oriented pipeline for processing of RNA-seq data in high performance cluster environments. aRNApipe is highly modular and can be easily migrated to any high performance computing (HPC) environment. The current applications included in aRNApipe combine the essential RNA-seq primary analyses, including quality control metrics, transcript align-ment, count generation, transcript fusion identification, alternative splicing, and sequence variant calling. aRNApipe is project-oriented and dynamic so users can easily update analyses to include or exclude samples or enable additional processing modules. Workflow parameters are easily set using a single configuration file that provides centralized tracking of all analytical processes. Finally, aRNApipe incorporates interactive web reports for sample tracking and a tool for managing the ge-nome assemblies available to perform an analysis. Availability and documentation: https://github.com/HudsonAlpha/aRNAPipe; DOI: 10.5281/zenodo.202950"}, {"title": "A comparison of reference-based algorithms for correcting cell-type heterogeneity in Epigenome-Wide Association Studies", "url": "https://www.biorxiv.org/content/early/2017/01/19/101709", "tag": "Bioinformatics", "abstract": "Background: Intra-sample cellular heterogeneity presents numerous challenges to the identification of biomarkers in large Epigenome-Wide Association Studies (EWAS). While a number of reference-based deconvolution algorithms have emerged, their potential remains underexplored and a comparative evaluation of these algorithms beyond tissues such as blood is still lacking. Results: Here we present a novel framework for reference-based inference, which leverages cell-type specific DNAse Hypersensitive Site (DHS) information from the NIH Epigenomics Roadmap to construct an improved reference DNA methylation database. We show that this leads to a marginal but statistically significant improvement of cell-count estimates in whole blood as well as in mixtures involving epithelial cell-types. Using this framework we compare a widely used state-of-the-art reference-based algorithm (called constrained projection) to two non-constrained approaches including CIBERSORT and a method based on robust partial correlations. We conclude that the widely-used constrained projection technique may not always be optimal. Instead, we find that the method based on robust partial correlations is generally more robust across a range of different tissue types and for realistic noise levels. We call the combined algorithm which uses DHS data and robust partial correlations for inference, EpiDISH (Epigenetic Dissection of Intra-Sample Heterogeneity). Finally, we demonstrate the added value of EpiDISH in an EWAS of smoking. Conclusions: Estimating cell-type fractions and subsequent inference in EWAS may benefit from the use of non-constrained reference-based cell-type deconvolution methods."}, {"title": "Modeling Hormonal Control of Cambium Proliferation", "url": "https://www.biorxiv.org/content/early/2017/01/19/101550", "tag": "Bioinformatics", "abstract": "Rise of atmospheric CO2 is one of the main causes of global warming. Catastrophic climate change can be avoided by reducing emissions and increasing sequestration of CO2. Trees are known to sequester CO2 during photosynthesis, and then store it as wood biomass. Thus, breeding of trees with higher wood yield would mitigate global warming as well as augment production of renewable construction materials, energy, and industrial feedstock. Wood is made of cellulose-rich xylem cells produced through proliferation of a specialized stem cell niche called cambium. Importance of cambium in xylem cells production makes it an ideal target for the tree breeding programs; however our knowledge about control of cambium proliferation remains limited. The morphology and regulation of cambium differs from stem cell niches that control axial growth. For this reason, translating the knowledge about axial growth to radial growth has limited use. Furthermore, genetic approaches cannot be easily applied because overlaying tissues conceal cambium from direct observation and complicate identification of mutants. To overcome the paucity of experimental tools in cambium biology, we constructed a Boolean network CARENET (CAmbium Regulation gene NETwork) for modelling cambium activity, which includes the key transcription factors WOX4 and HD-ZIP III as well as their potential regulators. Our simulations revealed that: (1) auxin, cytokinin, gibberellin, and brassinosteroids act cooperatively in promoting transcription of WOX4 and HD-ZIP III; (2) auxin and cytokinin pathways negatively regulate each other; (3) hormonal pathways act redundantly in sustaining cambium activity; (4) individual cells in the stem cell niches can have diverse molecular identities. CARENET can be extended to include components of other signalling pathways and be integrated with models of xylem and phloem differentiation. Such extended models would facilitate breeding trees with higher wood yield."}, {"title": "A Flow Procedure for the Linearization of Genome Sequence Graphs.", "url": "https://www.biorxiv.org/content/early/2017/01/18/101501", "tag": "Bioinformatics", "abstract": "Efforts to incorporate human genetic variation into the reference human genome have converged on the idea of a graph representation of genetic variation within a species, a genome sequence graph. A sequence graph represents a set of individual haploid reference genomes as paths in a single graph. When that set of reference genomes is sufficiently diverse, the sequence graph implicitly contains all frequent human genetic variations, including translocations, inversions, deletions, and insertions. In representing a set of genomes as a sequence graph one encounters certain challenges. One of the most important is the problem of graph linearization, essential both for efficiency of storage and access, as well as for natural graph visualization and compatibility with other tools. The goal of graph linearization is to order nodes of the graph in such a way that operations such as access, traversal and visualization are as efficient and effective as possible. A new algorithm for the linearization of sequence graphs, called the flow procedure, is proposed in this paper. Comparative experimental evaluation of the flow procedure against other algorithms shows that it outperforms its rivals in the metrics most relevant to sequence graphs."}, {"title": "Superbubbles, Ultrabubbles and Cacti", "url": "https://www.biorxiv.org/content/early/2017/01/18/101493", "tag": "Bioinformatics", "abstract": "A superbubble is a type of directed acyclic subgraph with single distinct source and sink vertices. In genome assembly and genet- ics, the possible paths through a superbubble can be considered to rep- resent the set of possible sequences at a location in a genome. Bidirected and biedged graphs are a generalization of digraphs that are increasingly being used to more fully represent genome assembly and variation prob- lems. Here we define snarls and ultrabubbles, generalizations of super- bubbles for bidirected and biedged graphs, and give an efficient algorithm for the detection of these more general structures. Key to this algorithm is the cactus graph, which we show encodes the nested decomposition of a graph into snarls and ultrabubbles within its structure. We propose and demonstrate empirically that this decomposition on bidirected and biedged graphs solves a fundamental problem by defining genetic sites for any collection of genomic variations, including complex structural vari- ations, without need for any single reference genome coordinate system. Furthermore, the nesting of the decomposition gives a natural way to describe and model variations contained within large variations, a case not currently dealt with by existing formats, e.g. VCF."}, {"title": "Genome Graphs", "url": "https://www.biorxiv.org/content/early/2017/01/18/101378", "tag": "Bioinformatics", "abstract": "There is increasing recognition that a single, monoploid reference genome is a poor universal reference structure for human genetics, because it represents only a tiny fraction of human variation. Adding this missing variation results in a structure that can be described as a mathematical graph: a genome graph. We demonstrate that, in comparison to the existing reference genome (GRCh38), genome graphs can substantially improve the fractions of reads that map uniquely and perfectly. Furthermore, we show that this fundamental simplification of read mapping transforms the variant calling problem from one in which many non-reference variants must be discovered de-novo to one in which the vast majority of variants are simply re-identified within the graph. Using standard benchmarks as well as a novel reference-free evaluation, we show that a simplistic variant calling procedure on a genome graph can already call variants at least as well as, and in many cases better than, a state-of-the-art method on the linear human reference genome. We anticipate that graph-based references will supplant linear references in humans and in other applications where cohorts of sequenced individuals are available."}, {"title": "Quality Assessment of High-throughput DNA Sequencing Data via Range analysis", "url": "https://www.biorxiv.org/content/early/2017/01/18/101469", "tag": "Bioinformatics", "abstract": "In the recent literature there appeared a number of studies for the quality assessment of sequencing data. These efforts, to a great extent, focused on reporting the statistical parameters regarding to the distribution of the quality scores and/or the base-calls in a FASTQ file. We investigate another dimension for the quality assessment motivated with the fact that reads including long intervals having fewer errors improve the performances of the post-processing tools in the down-stream analysis. Thus, the quality assessment procedures proposed in this study aim to analyze the segments on the reads that are above a certain quality. We define an interval of a read to be of desired quality when there are at most k quality scores less than or equal to a threshold value v, for some v and k provided by the user. We present the algorithm to detect those ranges and introduce new metrics computed from their lengths. These metrics include the mean values for the longest, shortest, average, cubic average, and average variation coefficient of the fragment lengths that are appropriate according to the v and k input parameters. We provide a new software tool QASDRA for quality assessment of sequencing data via range analysis. QASDRA, implemented in Python, and publicly available at https://github.com/ali-cp/QASDRA.git, creates the quality assessment report of an input FASTQ file according to the user specified k and v parameters. It also has the capabilities to filter out the reads according to the metrics introduced."}, {"title": "Promoter-Enhancer Interactions Identified from Hi-C Data using Probabilistic Models and Hierarchical Topological Domains", "url": "https://www.biorxiv.org/content/early/2017/01/18/101220", "tag": "Bioinformatics", "abstract": "Proximity-ligation methods as Hi-C allow us to map physical DNA-DNA interactions along the genome, and reveal its organization in topologically associating domains (TADs). As Hi-C data accumulate, computational methods were developed for identifying domain borders in multiple cell types and organisms. Here, we present PSYCHIC, a computational approach for analyzing Hi-C data and identifying Promoter-Enhancer interactions. We use a unified probabilistic model to segment the genome into domains, which we merge hierarchically and fit the Hi-C interaction map with a local background model. This allows us to estimate the expected number of interactions for every DNA-DNA pair, thus identifying over-represented interactions across the genome. By analyzing published Hi-C data in human and mouse, we identified hundreds of thousands of putative enhancers and their target genes in multiple cell types, and compiled an extensive genome-wide catalog of gene regulation in human and mouse."}, {"title": "The multiplex network of human diseases", "url": "https://www.biorxiv.org/content/early/2017/01/18/100370", "tag": "Bioinformatics", "abstract": "Untangling the complex interplay between phenotype and genotype is crucial to the effective characterization and subtyping of diseases. Here we build and analyze the multiplex network of 779 human diseases, which consists of a genotype-based layer and a phenotype-based layer. We show that diseases with common genetic constituents tend to share symptoms, and uncover how phenotype information helps boost genotype information. Moreover, we offer a flexible classification of diseases that considers their molecular underpinnings alongside their clinical manifestations. We detect cohesive groups of diseases that have high intra-group similarity at both the molecular and the phenotypic level. Inspecting these disease classes, we demonstrate the underlying pathways that connect diseases mechanistically. We observe monogenic disorders grouped together with complex diseases for which they increase the risk factor. We propose potentially new disease associations that arise as a unique feature of the information flow within and across the two layers."}, {"title": "Applying meta-analysis to Genotype-Tissue Expression data from multiple tissues to identify eQTLs and increase the number of eGenes", "url": "https://www.biorxiv.org/content/early/2017/01/16/100701", "tag": "Bioinformatics", "abstract": "During the last decade, with the advent of inexpensive microarray and RNA-seq technologies, there have been many expression quantitative trait loci (eQTL) studies for identifying genetic variants called eQTLs that regulate gene expression. Discovering eQTLs has been increasingly important as they may elucidate the functional consequence of non-coding variants identified from genome-wide association studies. Recently, several eQTL studies such as the Genotype-Tissue Expression (GTEx) consortium have made a great effort to obtain gene expression from multiple tissues. One advantage of these multi-tissue eQTL datasets is that they may allow one to identify more eQTLs by combining information across multiple tissues. Although a few methods have been proposed for multi-tissue eQTL studies, they are often computationally intensive and may not achieve optimal power because they do not consider a biological insight that a genetic variant regulates gene expression similarly in related tissues. In this paper, we propose an efficient meta-analysis approach for identifying eQTLs from large multi-tissue eQTL datasets. We name our method RECOV because it uses a random effects (RE) meta-analysis with an explicit covariance (COV) term to model the correlation of effect that eQTLs have across tissues. Our approach is faster than the previous approaches and properly controls the false-positive rate. We apply our approach to the real multi-tissue eQTL dataset from GTEx that contains 44 tissues, and show that our approach detects more eQTLs and eGenes than previous approaches."}, {"title": "The 3D genome organization of Drosophila melanogaster through data integration", "url": "https://www.biorxiv.org/content/early/2017/01/15/099911", "tag": "Bioinformatics", "abstract": "Genome structures are dynamic and non-randomly organized in the nucleus of higher eukaryotes. To maximize the accuracy and coverage of 3D genome structural models, it is important to integrate all available sources of experimental information about a genome's organization. It remains a major challenge to integrate such data from various complementary experimental methods. Here, we present an approach for data integration to determine a population of complete 3D genome structures that are statistically consistent with data from both genome-wide chromosome conformation capture (Hi-C) and lamina-DamID experiments. Our structures resolve the genome at the resolution of topological domains, and reproduce simultaneously both sets of experimental data. Importantly, this framework allows for structural heterogeneity between cells, and hence accounts for the expected plasticity of genome structures. As a case study we choose Drosophila melanogaster embryonic cells, for which both data types are available. Our 3D geome structures have strong predictive power for structural features not directly visible in the initial data sets, and reproduce experimental hallmarks of the D. melanogaster genome organization from independent and our own imaging experiments. Also they reveal a number of new insights about the genome organization and its functional relevance, including the preferred locations of heterochromatic satellites of differnet chromosomes, and observations about homologous pairing that cannot be directly observed in the original Hi-C or lamina-DamID data. To our knowledge our approach is the first that allows systematic integration of Hi-C and lamina DamID data for complete 3D genome structure calculation, while also explicitly considering genome structural variability. Keywords: 3D genome structure, higher order genome organization, population-based modeling, data integration, Hi-C, lamina-DamID, homologous pairing, Drosophila melanogaster."}, {"title": "DeepAD: Alzheimer\u2032s Disease Classification via Deep Convolutional Neural Networks using MRI and fMRI", "url": "https://www.biorxiv.org/content/early/2017/01/14/070441", "tag": "Bioinformatics", "abstract": "To extract patterns from neuroimaging data, various statistical methods and machine learning algorithms have been explored for the diagnosis of Alzheimer\u2032s disease among older adults in both clinical and research applications; however, distinguishing between Alzheimer\u2032s and healthy brain data has been challenging in older adults (age > 75) due to highly similar patterns of brain atrophy and image intensities. Recently, cutting-edge deep learning technologies have rapidly expanded into numerous fields, including medical image analysis. This paper outlines state-of-the-art deep learning-based pipelines employed to distinguish Alzheimer\u2032s magnetic resonance imaging (MRI) and functional MRI (fMRI) from normal healthy control data for a given age group. Using these pipelines, which were executed on a GPU-based high-performance computing platform, the data were strictly and carefully preprocessed. Next, scale- and shift-invariant low- to high-level features were obtained from a high volume of training images using convolutional neural network (CNN) architecture. In this study, fMRI data were used for the first time in deep learning applications for the purposes of medical image analysis and Alzheimer\u2032s disease prediction. These proposed and implemented pipelines, which demonstrate a significant improvement in classification output over other studies, resulted in high and reproducible accuracy rates of 99.9% and 98.84% for the fMRI and MRI pipelines, respectively. Additionally, for clinical purposes, subject-level classification was performed, resulting in an average accuracy rate of 94.32% and 97.88% for the fMRI and MRI pipelines, respectively. Finally, a decision making algorithm designed for the subject-level classification improved the rate to 97.77% for fMRI and 100% for MRI pipelines."}, {"title": "phylo-node: A Molecular Phylogenetic Toolkit using Node.js", "url": "https://www.biorxiv.org/content/early/2017/01/13/075101", "tag": "Bioinformatics", "abstract": "Background: Node.js is an open-source and cross-platform environment that provides a JavaScript codebase for back-end server-side applications. JavaScript has been used to develop very fast and user-friendly front-end tools for bioinformatic and phylogenetic analyses. However, no such toolkits are available using Node.js to conduct comprehensive molecular phylogenetic analysis. Results: To address this problem, I have developed, phylo-node, which was developed using Node.js and provides a stable and scalable toolkit that allows the user to perform diverse molecular and phylogenetic tasks. phylo-node can execute the analysis and process the resulting outputs from a suite of software options that provides tools for read processing and genome alignment, sequence retrieval, multiple sequence alignment, primer design, evolutionary modeling, and phylogeny reconstruction. Furthermore, phylo-node enables the user to deploy server dependent applications, and also provides simple integration and interoperation with other Node modules and languages using Node inheritance patterns, and a customized piping module to support the production of diverse pipelines. Conclusions: phylo-node is open-source and freely available to all users without sign-up or login requirements. All source code and user guidelines are openly available at the GitHub repository: https://github.com/dohalloran/phylo-node"}, {"title": "riboviz: analysis and visualization of ribosome profiling datasets", "url": "https://www.biorxiv.org/content/early/2017/01/12/100032", "tag": "Bioinformatics", "abstract": "Using high-throughput sequencing to monitor translation in vivo, ribosome profiling can provide critical insights into the dynamics and regulation of protein synthesis in a cell. Since its introduction in 2009, this technique has played a key role in driving biological discovery, and yet it requires a rigorous computational toolkit for widespread adoption. We developed a processing pipeline and browser-based visualization, riboviz, that allows convenient exploration and analysis of riboseq datasets. In implementation, riboviz consists of a comprehensive and flexible backend analysis pipeline that allows the user to analyze their private unpublished dataset, along with a web application for comparison with previously published public datasets. Availability and implementation: JavaScript and R source code and extra documentation are freely available from https://github.com/shahpr/RiboViz, while the web-application is live at www.riboviz.org."}, {"title": "CRISPR-RT: A web service for designing CRISPR-C2c2 crRNA with improved target specificity", "url": "https://www.biorxiv.org/content/early/2017/01/12/099895", "tag": "Bioinformatics", "abstract": "CRISPR-Cas systems have been successfully applied in genome editing. Recently, the CRISPR-C2c2 system has been reported as a tool for RNA editing. Here we describe CRISPR-RT (CRISPR RNA-Targeting), the first web service to help biologists design the crRNA with improved target specificity for the CRISPR-C2c2 system. CRISPR-RT allows users to set up a wide range of parameters, making it highly flexible for current and future research in CRISPR-based RNA editing. CRISPR-RT covers major model organisms and can be easily extended to cover other species. CRISPR-RT will empower researchers in RNA editing. It is available at http://bioinfolab.miamioh.edu/CRISPR-RT."}, {"title": "DeeperBind: Enhancing Prediction of Sequence Specificities of DNA Binding Proteins", "url": "https://www.biorxiv.org/content/early/2017/01/12/099754", "tag": "Bioinformatics", "abstract": "Transcription factors (TFs) are macromolecules that bind to cis-regulatory specific sub-regions of DNA promoters and initiate transcription. Finding the exact location of these binding sites (aka motifs) is important in a variety of domains such as drug design and development. To address this need, several in vivo and in vitro techniques have been developed so far that try to characterize and predict the binding specificity of a protein to different DNA loci. The major problem with these techniques is that they are not accurate enough in prediction of the binding affinity and characterization of the corresponding motifs. As a result, downstream analysis is required to uncover the locations where proteins of interest bind. Here, we propose DeeperBind, a long short term recurrent convolutional network for prediction of protein binding specificities with respect to DNA probes. DeeperBind can model the positional dynamics of probe sequences and hence reckons with the contributions made by individual sub-regions in DNA sequences, in an effective way. Moreover, it can be trained and tested on datasets containing varying-length sequences. We apply our pipeline to the datasets derived from protein binding microarrays (PBMs), an in-vitro high-throughput technology for quantification of protein-DNA binding preferences, and present promising results. To the best of our knowledge, this is the most accurate pipeline that can predict binding specificities of DNA sequences from the data produced by high-throughput technologies through utilization of the power of deep learning for feature generation and positional dynamics modeling."}, {"title": "Biowheel: interactive visualization and exploration of biomedical data", "url": "https://www.biorxiv.org/content/early/2017/01/11/099739", "tag": "Bioinformatics", "abstract": "We introduce Biowheel (https://biowheel.dibsvis.com/), a web-based award-winning data visualization tool, for exploring high-dimensional and heterogeneous biomedical data. Through interactive sorting and filtering of data, Biowheel enables researchers to quickly detect data outliers, evaluate data consistency, and discover mixed trends. Its interactive data presentation, visually-engaging design, and friendly user interface opens the door to easier, faster and better high-dimensional data interpretation for biomedical professionals with and without programming training."}, {"title": "Using pseudoalignment and base quality to accurately quantify microbial community composition", "url": "https://www.biorxiv.org/content/early/2017/01/09/097949", "tag": "Bioinformatics", "abstract": "Pooled DNA from multiple unknown organisms arises in a variety of contexts, for example microbial samples from ecological or human health research. Determining the composition of pooled samples can be difficult, especially at the scale of modern sequencing data and reference databases. Here we propose the novel pooled DNA classification method Karp. Karp combines the speed and low-memory requirements of k-mer based pseudoalignment with a likelihood framework that uses base quality information to better resolve multiply mapped reads. In this text we apply Karp to the problem of classifying 16S rRNA reads, commonly used in microbiome research. Using simulations, we show Karp is accurate across a variety of read lengths and when samples contain reads originating from organisms absent from the reference. We also assess performance in real 16S data, and show that relative to other widely used classification methods Karp can reveal stronger statistical association signals and should empower future discoveries."}, {"title": "Utilization of network graph connectivity to evaluate complications of inpatient medical and surgical populations as a method to prioritize quality improvement efforts", "url": "https://www.biorxiv.org/content/early/2017/01/09/099150", "tag": "Bioinformatics", "abstract": "Network graphs can provide a quantitative framework for identifying complications with significant volumes and strong relationships to other complications, as a method for prioritization of quality improvement work. Here we examine the application of network graphing techniques to acute care inpatient complications on acute care medical-surgical units of a quaternary care center. The 3M PPC software identified 66 complications among 106 unique patients with two or more complications during an inpatient hospital stay. The network graph highlighted renal failure without dialysis and septicemia and severe infections as highly connected complications in this population."}, {"title": "Automatic classification of diatoms of Merja fouarate", "url": "https://www.biorxiv.org/content/early/2017/01/09/099135", "tag": "Bioinformatics", "abstract": "This work consists of developing an automatic method of form analysis and classification of diatoms through processing of scanned images. The fundamental objective is to determine to what extent two diatoms are similar from the images stored in a database, and to conclude to what classes they belong. To do so, a comparison is made between a manual identification and an automatic identification, based on the ultimate points and the Freeman Chain Code."}, {"title": "Snaptron: querying and visualizing splicing across tens of thousands of RNA-seq samples", "url": "https://www.biorxiv.org/content/early/2017/01/09/097881", "tag": "Bioinformatics", "abstract": "As more and larger genomics studies appear, there is a growing need for comprehensive and queryable cross-study summaries. Snaptron is a search engine for summarized RNA sequencing data with a query planner that leverages R-tree, B-tree and inverted indexing strategies to rapidly execute queries over 146 million exon-exon splice junctions from over 70,000 human RNA-seq samples. Queries can be tailored by constraining which junctions and samples to consider. Snaptron can also rank and score junctions according to tissue specificity or other criteria. Further, Snaptron can rank and score samples according to the relative frequency of different splicing patterns. We outline biological questions that can be explored with Snaptron queries, including a study of novel exons in annotated genes, of exonization of repetitive element loci, and of a recently discovered alternative transcription start site for the ALK gene. Web app and documentation are at http://snaptron.cs.jhu.edu. Source code is at https://github.com/ChristopherWilks/snaptron under the MIT license."}, {"title": "Efficient inference of recent and ancestral recombination within bacterial populations", "url": "https://www.biorxiv.org/content/early/2017/01/09/059642", "tag": "Bioinformatics", "abstract": "Prokaryotic evolution is affected by horizontal transfer of genetic material through recombination. Inference of an evolutionary tree of bacteria thus relies on accurate identification of the population genetic structure and recombination-derived mosaicism. Rapidly growing databases represent a challenge for computational methods to detect recombinations in bacterial genomes. We introduce a novel algorithm called fastGEAR which identifies lineages in diverse microbial alignments, and recombinations between them and from external origins. The algorithm detects both recent recombinations (affecting a few isolates) and ancestral recombinations between detected lineages (affecting entire lineages), thus providing insight into recombinations affecting deep branches of the phylogenetic tree. In simulations, fastGEAR had comparable power to detect recent recombinations and outstanding power to detect the ancestral ones, compared to state-of-the-art methods, often with a fraction of computational cost. We demonstrate the utility of the method by analysing a collection of 616 whole-genomes of a recombinogenic pathogen Streptococcus pneumoniae, for which the method provided a high-resolution view of recombination across the genome. We examined in detail the penicillin-binding genes across the Streptococcus genus, demonstrating previously undetected genetic exchanges between different species at these three loci. Hence, fastGEAR can be readily applied to investigate mosaicism in bacterial genes across multiple species. Finally, fastGEAR correctly identified many known recombination hotspots and pointed to potential new ones. Matlab code and Linux/Windows executables are available at https://users.ics.aalto.fi/~pemartti/fastGEAR/"}, {"title": "A Complete Logical Approach to Resolve the Evolution and Dynamics of Mitochondrial Genome in Bilaterians", "url": "https://www.biorxiv.org/content/early/2017/01/06/098764", "tag": "Bioinformatics", "abstract": "A new method of genomic maps analysis based on formal logic is described. The purpose of the method is to 1) use mitochondrial genomic organisation of current taxa as datasets 2) calculate mutational steps between all mitochondrial gene arrangements and 3) reconstruct phylogenetic relationships according to these calculated mutational steps within a dendrogram under the assumption of maximum parsimony. Unlike existing methods mainly based on the probabilistic approach, the main strength of this new approach is that it calculates all the exact tree solutions with completeness and provides logical consequences as very robust results. Moreover, the method infers all possible hypothetical ancestors and reconstructs character states for all internal nodes (ancestors) of the trees. We started by testing the method using the deuterostomes as a study case. Then, with sponges as an outgroup, we investigated the mutational network of mitochondrial genomes of 47 bilaterian phyla and emphasised the peculiar case of chaetognaths. This pilot work showed that the use of formal logic in a hypothetico-deductive background such as phylogeny (where experimental testing of hypotheses is impossible) is very promising to explore mitochondrial gene rearrangements in deuterostomes and should be applied to many other bilaterian clades."}, {"title": "Software solutions for reproducible RNA-seq workflows", "url": "https://www.biorxiv.org/content/early/2017/01/06/099028", "tag": "Bioinformatics", "abstract": "Computational workflows typically consist of many tools that are usually distributed as compiled binaries or source code. Each of these software tools typically depends on other installed software, and performance could potentially vary due to versions, updates, and operating systems. We show here that the analysis of mRNA-seq data can depend on the computing environment, and we demonstrate that software containers represent practical solutions that ensure the reproducibility of RNAseq data analyses."}, {"title": "Building containerized workflows for RNA-seq data using the BioDepot-workflow-Builder (BwB)", "url": "https://www.biorxiv.org/content/early/2017/01/06/099010", "tag": "Bioinformatics", "abstract": "We present BioDepot-workflow-Builder (BwB), a portable and open-source tool for creating bioinformatics workflows with a simple drag-and-drop graphical user interface. The individual components of the workflows are Docker containers which are available from public repositories or provided by the user. The use of software containers ensures that workflows will give identical results across different operating systems and hardware architectures. The use of Docker also allows for individual components to be deployed on the cloud. The modularity and ease of customization and installation of bioinformatics tools using BwB allows for researchers to efficiently test new workflows and compare competing algorithms. Since BwB itself is packaged in a Docker container, the setup is minimal. In particular, users only need to install Docker and have access to a web browser to begin creating and running workflows. As a proof-of-concept case study, we illustrated the feasibility of BwB by developing widgets for the RNA-seq differential expression analysis workflow employed by the NIH BD2K-LINCS Drug Toxicity Signature Generation Center at Mount Sinai. The app and all the containers are available on the BioDepot repository (https://hub.docker.com/r/biodepot)."}, {"title": "MinorityReport, software for generalized analysis of causal genetic variants", "url": "https://www.biorxiv.org/content/early/2017/01/06/098731", "tag": "Bioinformatics", "abstract": "Background: The widespread availability of next generation genome sequencing technologies has enabled a wide range of variant detection applications, especially in cancer and inborn genetic disorders. For model systems and microorganisms, the same technology may be used to discover the causative mutations for any phenotype, including those generated in response to chemical perturbation. In the case of pathogenic organisms, these approaches have allowed the determination of drug targets by means of resistance selection followed by genome sequencing. Results: Here, we present open source software written in python, MinorityReport, to facilitate the comparison of any two sets of genome alignments for the purpose of rapidly identifying the spectrum of nonsynonymous changes, insertions or deletions, and copy number variations in a presumed mutant relative to its parent. Specifically, MinorityReport relates mapped sequence reads in SAM format output from any alignment tool for both the mutant and parent genome, relative to a reference genome, and produces the set of variants that distinguishes the mutant from the parent, all presented in an intuitive, straightforward report format. MinorityReport features tunable parameters for evaluating evidence and a scoring system that prioritizes reported variants based on relative proportions of read counts supporting the variant in the mutant versus parent data sets. We demonstrate the utility of MinorityReport using publicly available data sets that we previously published to find the determinants of resistance for novel anti-malarial drugs. Conclusions: MinorityReport is readily available (github.com/JeremyHorst/MinorityReport) to identify the genetic mechanisms of drug resistance in plasmodium, genotype-phenotype relationships in human diads, or genomic variations between any two related organisms."}, {"title": "Retrieving chromatin patterns from deep sequencing data using correlation functions", "url": "https://www.biorxiv.org/content/early/2017/01/06/054049", "tag": "Bioinformatics", "abstract": "Epigenetic modifications and other chromatin features partition the genome on multiple length scales. They define chromatin domains with distinct biological functions that come in sizes ranging from single modified DNA bases to several megabases in case of heterochromatic histone modifications. Due to chromatin folding, domains that are well separated along the linear nucleosome chain can form long-range interactions in three-dimensional space. It has now become a routine task to map epigenetic marks and chromatin structure by deep sequencing methods. However, assessing and comparing the properties of chromatin domains and their positional relationships across data sets without a priori assumptions remains challenging. Here, we introduce multi-scale correlation evaluation (MCORE), which uses the fluctuation spectrum of mapped sequencing reads to quantify and compare chromatin patterns over a broad range of length scales in a model-independent manner. We applied MCORE to map the chromatin landscape in mouse embryonic stem cells and differentiated neural cells. We integrated sequencing data from chromatin immunoprecipitation, RNA expression, DNA methylation and chromosome conformation capture experiments into network models that reflect the positional relationships among these features on different genomic scales. Furthermore, we used MCORE to compare our experimental data to models for heterochromatin reorganization during differentiation. The application of correlation functions to deep sequencing data complements current evaluation schemes and will support the development of quantitative descriptions of chromatin networks."}, {"title": "OrthoFiller: utilising data from multiple species to improve the completeness of genome annotations.", "url": "https://www.biorxiv.org/content/early/2017/01/05/098566", "tag": "Bioinformatics", "abstract": "Complete and accurate annotation of sequenced genomes is of paramount importance to their utility and analysis. Differences in gene prediction pipelines mean that genome sequences for a species can differ considerably in the quality and quantity of their predicted genes. Furthermore, genes that are present in genome sequences sometimes fail to be detected by computational gene prediction methods. Erroneously unannotated genes can lead to oversights and inaccurate assertions in biological investigations, especially for smaller-scale genome projects which rely heavily on computational prediction. Here we present OrthoFiller, a tool designed to address the problem of finding and adding such missing genes to genome annotations. OrthoFiller leverages information from multiple related species to identify those genes whose existence can be verified through comparison with known gene families, but which have not been predicted. By simulating missing gene annotations in real sequence datasets from both plants and fungi we demonstrate the accuracy and utility of OrthoFiller for finding missing genes and improving genome annotation. Furthermore, we show that applying OrthoFiller to existing complete genome annotations can identify and correct substantial numbers of erroneously missing genes in these two sets of species. We show that significant improvements in the completeness of genome annotations can be made by leveraging information from multiple species."}, {"title": "Glimma: interactive graphics for gene expression analysis", "url": "https://www.biorxiv.org/content/early/2017/01/05/096107", "tag": "Bioinformatics", "abstract": "Motivation: Summary graphics for RNA-sequencing and microarray gene expression analyses may contain upwards of tens of thousands of points. Details about certain genes or samples of interest are easily obscured in such dense summary displays. Incorporating interactivity into summary plots would enable additional information to be displayed on demand and facilitate intuitive data exploration. Results: The open-source Glimma package creates interactive graphics for exploring gene expression analysis with a few simple R commands. It extends popular plots found in the limma package, such as multi-dimensional scaling plots and mean-difference plots, to allow individual data points to be queried and additional annotation information to be displayed upon hovering or selecting particular points. It also offers links between plots so that more information can be revealed on demand. Glimma is widely applicable, supporting data analyses from a number of well established Bioconductor workflows (limma, edgeR and DESeq2) and uses D3/JavaScript to produce HTML pages with interactive displays that enable more effective data exploration by end-users. Results from Glimma can be easily shared between bioinformaticians and biologists, enhancing reporting capabilities while maintaining reproducibility. Availability and Implementation: The Glimma R package is available from http://bioconductor.org/packages/devel/bioc/html/Glimma.html."}, {"title": "Integrative pharmacogenomics to infer large-scale drug taxonomy", "url": "https://www.biorxiv.org/content/early/2017/01/05/046219", "tag": "Bioinformatics", "abstract": "Identification of drug targets and mechanism of action (MoA) for new and uncharacterized drugs is important for optimization of drug efficacy. Current MoA prediction approaches largely rely on prior information including side effects, therapeutic indication and/or chemo-informatics. Such information is not transferable or applicable for newly identified, previously uncharacterized small molecules. Therefore, a shift in the paradigm of MoA predictions is necessary towards development of unbiased approaches that can elucidate drug relationships and efficiently classify new compounds with basic input data. We propose a new integrative computational pharmacogenomic approach, referred to as Drug Network Fusion (DNF), to infer scalable drug taxonomies that relies only on basic drug characteristics towards elucidating drug-drug relationships. DNF is the first framework to integrate drug structural information, high-throughput drug perturbation and drug sensitivity profiles, enabling drug classification of new experimental compounds with minimal prior information. We demonstrate that the DNF taxonomy succeeds in identifying pertinent and novel drug-drug relationships, making it suitable for investigating experimental drugs with potential new targets or MoA. We highlight how the scalability of DNF facilitates identification of key drug relationships across different drug categories, and poses as a flexible tool for potential clinical applications in precision medicine. Our results support DNF as a valuable resource to the cancer research community by providing new hypotheses on the compound MoA and potential insights for drug repurposing. (Nehme El-Hachem and Deena M.A. Gendoo and Laleh Soltan Ghoraie contributed equally to this work.)"}, {"title": "Canu: scalable and accurate long-read assembly via adaptive k-mer weighting and repeat separation", "url": "https://www.biorxiv.org/content/early/2017/01/04/071282", "tag": "Bioinformatics", "abstract": "Long-read single-molecule sequencing has revolutionized de novo genome assembly and enabled the automated reconstruction of reference-quality genomes. However, given the relatively high error rates of such technologies, efficient and accurate assembly of large repeats and closely related haplotypes remains challenging. We address these issues with Canu, a successor of Celera Assembler that is specifically designed for noisy single-molecule sequences. Canu introduces support for nanopore sequencing, halves depth-of-coverage requirements, and improves assembly continuity while simultaneously reducing runtime by an order of magnitude on large genomes versus Celera Assembler 8.2. These advances result from new overlapping and assembly algorithms, including an adaptive overlapping strategy based on tf-idf weighted MinHash and a sparse assembly graph construction that avoids collapsing diverged repeats and haplotypes. We demonstrate that Canu can reliably assemble complete microbial genomes and near-complete eukaryotic chromosomes using either PacBio or Oxford Nanopore technologies, and achieves a contig NG50 of greater than 21 Mbp on both human and Drosophila melanogaster PacBio datasets. For assembly structures that cannot be linearly represented, Canu provides graph-based assembly outputs in graphical fragment assembly (GFA) format for analysis or integration with complementary phasing and scaffolding techniques. The combination of such highly resolved assembly graphs with long-range scaffolding information promises the complete and automated assembly of complex genomes."}, {"title": "A meta-analysis of bioinformatics software benchmarks reveals that publication-bias unduly influences software accuracy", "url": "https://www.biorxiv.org/content/early/2017/01/02/092205", "tag": "Bioinformatics", "abstract": "Computational biology has provided widely used and powerful software tools for testing and making inferences about biological data. In the face of increasing volumes of data, heuristic methods that trade software speed for mathematical completeness must be employed. We are interested in whether trade-offs between speed and accuracy are reasonable. Also, what factors are indicative of accurate software? In this work we mine published benchmarks of computational biology software, we collect data on the relative accuracy and speed of different software and then test to see what factors influence accuracy e.g. speed, author reputation, journal impact or recency. We found that author reputation, journal impact, the number of citations, software speed and age are not reliable predictors of software accuracy. This implies that useful bioinformatics software is not only the domain of famous senior researchers. In addition, we found that there exists an excess of slow and inaccurate software tools across multiple sub-disciplines of bioinformatics. Meanwhile, there are very few tools of middling accuracy and speed. We hypothesise that a strong publication bias unduly influences the publication and development of bioinformatic software tools. In other words, at present software that is not highly ranked on speed and not highly ranked on accuracy is difficult to publish due to editorial and reviewer practices. This leaves an unfortunate gap in the literature upon which future software refinements cannot be constructed."}, {"title": "Multi-Region Neural Representation: A novel model for decoding visual stimuli in human brains", "url": "https://www.biorxiv.org/content/early/2017/01/02/097675", "tag": "Bioinformatics", "abstract": "Multivariate Pattern (MVP) classification holds enormous potential for decoding visual stimuli in the human brain by employing task-based fMRI data sets. There is a wide range of challenges in the MVP techniques, i.e. decreasing noise and sparsity, defining effective regions of interest (ROIs), visualizing results, and the cost of brain studies. In overcoming these challenges, this paper proposes a novel model of neural representation, which can automatically detect the active regions for each visual stimulus and then utilize these anatomical regions for visualizing and analyzing the functional activities. Therefore, this model provides an opportunity for neuroscientists to ask this question: what is the effect of a stimulus on each of the detected regions instead of just study the fluctuation of voxels in the manually selected ROIs. Moreover, our method introduces analyzing snapshots of brain image for decreasing sparsity rather than using the whole of fMRI time series. Further, a new Gaussian smoothing method is proposed for removing noise of voxels in the level of ROIs. The proposed method enables us to combine different fMRI data sets for reducing the cost of brain studies. Experimental studies on 4 visual categories (words, consonants, objects and nonsense photos) confirm that the proposed method achieves superior performance to state-of-the-art methods."}, {"title": "grID: A CRISPR-Cas9 guide RNA Database and Resource for Genome-Editing", "url": "https://www.biorxiv.org/content/early/2016/12/30/097352", "tag": "Bioinformatics", "abstract": "CRISPR-Cas9 genome-editing is a revolutionary technology that is transforming biological research. The explosive growth and advances in CRISPR research over the last few years, coupled with the potential for clinical applications and therapeutics, is heralding a new era for genome engineering. To further support this technology platform and to provide a universal CRISPR annotation system, we introduce the grID database (http://crispr.technology), an extensive compilation of gRNA properties including sequence and variations, thermodynamic parameters, off-target analyses, and alternative PAM sites, among others. To aid in the design of optimal gRNAs, the website is integrated with other prominent databases, providing a wealth of additional resources to guide users from in silico analysis through experimental CRISPR targeting. Here, we make available all the tools, protocols, and plasmids that are needed for successful CRISPR-based genome targeting."}, {"title": "Training Genotype Callers with Neural Networks", "url": "https://www.biorxiv.org/content/early/2016/12/30/097469", "tag": "Bioinformatics", "abstract": "We present an open source software toolkit for training deep learning models to call genotypes in high-throughput sequencing data. The software supports SAM, BAM, CRAM and Goby alignments and the training of models for a variety of experimental assays and analysis protocols. We evaluate this software in the Illumina platinum whole genome datasets and find that a deep learning model trained on 80% of the genome achieves a 0.986% accuracy on variants (genotype concordance) when trained with 10% of the data from a genome. The software is distributed at https://github.com/CampagneLaboratory/variationanalysis. The software makes it possible to train genotype calling models on consumer hardware with CPUs or GPU(s). It will enable individual investigators and small laboratories to train and evaluate their own models and to make open source contributions. We welcome contributions to extend this early prototype or evaluate its performance on other gold standard datasets."}, {"title": "tensorBF: an R package for Bayesian tensor factorization", "url": "https://www.biorxiv.org/content/early/2016/12/29/097048", "tag": "Bioinformatics", "abstract": "With recent advancements in measurement technologies, many multi-way and tensor datasets have started to emerge. Exploiting the natural tensor structure in the data has been shown to be advantageous for both explorative and predictive studies in several application areas of bioinformatics and computational biology. Therefore, there has subsequently arisen a need for robust and flexible tools for effectively analyzing tensor data sets. We present the R package tensorBF, which is the first R package providing Bayesian factorization of a tensor. Our package implements a generative model that automatically identifies the number of factors needed to explain the tensor, overcoming a key limitation of traditional tensor factorizations. We also recommend best practices when using tensor factorizations for both, explorative and predictive analysis with an example application on drug response dataset. The package also implements tools related to the normalization of data, informative noise priors and visualization. Availability: The package is available at https://cran.r-project.org/package=tensorBF"}, {"title": "Integrated deep learned transcriptomic and structure-based predictor of clinical trials outcomes", "url": "https://www.biorxiv.org/content/early/2016/12/29/095653", "tag": "Bioinformatics", "abstract": "Despite many recent advances in systems biology and a marked increase in the availability of high-throughput biological data, the productivity of research and development in the pharmaceutical industry is on the decline. This is primarily due to clinical trial failure rates reaching up to 95% in oncology and other disease areas. We have developed a comprehensive analytical and computational pipeline utilizing deep learning techniques and novel systems biology analytical tools to predict the outcomes of phase I/II clinical trials. The pipeline predicts the side effects of a drug using deep neural networks and estimates drug-induced pathway activation. It then uses the predicted side effect probabilities and pathway activation scores as an input to train a classifier which predicts clinical trial outcomes. This classifier was trained on 577 transcriptomic datasets and has achieved a cross-validated accuracy of 0.83. When compared to a direct gene-based classifier, our multi-stage approach dramatically improves the accuracy of the predictions. The classifier was applied to a set of compounds currently present in the pipelines of several major pharmaceutical companies to highlight potential risks in their portfolios and estimate the fraction of clinical trials that were likely to fail in phase I and II."}, {"title": "Bio.Ontology - Python tools for enrichment analysis and visualization of ontologies", "url": "https://www.biorxiv.org/content/early/2016/12/28/097139", "tag": "Bioinformatics", "abstract": "Motivation: Functional annotation and enrichment analysis based on ontologies has become one of the standard methods of analysis of experimental results. Over the past decade, many methods have been proposed for statistical quantification of enrichment of different functional terms and many implementations of these methods are available. As the popularity of these methods grows, the need for tools facilitating their automation increases. Results: We present a complete Python library for statistical enrichment analysis of gene sets and gene rankings compatible with most available biological ontologies. It allows the user to perform all necessary steps: reading the ontologies and gene annotations in multiple formats; performing enrichment analysis using various methods and visualizing the results as readable reports. Importantly, our library includes methods for correcting for multiple hypotheses testing including computation of False Discovery Rates."}, {"title": "Quantifying variation by the ratio of CSS to UCSS", "url": "https://www.biorxiv.org/content/early/2016/12/26/096784", "tag": "Bioinformatics", "abstract": "The variance, the average of squared deviations of data values from their mean, is the most widely used criterion for measuring the variation. Small amounts of the variance indicate that the values tend to be close to the mean and its high amounts are indicative of more dispersion around the mean. However, when dealing with a single variable or variables with different measuring units, variance can not give us a proper understanding of the actual extent of variation. RCU, the ratio of corrected sum of squares (CSS) to the uncorrected sum of squares (UCSS) can quantify variation too. The values of RCU vary from zero, for a situation in which all values are the same, to 100 % when they are completely different or symmetric. To compare the efficiency of RCU and variance for measuring variation, data of seven variables with different units of measurement for 17 wheat cultivars including yield (g/plant), spikes (numbers), height (cm), earliness (days), viability (%), hectoliter (kg/hectoliter) and EC (micromohs/cm) were used. Variance of these variables respectively was 83.34, 72.01, 353.23, 81.48, 5.31, 69.52 and 7167.47. The highest and lowest RCU was obtained for spikes (9.38 %) and viability (0.05 %), respectively. The RCU for yield, height, earliness, hectoliter and EC was 8.97, 2.67, 0.29, 1.30 and 2.85 %, respectively. The RCUs were somewhat similar to coefficient of variation of the variables. Based on the RCU, the extent of variation was medium for spikes and viability and was low for the other variables. As a result RCU could be used as a simple criterion to quantify the actual extent of variation in the data."}, {"title": "Viral Quasispecies Reconstruction via Correlation Clustering", "url": "https://www.biorxiv.org/content/early/2016/12/26/096768", "tag": "Bioinformatics", "abstract": "RNA viruses are characterized by high mutation rates that give rise to populations of closely related viral genomes, the so-called viral quasispecies. The underlying genetic heterogeneity occurring as a result of natural mutation-selection process enables the virus to adapt and proliferate in face of changing conditions over the course of an infection. Determining genetic diversity (i.e., inferring viral haplotypes and their proportions in the population) of an RNA virus is essential for the understanding of its origin and mutation patterns, and the development of effective drug treatments. In this paper we present QSdpR, a novel correlation clustering formulation of the quasispecies reconstruction problem which relies on semidefinite programming to accurately estimate the sub-species and their frequencies in a mixed population. Extensive comparisons with existing methods are presented on both synthetic and real data, demonstrating efficacy and superior performance of QSdpR."}, {"title": "K-mer Motif Multinomial Mixtures", "url": "https://www.biorxiv.org/content/early/2016/12/24/096735", "tag": "Bioinformatics", "abstract": "Motivation: The advent of inexpensive high-throughput sequencing (HTS) places new demands on motif discovery algorithms. To confront the challenges and embrace the opportunities presented by the growing wealth of information tied up in HTS datasets, we developed K-mer motif multinomial mixtures (KMMMs), a flexible class of Bayesian models for identifying multiple motifs in sequence sets using K-mer tables. Advantages of this framework are inference with time and space complexities that only scale with K, and the ability to be incorporated into larger Bayesian models. Results: We derived a class of probabilistic models of K-mer tables generated from sequence containing multiple motifs. KMMMs model the K-mer table as a multinomial mixture, with motif and background components, which are distributions over K-mers overlapping with each of the latent motifs and over K-mers that do not overlap with any motif, respectively. The framework casts motif discovery as a posterior inference problem, and we present several approximate inference methods that provide accurate reconstructions of motifs in synthetic data. Finally we apply the method to discover motifs in DNAse hypersensitive sites and ChIP-seq peaks obtained from the ENCODE project."}, {"title": "RealTime Heart Rate Monitoring Using Photoplethysmographic (PPG) Signals During Intensive Physical Exercises", "url": "https://www.biorxiv.org/content/early/2016/12/24/092627", "tag": "Bioinformatics", "abstract": "Heart Rate (HR) is a fundamental vital sign, monitoring which provides essential information for automated healthcare systems. The emerging technology of Photoplethysmograph (PPG) is shown as a feasible candidate for such applications; however, Motion Artifacts (MA) hinder efficient HR estimation using PPG, especially in situations involving physical activities. It is previously shown that even in the presence of sever MA, HR is still traceable with the help of simultaneous acceleration data although at high computational expenses. In this paper, we propose a novel framework, that not only improves the accuracy in HR estimation, but also achieves realtime performance by significantly reducing the complexity of system; mainly due to alleviation of the need for computationally demanding MA cancellation methods. Utilizing an spectrum estimation model (autoregressive) that suits well to the inherent PPG generation process, and benefiting from further intrinsic properties of the environment (e.g., the venous pulsation phenomenon); our framework achieves realtime and delayed (post-processed) Average Absolute Errors (AAE) of 1.19 and 0.99 Beats Per Minute (BPM) respectively, on the 12 benchmark recordings in which subjects run at speeds of up to 15 km/h maximum. Moreover, the system makes standalone implementation feasible by processing input frames (2 channel PPG and 3D ACC) in < 0.004 times of the frame duration, operating on a 3.2 GHz processor. This study provides wearable healthcare technologies with a robust framework for accurate HR monitoring; at considerably low computational costs."}, {"title": "diffloop: a computational framework for identifying and analyzing differential DNA loops from sequencing data", "url": "https://www.biorxiv.org/content/early/2016/12/24/087338", "tag": "Bioinformatics", "abstract": "The three-dimensional architecture of DNA within the nucleus is a key determinant of interactions between genes, regulatory elements, and transcriptional machinery. As a result, differences in loop structure are associated with differences in gene expression and cell state. Here, we introduce diffloop, an R/Bioconductor package for identifying differential DNA looping between samples. The package additionally provides a suite of functions for the quality control, statistical testing, annotation and visualization of DNA loops. We demonstrate this functionality by detecting differences in DNA loops between ENCODE ChIA-PET datasets and relate looping to differences in epigenetic state and gene expression."}, {"title": "Inferring Weighted Gene Annotations from Expression Dat", "url": "https://www.biorxiv.org/content/early/2016/12/24/096677", "tag": "Bioinformatics", "abstract": "Annotating genes with information describing their role in the cell is a fundamental goal in biology, and essential for interpreting data-rich assays such as microarray analysis and RNA-Seq. Gene annotation takes many forms, from Gene Ontology (GO) terms, to tissues or cell types of significant expression, to putative regulatory factors and DNA sequences. Almost invariably in gene databases, annotations are connected to genes by a Boolean relationship, e.g., a GO term either is or is not associated with a particular gene. While useful for many purposes, Boolean-type annotations fail to capture the varying degrees by which some annotations describe their associated genes and give no indication of the relevance of annotations to cellular logistical activities such as gene expression. We hypothesized that weighted annotations could prove useful for understanding gene function and for interpreting gene expression data, and developed a method to generate these from Boolean annotations and a large compendium of gene expression data. The method uses an independent component analysis-based approach to find gene modules in the compendium, and then assigns gene-specific weights to annotations proportional to the degree to which they are shared among members of the module, with the reasoning that the more an annotation is shared by genes in a module, the more likely it is to be relevant to their function and, therefore, the higher it should be weighted. In this paper, we show that analysis of expression data with module-weighted annotations appears to be more resistant to the confounding effect of gene-gene correlations than non-weighted annotation enrichment analysis, and show several examples in which module-weighted annotations provide biological insights not revealed by Boolean annotations. We also show that application of the method to a simple form of genetic regulatory annotation, namely, the presence or absence of putative regulatory words (oligonucleotides) in gene promoters, leads to module-weighted words that closely match known regulatory sequences, and that these can be used to quickly determine key regulatory sequences in differential expression data."}, {"title": "Unicycler: resolving bacterial genome assemblies from short and long sequencing reads", "url": "https://www.biorxiv.org/content/early/2016/12/22/096412", "tag": "Bioinformatics", "abstract": "The Illumina DNA sequencing platform generates accurate but short reads, which can be used to produce accurate but fragmented genome assemblies. Pacific Biosciences and Oxford Nanopore Technologies DNA sequencing platforms generate long reads that can produce more complete genome assemblies, but the sequencing is more expensive and error prone. There is significant interest in combining data from these complementary sequencing technologies to generate more accurate \"hybrid\" assemblies. However, few tools exist that truly leverage the benefits of both types of data, namely the accuracy of short reads and the structural resolving power of long reads. Here we present Unicycler, a new tool for assembling bacterial genomes from a combination of short and long reads, which produces assemblies that are accurate, complete and cost-effective. Unicycler builds an initial assembly graph from short reads using the de novo assembler SPAdes and then simplifies the graph using information from short and long reads. Unicycler utilises a novel semi-global aligner, which is used to align long reads to the assembly graph. Tests on both synthetic and real reads show Unicycler can assemble larger contigs with fewer misassemblies than other hybrid assemblers, even when long read depth and accuracy are low. Unicycler is open source (GPLv3) and available at github.com/rrwick/Unicycler."}, {"title": "The DOE Systems Biology Knowledgebase (KBase)", "url": "https://www.biorxiv.org/content/early/2016/12/22/096354", "tag": "Bioinformatics", "abstract": "The U.S. Department of Energy Systems Biology Knowledgebase (KBase) is an open-source software and data platform designed to meet the grand challenge of systems biology - predicting and designing biological function from the biomolecular (small scale) to the ecological (large scale). KBase is available for anyone to use, and enables researchers to collaboratively generate, test, compare, and share hypotheses about biological functions; perform large-scale analyses on scalable computing infrastructure; and combine experimental evidence and conclusions that lead to accurate models of plant and microbial physiology and community dynamics. The KBase platform has (1) extensible analytical capabilities that currently include genome assembly, annotation, ontology assignment, comparative genomics, transcriptomics, and metabolic modeling; (2) a web-browser-based user interface that supports building, sharing, and publishing reproducible and well-annotated analyses with integrated data; (3) access to extensive computational resources; and (4) a software development kit allowing the community to add functionality to the system."}, {"title": "A Unique Ribosome Signature Reveals Bacterial Translation Initiation Sites", "url": "https://www.biorxiv.org/content/early/2016/12/22/095893", "tag": "Bioinformatics", "abstract": "While methods for annotation of genes are increasingly reliable the exact identification of the translation initiation site remains a challenging problem. Since the N-termini of proteins often contain regulatory and targeting information developing a robust method for start site identification is crucial. Ribosome profiling reads show distinct patterns of read length distributions around translation initiation sites. These patterns are typically lost in standard ribosome profiling analysis pipelines, when reads from footprints are adjusted to determine the specific codon being translated. Using these unique signatures we build a model capable of predicting translation initiation sites and demonstrate its high accuracy using N-terminal proteomics. Applying this to prokaryotic samples, we re-annotate translation initiation sites and provide evidence of N-terminal truncations and elongations of annotated coding sequences. These re-annotations are supported by the presence of Shine-Dalgarno sequences, structural and sequence based features and N-terminal peptides. Finally, our model identifies 61 novel genes previously undiscovered in the genome."}, {"title": "RSAT matrix-clustering: dynamic exploration and redundancy reduction of transcription factor binding motif collections", "url": "https://www.biorxiv.org/content/early/2016/12/21/065565", "tag": "Bioinformatics", "abstract": "Transcription Factor (TF) databases contain multitudes of motifs from various sources, from which non-redundant collections are derived by manual curation. The advent of high-throughput methods stimulated the production of novel collections with increasing numbers of motifs. Meta-databases, built by merging these collections, contain redundant versions, because available tools are not suited to automatically identify and explore biologically relevant clusters among thousands of motifs. Motif discovery from genome-scale data sets (e.g. ChIP-seq peaks) also produces redundant motifs, hampering the interpretation of results. We present matrix-clustering, a versatile tool that clusters similar TFBMs into multiple trees, and automatically creates non-redundant collections of motifs. A feature unique to matrix-clustering is its dynamic visualisation of aligned TFBMs, and its capability to simultaneously treat multiple collections from various sources. We demonstrate that matrix-clustering considerably simplifies the interpretation of combined results from multiple motif discovery tools and highlights biologically relevant variations of similar motifs. By clustering 24 entire databases (>7,500 motifs), we show that matrix-clustering correctly groups motifs belonging to the same TF families, and can drastically reduce motif redundancy. matrix-clustering is integrated within the RSAT suite (http://rsat.eu/), accessible through a user-friendly web interface or command-line for its integration in pipelines."}, {"title": "Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification", "url": "https://www.biorxiv.org/content/early/2016/12/20/095794", "tag": "Bioinformatics", "abstract": "Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods requires great effort to annotate the training data by costly manual labeling and specialized computational models to detect these annotations during test. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned costly need to annotate the training data. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed deep networks compared to previous work using segmentation and detection annotations in the training."}, {"title": "Adversarial Deep Structural Networks for Mammographic Mass Segmentation", "url": "https://www.biorxiv.org/content/early/2016/12/20/095786", "tag": "Bioinformatics", "abstract": "Mass segmentation is an important task in mammogram analysis, providing effective morphological features and regions of interest (ROI) for mass detection and classification. Inspired by the success of using deep convolutional features for natural image analysis and conditional random fields (CRF) for structural learning, we propose an end-to-end network for mammographic mass segmentation. The network employs a fully convolutional network (FCN) to model potential function, followed by a CRF to perform structural learning. Because the mass distribution varies greatly with pixel position, the FCN is combined with position priori for the task. Due to the small size of mammogram datasets, we use adversarial training to control over-fitting. Four models with different convolutional kernels are further fused to improve the segmentation results. Experimental results on two public datasets, INbreast and DDSM-BCRP, show that our end-to-end network combined with adversarial training achieves the-state-of-the-art results."}, {"title": "Partitioned learning of deep Boltzmann machines for SNP data", "url": "https://www.biorxiv.org/content/early/2016/12/20/095638", "tag": "Bioinformatics", "abstract": "Learning the joint distributions of measurements, and in particular identification of an appropriate low-dimensional manifold, has been found to be a powerful ingredient of deep leaning approaches. Yet, such approaches have hardly been applied to single nucleotide polymorphism (SNP) data, probably due to the high number of features typically exceeding the number of studied individuals. After a brief overview of how deep Boltzmann machines (DBMs), a deep learning approach, can be adapted to SNP data in principle, we specifically present a way to alleviate the dimensionality problem by partitioned learning. We propose a sparse regression approach to coarsely screen the joint distribution of SNPs, followed by training several DBMs on SNP partitions that were identified by the screening. Aggregate features representing SNP patterns and the corresponding SNPs are extracted from the DBMs by a combination of statistical tests and sparse regression. In simulated case-control data, we show how this can uncover complex SNP patterns and augment results from univariate approaches, while maintaining type 1 error control. Time-to-event endpoints are considered in an application with acute myeloid lymphoma patients, where SNP patterns are modeled after a pre-screening based on gene expression data. The proposed approach identified three SNPs that seem to jointly influence survival in a validation data set. This indicates the added value of jointly investigating SNPs compared to standard univariate analyses and makes partitioned learning of DBMs an interesting complementary approach when analyzing SNP data."}, {"title": "MetaSRA: normalized sample-specific metadata for the Sequence Read Archive", "url": "https://www.biorxiv.org/content/early/2016/12/19/090506", "tag": "Bioinformatics", "abstract": "Motivation: The NCBI's Sequence Read Archive (SRA) promises great biological insight if one could analyze the data in the aggregate; however, the data remain largely underutilized, in part, due to the poor structure of the metadata associated with each sample. The rules governing submissions to the SRA do not dictate a standardized set of terms that should be used to describe the biological samples from which the sequencing data are derived. As a result, the metadata include many synonyms, spelling variants, and references to outside sources of information. Furthermore, manual annotation of the data remains intractable due to the large number of samples in the archive. For these reasons, it has been difficult to perform large-scale analyses that study the relationships between biomolecular processes and phenotype across diverse diseases, tissues, and cell types present in the SRA. Results: We present MetaSRA, a database of normalized SRA sample-specific metadata following a schema inspired by the metadata organization of the ENCODE project. This schema involves mapping samples to terms in biomedical ontologies, labeling each sample with a sample-type category, and extracting real-valued properties. We automated these tasks via a novel computational pipeline. Availability: The MetaSRA database is available at http://deweylab.biostat.wisc.edu/metasra. Software implementing our computational pipeline is available at https://github.com/deweylab/metasra-pipeline."}, {"title": "Mortality prediction in sepsis via gene expression analysis: a community approach", "url": "https://www.biorxiv.org/content/early/2016/12/19/095489", "tag": "Bioinformatics", "abstract": "Improved risk stratification and prognosis in sepsis is a critical unmet need. Clinical severity scores and available assays such as blood lactate reflect global illness severity with suboptimal performance, and do not specifically reveal the underlying dysregulation of sepsis. Here three scientific groups were invited to independently generate prognostic models for 30-day mortality using 12 discovery cohorts (N=650) containing transcriptomic data collected from primarily community-onset sepsis patients. Predictive performance was validated in 5 cohorts of community-onset sepsis patients (N=189) in which the models showed summary AUROCs ranging from 0.765-0.89. Similar performance was observed in 4 cohorts of hospital-acquired sepsis (N=282). Combining the new gene-expression-based prognostic models with prior clinical severity scores led to significant improvement in prediction of 30-day mortality (p<0.01). These models provide an opportunity to develop molecular bedside tests that may improve risk stratification and mortality prediction in patients with sepsis, improving both resource allocation and prognostic enrichment in clinical trials."}, {"title": "In silico epitope prediction and 3D model analysis of Peste des petits ruminants virus nucleoprotein (PPRV N)", "url": "https://www.biorxiv.org/content/early/2016/12/19/095505", "tag": "Bioinformatics", "abstract": "Peste des petits ruminants virus (PPRV) is an acute, highly contagious viral disease of small ruminants. It is endemic to sub-Saharan Africa, Asia and the Arabian Peninsula. The disease is a major constraint to food security, causing significant economic losses to subsistence farmers in affected areas. The nucleoprotein of morbilliviruses is highly immunogenic and produced in large quantities in virus infected cells. This makes it a suitable target for the immune response. In this study, B-cell and T-cell epitopes of PPRV Nig/75/1 nucleoprotein were predicted using a suite of in silico tools. Forty-six T-cell epitopes were predicted, of which 38 were MHC-I binding while eight were MHC-II binding. Of the 19 B-cell epitopes predicted, 15 were linear epitopes while four were discontinuous epitopes. Homology modelling of PPRV-N was done to elucidate the 3D structure of the protein and conformational epitopes. Conservation analysis of the discontinuous epitopes gave an indication into the similarity of the selected epitopes with other isolates of PPRV. Predicted epitopes may form an important starting point for serological screening and diagnostic tools against PPRV. Experimental validation of the predicted epitopes will assist in selection of promising candidates for consideration as antigen-based diagnostic tools. Such diagnostic tools would play a role in the global fight and possible eradication of PPR."}, {"title": "Hardy Weinberg Exact Test In Large Scale Variant Calling Quality Control", "url": "https://www.biorxiv.org/content/early/2016/12/19/095521", "tag": "Bioinformatics", "abstract": "Hardy Weinberg Equilibrium (HWE) test is widely used as a quality control measure to detect sequencing artifacts like mismapping, allelic dropout and biases. However, in the high throughput sequencing era, where the sample size is beyond a thousand scale, the utility of HWE test in reducing the false positive rate remains unclear. In this paper, we demonstrate that HWE test has limited power in identifying sequencing artifacts when the variant allele frequency is lower than 1% in a variant call set produced from more than five thousand whole genome sequenced samples from two homogeneous populations. We develop a novel strategy of implementing HWE filtering in which we incorporate site frequency spectrum information and determine the p-value cutoff which optimizes the tradeoff between sensitivity and specificity. The novel strategy is shown to outperform the exact test of HWE with an empirical constant p-value cutoff regardless of the sequencing sample size. We also present best practice recommendations for identifying possible sources of false positives from large sequencing datasets based on an analysis of intrinsic biases in the variant calling process. Our novel strategy of determining the HWE test p-value cutoff and applying the test to the common variants provides a practical approach for the variant level quality controls in the upcoming sequencing projects with tens to hundreds of thousand of samples."}, {"title": "RNAtor: an Android-based application for biologists to plan RNA sequencing experiments.", "url": "https://www.biorxiv.org/content/early/2016/12/19/095315", "tag": "Bioinformatics", "abstract": "RNA sequencing (RNA-seq) is a powerful technology for identification of novel transcripts (coding, non-coding and splice variants), understanding of transcript structures and estimation of gene and/or allelic expression. There are specific challenges that biologists face in determining the number of replicates to use, total number of sequencing reads to generate for detecting marginally differentially expressed transcripts and the number of lanes in a sequencing flow cell to use for the production of right amount of information. Although past studies attempted answering some of these questions, there is a lack of accessible and biologist-friendly mobile applications to answer these questions. Keeping this in mind, we have developed RNAtor, a mobile application for Android platforms, to aid biologists in correctly designing their RNA-seq experiments. The recommendations from RNAtor are based on simulations and real data. Availability and Implementation The Android version of RNAtor is available on Google Play Store and the code from GitHub (https://github.com/binaypanda/RNAtor)."}, {"title": "poRe GUIs for parallel and real-time processing of MinION sequence data", "url": "https://www.biorxiv.org/content/early/2016/12/19/094979", "tag": "Bioinformatics", "abstract": "Motivation: Oxford Nanopore's MinION device has matured rapidly and is now capable of producing over one million reads and several gigabases of sequence data per run. The nature of the MinION output re-quires new tools that are easy to use by scientists with a range of computational skills and which enable quick and simple QC and data extraction from MinION runs. Results: We have developed two GUIs for the R package poRe that allow parallel and real-time processing of MinION datasets. Both GUIs are capable of extracting sequence- and meta- data from large MinION da-tasets via a friendly point-and-click interface using commodity hardware. Availability: The GUIs are packaged within poRe which is available on SourceForge: https://sourceforge.net/projects/rpore/files/. Documentation is available on GitHub: https://github.com/mw55309/poRe_docs Contact: mick.watson@roslin.ed.ac.uk"}, {"title": "Genome Wide Computational Prediction of miRNAs in Kyasanur Forest Disease Virus and their Targeted Genes in Human", "url": "https://www.biorxiv.org/content/early/2016/12/19/095083", "tag": "Bioinformatics", "abstract": "RNAs are versatile biomolecules and can be coding or non-coding. Among the non-coding RNAs, miRNAs are small endogenous molecules that play important role in posttranscriptional gene regulation. miRNAs are identified in viruses too and involved in down regulation of host genes. Flavivirus family members are classified in to two groups: mosquito-borne flaviviruses (MBFV) and tick-borne flaviviruses (TBFV). Kyasanur forest disease virus (KFDV) found in India in 1957 (Karnataka) relates to TBFV. Virus has been diffuse to new areas in India and needs attention as it can cause severe hemorrhagic fever. Here in this study, we scanned the virus genome for prediction of miRNAs that can inhibit host target genes. VMir, tool was used for extraction of pre-miRNAs. A total of four miRNAs were found and submitted to ViralMir for classification in to real or pseudo. Interestingly, all four pre-miRNAs were classified as real. Eight mature miRNAs were located in pre-miRNAs by Mature Bayes. A total of 539 human target genes has been identified by using miRDB but ANGPT1 (angiopoietin 1) and TFRC (transferrin receptor) genes were screened to play role in hemorrhagic fever and neurological problems. GO analysis of target genes also supported the evidences. Keywords: Kyasanur forest disease virus, Flavivirus, miRNA, target prediction *Corresponding author: email: sandeep.5sep@yahoo.co.in"}, {"title": "GeNNet: An Integrated Platform for Unifying Scientific Workflow Management and Graph Databases for Transcriptome Data Analysis", "url": "https://www.biorxiv.org/content/early/2016/12/18/095257", "tag": "Bioinformatics", "abstract": "Background: There are many steps in analyzing transcriptome data, from the acquisition of raw data to the selection of a subset of representative genes that explain a scientific hypothesis. The data produced may additionally be integrated with other biological databases, such as Protein-Protein Interactions and annotations. However, the results of these analyses remain fragmented, imposing difficulties, either for posterior inspection of results, or for meta-analysis by the incorporation of new related data. Integrating databases and tools into scientific workflows, orchestrating their execution, and managing the resulting data and its respective metadata are challenging tasks. Running in-silico experiments to structure and compose the information as needed for analysis is a daunting task. Different programs may need to be applied and different files are produced during the experiment cycle. In this context, the availability of a platform supporting experiment execution is paramount. Results: We present GeNNet, an integrated transcriptome analysis platform that unifies scientific workflows with graph databases for selecting relevant genes according to the evaluated biological systems. GeNNet includes pre-loaded biological data, pre-processes raw microarray data and conducts a series of analyses including normalization, differential expression inference, clusterization and geneset enrichment analysis. To demonstrate the features of GeNNet, we performed case studies with data retrieved from GEO, particularly using a single-factor experiment. As a result, we obtained differentially expressed genes for which biological functions were analyzed. The results are integrated into GeNNet-DB, a database about genes, clusters, experiments and their properties and relationships. The resulting graph database is explored with queries that demonstrate the expressiveness of this data model for reasoning about gene regulatory networks. Conclusions: GeNNet is the first platform to integrate the analytical process of transcriptome data with graph database. It provides a comprehensive set of tools that would otherwise be challenging for non-expert users to install and use. Developers as well can add new functionality to each component of GeNNet. The resulting data allows for testing previous hypotheses about an experiment as well as exploring new ones through the interactive graph database environment. It enables the analysis of different data on humans, rhesus, mice and rat coming from Affymetrix platforms."}, {"title": "Systematic assessment of multi-gene predictors of pan-cancer cell line sensitivity to drugs exploiting gene expression data", "url": "https://www.biorxiv.org/content/early/2016/12/18/095224", "tag": "Bioinformatics", "abstract": "Selected gene mutations are routinely used to guide the selection of cancer drugs for a given patient tumour. Large pharmacogenomic data sets were introduced to discover more of these single-gene markers of drug sensitivity. Very recently, machine learning regression has been used to investigate how well cancer cell line sensitivity to drugs is predicted depending on the type of molecular profile. The latter has revealed that gene expression data is the most predictive profile in the pan-cancer setting. However, no study to date has exploited GDSC data to systematically compare the performance of machine learning models based on multi-gene expression data against that of widely-used single-gene markers based on genomics data. Here we present this systematic comparison using Random Forest (RF) classifiers exploiting the expression levels of 13,321 genes and an average of 501 tested cell lines per drug. To account for time-dependent batch effects in IC50 measurements, we employ independent test sets generated with more recent GDSC data than that used to train the predictors and show that this is a more realistic validation than K-fold cross-validation. Across 127 GDSC drugs, our results show that the single-gene markers unveiled by the MANOVA analysis tend to achieve higher precision than these RF-based multi-gene models, at the cost of generally having a poor recall (i.e. correctly detecting only a small part of the cell lines sensitive to the drug). Regarding overall classification performance, about two thirds of the drugs are better predicted by multi-gene RF classifiers. Among the drugs with the most predictive of these models, we found pyrimethamine, sunitinib and 17-AAG."}, {"title": "Benchmarking BarraCUDA on Epigenetic DNA and nVidia Pascal GPUs", "url": "https://www.biorxiv.org/content/early/2016/12/17/095075", "tag": "Bioinformatics", "abstract": "Typically BarraCUDA uses CUDA graphics cards to map DNA reads to the human genome. Previously its software source code was genetically improved for short paired end next generation sequences. On longer, 150 base paired end noisy Cambridge Epigenetix's data, a Pascal GTX 1080 processes about 10000 strings per second, comparable with twin nVidia Tesla K40."}, {"title": "Computational studies of P-glycoprotein polymorphisms in antiepileptic drug resistance mechanisms", "url": "https://www.biorxiv.org/content/early/2016/12/17/095059", "tag": "Bioinformatics", "abstract": "The treatment of epilepsy using antiepileptogenic drugs is frequently complicated by drug resistance, leading to drug failure in more than one-third of cases. Human P-glycoprotein (hPGP), coded by MDR1 and belonging to the ABC superfamily, is a membrane efflux transporter that has been identified as an epileptogenic mediator. The ability of hPGP to bind a broad spectrum of substrates could be implicated in the emergence of drug resistance in epilepsy treatment. Single-nucleotide polymorphisms (SNPs) in MDR1 could compound the aberrant changes in hPGP activity causing and enhancing drug resistance. Bioinformatics approaches were used to assess the functional impact of 20 missense MDR1 polymorphisms and of these, five MDR1 polymorphisms were prioritised for further study. The structures of the wildtype and five mutant hPGP were modelled using the mouse PGP structure as the template. Docking studies of the wildtype and mutant PGP with four standard FDA-aprroved anti-epileptic drugs were carried out. Our results revealed that the drug binding site with respect to the wildtype protein was constant. However the hPGP mutant proteins displayed a repertoire of binding sites with stronger binding affinities towards the drug. Our studies indicate that specific polymorphisms in MDR1 could drive conformational changes of PGP structure, facilitating novel contacts with drug-substrates and eventually transporting the drug out of the cell, leading to pharmacoresistance."}, {"title": "Accuracy in wrist-worn, sensor-based measurements of heart rate and energy expenditure in a diverse cohort", "url": "https://www.biorxiv.org/content/early/2016/12/17/094862", "tag": "Bioinformatics", "abstract": "Background: The ability to measure activity and physiology through wrist-worn devices provides an opportunity for cardiovascular medicine. However, the accuracy of commercial devices is largely unknown. Objective: To assess the accuracy of seven commercially available wrist-worn devices in estimating heart rate (HR) and energy expenditure (EE) and to propose a wearable sensor evaluation framework. Methods: We evaluated the Apple Watch, Basis Peak, Fitbit Surge, Microsoft Band, Mio Alpha 2, PulseOn, and Samsung Gear S2. Participants wore devices while being simultaneously assessed with continuous telemetry and indirect calorimetry while sitting, walking, running, and cycling. Sixty volunteers (29 male, 31 female, age 38 +/- 11 years) of diverse age, height, weight, skin tone, and fitness level were selected. Error in HR and EE was computed for each subject/device/activity combination. Results: Devices reported the lowest error for cycling and the highest for walking. Device error was higher for males, greater body mass index, darker skin tone, and walking. Six of the devices achieved a median error for HR below 5% during cycling. No device achieved an error in EE below 20 percent. The Apple Watch achieved the lowest overall error in both HR and EE, while the Samsung Gear S2 reported the highest. Conclusions: Most wrist-worn devices adequately measure HR in laboratory-based activities, but poorly estimate EE, suggesting caution in the use of EE measurements as part of health improvement programs. We propose reference standards for the validation of consumer health devices (http://precision.stanford.edu/)."}, {"title": "An Integrated Risk Predictor for Pulmonary Nodules", "url": "https://www.biorxiv.org/content/early/2016/12/17/094920", "tag": "Bioinformatics", "abstract": "It is estimated that over 1.5 million lung nodules are detected annually in the United States. Most of these are benign but frequently undergo invasive and costly procedures to rule out malignancy. A risk predictor that can accurately differentiate benign and malignant lung nodules could be used to more efficiently route benign lung nodules to non-invasive observation by CT surveillance and route malignant lung nodules to invasive procedures. The majority of risk predictors developed to date are based exclusively on clinical risk factors, imaging technology or molecular markers. Assessed here are the relative performances of previously reported clinical risk factors and proteomic molecular markers for assessing cancer risk in lung nodules. From this analysis an integrated model incorporating clinical risk factors and proteomic molecular markers is developed and its performance assessed on a previously reported prospective collection of lung nodules that enrolled 475 patients from 12 sites with lung nodules between 8 and 30mm in diameter. In this analysis it is found that the molecular marker is most predictive. However, the integration of clinical and molecular markers is superior to both clinical and molecular markers separately."}, {"title": "DataMed: Finding useful data across multiple biomedical data repositories", "url": "https://www.biorxiv.org/content/early/2016/12/17/094888", "tag": "Bioinformatics", "abstract": "The value of broadening searches for data across multiple repositories has been identified by the biomedical research community. As part of the NIH Big Data to Knowledge initiative, we work with an international community of researchers, service providers and knowledge experts to develop and test a data index and search engine, which are based on metadata extracted from various datasets in a range of repositories. DataMed is designed to be, for data, what PubMed has been for the scientific literature. DataMed supports Findability and Accessibility of datasets. These characteristics - along with Interoperability and Reusability - compose the four FAIR principles to facilitate knowledge discovery in today's big data-intensive science landscape."}, {"title": "Model-based branching point detection in single-cell data by K-Branches clustering", "url": "https://www.biorxiv.org/content/early/2016/12/15/094532", "tag": "Bioinformatics", "abstract": "Motivation: The identification of heterogeneities in cell populations by utilizing single-cell technologies such as single-cell RNA-Seq, enables inference of cellular development and lineage trees. Several methods have been proposed for such inference from high-dimensional single-cell data. They typically assign each cell to a branch in a differentiation trajectory. However, they commonly assume specific geometries such as tree-like developmental hierarchies and lack statistically sound methods to decide on the number of branching events. Results: We present K-Branches, a solution to the above problem by locally fitting half-lines to single-cell data, introducing a clustering algorithm similar to K-Means. These halflines are proxies for branches in the differentiation trajectory of cells. We propose a modified version of the GAP statistic for model selection, in order to decide on the number of lines that best describe the data locally. In this manner, we identify the location and number of subgroups of cells that are associated with branching events and full differentiation, respectively. We evaluate the performance of our method on single-cell RNA-Seq data describing the differentiation of myeloid progenitors during hematopoiesis, single-cell qPCR data of mouse blastocyst development and artificial data."}, {"title": "A computational method for detection of structural variants using Deviant Reads and read pair Orientation: DevRO", "url": "https://www.biorxiv.org/content/early/2016/12/15/094474", "tag": "Bioinformatics", "abstract": "Background: Next generation sequencing (NGS) technology has made it possible to perform high-resolution screens for structural variants. Computational methods for detection of structural variants utilize paired-end mapping information, depth of coverage, split reads, or some combination of such data. The available methods are particularly designed to detect structural variants in single genomes or multiple genomes in a pairwise manner. The aim of this study was to develop a bioinformatics pipeline for detection of large structural variants using multiple pooled populations. Results: Here we describe the method \"DevRO\", developed to enable identification of structural variants using short insert paired-ends and long-range mate-pairs. DevRO uses paired-end mapping information from both types of libraries for identification of inversions, deletions and duplications followed by read depth information to screen for copy number variants. DevRO can detect structural variants in multiple populations without the need for pairwise comparisons. It uses a combined approach based on (i) paired-end mapping and (ii) depth of coverage that gives power to the study as compared to traditional methods that are based on either of these. DevRO is also designed to detect deletions in the reference assembly, which is an added functionality as compared to available methods. Conclusion: We report a bioinformatics pipeline \"DevRO\" for detection of structural variants using paired-end mapping and depth of coverage methods tested on sequencing reads from multiple pooled rabbit populations. This method is useful when large numbers of populations have been re-sequenced as compared to traditional methods that can detect structural variants in a pairwise manner."}, {"title": "MECAT: an ultra-fast mapping, error correction and de novo assembly tool for single-molecule sequencing reads", "url": "https://www.biorxiv.org/content/early/2016/12/15/089250", "tag": "Bioinformatics", "abstract": "The high computational cost of current assembly methods for the long, noisy single molecular sequencing (SMS) reads has prevented them from assembling large genomes. We introduce an ultra-fast alignment method based on a novel global alignment score. For large human SMS data, our method is 7X faster than MHAP for pairwise alignment and 15X faster than BLASR for reference mapping. We develop a Mapping, Error Correction and de novo Assembly Tool (MECAT) by integrating our new alignment and error correction methods, with the Celera Assembler. MECAT is capable of producing high quality de novo assembly of large genome from SMS reads with low computational cost. MECAT produces reference-quality assemblies of Saccharomyces cerevisiae, Arabidopsis thaliana, Drosophila melanogaster and reconstructs the human CHM1 genome with 15% longer NG50 in only 7600 CPU core hours using 54X SMS reads and a Chinese Han genome in 19200 CPU core hours using 102X SMS reads."}, {"title": "BuddySuite: Command-line toolkits for manipulating sequences, alignments, and phylogenetic trees", "url": "https://www.biorxiv.org/content/early/2016/12/15/040675", "tag": "Bioinformatics", "abstract": "The ability to manipulate sequence, alignment, and phylogenetic tree files has become an increasingly important skill in the life sciences, whether to generate summary information or to prepare data for further downstream analysis. The command line can be an extremely powerful environment for interacting with these resources, but only if the user has the appropriate general-purpose tools on hand. BuddySuite is a collection of four independent yet interrelated command-line toolkits that facilitate each step in the workflow of sequence discovery, curation, alignment, and phylogenetic reconstruction. Most common sequence, alignment, and tree file formats are automatically detected and parsed, and over 100 tools have been implemented for manipulating these data. The project has been engineered to easily accommodate the addition of new tools, it is written in the popular programming language Python, and is hosted on the Python Package Index and GitHub to maximize accessibility. Documentation for each BuddySuite tool, including usage examples, is available at http://tiny.cc/buddysuite_wiki. All software is open source and freely available through http://research.nhgri.nih.gov/software/BuddySuite."}, {"title": "Shared Nearest Neighbor clustering in a Locality Sensitive Hashing framework", "url": "https://www.biorxiv.org/content/early/2016/12/15/093898", "tag": "Bioinformatics", "abstract": "We present a new algorithm to cluster high dimensional sequence data, and its application to the field of metagenomics, which aims to reconstruct individual genomes from a mixture of genomes sampled from an environmental site, without any prior knowledge of reference data (genomes) or the shape of clusters. Such problems typically cannot be solved directly with classical approaches seeking to estimate the density of clusters, e.g., using the shared nearest neighbors rule, due to the prohibitive size of contemporary sequence datasets. We explore here a new method based on combining the shared nearest neighbor (SNN) rule with the concept of Locality Sensitive Hashing (LSH). The proposed method, called LSH-SNN, works by randomly splitting the input data into smaller-sized subsets (buckets) and employing the shared nearest neighbor rule on each of these buckets. Links can be created among neighbors sharing a sufficient number of elements, hence allowing clusters to be grown from linked elements. LSH-SNN can scale up to larger datasets consisting of millions of sequences, while achieving high accuracy across a variety of sample sizes and complexities."}, {"title": "Domain prediction with probabilistic directional context", "url": "https://www.biorxiv.org/content/early/2016/12/14/094284", "tag": "Bioinformatics", "abstract": "Motivation: Protein domain prediction is one of the most powerful approaches for sequence-based function prediction. While domain instances are typically predicted independently of each other, newer approaches have demonstrated improved performance by rewarding domain pairs that frequently co-occur within sequences. However, most of these approaches have ignored the order in which domains preferentially co-occur and have also not modeled domain co-occurrence probabilistically. Results: We introduce a probabilistic approach for domain prediction that models \"directional\" domain context. Our method is the first to score all domain pairs within a sequence while taking their order into account, even for non-sequential domains. We show that our approach extends a previous Markov model-based approach to additionally score all pairwise terms, and that it can be interpreted within the context of Markov random fields. We formulate our underlying combinatorial optimization problem as an integer linear program, and demonstrate that it can be solved quickly in practice. Finally, we perform extensive evaluation of domain context methods and demonstrate that incorporating context increases the number of domain predictions by ~15%, with our approach dPUC2 (Domain Prediction Using Context) outperforming all competing approaches. Availability: dPUC2 is available at http://github.com/alexviiia/dpuc2 ."}, {"title": "Deep learning is effective for the classification of OCT images of normal versus Age-related Macular Degeneration", "url": "https://www.biorxiv.org/content/early/2016/12/14/094276", "tag": "Bioinformatics", "abstract": "Objective: The advent of Electronic Medical Records (EMR) with large electronic imaging databases along with advances in deep neural networks with machine learning has provided a unique opportunity to achieve milestones in automated image analysis. Optical coherence tomography (OCT) is the most commonly obtained imaging modality in ophthalmology and represents a dense and rich dataset when combined with labels derived from the EMR. We sought to determine if deep learning could be utilized to distinguish normal OCT images from images from patients with Age-related Macular Degeneration (AMD). Design: EMR and OCT database study Subjects: Normal and AMD patients who had a macular OCT. Methods: Automated extraction of an OCT imaging database was performed and linked to clinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg Spectralis, and each OCT scan was linked to EMR clinical endpoints extracted from EPIC. The central 11 images were selected from each OCT scan of two cohorts of patients: normal and AMD. Cross-validation was performed using a random subset of patients. Receiver operator curves (ROC) were constructed at an independent image level, macular OCT level, and patient level. Main outcome measure: Area under the ROC. Results: Of a recent extraction of 2.6 million OCT images linked to clinical datapoints from the EMR, 52,690 normal macular OCT images and 48,312 AMD macular OCT images were selected. A deep neural network was trained to categorize images as either normal or AMD. At the image level, we achieved an area under the ROC of 92.78% with an accuracy of 87.63%. At the macula level, we achieved an area under the ROC of 93.83% with an accuracy of 88.98%. At a patient level, we achieved an area under the ROC of 97.45% with an accuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were 92.64% and 93.69% respectively. Conclusions: Deep learning techniques achieve high accuracy and is effective as a new image classification technique. These findings have important implications in utilizing OCT in automated screening and the development of computer aided diagnosis tools in the future."}, {"title": "Machine-learning annotation of human splicing branchpoints", "url": "https://www.biorxiv.org/content/early/2016/12/14/094003", "tag": "Bioinformatics", "abstract": "Background: The branchpoint element is required for the first lariat-forming reaction in splicing. However due to difficulty in experimentally mapping at a genome-wide scale, current catalogues are incomplete. Results: We have developed a machine-learning algorithm trained with empirical human branchpoint annotations to identify branchpoint elements from primary genome sequence alone. Using this approach, we can accurately locate branchpoints elements in 85% of introns in current gene annotations. Consistent with branchpoints as basal genetic elements, we find our annotation is unbiased towards gene type and expression levels. A major fraction of introns was found to encode multiple branchpoints raising the prospect that mutational redundancy is encoded in key genes. We also confirmed all deleterious branchpoint mutations annotated in clinical variant databases, and further identified thousands of clinical and common genetic variants with similar predicted effects. Conclusions: We propose the broad annotation of branchpoints constitutes a valuable resource for further investigations into the genetic encoding of splicing patterns, and interpreting the impact of common- and disease-causing human genetic variation on gene splicing."}, {"title": "On expert curation and sustainability: UniProtKB/Swiss-Prot as a case study", "url": "https://www.biorxiv.org/content/early/2016/12/14/094011", "tag": "Bioinformatics", "abstract": "Biological knowledgebases, such as UniProtKB/Swiss-Prot, constitute an essential component of daily scientific research by offering distilled, summarized, and computable knowledge extracted from the literature by expert curators. While knowledgebases play an increasingly important role in the scientific community, the question of their sustainability is raised due to the growth of biomedical literature. By using UniProtKB/Swiss-Prot as a case study, we address this question by using different literature triage approaches. With the assistance of the PubTator text-mining tool, we tagged more than 10,000 articles to assess the ratio of papers relevant for curation. We first show that curators read and evaluate many more papers than they curate, and that measuring the number of curated publications is insufficient to provide a complete picture. We show that a large fraction of published papers found in PubMed is not relevant for curation in UniProtKB/Swiss-Prot and demonstrate that, despite appearances, expert curation is sustainable."}, {"title": "Disentangled Long-Read De Bruijn Graphs via Optical Maps", "url": "https://www.biorxiv.org/content/early/2016/12/14/094235", "tag": "Bioinformatics", "abstract": "Pacific Biosciences (PacBio), the main third generation sequencing technology can produce scalable, high-throughput, unprecedented sequencing results through long reads with uniform coverage. Although these long reads have been shown to increase the quality of draft genomes in repetitive regions, fundamental computational challenges remain in overcoming their high error rate and assembling them efficiently. In this paper we show that the de Bruijn graph built on the long reads can be efficiently and substantially disentangled using optical mapping data as auxiliary information. Fundamental to our approach is the use of the positional de Bruijn graph and a succinct data structure for constructing and traversing this graph. Our experimental results show that over 97.7% of directed cycles have been removed from the resulting positional de Bruijn graph as compared to its non-positional counterpart. Our results thus indicate that disentangling the de Bruijn graph using positional information is a promising direction for developing a simple and efficient assembly algorithm for long reads."}, {"title": "HICL table can manipulate all proteins in human complete proteome", "url": "https://www.biorxiv.org/content/early/2016/12/14/093971", "tag": "Bioinformatics", "abstract": "Background The data of human complete proteome in the databases of Universal Protein Resource (UniProt) or National Center for Biotechnology Information(NCBI) were disorderly organized and hardly handled by an ordinary biologist. Results The HICL table enable an ordinary biologist efficiently to handle the human complete proteome with 67911 entries, to get an overview on the distribution of the physicochemical features of all proteins in the human complete proteome, to perceive the details of the distribution patterns of the physicochemical features in some protein family members and protein variants, to find some particular proteins. Moreover, two discoveries were made via the HICL table: (1) The amino aicds(Asp,Glu) have symmetrical trend of the distributions versus pI, but the amino aicds(Arg, Lys) have local asymmetrical trend of the distributions versus pI in human complete proteome. (2) Protein sequence, besides amino acid properties, can in theory influence the modal distribution of protein isoelectric points. Conclusion I has created the HICL table as a robust tool for orderly managing 67911 proteins in human complete proteome by their physicochemical features, the names and sequences. Any proteins with the particular physicochemical features can be screened out from the human complete proteome via the HICL table. In addition, the unbalanced distribution of the amino aicds(Arg, Lys) in high pI proteins of human complete proteome and the effect of protein sequence on modal distribution of protein isoelectric points have been discovered through the HICL table."}, {"title": "Easy and Accurate Reconstruction of Whole HIV Genomes from Short-Read Sequence Data", "url": "https://www.biorxiv.org/content/early/2016/12/13/092916", "tag": "Bioinformatics", "abstract": "Next-generation sequencing has yet to be widely adopted for HIV. The difficulty of accurately reconstructing the consensus sequence of a quasispecies from reads (short fragments of DNA) in the presence of rapid between- and within-host evolution may have presented a barrier. In particular, mapping (aligning) reads to a reference sequence leads to biased loss of information; this bias can distort epidemiological and evolutionary conclusions. De novo assembly avoids this bias by effectively aligning the reads to themselves, producing a set of sequences called contigs. However contigs provide only a partial summary of the reads, misassembly may result in their having an incorrect structure, and no information is available at parts of the genome where contigs could not be assembled. To address these problems we developed the tool shiver to preprocess reads for quality and contamination, then map them to a reference tailored to the sample using corrected contigs supplemented with existing reference sequences. Run with two commands per sample, it can easily be used for large heterogeneous data sets. We use shiver to reconstruct the consensus sequence and minority variant information from paired-end short-read data produced with the Illumina platform, for 65 existing publicly available samples and 50 new samples. We show the systematic superiority of mapping to shiver's constructed reference over mapping the same reads to the standard reference HXB2: an average of 29 bases per sample are called differently, of which 98.5% are supported by higher coverage. We also provide a practical guide to working with imperfect contigs."}, {"title": "DiffTAD: Detecting Differential contact frequency in Topologically Associating Domains Hi-C experiments between conditions", "url": "https://www.biorxiv.org/content/early/2016/12/13/093625", "tag": "Bioinformatics", "abstract": "Motivation: In recent years, the interest in analyzing chromosome conformation by Hi-C and related techniques has grown. It has been shown that contact frequency matrices obtained by these methods cor- relate with other methods of measurement of activity such as transcrip- tomics and histone modification assays. This brings a question of testing for differential contact frequency between experiments to the field. Results: In this work, we provide a freely available software that imple- ments two statistical methods for testing the significance of differential contact frequency in topological domains between two experiments. One method follows an empirical, permutation based approach to computing p- values, while the other is a parametric test based on the Poisson-Binomial distribution. Availability: The software is freely available on the GNU General Public License at https://bitbucket.org/rzaborowski/differential-analysis"}, {"title": "VarMatch: robust matching of small variant datasets using flexible scoring schemes", "url": "https://www.biorxiv.org/content/early/2016/12/13/062943", "tag": "Bioinformatics", "abstract": "Small variant calling is an important component of many analyses, and, in many instances, it is important to determine the set of variants which appear in multiple callsets. Variant matching is complicated by variants that have multiple equivalent representations. Normalization and decomposition algorithms have been proposed, but are not robust to different representation of complex variants. Variant matching is also usually done to maximize the number of matches, as opposed to other optimization criteria. We present the VarMatch algorithm for the variant matching problem. Our algorithm is based on a theoretical result which allows us to partition the input into smaller subproblems without sacrificing accuracy. VarMatch is robust to different representation of complex variants and is particularly effective in low complexity regions or those dense in variants. It also implements different optimization criteria, such as edit distance, that can lead to different results and affect conclusions about algorithm performance. Finally, the VarMatch software provides summary statistics, annotations, and visualizations that are useful for understanding callers' performance. Availability: VarMatch is freely available at: https://github.com/medvedevgroup/varmatch"}, {"title": "Simple adjustment of the sequence weight algorithm remarkably enhances PSI-BLAST performance", "url": "https://www.biorxiv.org/content/early/2016/12/13/092742", "tag": "Bioinformatics", "abstract": "PSI-BLAST, an extremely popular tool for sequence similarity search, features the utilization of Position Specific Scoring Matrix (PSSM) constructed from a multiple sequence alignment (MSA). PSSM allows the detection of more distant homologs than a general amino acid substitution matrix does. An accurate estimation of the weights of sequences in an MSA is crucially important for PSSM construction. PSI-BLAST divides a given MSA into multiple blocks, for which sequence weights are calculated. When the block width becomes very narrow, the sequence weight calculation can be difficult. We demonstrate that PSI-BLAST indeed generates a significant fraction of blocks having widths less than 5, thereby degrading the PSI-BLAST performance. We revised the code of PSI-BLAST to prevent the blocks from being narrower than a given minimum block width (MBW). We designate the modified application of PSI-BLAST as PSI-BLASTexB. When MBW is 25, PSI-BLASTexB notably outperforms PSI-BLAST consistently for three independent benchmark sets. The performance boost is even more drastic when an MSA, instead of a sequence, was used as a query. Our results demonstrate that the generation of narrow-width blocks during the sequence weight calculation is a critically important factor that restricts the PSI-BLAST search performance. By preventing narrow blocks, PSI-BLASTexB remarkably upgrades the PSI-BLAST performance."}, {"title": "http://dx.doi.org/10.1101/079087 CONTINUATION: Evaluation of adaptive somatic models in a gold standard whole genome somatic dataset", "url": "https://www.biorxiv.org/content/early/2016/12/13/093534", "tag": "Bioinformatics", "abstract": "In http://dx.doi.org/10.1101/079087, we presented adaptive models for calling somatic mutations in high-throughput sequencing data. These models were developed by training deep neural networks with semi-simulated data. In this continuation, I evaluate how such models can predict known somatic mutations in a real dataset. To address this question, I tested the approach using samples from the International Cancer Genome Consortium (ICGC) and the previously published ground-truth mutations (GoldSet). This evaluation revealed that training models with semi-simulation does produce models that exhibit strong performance in real datasets. I found a linear relationship between the performance observed on a semi-simulated validation set and independent ground-truth in the gold set (r^2=0.952, P<2-16). I also found that semi-simulation can be used to pre-train models before continuing training with true labels and that this pre-training improves model performance substantially on the real dataset compared to training models only with the real dataset. The best model pre-trained with semi-simulation achieved an AUC of 0.969 [0.957-0.982] (95% confidence interval) compared to 0.911 [0.890-0.932] when training with real labels only. These data demonstrate that semi-simulation can be a very effective approach to training filtering and ranking probabilistic models."}, {"title": "Correcting Chimeric Crosstalk in Single Cell RNA-seq Experiments", "url": "https://www.biorxiv.org/content/early/2016/12/12/093237", "tag": "Bioinformatics", "abstract": "As part of the process of preparing scRNA-seq libraries, a diverse template is typically amplified by PCR. During amplification, spurious chimeric molecules can be formed between molecules originating in different cells. While several computational and experimental strategies have been suggested to mitigate the impact of chimeric molecules, they have not been addressed in the context of scRNA-seq experiments. We demonstrate that chimeras become increasingly problematic as samples are sequenced deeply and propose two computational solutions. The first is unsupervised and relies only on cell barcode and UMI information. The second is a supervised approach built on labeled data and a set of molecule specific features. The classifier can accurately identify most of the contaminating molecules in a deeply sequenced species mixing scRNA-seq dataset. Code is publicly available at https://github.com/asncd/schimera."}, {"title": "Designing fecal microbiota transplant trials that account for differences in donor stool efficacy", "url": "https://www.biorxiv.org/content/early/2016/12/12/065383", "tag": "Bioinformatics", "abstract": "Fecal microbiota transplantation (FMT) is a highly effective intervention for patients suffering from recurrent Clostridium difficile, a common hospital-acquired infection. FMT's success as a therapy for C. difficile has inspired interest in performing clinical trials that experiment with FMT as a therapy for conditions like inflammatory bowel disease, obesity, diabetes, and Parkinson's disease. Results from clinical trials that use FMT to treat inflammatory bowel disease suggest that, for at least one condition beyond C. difficile, most FMT donors produce stool that is not efficacious. The optimal strategies for identifying and using efficacious donors have not been investigated. We therefore formulated an optimal Bayesian response-adaptive donor selection strategy and a computationally-tractable myopic heuristic. This algorithm computes the probability that a donor is efficacious by updating prior expectations about the efficacy of FMT, the placebo rate, and the fraction of donors that are efficacious. In simulations designed to mimic a recent FMT clinical trial, for which traditional power calculations predict ~100% statistical power, we found that accounting for differences in donor efficacy reduced the predicted statistical power to ~9%. For these simulations, using the Bayesian allocation strategy more than quadrupled the statistical power to ~39%. We use the results of similar simulations to make recommendations about the number of patients, number of donors, and choice of clinical endpoint that clinical trials should use to optimize their ability to detect if FMT is effective for treating a condition."}, {"title": "BioMake: a GNU Make-compatible utility for declarative workflow management", "url": "https://www.biorxiv.org/content/early/2016/12/12/093245", "tag": "Bioinformatics", "abstract": "The Unix \"make\" program is widely used in bioinformatics pipelines, but suffers from problems that limit its application to large analysis datasets. These include reliance on file modification times to determine whether a target is stale, lack of support for parallel execution on clusters, and restricted flexibility to extend the underlying logic program. We present BioMake, a make-like utility that is compatible with most features of GNU Make and adds support for popular cluster-based job-queue engines, MD5 signatures as an alternative to timestamps, and logic programming extensions in Prolog. BioMake is available from https://github. com/evoldoers/biomake under the Creative Commons Attribution 3.0 US license. The only dependency is SWI-Prolog, available from http://www. swi-prolog.org/. Contact: Ian Holmes ihholmes+biomake@gmail.com or Chris Mungall cmungall+biomake@gmail.com."}, {"title": "Historian: accurate reconstruction of ancestral sequences and evolutionary rates", "url": "https://www.biorxiv.org/content/early/2016/12/11/093161", "tag": "Bioinformatics", "abstract": "Reconstruction of ancestral sequence histories, and estimation of parameters like indel rates, are improved by using explicit evolutionary models and summing over uncertain alignments. The previous best tool for this purpose (according to simulation benchmarks) was ProtPal, but this tool was too slow for practical use. Historian combines an efficient reimplementation of the ProtPal algorithm with performance-improving heuristics from other alignment tools. Simulation results on fidelity of rate estimation via ancestral reconstruction, along with evaluations on the structurally-informed alignment dataset BAliBase 3.0, recommend Historian over other alignment tools for evolutionary applications. Historian is available at https://github.com/ihh/indelhistorian under the Creative Commons Attribution 3.0 US license. Contact: Ian Holmes ihholmes+historian@gmail.com."}, {"title": "Cox-nnet: an artificial neural network Cox regression for prognosis prediction", "url": "https://www.biorxiv.org/content/early/2016/12/11/093021", "tag": "Bioinformatics", "abstract": "Artificial neural networks (ANN) are computing architectures with massively parallel interconnections of simple neurons and has been applied to biomedical fields such as imaging analysis and diagnosis. We have developed a new ANN framework called Cox-nnet to predict patient prognosis from high throughput transcriptomics data. In over 10 TCGA RNA-Seq data sets, Cox-nnet achieves a statistically significant increase in predictive accuracy, compared to the other three methods including Cox-proportional hazards (Cox-PH), Random Forests Survival and CoxBoost. Cox-nnet also offers richer biological information, from both pathway and gene levels. The outputs from the hidden layer node can be utilized as a new approach for survival-sensitive dimension reduction. In summary, we have developed a new method for more accurate and efficient prognosis prediction, with functional biological insights. The source code is freely available at https://github.com/lanagarmire/cox-nnet."}, {"title": "SNVPhyl: A Single Nucleotide Variant Phylogenomics pipeline for microbial genomic epidemiology", "url": "https://www.biorxiv.org/content/early/2016/12/10/092940", "tag": "Bioinformatics", "abstract": "Motivation: The recent widespread application of whole-genome sequencing (WGS) for microbial disease investigations has spurred the development of new bioinformatics tools, including a notable proliferation of phylogenomics pipelines designed for infectious disease surveillance and outbreak investigation. Transitioning the use of WGS data out of the research lab and into the front lines of surveillance and outbreak response requires user-friendly, reproducible, and scalable pipelines that have been well validated. Results: SNVPhyl (Single Nucleotide Variant Phylogenomics) is a bioinformatics pipeline for identifying high-quality SNVs and constructing a whole genome phylogeny from a collection of WGS reads and a reference genome. Individual pipeline components are integrated into the Galaxy bioinformatics framework, enabling data analysis in a user-friendly, reproducible, and scalable environment. We show that SNVPhyl can detect SNVs with high sensitivity and specificity and identify and remove regions of high SNV density (indicative of recombination). SNVPhyl is able to correctly distinguish outbreak from non-outbreak isolates across a range of variant-calling settings, sequencing-coverage thresholds, or in the presence of contamination. Availability: SNVPhyl is available as a Galaxy workflow, Docker and virtual machine images, and a Unix-based command-line application. SNVPhyl is released under the Apache 2.0 license and available at http://snvphyl.readthedocs.io/ or at https://github.com/phac-nml/snvphyl-galaxy."}, {"title": "High-dimensional regression over disease subgroups", "url": "https://www.biorxiv.org/content/early/2016/12/09/092825", "tag": "Bioinformatics", "abstract": "We consider high-dimensional regression over subgroups of observations. Our work is motivated by biomedical problems, where disease subtypes, for example, may differ with respect to underlying regression models, but sample sizes at the subgroup-level may be limited. We focus on the case in which subgroup-specific models may be expected to be similar but not necessarily identical. Our approach is to treat subgroups as related problem instances and jointly estimate subgroup-specific regression coefficients. This is done in a penalized framework, combining an l1 term with an additional term that penalizes differences between subgroup-specific coefficients. This gives solutions that are globally sparse but that allow information-sharing between the subgroups. We present algorithms for estimation and empirical results on simulated data and using Alzheimer's disease, amyotrophic lateral sclerosis and cancer datasets. These examples demonstrate the gains our approach can offer in terms of prediction and the ability to estimate subgroup-specific sparsity patterns."}, {"title": "LOCALIZER: subcellular localization prediction of plant and effector proteins in the plant cell", "url": "https://www.biorxiv.org/content/early/2016/12/09/092726", "tag": "Bioinformatics", "abstract": "Pathogens are able to deliver effector proteins into plant cells to enable infection. Some effectors have been found to enter subcellular compartments by mimicking host targeting sequences. Although many computational methods exist to predict plant protein subcellular localization, they perform poorly for effectors. We introduce LOCALIZER for predicting plant and effector protein localization to chloroplasts, mitochondria, and nuclei. LOCALIZER shows greater prediction accuracy for chloroplast and mitochondrial targeting compared to other methods for 652 plant proteins. For 108 eukaryotic effectors, LOCALIZER outperforms other methods and predicts a previously unrecognized chloroplast transit peptide for the ToxA effector, which we show translocates into tobacco chloroplasts. Secretome-wide predictions and confocal microscopy reveal that rust fungi might have evolved multiple effectors that target chloroplasts or nuclei. LOCALIZER is the first method for predicting effector localisation in plants and is a valuable tool for prioritizing effector candidates for functional investigations. LOCALIZER is available at http://localizer.csiro.au/."}, {"title": "Characterization of Dependencies Between Growth and Division in Budding Yeast", "url": "https://www.biorxiv.org/content/early/2016/12/08/082735", "tag": "Bioinformatics", "abstract": "Cell growth and division are processes vital to the proliferation and development of life. Coordination between these two processes has been recognized for decades in a variety of organisms. In the budding yeast Saccharomyces cerevisiae, this coordination or 'size control' appears as an inverse correlation between cell size and the rate of cell-cycle progression, routinely observed in G1 prior to cell division commitment. Beyond this point, cells are presumed to complete S/G2/M at similar rates and in a size-independent manner. As such, studies of dependence between growth and division have focused on G1. Moreover, coordination between growth and division has commonly been analyzed within the cycle of a single cell without accounting for correlations in growth and division characteristics between cycles of related cells. In a comprehensive analysis of three published time-lapse microscopy datasets, we analyze both intra- and inter-cycle dependencies between growth and division, revisiting assumptions about the coordination between these two processes. Interestingly, we find evidence (1) that S/G2/M durations are systematically longer in daughters than in mothers, (2) of dependencies between S/G2/M and size at budding that echo the classical G1 dependencies, and, (3) in contrast with recent bacterial studies, of negative dependencies between size at birth and size accumulated during the cell cycle. In addition, we develop a novel hierarchical model to uncover inter-cycle dependencies, and we find evidence for such dependencies in cells growing in sugar-poor environments. Our analysis highlights the need for experimentalists and modelers to account for new sources of cell-to-cell variation in growth and division, and our model provides a formal statistical framework for the continued study of dependencies between biological processes."}, {"title": "Detection and characterization of low and high genome coverage regions using an efficient running median and a double threshold approach.", "url": "https://www.biorxiv.org/content/early/2016/12/08/092478", "tag": "Bioinformatics", "abstract": "Motivation: Next Generation Sequencing (NGS) provides researchers with powerful tools to investigate both prokaryotic and eukaryotic genetics. An accurate assessment of reads mapped to a specific genome consists of inspecting the genome coverage as number of reads mapped to a specific genome location. Most current methods use the average of the genome coverage (sequencing depth) to summarize the overall coverage. This metric quickly assess the sequencing quality but ignores valuable biological information like the presence of repetitive regions or deleted genes. The detection of such information may be challenging due to a wide spectrum of heterogeneous coverage regions, a mixture of underlying models or the presence of a non-constant trend along the genome. Using robust statistics to systematically identify genomic regions with unusual coverage is needed to characterize these regions more precisely. Results: We implemented an efficient running median algorithm to estimate the genome coverage trend. The distribution of the normalized genome coverage is then estimated using a Gaussian mixture model. A z-score statistics is then assigned to each base position and used to separate the central distribution from the regions of interest (ROI) (i.e., under- and over-covered regions). Finally, a double threshold mechanism is used to cluster the genomic ROIs. HTML reports provide a summary with interactive visual representations of the genomic ROIs. Availability: An implementation of the genome coverage characterization is available within the Sequana project. The standalone application is called sequana_coverage. The source code is available on GitHub (http://github.com/sequana/sequana), and documentation on ReadTheDocs (http://sequana.readtheodcs.org). An example of HTML report is provided on http://sequana.github.io ."}, {"title": "Complete fold annotation of the human proteome using a novel structural feature space", "url": "https://www.biorxiv.org/content/early/2016/12/07/092379", "tag": "Bioinformatics", "abstract": "Recognition of protein structural fold is the starting point for many structure prediction tools and protein function inference. Fold prediction is computationally demanding and recognizing novel folds is difficult such that the majority of proteins have not been annotated for fold classification. Here we describe a new machine learning approach using a novel feature space that can be used for accurate recognition of all 1,221 currently known folds and inference of unknown novel folds. We show that our method achieves better than 96% accuracy even when many folds have only one training example. We demonstrate the utility of this method by predicting the folds of 34,330 human protein domains and showing that these predictions can yield useful insights into biological function, including the prediction of functional motifs relevant to human diseases. Our method can be applied to de novo fold prediction of entire proteomes and identify candidate novel fold families."}, {"title": "Optimized implementations of voxel-wise degree centrality and local functional connectivity density mapping in AFNI", "url": "https://www.biorxiv.org/content/early/2016/12/07/067702", "tag": "Bioinformatics", "abstract": "Degree centrality (DC) and local functional connectivity density (lFCD) are statistics calculated from brain connectivity graphs that measure how important a brain region is to the graph. DC (a.k.a. global functional connectivity density) is calculated as the number of connections a region has with the rest of the brain (binary DC), or the sum of weights for those connections (weighted DC). lFCD was developed to be a surrogate measure of DC that is faster to calculate by restricting its computation to regions that are spatially adjacent. Although both of these measures are popular for investigating inter-individual variation in brain connectivity, efficient neuroimaging tools for computing them are scarce. The goal of this Brainhack project was to contribute optimized implementations of these algorithms to the widely used, open source, AFNI software package."}, {"title": "Metacoder: An R Package for Visualization and Manipulation of Community Taxonomic Diversity Data", "url": "https://www.biorxiv.org/content/early/2016/12/07/071019", "tag": "Bioinformatics", "abstract": "Community-level data, the type generated by an increasing number of metabarcoding studies, is often graphed as stacked bar charts or pie graphs that use color to represent taxa. These graph types do not convey the hierarchical structure of taxonomic classifications and are limited by the use of color for categories. As an alternative, we developed metacoder, an R package for easily parsing, manipulating, and graphing publication-ready plots of hierarchical data. Metacoder includes a dynamic and flexible function that can parse most text-based formats that contain taxonomic classifications, taxon names, taxon identifiers, or sequence identifiers. Metacoder can then subset, sample, and order this parsed data using a set of intuitive functions that take into account the hierarchical nature of the data. Finally, an extremely flexible plotting function enables quantitative representation of up to 4 arbitrary statistics simultaneously in a tree format by mapping statistics to the color and size of tree nodes and edges. Metacoder also allows exploration of barcode primer bias by integrating functions to run digital PCR. Although it has been designed for data from metabarcoding research, metacoder can easily be applied to any data that has a hierarchical component such as gene ontology or geographic location data. Our package complements currently available tools for community analysis and is provided open source with an extensive online user manual."}, {"title": "Characterizing Xenopus tropicalis endurance capacities with multilevel transcriptomics", "url": "https://www.biorxiv.org/content/early/2016/12/02/091280", "tag": "Bioinformatics", "abstract": "Vertebrate endurance capacity is a phenotype with considerable genetic heterogeneity. RNA-Seq technologies are an ideal tool to investigate the involved genes and processes, but several challenges exist when the phenotype of interest has a complex genetic background. Difficulties manifest at the level of results interpretation because commonly used statistical methods are designed to identify strongly associated genes. If an observed phenotype can be achieved though multiple distinct genetic mechanisms then typical gene-centric methods come with the attached risk that signal may be lost or misconstrued. Gene set analysis (GSA) methods are now widely accepted as a means to address some of the shortcomings of gene-by-gene analysis methods. We carry out both gene level and gene set level analyses on Xenopus tropicalis to identify the genetic factors that contribute to endurance heterogeneity. A typical workflow might consider gene level and pathway level analyses, but in this work we propose an additional focus at the intermediate level of functional modules. We generate functional modules for GSA testing in order to be explicit in how ontology information is used with respect to the functional genomics of Xenopus. Additionally, we make use of multiple assemblies to corroborate implicated genes and processes. We identified 42 core genes, 10 functional modules, and 14 pathways based on gene expression differences between endurant and non-endurant frogs. The majority of the genes and processes are readily associated with muscle contraction or catabolism. A substantial number of these genes are involved in lipid metabolic processes, suggesting an important role in frog endurance heterogeneity. Unsurprisingly, many of the gene expression differences between endurant and non-endurant frogs can be distilled down to the capacity to utilize substrate for energy, but at the individual level frogs appear to make use of diverse machinery to achieve these differences."}, {"title": "Predicting off-target effects for end-to-end CRISPR guide design", "url": "https://www.biorxiv.org/content/early/2016/12/02/078253", "tag": "Bioinformatics", "abstract": "The CRISPR-Cas9 system provides unprecedented genome editing capabilities. However, off-target effects lead to sub-optimal usage and additionally are a bottleneck in development of therapeutic uses. Herein, we introduce the first machine learning-based approach to this problem, yielding a state-of-the-art predictive model for CRISPR-Cas9 off-target effects which outperforms all other guide design services. Our approach, Elevation, consists of two inter-related machine learning models--one for scoring individual guide-target pairs and another which aggregates guide-target scores into a single, overall guide summary score. Through systematic investigation, we demonstrate that Elevation performs substantially better than competing approaches on both of these tasks. Additionally, we are the first to systematically evaluate approaches on the guide summary score problem; we show that the most widely-used method (and one re-implemented by several other servers) performs no better than random at times, whereas Elevation consistently outperformed it, sometimes by an order of magnitude. In our analyses, we also introduce a method to balance errors on truly active guides with those which are truly inactive, encapsulating a range of practical use cases, thereby showing that Elevation is consistently superior across the entire range. We thus contribute a new evaluation metric for benchmarking off-target modeling. Finally, because of the large computational demands of our tasks, we have developed a cloud-based service for end-to-end guide design which incorporates our previously reported on-target model, Azimuth, as well as our new off-target model, Elevation."}, {"title": "Entropy and codon bias in HIV-1", "url": "https://www.biorxiv.org/content/early/2016/12/02/052274", "tag": "Bioinformatics", "abstract": "For the heterologous gene expression systems, the codon bias has to be optimized according to the host for efficient expression. Although DNA viruses show a correlation on codon bias with their hosts, HIV genes show low correlation for both nucleotide composition and codon usage bias with its human host which limits the efficient expression of HIV genes. Despite this variation, HIV is efficient at infecting hosts and multiplying in large number. In this study, first, the degree of codon adaptation is calculated as codon adaptation index (CAI) and compared with the expected threshold value (eCAI) determined from the sequences with the same nucleotide composition as that of the HIV-1 genome. Then, information theoretic analysis of nine genes of HIV-1 based on codon statistics of the HIV-1 genome, individual genes and codon usage of human genes is done. Comparison of codon adaptation indices with their respective threshold values shows that the CAI lies very close to the threshold values. Despite not being well adapted to the codon usage bias of human hosts, it was found that the Shannon entropies of the nine genes based on overall codon statistics of HIV-1 genome are very similar to the entropies calculated from codon usage of human genes. Similarly, for the HIV-1 genome sequence analyzed, the codon statistics of the third reading frame has the highest bias representing minimum entropy and hence the maximum information."}, {"title": "Tumor Origin Detection with Tissue-Specific miRNA and DNA methylation Markers", "url": "https://www.biorxiv.org/content/early/2016/12/01/090746", "tag": "Bioinformatics", "abstract": "Motivation: Cancer of unknown primary origin constitutes 3-5% of all human malignancies. Pa-tients with these carcinomas present with metastases without an established primary site, which may not be found even by thorough histological search methods. Patients with cancer of unknown primary origin always have poor prognosis and hardly have efficient treatment since most cancers respond well to specific chemotherapy or hormone drugs. Many studies have proposed classifiers based on miRNAs or mRNAs to predict the tumor origins, but few study focus on high-dimensional DNA methylation profiles. Results: We introduced three classifiers with novel feature selection algorithm combined with ran-dom forest to effectively identify highly tissue-specific epigenetics biomarkers such as microRNAs and CpG sites, which can help us predict the origin site of tumors. This algorithm, incorporating differential analysis and descending dimension algorithm, was applied on 14 histological tissues and over 5000 samples based on miRNA expression and DNA methylation profiles to assign given primary tumor to its origin tissue. Our study shows all of these three classifiers have an overall ac-curacy of 87.78% (72.55%-97.54%) based on miRNA datasets and an accuracy of 96.43% (MRMD: 87.85%-99.76%) or 97.06% (PCA: 92.44%-100%) based on DNA methylation datasets on predict-ing the origin of tumors and suggests that the biomarkers we selected can efficiently predict the origin of tumors and allow the clinicians to avoid adjuvant systemic therapy or to choose less ag-gressive therapeutic options. We also developed a user-friendly webserver which enables users to predict the origin site of tumors by uploading the miRNAs expression or DNA methylation profiles of those cancers. Availability: The webserver, data, and code are accessible free of charge at http://server.malab.cn/MMCOP/ Contact: zouquan@nclab.net Supplementary information: Supplementary data are available at Bioinformatics online."}, {"title": "The unbalanced distribution of the amino aicds(Arg, Lys) in high pI proteins of complete proteomes, its origin and evolution", "url": "https://www.biorxiv.org/content/early/2016/12/01/090290", "tag": "Bioinformatics", "abstract": "No evolutionary signature has been found in the complete proteomes of eukaryotes by now, although amino acid composition signature of the complete proteomes was discovered as molecular signature of the adaptation for thermophiles and halophiles. Arginine and lysine respectively have the guanidinium group and the \u03b5-amino group as the ionizable side chain groups with different pKa values of about 12.5 and 10.5. The trends of their distribution seem similar in the range of about pIs\ufe6410.0 and diverge in the range of about pIs\u226510.0 in most complete proteomes of 387 species from the three domains of life. The complete proteome of Reticulomyxa filose is the one of only in 287 eukaryotic complete proteomes that has a predominance of the trend of lysine over that of arginine in high pI proteins. The unbalanced distribution of the amino aicds(Arg, Lys) in high pI proteins of complete proteomes may originally come from different pKa values of arginine and lysine and be developed by the influences of an average lysine level of a proteome and evolution. Because of this unbalanced distribution, the pattern of arginine and lysine distribution in high pI proteins of some complete proteomes can form a particular proteomic structure as evolutionary signature that may be shaped by massive natural selection in molecular level from hundreds to ten thousands of proteins in the complete proteomes of many animals and green plants."}, {"title": "A comparative microarray analysis identifies a conserved gene expression signature between airway injury and lung cancer", "url": "https://www.biorxiv.org/content/early/2016/11/30/090605", "tag": "Bioinformatics", "abstract": "Lung squamous cell carcinoma (SqCC) accounts for 30% of lung cancers, with over 400,000 deaths per year worldwide. Although evidence suggests that chronic lung injury drives carcinogenesis, a comprehensive understanding of this process remains elusive. Here, I used a comparative microarray analysis to identify gene expression differences shared between airway injury and squamous lung cancer. Of the 667 genes that exhibited differential expression following murine polidocanol and SO2 injury, 40.6% were additionally dysregulated in human SqCC. Among these, 150 genes were consistently upregulated and 54 downregulated relative to all controls. Examples included genes associated with increased cell cycling, aberrant cytokinesis and DNA repair, and enhanced tumour cell invasion and metastases. For 88.2% of identified genes, altered expression was associated with increased SqCC progression and patient mortality. These results establish a novel gene expression signature linking airway injury and lung cancer pathogenesis."}, {"title": "Neptune: A Bioinformatics Tool for Rapid Discovery of Genomic Variation in Bacterial Populations", "url": "https://www.biorxiv.org/content/early/2016/11/30/032227", "tag": "Bioinformatics", "abstract": "The ready availability of vast amounts of genomic sequence data has created the need to rethink comparative genomics algorithms using \"big data\" approaches. Neptune is an efficient system for rapidly locating differentially abundant genomic content in bacterial populations using an exact k-mer matching strategy, while accommodating k-mer mismatches. Neptune's loci discovery process identifies sequences that are sufficiently common to a group of target sequences and sufficiently absent from non-targets using probabilistic models. Neptune uses parallel computing to efficiently identify and extract these loci from draft genome assemblies without requiring multiple sequence alignments or other computationally expensive comparative sequence analyses. Tests on simulated and real data sets showed that Neptune rapidly identifies regions that are both sensitive and specific. We demonstrate that this system can identify trait-specific loci from different bacterial lineages. Neptune is broadly applicable for comparative bacterial analyses, yet will particularly benefit pathogenomic applications, owing to efficient and sensitive discovery of differentially abundant genomic loci."}, {"title": "DMAP - Graphical representation of physical and genetic map correlation", "url": "https://www.biorxiv.org/content/early/2016/11/29/090415", "tag": "Bioinformatics", "abstract": "Next-generation sequencing approaches coupled with appropriate assembly software can provide draft genome sequences of complex organisms as a series of unordered contigs in a timely and cost effective manner. Likewise, high throughput mapping technologies such as DArT and SNP platforms can provide a high density of sequence-anchored markers with which high resolution genetic maps can be constructed. Visualising and interpreting these data requires a new generation of tools as the volume of data leads to considerable redundancy and information overload in graphical representation. DMAP provides a highly configurable visual representation of physical and genetic map correlation, reducing data representation to an aesthetically acceptable degree. It also calculates an optimal orientation for the ordered sequence contigs, highlighting markers that are anomalous and contigs which may be in erroneous positions. Output is as PDF, allowing subsequent refinement prior to print publication and vector based representation for online supplementary figures. The perl scripts have few dependencies and code is freely available under a creative commons license (CC-BY) from the author's GitHub repository at http://github.com/davidmam/DMAP.git ."}, {"title": "Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model", "url": "https://www.biorxiv.org/content/early/2016/11/28/073239", "tag": "Bioinformatics", "abstract": "Motivation: Protein contacts contain key information for the understanding of protein structure and function and thus, contact prediction from sequence is an important problem. Recently exciting progress has been made on this problem, but the predicted contacts for proteins without many sequence homologs is still of low quality and not extremely useful for de novo structure prediction. Method: This paper presents a new deep learning method that predicts contacts by integrating both evolutionary coupling (EC) and sequence conservation information through an ultra-deep neural network formed by two deep residual neural networks. The first residual network conducts a series of 1-dimensional convolutional transformation of sequential features; the second residual network conducts a series of 2-dimensional convolutional transformation of pairwise information including output of the first residual network, EC information and pairwise potential. By using very deep residual networks, we can model very complex relationship between sequence and contact map as well as long-range interdependency between contacts and thus, obtain high-quality contact prediction. Results: Our method greatly outperforms existing contact prediction methods and leads to much more accurate contact-assisted protein folding. Tested on the 105 CASP11 targets, 76 CAMEO test proteins and 398 membrane proteins, the average top L long-range prediction accuracy obtained our method, the representative EC method CCMpred and the CASP11 winner MetaPSICOV is 0.47, 0.21 and 0.30, respectively; the average top L/10 long-range accuracy of our method, CCMpred and MetaPSICOV is 0.77, 0.47 and 0.59, respectively. Ab initio folding using our predicted contacts as restraints can yield correct folds (i.e., TMscore>0.6) for 203 of the 579 test proteins, while that using MetaPSICOV- and CCMpred-predicted contacts can do so for only 79 and 62 of them, respectively. Further, our contact-assisted models also have much better quality than template-based models (especially for membrane proteins). Using our predicted contacts as restraints, we can (ab initio) fold 208 of the 398 membrane proteins with TMscore>0.5. By contrast, when the training proteins of our method are used as templates, homology modeling can only do so for 10 of them. One interesting finding is that even if we train our prediction models with only non-membrane proteins, our method works very well on membrane protein contact prediction. In the recent blind CAMEO benchmark, our method successfully folded one mainly-beta protein of 182 residues, one alpha+beta protein of 125 residues, one mainly-alpha protein of 140 residues, one mainly-alpha protein of 217 residues and one alpha+beta protein of 260 residues, all of which have a novel fold and only 0.3L-2.3L effective sequence homologs."}, {"title": "DNA sequence+shape kernel enables alignment-free modeling of transcription factor binding", "url": "https://www.biorxiv.org/content/early/2016/11/27/089441", "tag": "Bioinformatics", "abstract": "Motivation: Transcription factors (TFs) bind to specific DNA sequence motifs. Several lines of evidence suggest that TF-DNA binding is mediated in part by properties of the local DNA shape: the width of the minor groove, the relative orientations of adjacent base pairs, etc. Several methods have been developed to jointly account for DNA sequence and shape properties in predicting TF binding affinity. However, a limitation of these methods is that they typically require a training set of aligned TF binding sites. Results: We describe a sequence+shape kernel that leverages DNA sequence and shape information to better understand protein-DNA binding preference and affinity. This kernel extends an existing class of k-mer based sequence kernels, based on the recently described di-mismatch kernel. Using three in vitro benchmark datasets, derived from universal protein binding microarrays (uPBMs), genomic context PBMs (gcPBMs) and SELEX-seq data, we demonstrate that incorporating DNA shape information improves our ability to predict protein-DNA binding affinity. In particular, we observe that (1) the k-spectrum+shape model performs better than the classical k-spectrum kernel, particularly for small k values; (2) the di- mismatch kernel performs better than the k-mer kernel, for larger k; and (3) the di-mismatch+shape kernel performs better than the di-mismatch kernel for intermediate k values. Availability: The software is available at https://bitbucket.org/wenxiu/sequence-shape.git"}, {"title": "Baseline mutation profiling of 1134 samples of circulating cell-free DNA and blood cells from healthy individuals", "url": "https://www.biorxiv.org/content/early/2016/11/26/089813", "tag": "Bioinformatics", "abstract": "The molecular alteration in circulating cell-free DNA (cfDNA) in plasma can reflect the status of the human body in a timely manner. Hence, cfDNA has emerged as important biomarkers in clinical diagnostics, particularly in cancer. However, somatic mutations are also commonly found in healthy individuals, which extensively interfere with the diagnostic results in cancer. This study was designed to examine the background somatic mutations in white blood cells (WBC) and cfDNA for healthy controls based on the sequencing data from 1134 samples, to understand the patterns and origin of mutations detected in cfDNA. We determined the mutation frequencies in both the WBC and cfDNA groups of the samples by a panel of 50 cancer-associated genes which covered 20K nucleotide regions using ultra-deep sequencing with average depth >40000 folds. Our results showed that most of mutations in cfDNA originated from WBC. We also observed that NPM1 gene was the most frequently mutant gene in both WBC and cfDNA. Our study highlighted the importance of sequencing both cfDNA and WBC, to improve the sensitivity and accuracy for calling cancer-related mutations from circulating tumor DNA, and shielded light on developing the early cancer diagnosis by cfDNA sequencing."}, {"title": "PoGo: Jumping from Peptides to Genomic Loci", "url": "https://www.biorxiv.org/content/early/2016/11/26/079772", "tag": "Bioinformatics", "abstract": "Current tools for visualization and integration of proteomics with other omics datasets are inadequate for large-scale studies and capture only basic sequence identity information. We developed PoGo for mapping peptides identified through mass spectrometry to a reference genome to overcome these limitations. PoGo exhibited superior performance over other tools on benchmarking with large-scale human tissue and cancer phosphoproteome datasets. Additionally, extended functionality enables representation of single nucleotide variants, post-translational modifications and quantitative features."}, {"title": "FlyLimbTracker: an active contour based approach for leg segment tracking in unmarked, freely behaving Drosophila", "url": "https://www.biorxiv.org/content/early/2016/11/26/089714", "tag": "Bioinformatics", "abstract": "Understanding the biological underpinnings of movement and action requires the development of tools for precise, quantitative, and high-throughput measurements of animal behavior. Drosophila melanogaster provides an ideal model for developing such tools: the fly has unparalleled genetic accessibility and depends on a relatively compact nervous system to generate sophisticated limbed behaviors including walking, reaching, grooming, courtship, and boxing. Here we describe a method that uses active contours to semi-automatically track body and leg segments from video image sequences of unmarked, freely behaving Drosophila. We show that this approach is robust to wide variations in video spatial and temporal resolution and that it can be used to measure leg segment motions during a variety of locomotor and grooming behaviors. FlyLimbTracker, the software implementation of this method, is open-source and our approach is generalizable. This opens up the possibility of tracking leg movements in other species by modifications of underlying active contour models."}, {"title": "Graph regularized, semi-supervised learning improves annotation of de novo transcriptomes", "url": "https://www.biorxiv.org/content/early/2016/11/25/089417", "tag": "Bioinformatics", "abstract": "We present a new method, GRASS, for improving an initial annotation of de novo transcriptomes. GRASS makes the shared-sequence relationships between assembled contigs explicit in the form of a graph, and applies an algorithm that performs label propagation to transfer annotations between related contigs and modifies the graph topology iteratively. We demonstrate that GRASS increases the completeness and accuracy of the initial annotation, allows for improved differential analysis, and is very efficient, typically taking 10s of minutes."}, {"title": "Development of a multivariate clinical prediction model for the diagnosis of mild stroke/TIA in physician first-contact patient settings", "url": "https://www.biorxiv.org/content/early/2016/11/22/089227", "tag": "Bioinformatics", "abstract": "OBJECTIVE: To develop a clinical prediction model for diagnosing mild stroke/transient ischemic attack (TIA) in first-contact patient settings. DESIGN: Retrospective study design utilizing logistic regression modeling of patient clinical symptoms collected from patient chart histories and referral data. SETTING: Regional fast-track TIA clinic on Vancouver Island, Canada, accepting referrals from emergency departments (ED) and general practice (GP). PARTICIPANTS: Model development: 4187 ED and GP referred patients from 2008-2011 who were assessed at the TIA clinic. Temporal hold-out validation: 1953 ED and GP referred patients from 2012-2013 assessed at the same clinic. OUTCOMES: Diagnosis of mild stroke/TIA by clinic neurologists. RESULTS: 123 candidate predictors were assessed using univariate feature selection for inclusion in the model, and culminated in the selection of 50 clinical features. Post-hoc investigation of the selected predictors revealed 12 clinically relevant interaction terms. Model performance on the temporal hold-out validation set achieved a sensitivity/specificity of 71.8% / 72.8% using the ROC01 cutpoint (\u2265 0.662), and an AUC of 79.9% (95% CI, 77.9%-81.9%). In comparison, the ABCD2 score (\u2265 4) achieved a sensitivity/specificity of 70.4% / 54.5% and an AUC of 67.5% (95% CI, 65.2%-69.9%). The logistic regression model demonstrated good calibration on the hold-out set (\u03b20 = -0.257); \u03b2linear = 1.047). CONCLUSIONS: The developed diagnostic model performs better than the ABCD2 score at diagnosing mild stroke/TIA on the basis of clinical symptoms. The model has the potential to replace the use of the prognostic ABCD2 score in diagnostic medical contexts in which the ABCD2 score is currently used, such as patient triage."}, {"title": "Multiple Confounders Correction with Regularized Linear Mixed Effect Models, with Application in Biological Processes", "url": "https://www.biorxiv.org/content/early/2016/11/22/089052", "tag": "Bioinformatics", "abstract": "In this paper, we inspect the performance of regularized linear mixed effect models, as an extension of linear mixed effect model, when multiple confounding factors coexist. We first review its parameter estimation algorithms before we introduce three different methods for multiple confounding factors correction, namely concatenation, sequence, and interpolation. Then we investigate the performance on variable selection task and predictive task on three different data sets, synthetic data set, semi-empirical synthetic data set based on genome sequences and brain wave data set connecting to confused mental states. Our results suggest that sequence multiple confounding factors corrections behave the best when different confounders contribute equally to response variables. On the other hand, when various confounders affect the response variable unevenly, results mainly rely on the degree of how the major confounder is corrected."}, {"title": "Constructing high-density linkage maps with MapDisto 2.0", "url": "https://www.biorxiv.org/content/early/2016/11/22/089177", "tag": "Bioinformatics", "abstract": "We present the second major release of MapDisto, a multi-platform, user-friendly computer program for constructing genetic linkage maps of experimental populations. This version includes several new major features: (i) handling of very large genotyping datasets like the ones generated by genotyping-by-sequencing (GBS); (ii) direct importation and conversion of Variant Call Format (VCF) files; (iii) detection of linkage, i.e. construction of linkage groups in case of segregation distortion; (iv) data imputation on VCF files using a new approach, called LB-Impute. These features operate through inclusion of new Java modules that are installed and used transparently by MapDisto; (v) QTL detection via a new R/qtl graphical interface."}, {"title": "Predicting gene expression level in E. coli from mRNA sequence information", "url": "https://www.biorxiv.org/content/early/2016/11/22/089102", "tag": "Bioinformatics", "abstract": "Motivation: The accurate characterization of the translational mechanism is crucial for enhancing our understanding of the relationship between genotype and phenotype. In particular, predicting the impact of the genetic variants on gene expression will allow to optimize specific pathways and functions for engineering new biological systems. In this context, the development of accurate methods for predicting translation efficiency from the nucleotide sequence is a key challenge in computational biology. Methods: In this work we present PGExpress, a binary classifier to discriminate between mRNA sequences with low and high translation efficiency in E. coli. PGExpress algorithm takes as input 12 features corresponding to RNA folding and anti-Shine-Dalgarno hybridization free energies. The method was trained on a set of 1,772 sequence variants (WT-High) of 137 essential E. coli genes. For each gene, we considered 13 sequence variants of the first 33 nucleotides encoding for the same amino acids followed by the superfolder GFP. Each gene variant is represented sequence blocks that include the Ribosome Binding Site (RBS), the first 33 nucleotides of the coding region (C33), the remaining part of the coding region (CC), and their combinations. Results: Our logistic regression-based tool (PGExpress) was trained using a 20-fold gene-based cross-validation procedure on the WT-High dataset. In this test PGExpress achieved an overall accu-racy of 74%, a Matthews correlation coefficient 0.49 and an Area Under the Receiver Operating Characteristic Curve (AUC) of 0.81. Tested on 3 sets of sequences with different Ribosome Binding Sites, PGExpress reaches similar AUC. Finally, we validated our method by performing in-house experiments on five newly generated mRNA sequence variants. The predictions of the expression level of the new variants are in agreement with our experimental results in E. coli."}, {"title": "SCODE: An efficient regulatory network inference algorithm from single-cell RNA-Seq during differentiation", "url": "https://www.biorxiv.org/content/early/2016/11/22/088856", "tag": "Bioinformatics", "abstract": "The analysis of RNA-Seq data from individual differentiating cells enables us to reconstruct the differentiation process and the degree of differentiation (in pseudo-time) of each cell. Such analyses can reveal detailed expression dynamics and functional relationships for differentiation. To further elucidate differentiation processes, more insight into gene regulatory networks is required. The pseudo-time can be regarded as time information and, therefore, single-cell RNA-Seq data are time-course data with high time resolution. Although time-course data are useful for inferring networks, conventional inference algorithms for such data suffer from high time complexity when the number of samples and genes is large. Therefore, a novel algorithm is necessary to infer networks from single-cell RNA-Seq during differentiation. In this study, we developed the novel and efficient algorithm SCODE to infer regulatory networks, based on ordinary differential equations. We applied SCODE to three single-cell RNA-Seq datasets and confirmed that SCODE can reconstruct observed expression dynamics. We evaluated SCODE by comparing its inferred networks with use of a DNaseI-footprint based network. The performance of SCODE was best for two of the datasets and nearly best for the remaining dataset. We also compared the runtimes and showed that the runtimes for SCODE are significantly shorter than for alternatives. Thus, our algorithm provides a promising approach for further single-cell differentiation analyses. The R source code of SCODE is available at https://github.com/hmatsu1226/SCODE."}, {"title": "HiC-Spector: A matrix library for spectral and reproducibility analysis of Hi-C contact maps", "url": "https://www.biorxiv.org/content/early/2016/11/21/088922", "tag": "Bioinformatics", "abstract": "Summary: Genome-wide proximity ligation based assays like Hi-C have opened a window to the 3D organization of the genome. In so doing, they present data structures that are different from conventional 1D signal tracks. To exploit the 2D nature of Hi-C contact maps, matrix techniques like spectral analysis are particularly useful. Here, we present HiC-spector, a collection of matrix-related functions for analyzing Hi-C contact maps. In particular, we introduce a novel reproducibility metric for quantifying the similarity between contact maps based on spectral decomposition. The metric successfully separates contact maps mapped from Hi-C data coming from biological replicates, pseudo-replicates and different cell types. Availability: Source code in Julia and the documentation of HiC-spector can be freely obtained at https://github.com/gersteinlab/HiC_spector"}, {"title": "ConfocalGN : a minimalistic confocal image simulator.", "url": "https://www.biorxiv.org/content/early/2016/11/21/088906", "tag": "Bioinformatics", "abstract": "SUMMARY : We developed a user-friendly software to generate synthetic confocal microscopy images from a ground truth specified as a 3D bitmap with pixels of arbitrary size. The software can analyze a real confocal stack to derivate noise parameters and will use them directly to generate new images with similar noise characteristics. Such synthetic images can then be used to assert the quality and robustness of an image analysis pipeline, as well as be used to train machine-learning image analysis procedures. We illustrate the approach with closed curves corresponding to the microtubule ring present in blood platelets. AVAILABILITY AND IMPLEMENTATION : ConfocalGN is written in Malab but does not require any toolbox. The source code is distributed under the GPL 3.0 licence on https://github.com/SergeDmi/ConfocalGN."}, {"title": "Isolator: accurate and stable analysis of isoform-level expression in RNA-Seq experiments", "url": "https://www.biorxiv.org/content/early/2016/11/20/088765", "tag": "Bioinformatics", "abstract": "While RNA-Seq has enabled great progress towards the goal of wide-scale isoform-level mRNA quantification, short reads have limitations when resolving complex or similar sets of isoforms. As a result, estimates of isoform abundance carry far more uncertainty than those made at the gene level. When confronted with this uncertainty, commonly used methods produce estimates that are often high-variance---small perturbations in the data often produce dramatically different results, confounding downstream analysis. We introduce a new method, Isolator, which analyzes all samples in an experiment in unison using a simple Bayesian hierarchical model. Combined with aggressive bias correction, it produces estimates that are simultaneously accurate and show high agreement between samples. In a comprehensive comparison of accuracy and variance, we show that this property is unique to Isolator. We further demonstrate that the approach of modeling an entire experiment enables new analyses, which we demonstrate by examining splicing monotonicity across several time points in the development of human cardiomyocyte cells."}, {"title": "SWALO: scaffolding with assembly likelihood optimization", "url": "https://www.biorxiv.org/content/early/2016/11/20/081786", "tag": "Bioinformatics", "abstract": "Scaffolding i.e. ordering and orienting contigs is an important step in genome assembly. We present a method for scaffolding based on likelihoods of genome assemblies. Generative models for sequencing are used to obtain maximum likelihood estimates of gaps between contigs and to estimate whether linking contigs into scaffolds would lead to an increase in the likelihood of the assembly. We then link contigs if they can be unambiguously joined or if the corresponding increase in likelihood is substantially greater than that of other possible joins of those contigs. The method is implemented in a tool called SWALO with approximations to make it efficient and applicable to large datasets. Analysis on real and simulated datasets reveals that it consistently makes more or similar number of correct joins as other scaffolders while linking very few contigs incorrectly, thus outperforming other scaffolders and demonstrating that substantial improvement in genome assembly may be achieved through the use of statistical models. SWALO is freely available for download at https://atifrahman.github.io/SWALO/."}, {"title": "A SIMPLE pipeline for mapping point mutations", "url": "https://www.biorxiv.org/content/early/2016/11/20/088591", "tag": "Bioinformatics", "abstract": "A forward genetic screen is one of the best methods for revealing the regulatory functions of genes. In plants, this technique is highly efficient since it is relatively easy to grow and screen the phenotypes of hundreds or thousands of individuals. The cost-efficiency and ease of data production afforded by next-generation sequencing techniques have created new opportunities for mapping induced mutations. The principles of genetic mapping remain the same. However, the details have changed, which allows for rapid mapping of causal mutations. Current mapping tools are often not user-friendly and complicated or require extensive preparation steps. To simplify the process of mapping new mutations, we developed a pipeline that takes next generation sequencing fastq files as input, calls on several well-established and freely available genome-analysis tools, and outputs the most likely causal DNA change(s). The pipeline has been validated in Arabidopsis and can be readily applied to other species."}, {"title": "Chromosome assembly of large and complex genomes using multiple references", "url": "https://www.biorxiv.org/content/early/2016/11/19/088435", "tag": "Bioinformatics", "abstract": "Despite the rapid development of sequencing technologies, assembly of mammalian-scale genomes into complete chromosomes remains one of the most challenging problems in bioinformatics. To help address this difficulty, we developed Ragout, a reference-assisted assembly tool that now works for large and complex genomes. Taking one or more target assemblies (generated from an NGS assembler) and one or multiple related reference genomes, Ragout infers the evolutionary relationships between the genomes and builds the final assemblies using a genome rearrangement approach. Using Ragout, we transformed NGS assemblies of 15 different Mus musculus and one Mus spretus genomes into sets of complete chromosomes, leaving less than 5% of sequence unlocalized per set. Various benchmarks, including PCR testing and realigning of long PacBio reads, suggest only a small number of structural errors in the final assemblies, comparable with direct assembly approaches. Additionally, we applied Ragout to Mus caroli and Mus pahari genomes, which exhibit karyotype-scale variations compared to other genomes from the Muridae family. Chromosome color maps confirmed most large-scale rearrangements that Ragout detected."}, {"title": "Scalable variational inference for super resolution microscopy", "url": "https://www.biorxiv.org/content/early/2016/11/19/081703", "tag": "Bioinformatics", "abstract": "Super-resolution microscopy methods (e.g. STORM or PALM imaging) have become essential tools in biology, opening up a va- riety of new questions that were previously inaccessible with standard light microscopy methods. In this paper we develop new Bayesian image processing methods that extend the reach of super-resolution mi- croscopy even further. Our method couples variational inference techniques with a data summarization based on Laplace approxi- mation to ensure computational scalability. Our formulation makes it straightforward to incorporate prior information about the underlying sample to further improve ac- curacy. The proposed method obtains dra- matic resolution improvements over previ- ous methods while retaining computational tractability."}, {"title": "UNCROSS: Filtering of high-frequency cross-talk in 16S amplicon reads", "url": "https://www.biorxiv.org/content/early/2016/11/19/088666", "tag": "Bioinformatics", "abstract": "Next-generation amplicon sequencing is widely used for surveying biological diversity in applications such as microbial metagenomics, immune system repertoire analysis and targeted tumor sequencing of cancer-associated genes. In such studies, assignment of reads to incorrect samples (cross-talk) is a well-documented problem that is rarely considered in practice. By considering unexpected OTUs in artificial (mock) samples, I estimate that cross-talk occurred for ~2% of the reads in one Illumina GAIIx run and eleven Illumina MiSeq runs targeting 16S ribosomal RNA. I also describe UNCROSS, an algorithm for detecting and filtering cross-talk in OTU tables."}, {"title": "GLASS: assisted and standardized assessment of gene variations from Sanger sequence trace data", "url": "https://www.biorxiv.org/content/early/2016/11/17/088401", "tag": "Bioinformatics", "abstract": "Motivation: Sanger sequencing remains the reference method for sequence variant detection, especially in a clinical setting. However, chromatogram interpretation often requires manual inspection and in some cases considerable expertise. Additionally, variant reporting and nomenclature is typically left to the user, which can lead to inconsistencies. Results: We introduce GLASS, a tool built to assist with the assessment of gene variations in Sanger sequencing data. Critically, it provides a standardized variant output as recommended by the Human Genome Variation Society. Availability: The program is freely available online at http://bat.infspire.org/genomepd/glass/."}, {"title": "HiC-bench: comprehensive and reproducible Hi-C data analysis designed for parameter exploration and benchmarking", "url": "https://www.biorxiv.org/content/early/2016/11/17/084954", "tag": "Bioinformatics", "abstract": "Chromatin conformation capture techniques have evolved rapidly over the last few years and have provided new insights into genome organization at an unprecedented resolution. Analysis of Hi-C data is complex and computationally intensive involving multiple tasks and requiring robust quality assessment at each step of the analysis. This has led to the development of several tools and methods for processing Hi-C data. However, most of the existing tools do not cover all aspects of the analysis and only offer few quality assessment options. Additionally, availability of a multitude of tools makes scientists wonder how these tools and associated parameters can be optimally used, and how potential discrepancies can be interpreted and resolved. Most importantly, investigators need to be ensured that slight changes in parameters and/or methods do not affect the conclusions of their studies. Finally, any analysis, no matter how complex, should be reproducible by keeping track of the tool versions, parameters and input data. To address these issues (compare, explore and reproduce), we introduce HiC-bench, a configurable computational platform for comprehensive and reproducible analysis of Hi-C sequencing data. HiC-bench performs all common Hi-C analysis tasks, such as alignment, filtering, contact matrix generation and normalization, identification of topological domains, scoring and annotation of specific interactions using both published tools and our own. We have also embedded various tasks that perform quality assessment and visualization. HiC-bench is implemented as a data flow platform with an emphasis on analysis reproducibility. Additionally, the user can readily perform parameter exploration and comparison of different tools in a combinatorial manner that takes into account all desired parameter settings in each pipeline task. This unique feature facilitates the design and execution of complex benchmark studies that may involve combinations of multiple tool/parameter choices in each step of the analysis. To demonstrate the usefulness of our platform, we performed a comprehensive benchmark of existing and new TAD callers exploring different matrix correction methods, parameter settings and sequencing depths. Users can extend our pipeline by adding more tools as they become available. HiC-bench is distributed as free open-source software on GitHub and Zenodo, and our bioinformatics team offers installation and usage support."}, {"title": "A community-based collaboration to build prediction models for short-term discontinuation of docetaxel in metastatic castration-resistant prostate cancer", "url": "https://www.biorxiv.org/content/early/2016/11/17/087809", "tag": "Bioinformatics", "abstract": "Background: Docetaxel has a demonstrated survival benefit for metastatic castration-resistant prostate cancer (mCRPC). However, 10-20% of patients discontinue docetaxel prematurely because of toxicity-induced adverse events, and managing risk factors for toxicity remains an ongoing challenge for health care providers and patients. Prospective identification of high-risk patients for early discontinuation has the potential to assist clinical decision-making and can improve the design of more efficient clinical trials. In partnership with Project Data Sphere (PDS), a non-profit initiative facilitating clinical trial data-sharing, we designed an open-data, crowdsourced DREAM (Dialogue for Reverse Engineering Assessments and Methods) Challenge for developing models to predict early discontinuation of docetaxel Methods: Data from the comparator arms of four phase III clinical trials in first-line mCRPC were obtained from PDS, including 476 patients treated with docetaxel and prednisone from the ASCENT2 trial, 598 patients treated with docetaxel, prednisone/prednisolone, and placebo in the VENICE trial, 526 patients treated with docetaxel, prednisone, and placebo in the MAINSAIL trial, and 528 patients treated with docetaxel and placebo in the ENTHUSE 33 trial. Early discontinuation was defined as treatment stoppage within three months due to adverse treatment effects. Over 150 clinical features including laboratory values, medical history, lesion measures, prior treatment, and demographic variables were curated and made freely available for model building for all four trials. The ASCENT2, VENICE, and MAINSAIL trial data sets formed the training set that also included patient discontinuation status. The ENTHUSE 33 trial, with patient discontinuation status hidden, was used as an independent validation set to evaluate model performance. Prediction performance was assessed using area under the precision-recall curve (AUPRC) and the Bayes factor was used to compare the performance between prediction models. Results: The frequency of early discontinuation was similar between training (ASCENT2, VENICE, and MAINSAIL) and validation (ENTHUSE 33) sets, 12.3% versus 10.4% of docetaxel-treated patients, respectively. In total, 34 independent teams submitted predictions from 61 different models. AUPRC ranged from 0.088 to 0.178 across submissions with a random model performance of 0.104. Seven models with comparable AUPRC scores (Bayes factor \u2264 3) were observed to outperform all other models. A post-challenge analysis of risk predictions generated by these seven models revealed three distinct patient subgroups: patients consistently predicted to be at high-risk or low-risk for early discontinuation and those with discordant risk predictions. Early discontinuation events were two-times higher in the high- versus low-risk subgroup and baseline clinical features such as presence/absence of metastatic liver lesions, and prior treatment with analgesics and ACE inhibitors exhibited statistically significant differences between the high- and low-risk subgroups (adjusted P < 0.05). An ensemble-based model constructed from a post-Challenge community collaboration resulted in the best overall prediction performance (AUPRC = 0.230) and represented a marked improvement over any individual Challenge submission. An online predictor can be found at: http://dream.web.tool.aicml.ca/ Findings: Our results demonstrate that routinely collected clinical features can be used to prospectively inform clinicians of mCRPC patients' risk to discontinue docetaxel treatment early due to adverse events and to the best of our knowledge is the first to establish performance benchmarks in this area. This work also underscores the \"wisdom of crowds\" approach by demonstrating that improved prediction of patient outcomes is obtainable by combining methods across an extended community. These findings were made possible because data from separate trials were made publicly available and centrally compiled through PDS."}, {"title": "Exploiting Next Generation Sequencing to solve the Haplotyping puzzle in Polyploids: a Simulation study", "url": "https://www.biorxiv.org/content/early/2016/11/16/088112", "tag": "Bioinformatics", "abstract": "Haplotypes are the units of inheritance in an organism, and many genetic analyses depend on their precise determination. Methods for haplotyping single individuals use the phasing information available in Next Generation Sequencing reads, by matching overlapping SNPs while penalizing post hoc nucleotide corrections made. Haplotyping diploids is relatively easy, but the complexity of the problem increases drastically for polyploid genomes, which are found in both model organisms and in economically relevant plant and animal species. While a number of tools are available for haplotyping polyploids, the effects of the genomic makeup and the sequencing strategy followed on the accuracy of these methods have hitherto not been thoroughly evaluated. We developed the simulation pipeline haplosim to evaluate the performance of haplotype estimation algorithms for polyploids: HapCompass, HapTree and SDhaP, in settings varying in sequencing approach, ploidy levels and genomic diversity, using tetraploid potato as the model. Our results show that sequencing depth is the major determinant of haplotype estimation quality, that 1kb PacBio CCS reads and Illumina reads with large insert-sizes are competitive, and that all methods fail to produce good haplotypes when ploidy levels increase. Comparing the three methods, HapTree produces the most accurate estimates, but also consumes the most resources. There is clearly room for improvement in polyploid haplotyping algorithms."}, {"title": "Computational evidence of a compound with nicotinic \u03b14\u03b22-Ach receptor partial agonist properties as possible coadjuvant for the treatment of obesity", "url": "https://www.biorxiv.org/content/early/2016/11/16/088138", "tag": "Bioinformatics", "abstract": "Background. Nowadays, the search for new anti-obesity drugs is oriented to the use of anti-addiction medications like bupropion and naltrexone. Other compounds like varenicline may be also useful to treat obesity. However, the low effectiveness of the former or the high number of adverse effects of the latter makes it necessary to search for new therapeutic agents. Methods. Screening database selected for the computational experiments was DrugBank. 3D global shape comparison with varenicline was performed by means of the Ligand Based Virtual Screening tool WEGA v2015. A pharmacophore model based in the structure of varenicline was created by means of LigandScout v4.08. The in-silico screening was performed using Relative Pharmacophore Fit (RPF) scoring function implemented in LigandScout. Up to 3 mismatches with varenicline pharmacophore model were allowed for hits retrieving. Results. Drugbank database was screened in silico to find alternative molecules to varenicline, and the compound cevimeline was found to have strong similarity to varenicline in terms of 3D shape and pharmacophoric features. Thus, we propose this hit may interact with nicotinic \u03b14\u03b22-Ach receptor in the same mode as varenicline does. Discussion. The functional activities of this compound and its validity as a drug therapy for obesity treatment must be confirmed in further in vitro, in vivo and preclinical studies; however, attending to our screening procedure, this compound should be a promising therapy for such a complex disorder such as obesity."}, {"title": "Detecting Selection Signals In Plasmodium falciparum Using Identity-By-Descent Analysis", "url": "https://www.biorxiv.org/content/early/2016/11/16/088039", "tag": "Bioinformatics", "abstract": "Detection of selection signals using genomic data is vital for tracking drug resistance loci in microorganisms that cause disease, such as malaria, and monitoring of such loci is crucial for disease control efforts. Here we present a novel method of detecting relatively recent selection using identity-by-descent approaches suitable for multiclonal, recombining microorganisms. Application of this new method to a large whole genome sequencing study of Plasmodium falciparum identifies many well-known signatures such as crt and k13, associated with chloroquine resistance and artemisinin resistance respectively, and through relatedness networks shows how these signatures are distributed in Southeast Asia, Africa and Oceania. Using these networks, we confirmed an independent origin of chloroquine resistance in Papua New Guinea and the spread of multiple artemisinin resistance mutations in Southeast Asia. We also found two novel signals of selection not yet investigated in detail."}, {"title": "Efficient Heuristic for Decomposing a Flow with Minimum Number of Paths", "url": "https://www.biorxiv.org/content/early/2016/11/16/087759", "tag": "Bioinformatics", "abstract": "Motivated by transcript assembly and multiple genome assembly problems, in this paper, we study the following minimum path flow decomposition problem: given a directed acyclic graph G=(V, E) with source s and sink t and a flow f, compute a set of s-t paths P and assign weight w(p) for p \u2208 P such that f(e) = \u03a3p\u2208P:e\u2208pw(p), \u2200e \u2208 E, and |P| is minimized. We propose an efficient pseudo-polynomial-time heuristic for this problem based on novel insights. Our heuristic gives a framework that consists of several components, providing a roadmap for continuing development of better heuristics. Through experimental studies on both simulated and transcript assembly instances, we show that our algorithm significantly improves the previous state-of-the-art algorithm. Implementation of our algorithm is available at https://github.com/Kingsford-Group/catfish."}, {"title": "Efficient randomization of biological networks while preserving functional characterization of individual nodes", "url": "https://www.biorxiv.org/content/early/2016/11/16/069245", "tag": "Bioinformatics", "abstract": "Networks are popular and powerful tools to describe and model biological processes. Many computational methods have been developed to infer biological networks from literature, high-throughput experiments, and combinations of both. Additionally, a wide range of tools has been developed to map experimental data onto reference biological networks, in order to extract meaningful modules. Many of these methods assess results' significance against null distributions of randomized networks. However, these standard unconstrained randomizations do not preserve the functional characterization of the nodes in the reference networks (i.e. their degrees and connection signs), hence including potential biases in the assessment. Building on our previous work about rewiring bipartite and undirected-unweighted networks, we propose a method for rewiring any type of unweighted networks. In particular we formally demonstrate that the problem of rewiring a signed and directed network preserving its functional connectivity (F-rewiring) reduces to the problem of rewiring two induced bipartite networks. Additionally, we reformulate the lower bound to the iterations' number of the switching-algorithm to make it suitable for the F-rewiring of networks of any size. Finally, we present BiRewire3, an open-source Bioconductor software enabling the F-rewiring of any type of unweighted network. We illustrate its application to a case study about the identification of modules from gene expression data mapped on protein interaction networks, and a second one focused on building logic models from more complex signed-directed reference signaling networks and phosphoproteomic data. BiRewire3 it is freely available at https://www.bioconductor.org/packages/BiRewire/, and it should have a broad application as it allows an efficient and analytically derived statistical assessment of results from any network biology tool."}, {"title": "Bit-twiddling on Nucleotides", "url": "https://www.biorxiv.org/content/early/2016/11/16/082214", "tag": "Bioinformatics", "abstract": "Bits, nucleotides and speed; Analyzing sequence data is one of the main focuses of bioinformatics. In this note we shall analyze four common tasks: computing the GC-content, hashing k-mers, computing the reverse complement and counting transversions. We present improved algorithms that show superior performance compared to na\u00efve approaches."}, {"title": "SplitThreader: Exploration and analysis of rearrangements in cancer genomes", "url": "https://www.biorxiv.org/content/early/2016/11/15/087981", "tag": "Bioinformatics", "abstract": "Genomic rearrangements and associated copy number changes are important drivers in cancer as they can alter the expression of oncogenes and tumor suppressors, create gene fusions, and misregulate gene expression. Here we present SplitThreader (http://splitthreader.com), an open- source interactive web application for analysis and visualization of genomic rearrangements and copy number variation in cancer genomes. SplitThreader constructs a sequence graph of genomic rearrangements in the sample and uses a priority queue breadth-first search algorithm on the graph to search for novel interactions. This is applied to detect gene fusions and other novel sequences, as well as to evaluate distances in the rearranged genome between any genomic regions of interest, especially the repositioning of regulatory elements and their target genes. SplitThreader also analyzes each variant to categorize it by its relation to other variants and by its copy number concordance. This identifies balanced translocations, identifies simple and complex variants, and suggests likely false positives when copy number is not concordant across a candidate breakpoint. It also provides explanations when multiple variants affect the copy number state and obscure the contribution of a single variant, such as a deletion within a region that is overall amplified. Together, these categories triage the variants into groups and provide a starting point for further systematic analysis and manual curation. To demonstrate its utility, we apply SplitThreader to three cancer cell lines, MCF-7 and A549 with Illumina paired- end sequencing, and SK-BR-3, with long-read PacBio sequencing. Using SplitThreader, we examine the genomic rearrangements responsible for previously observed gene fusions in SK-BR-3 and MCF-7, and discover many of the fusions involved a complex series of multiple genomic rearrangements. We also find notable differences in the types of variants between the three cell lines, in particular a much higher proportion of reciprocal variants in SK-BR-3 and a distinct clustering of interchromosomal variants in SK-BR-3 and MCF-7 that is absent in A549."}, {"title": "Scalable latent-factor models applied to single-cell RNA-seq data separate biological drivers from confounding effects", "url": "https://www.biorxiv.org/content/early/2016/11/15/087775", "tag": "Bioinformatics", "abstract": "Single-cell RNA-sequencing (scRNA-seq) allows heterogeneity in gene expression levels to be studied in large populations of cells. Such heterogeneity can arise from both technical and biological factors, thus making decomposing sources of variation extremely difficult. We here describe a computationally efficient model that uses prior pathway annotation to guide inference of the biological drivers underpinning the heterogeneity. Moreover, we jointly update and improve gene set annotation and infer factors explaining variability that fall outside the existing annotation. We validate our method using simulations, which demonstrate both its accuracy and its ability to scale to large datasets with up to 100,000 cells. Moreover, through applications to real data we show that our model can robustly decompose scRNA-seq datasets into interpretable components and facilitate the identification of novel sub- populations."}, {"title": "ROSE: a deep learning based framework for predicting ribosome stalling", "url": "https://www.biorxiv.org/content/early/2016/11/15/067108", "tag": "Bioinformatics", "abstract": "We present a deep learning based framework, called ROSE, to accurately predict ribosome stalling events in translation elongation from coding sequences based on high-throughput ribosome profiling data. Our validation results demonstrate the superior performance of ROSE over conventional prediction models. ROSE provides an effective index to estimate the likelihood of translational pausing at codon resolution and understand diverse putative regulatory factors of ribosome stalling. Also, the ribosome stalling landscape computed by ROSE can recover the functional interplay between ribosome stalling and cotranslational events in protein biogenesis, including protein targeting by the signal recognition particle (SRP) and protein secondary structure formation."}, {"title": "Automatic Tracing of Ultra-Volume of Neuronal Images", "url": "https://www.biorxiv.org/content/early/2016/11/14/087726", "tag": "Bioinformatics", "abstract": "Despite substantial advancement in the automatic tracing of neurons' morphology in recent years, it is challenging to apply the existing algorithms to very large image datasets containing billions or more voxels. We introduce UltraTracer, a solution designed to extend any base neuron-tracing algorithm to be able to trace virtually unlimited data volumes. We applied this approach to neuron-tracing algorithms with completely different design principles and tested on challenging human and mouse neuron datasets that have hundreds of billions of voxels. Results indicate that UltraTracer is scalable, accurate, and about 3 to 6 times more efficient compared to other state-of-the-art approaches."}, {"title": "WhatsHap: fast and accurate read-based phasing", "url": "https://www.biorxiv.org/content/early/2016/11/14/085050", "tag": "Bioinformatics", "abstract": "Read-based phasing allows to reconstruct the haplotype structure of a sample purely from sequencing reads. While phasing is a required step for answering questions about population genetics, compound heterozygosity, and to aid in clinical decision making, there has been a lack of an accurate, usable and standards-based software. WhatsHap is a production-ready tool for highly accurate read-based phasing. It was designed from the beginning to leverage third-generation sequencing technologies, whose long reads can span many variants and are therefore ideal for phasing. WhatsHap works also well with second-generation data, is easy to use and will phase not only SNVs, but also indels and other variants. It is unique in its ability to combine read-based with genetic phasing, allowing to further improve accuracy if multiple related samples are provided."}, {"title": "MARS: Motif Assessment and Ranking Suite for transcription factor binding motifs", "url": "https://www.biorxiv.org/content/early/2016/11/14/065615", "tag": "Bioinformatics", "abstract": "We describe MARS (Motif Assessment and Ranking Suite), a web-based suite of tools used to evaluate and rank PWM-based motifs. The increased number of learned motif models that are spread across databases and in different PWM formats, leading to a choice dilemma among the users, is our motivation. This increase has been driven by the difficulty of modelling transcription factor binding sites and the advance in high-throughput sequencing technologies at a continually reducing cost. Therefore, several experimental techniques have been developed resulting in diverse motif-finding algorithms and databases. We collate a wide variety of available motifs into a benchmark database, including the corresponding experimental ChIP-seq and PBM data obtained from ENCODE and UniPROBE databases, respectively. The implemented tools include: a data-independent consistency-based motif assessment and ranking (CB-MAR), which is based on the idea that \u2018correct motifs\u2019 are more similar to each other while incorrect motifs will differ from each other; and a scoring and classification-based algorithms, which rank binding models by their ability to discriminate sequences known to contain binding sites from those without. The CB-MAR and scoring techniques have a 0.86 and 0.73 median rank correlation using ChIP-seq and PBM respectively. Best motifs selected by CB-MAR achieve a mean AUC of 0.75, comparable to those ranked by held out data at 0.76 -- this is based on ChIP-seq motif discovery using five algorithms on 110 transcription factors. We have demonstrated the benefit of this web server in motif choice and ranking, as well as in motif discovery. It can be accessed at http://www.bioinf.ict.ru.ac.za/."}, {"title": "Ensembl Core Software Resources: storage and programmatic access for DNA sequence and genome annotation", "url": "https://www.biorxiv.org/content/early/2016/11/11/087239", "tag": "Bioinformatics", "abstract": "The Ensembl software resources are a stable infrastructure to store, access and manipulate genome assemblies and their functional annotations. The Ensembl \"Core\" database and Application Programming Interface (API) was our first major piece of software infrastructure and remains at the centre of all of our genome resources. Since its initial design more than fifteen years ago, the number of publicly available genomic, transcriptomic and proteomic datasets has grown enormously, accelerated by continuous advances in DNA sequencing technology. Initially intended to provide annotation for the reference human genome, we have extended our framework to support the genomes of all species as well as richer assembly models. Cross-referenced links to other informatics resources facilitate searching our database with a variety of popular identifiers such as UniProt and RefSeq. Our comprehensive and robust framework storing a large diversity of genome annotations in one location serves as a platform for other groups to generate and maintain their own tailored annotation. Our databases and APIs are publicly available and all of our source code is released with a permissive Apache v2.0 licence at http://github.com/Ensembl."}, {"title": "lncRNA-screen: an interactive platform for computationally screening long non-coding RNAs in large genomics datasets", "url": "https://www.biorxiv.org/content/early/2016/11/10/087080", "tag": "Bioinformatics", "abstract": "Long non-coding RNAs (lncRNAs) have emerged as a class of factors that are important for regulating development and cancer. Computational prediction of lncRNAs from ultra-deep RNA sequencing has been successful in identifying candidate lncRNAs. However, the complexity of handling and integrating different types of genomics data poses significant challenges to experimental laboratories that lack extensive genomics expertise. To address this issue, we have developed lncRNA-screen, a comprehensive pipeline for computationally screening putative lncRNA transcripts over large multimodal datasets. The main objective of this work is to facilitate the computational discovery of lncRNA candidates to be further examined by functional experiments. lncRNA-screen provides a fully automated easy-to-run pipeline which performs data download, RNA-seq alignment, assembly, quality assessment, transcript filtration, novel lncRNA identification, coding potential estimation, expression level quantification, histone mark enrichment profile integration, differential expression analysis, annotation with other type of segmented data (CNVs, SNPs, Hi-C, etc.) and visualization. Importantly, lncRNA-screen generates an interactive report summarizing all interesting lncRNA features including genome browser snapshots and lncRNA-mRNA interactions based on Hi-C data. In summary, our pipeline provides a comprehensive solution for lncRNA discovery and an intuitive interactive report for identifying promising lncRNA candidates. lncRNA-screen is available as free open-source software on GitHub."}, {"title": "A Deep Boosting Based Approach for Capturing the Sequence Binding Preferences of RNA-Binding Proteins from High-Throughput CLIP-Seq Data", "url": "https://www.biorxiv.org/content/early/2016/11/10/086421", "tag": "Bioinformatics", "abstract": "Characterizing the binding behaviors of RNA-binding proteins (RBPs) is important for understanding their functional roles in gene expression regulation. However, current high-throughput experimental methods for identifying RBP targets, such as CLIP-seq and RNAcompete, usually suffer from the false positive and false negative issues. Here, we develop a deep boosting based machine learning approach, called DeBooster, to accurately model the binding sequence preferences and identify the corresponding binding targets of RBPs from CLIP-seq data. Comprehensive validation tests have shown that DeBooster can outperform other state-of-the-art approaches in predicting RBP targets and recover false negatives that are common in current CLIP-seq data. In addition, we have demonstrated several new potential applications of DeBooster in understanding the regulatory functions of RBPs, including the binding effects of the RNA helicase MOV10 on mRNA degradation, the influence of different binding behaviors of the ADAR proteins on RNA editing, as well as the antagonizing effect of RBP binding on miRNA repression. Moreover, DeBooster may provide an effective index to investigate the effect of pathogenic mutations in RBP binding sites, especially those related to splicing events. We expect that DeBooster will be widely applied to analyze large-scale CLIP-seq experimental data and can provide a practically useful tool for novel biological discoveries in understanding the regulatory mechanisms of RBPs."}, {"title": "EthSEQ: ethnicity annotation from whole exome sequencing data", "url": "https://www.biorxiv.org/content/early/2016/11/10/085837", "tag": "Bioinformatics", "abstract": "Whole exome sequencing (WES) is widely utilized both in translational cancer genomics studies and in the setting of precision medicine. Stratification of individual's ethnicity is fundamental for the correct interpretation of personal genomic variation impact. We implemented EthSEQ to provide reliable and rapid ethnicity annotation from whole exome sequencing individual's data and validated it on 1,000 Genome Project and TCGA data demonstrating high precision (>99%). EthSEQ can be integrated into any WES based processing pipeline and exploits multi-core capabilities. Source code, manual and other data is available at http://demichelislab.unitn.it/EthSEQ."}, {"title": "Comparison of Clustering Methods for High-Dimensional Single-Cell Flow and Mass Cytometry Data", "url": "https://www.biorxiv.org/content/early/2016/11/10/047613", "tag": "Bioinformatics", "abstract": "Recent technological developments in high-dimensional flow cytometry and mass cytometry (CyTOF) have made it possible to detect expression levels of dozens of protein markers in thousands of cells per second, allowing cell populations to be characterized in unprecedented detail. Traditional data analysis by \"manual gating\" can be inefficient and unreliable in these high-dimensional settings, which has led to the development of a large number of automated analysis methods. Methods designed for unsupervised analysis use specialized clustering algorithms to detect and define cell populations for further downstream analysis. Here, we have performed an up-to-date, extensible performance comparison of clustering methods for high-dimensional flow and mass cytometry data. We evaluated methods using several publicly available data sets from experiments in immunology, containing both major and rare cell populations, with cell population identities from expert manual gating as the reference standard. Several methods performed well, including FlowSOM, X-shift, PhenoGraph, Rclusterpp, and flowMeans. Among these, FlowSOM had extremely fast runtimes, making this method well-suited for interactive, exploratory analysis of large, high-dimensional data sets on a standard laptop or desktop computer. These results extend previously published comparisons by focusing on high-dimensional data and including new methods developed for CyTOF data. R scripts to reproduce all analyses are available from GitHub (https://github.com/lmweber/cytometry-clustering-comparison), and pre-processed data files are available from FlowRepository (FR-FCM-ZZPH), allowing our comparisons to be extended to include new clustering methods and reference data sets."}, {"title": "Establishing wonder oil, Solanesol, as a novel inhibitor for Focal Adhesive Kinase by in silico strategies", "url": "https://www.biorxiv.org/content/early/2016/11/09/086660", "tag": "Bioinformatics", "abstract": "Focal adhesion kinase (FAK) plays a primary role in regulating the activity of many signaling molecules. Increased FAK expression has been implicated in a series of cellular processes, including cell migration and survival. Inhibiting the activity of FAK for cancer therapy is currently under investigation. Hence, FAK and its inhibitors has been the subject of intensive research. To understand the structural factors affecting inhibitory potency, kinetic analysis, molecular docking and molecular dynamics simulation were studied in this project. Though, Solanesol was found have inhibitory activities towards FAK, no in silico tests were ever done on the same. Due to high flexibility of Solanesol (Rotatable bonds = 25), it is difficult to analyze using normal docking protocols. This paper introduces a novel method to dock and analyze molecules with high flexibility based on weighed contact based scoring method. This method uses blind docking technique, which was developed for protein peptide docking method, to generate conformations which were used to calculate contact based weights of residues. This method reveals the possible binding site for the small molecule. An exhaustive docking search on the acquired area reveals the docked confirmation of the compound. The final docked conformation was subjected to molecular dynamics to understand of binding stability. This study is in a good agreement with experimental results which shows Solanesol binds at ATP binding site and inhibit the phosphorylation of Focal Adhesion Kinase."}, {"title": "Unified Framework for Representing and Ranking", "url": "https://www.biorxiv.org/content/early/2016/11/09/086678", "tag": "Bioinformatics", "abstract": "In the database retrieval and nearest neighbor classification tasks, the two basic problems are to represent the query and database objects, and to learn the ranking scores of the database objects to the query. Many studies have been conducted for the representation learning and the ranking score learning problems, however, they are always learned independently from each other. In this paper, we argue that there are some inner relationships between the representation and ranking of database objects, and try to investigate their relationships by learning them in a unified way. To this end, we proposed the Unified framework for Representation and Ranking (UR2) of objects for the database retrieval and nearest neighbor classification tasks. The learning of representation parameter and the ranking scores are modeled within one single unified objective function. The objective function is optimized alternately with regarding to representation parameter and the ranking scores. Based on the optimization results; iterative algorithms are developed to learn the representation parameter and the ranking scores on a unified way. Moreover, with two different formulas of representation (feature selection and subspace learning), we give two versions of UR2. The proposed algorithms are tested on two challenging tasks - MRI image based brain tumor retrieval and nearest neighbor classification based protein identification. The experiments show the advantage of the proposed unified framework over the state-of-the-art independent representation and ranking methods."}, {"title": "Rotamer libraries for the high-resolution design of beta-amino acid foldamers", "url": "https://www.biorxiv.org/content/early/2016/11/08/086389", "tag": "Bioinformatics", "abstract": "\u03b2-amino acids offer attractive opportunities to develop biologically active peptidomimetics, either employed alone or in conjunction with natural \u03b1-amino acids. Owing to their potential for unique conformational preferences that deviate considerably from \u03b1-peptide geometries, \u03b2-amino acids greatly expand the possible chemistries and physical properties available to polyamide foldamers. Complete in silico support for designing new molecules incorporating nonnatural amino acids typically requires representing their side chain conformations as sets of discrete rotamers for model refinement and sequence optimization. Such rotamer libraries are key components of several state of the art design frameworks. Here we report the development, incorporation in to the Rosetta macromolecular modeling suite, and validation of rotamer libraries for \u03b23-amino acids."}, {"title": "ploidyNGS: Visually exploring ploidy with Next Generation Sequencing data", "url": "https://www.biorxiv.org/content/early/2016/11/08/086488", "tag": "Bioinformatics", "abstract": "Summary: ploidyNGS is a model-free, open source tool to visualize and explore ploidy levels in a newly sequenced genome, exploiting short read data. We tested ploidyNGS using both simulated and real NGS data of the model yeast Saccharomyces cerevisiae. ploidyNGS allows the identification of the ploidy level of a newly sequenced genome in a visual way. Availability and implementation: ploidyNGS is available under the GNU General Public License (GPL) at https://github.com/diriano/ploidyNGS. ploidyNGS is implemented in Python and R."}, {"title": "Predicting the phenotype of Mendelian disease missense mutations using amino acid conservation and protein stability change", "url": "https://www.biorxiv.org/content/early/2016/11/08/086470", "tag": "Bioinformatics", "abstract": "Many Mendelian diseases are caused by recessive, loss-of-function missense mutations. On a gene-by-gene basis, it has been demonstrated that missense mutations cause, among other defects, protein misfolding, protein instability, protein mistransport, which strongly suggests that pathogenic missense mutations do not occur at random positions. Based on those observations, we predicted that Mendelian disease missense mutations are enriched in evolutionarily-conserved amino acids. In a pilot set of 260 Mendelian diseases genes affecting cellular organelles we show that missense mutations indeed occur in amino acids that are significantly more conserved than the average amino acid in the protein based on three different scoring methods (Jensen Shannon Divergence p = 7.78E-03, Shannon Entropy p = 1.68E-13, Sum of Pairs p = 1.55E-17). In order to understand how these results might be related to clinical phenotypes in humans or preclinical phenotypes in model organisms, we calculated the protein stability change upon mutation (\u0394\u0394Gu) using EASE-MM and found that, on average, pathogenic mutations cause a stability change of greater magnitude than benign mutations (p = 4.414428E-23). Finally, we performed a computational case study on NPC1, the gene responsible for 95% of diagnosed cases of the lysosomal storage disorder Niemann-Pick Type C using a set of 411 missense mutations from the Exome Aggregation Consortium."}, {"title": "Measuring the importance of annotation granularity to the detection of semantic similarity between phenotype profiles", "url": "https://www.biorxiv.org/content/early/2016/11/08/086306", "tag": "Bioinformatics", "abstract": "In phenotype annotations curated from the biological and medical literature, considerable human effort must be invested to select ontological classes that capture the expressivity of the original natural language descriptions, and finer annotation granularity can also entail higher computational costs for particular reasoning tasks. Do coarse annotations suffice for certain applications? Here, we measure how annotation granularity affects the statistical behavior of semantic similarity metrics. We use a randomized dataset of phenotype profiles drawn from 57,051 taxon-phenotype annotations in the Phenoscape Knowledgebase. We compared query profiles having variable proportions of matching phenotypes to subject database profiles using both pairwise and groupwise Jaccard (edge-based) and Resnik (node-based) semantic similarity metrics, and compared statistical performance for three different levels of annotation granularity: entities alone, entities plus attributes, and entities plus qualities (with implicit attributes). All four metrics examined showed more extreme values than expected by chance when approximately half the annotations matched between the query and subject profiles, with a more sudden decline for pairwise statistics and a more gradual one for the groupwise statistics. Annotation granularity had a negligible effect on the position of the threshold at which matches could be discriminated from noise. These results suggest that coarse annotations of phenotypes, at the level of entities with or without attributes, may be sufficient to identify phenotype profiles with statistically significant semantic similarity."}, {"title": "Exploiting expression patterns across multiple gene isoforms to identify radiation response biomarkers in early-stage breast cancer patients.", "url": "https://www.biorxiv.org/content/early/2016/11/08/086322", "tag": "Bioinformatics", "abstract": "In an effort to understand the underlying biology of radiation response along with whole transcriptome effects of preoperative radiotherapy in early-stage breast tumors, we propose two efficient score-based statistical methods that exploit gene expression patterns across all available gene transcript isoforms and identify potential biomarkers in the form of differentially expressed genes and differentially enriched gene-sets. We demonstrate the effectiveness of these two methods using extensive simulation studies that show that both of our methods give improved performance, in terms of statistical power, over the most commonly used methods. By exploiting radiation-induced changes in all available gene transcript isoforms, we identified several statistically significant differentially expressed genes related to PI3K-AKT and JAK-STAT signaling pathways along with radiation-induced oncogenic signaling pathways and tumor microenvironment gene signatures that could be potential targets to improve response to radiotherapy in breast tumors."}, {"title": "Development and assessment of fully automated and globally transitive geometric morphometric methods, with application to a biological comparative dataset with high interspecific variation", "url": "https://www.biorxiv.org/content/early/2016/11/07/086280", "tag": "Bioinformatics", "abstract": "Automated geometric morphometric methods are promising tools for shape analysis in comparative biology: they improve researchers' abilities to quantify biological variation extensively (by permitting more specimens to be analyzed) and intensively (by characterizing shapes with greater fidelity). Although use of these methods has increased, automated methods have some notable limitations: pairwise correspondences are frequently inaccurate or lack transitivity (i.e., they are not defined with reference to the full sample). In this study, we reassess the accuracy of two previously published automated methods, cPDist [1] and auto3Dgm [2], and evaluate several modifications to these methods. We show that a substantial fraction of alignments and pairwise maps between specimens of highly dissimilar geometries were inaccurate in the study of Boyer et al. [1], despite a taxonomically sensitive variance structure of continuous Procrustes distances. We also show these inaccuracies can be remedied by utilizing a globally informed methodology within a collection of shapes, instead of only comparing shapes in a pairwise manner (c.f. [2]). Unfortunately, while global information generally enhances maps between dissimilar objects, it can degrade the quality of correspondences between similar objects due to the accumulation of numerical error. We explore a number of approaches to mitigate this degradation, quantify the performance of these approaches, and compare the generated pairwise maps (as well as the shape space characterized by these maps) to a \"ground truth\" obtained from landmarks manually collected by geometric morphometricians. Novel methods both improve the quality of the pairwise correspondences relative to cPDist, and achieve a taxonomic distinctiveness comparable to auto3Dgm."}, {"title": "Deep learning with feature embedding for compound-protein interaction prediction", "url": "https://www.biorxiv.org/content/early/2016/11/07/086033", "tag": "Bioinformatics", "abstract": "Accurately identifying compound-protein interactions in silico can deepen our understanding of the mechanisms of drug action and significantly facilitate the drug discovery and development process. Traditional similarity-based computational models for compound-protein interaction prediction rarely exploit the latent features from current available large-scale unlabelled compound and protein data, and often limit their usage on relatively small-scale datasets. We propose a new scheme that combines feature embedding (a technique of representation learning) with deep learning for predicting compound-protein interactions. Our method automatically learns the low-dimensional implicit but expressive features for compounds and proteins from the massive amount of unlabelled data. Combining effective feature embedding with powerful deep learning techniques, our method provides a general computational pipeline for accurate compound-protein interaction prediction, even when the interaction knowledge of compounds and proteins is entirely unknown. Evaluations on current large-scale databases of the measured compound-protein affinities, such as ChEMBL and BindingDB, as well as known drug-target interactions from DrugBank have demonstrated the superior prediction performance of our method, and suggested that it can offer a useful tool for drug development and drug repositioning."}, {"title": "Quark enables semi-reference-based compression of RNA-seq data", "url": "https://www.biorxiv.org/content/early/2016/11/05/085878", "tag": "Bioinformatics", "abstract": "Motivation: The past decade has seen an exponential increase in biological sequencing capacity, and there has been a simultaneous effort to help organize and archive some of the vast quantities of sequencing data that are being generated. While these developments are tremendous from the perspective of maximizing the scientific utility of available data, they come with heavy costs. The storage and transmission of such vast amounts of sequencing data is expensive. Results: We present Quark, a semi reference-based compression tool designed for RNA-seq data. Quark makes use of a reference sequence when encoding reads, but produces a representation that can be decoded independently, without the need for a reference. This allows Quark to achieve markedly better compression rates than existing reference-free schemes, while still relieving the burden of assuming a specific, shared reference sequence between the encoder and decoder. We demonstrate that Quark achieves state-of-the-art compression rates, and that, typically, only a small fraction of the reference sequence must be encoded along with the reads to allow reference-free decompression. Availability: Quark is implemented in C++11, and is available under a GPLv3 license at www.github.com/COMBINE-lab/quark."}, {"title": "CAMSA: a Tool for Comparative Analysis and Merging of Scaffold Assemblies", "url": "https://www.biorxiv.org/content/early/2016/11/04/069153", "tag": "Bioinformatics", "abstract": "Motivation: Despite the recent progress in genome sequencing and assembly, many of the currently available assembled genomes come in a draft form. Such draft genomes consist of a large number of genomic fragments (scaffolds), whose positions and orientations along the genome are unknown. While there exists a number of methods for reconstruction of the genome from its scaffolds, utilizing various computational and wet-lab techniques, they often can produce only partial error-prone scaffold assemblies. It therefore becomes important to compare and merge scaffold assemblies produced by different methods, thus combining their advantages and highlighting present conflicts for further investigation. These tasks may be labor intensive if performed manually. Results: We present CAMSA - a tool for comparative analysis and merging of two or more given scaffold assemblies. The tool (i) creates an extensive report with several comparative quality metrics; (ii) constructs the most confident merged scaffold assembly; and (iii) provides an interactive framework for a visual comparative analysis of the given assemblies. Among the CAMSA features, only scaffold merging can be evaluated in comparison to existing methods. Namely, it resembles the functionality of assembly reconciliation tools, although their primary targets are somewhat different. Our evaluations show that CAMSA produces merged assemblies of comparable or better quality than existing assembly reconciliation tools while being the fastest in terms of the total running time. Availability: CAMSA is distributed under the MIT license and is available at http://cblab.org/camsa/."}, {"title": "Sequence kernel association tests for large sets of markers: tail probabilities for large quadratic forms", "url": "https://www.biorxiv.org/content/early/2016/11/04/085639", "tag": "Bioinformatics", "abstract": "The Sequence Kernel Association Test (SKAT) is widely used to test for associations between a phenotype and a set of (usually rare) genetic variants. Evaluating tail probabilities or quantiles of the null distribution for SKAT requires computing the eigenvalues of a matrix related to the genotype covariance between markers. Extracting the full set of eigenvalues of this matrix (an n x n matrix, for n subjects) has computational complexity proportional to n^3. As SKAT is used when n>10^4 is common, this step becomes a major bottleneck in its use. We propose fastSKAT, a new computationally-inexpensive but accurate approximations to the tail probabilities, in which the k largest eigenvalues of a weighted genotype covariance matrix or the largest singular values of a weighted genotype matrix are extracted, and a single term based on the Satterthwaite approximation is used for the remaining eigenvalues. While the method is not particularly sensitive to the choice of k, we also describe how to choose its value, and show how fastSKAT can automatically alert users to the rare cases where the choice may affect results. As well as providing faster implementation of SKAT, the new method also enables entirely new applications of SKAT, that were not possible before; we give examples grouping variants by topologically assisted domains, and comparing chromosome-wide association by class of histone marker."}, {"title": "Secure Wavelet Matrix: Alphabet-Friendly Privacy-Preserving String Search", "url": "https://www.biorxiv.org/content/early/2016/11/04/085647", "tag": "Bioinformatics", "abstract": "Motivation: Privacy-preserving substring matching is an important task for sensitive biological/biomedical sequence database searches. It enables a user to obtain only a substring match while his/her query is concealed to a server. The previous approach for this task is based on a linear-time algorithm in terms of alphabet size |\u03a3|. Therefore, a more efficient method is needed to deal with strings with large alphabet size such as a protein sequence, time-series data, and a clinical document. Results: We present a novel algorithm that can search a string in logarithmic time of |\u03a3|. In our algorithm, named secure wavelet matrix (sWM), we use an additively homomorphic encryption to build an efficient data structure called a wavelet matrix. In an experiment using a simulated string of length 10,000 whose alphabet size ranges from 4 to 1024, the run time of the sWM was an order of magnitude faster than that of the previous method. We also tested the sWM on all sequences of one protein family in Pfam (9,826 residues in total) and clinical texts written in a natural language (77,712 letters in total). By using a laptop computer for the user and a desktop PC for the server, we found that its run time was \u22482.5 s (user) and \u22486.7 s (server) for the protein sequences and \u224810 s (user) and \u224860 s (server) for the clinical texts. Availability: https://github.com/cBioLab/sWM"}, {"title": "Assessment of Antibody Library Diversity through Next Generation Sequencing and Technical Error Compensation.", "url": "https://www.biorxiv.org/content/early/2016/11/03/085498", "tag": "Bioinformatics", "abstract": "Antibody libraries are important resources to derive antibodies to be used for a wide range of applications, from structural and functional studies to intracellular protein interference studies to developing new diagnostics and therapeutics. Whatever the goal, the key parameter for an antibody library is its diversity, i.e. the number of distinct elements in the collection, which directly reflects the probability of finding in the library an antibody against a given antigen, of sufficiently high affinity. Quantitative evaluation of antibody library diversity and quality has been for a long time inadequately addressed, due to the high similarity and length of the sequences of the library. Diversity was usually inferred by the transformation efficiency and tested either by fingerprinting and/or sequencing of a few hundred random library elements. Inferring diversity from such a small sample is, however, very rudimental and gives limited information about the real complexity, because complexity does not scale linearly with sample size. Next-generation sequencing (NGS) has opened new ways to tackle the antibody library diversity quality assessment. However, much remains to be done to fully exploit the potential of NGS for the quantitative analysis of antibody repertoires and to overcome current limitations. To obtain a more reliable antibody library complexity estimate here we show a new, PCR-free, NGS approach to sequence antibody libraries on Illumina platform, coupled to a new bioinformatic analysis and software (Diversity Estimator of Antibody Library, DEAL) that allows to reliably estimate the diversity, taking in consideration the sequencing error."}, {"title": "RNA-protein binding motifs mining with a new hybrid deep learning based cross-domain knowledge integration approach", "url": "https://www.biorxiv.org/content/early/2016/11/03/085191", "tag": "Bioinformatics", "abstract": "RNA plays important roles in cells through the interactions with proteins known as the RNA-binding proteins (RBP) and their binding motifs can provide crucial understanding of the post-transcriptional regulation of RNAs. How the RBPs correctly recognize the target RNA and why they bind specific positions is still far from clear. Artificial intelligence-based computational algorithms are widely acknowledged to be capable of speeding up this process. Although many automatic tools have been developed to predict the RNA-protein binding sites from the rapidly growing multi-resource data, e.g. from sequence, structure data etc, their different domain specific features and formats have posed significant computational challenges. One of current most difficulties is the cross-source shared common knowledge is usually at a higher abstraction level beyond the observed data, resulting in a low efficiency of direct integration of observed data in different domains. The other difficulty is how to interpret the prediction results. Existing approaches tend to terminate after outputting the potential discrete binding sites on the sequence, but how to assemble them into the meaningful binding motifs is a topic worth of further investigation. In viewing of these challenges, we proposed a deep learning-based framework (iDeep) by using a novel hybrid convolutional neural network and deep belief network to predict the RBP interaction sites and motifs on RNAs. This new protocol is featured by transforming the original observed data into a high-level abstraction feature space using multiple layers of learning blocks, where the shared representations across different domains are integrated. This bottom to up abstraction strategy is also very helpful to remove the noise effects in the observed data. To validate our iDeep method, we performed experiments on 31 large-scale CLIP-seq datasets, and our results show that by integrating multiple sources of data, the average AUC can be improved by 8% compared to the best single-source-based predictor; and through cross-domain knowledge integration at an abstraction level, it outperforms the state-of-the-art predictors by 6%. Besides the overall enhanced prediction performance, the convolutional neural network module embedded in iDeep is also able to automatically capture the interpretable RNA sequential binding motifs for RBPs. Large-scale experiments show that these mined binding motifs agree well with the experimentally verified results, suggesting iDeep is a promising approach in the real-world applications. The iDeep is available at https://github.com/xypan1232/iDeep."}, {"title": "Duplicates, redundancies, and inconsistencies in the primary nucleotide databases: a descriptive study", "url": "https://www.biorxiv.org/content/early/2016/11/03/085019", "tag": "Bioinformatics", "abstract": "GenBank, the EMBL European Nucleotide Archive, and the DNA DataBank of Japan, known collectively as the International Nucleotide Sequence Database Collaboration or INSDC, are the three most significant nucleotide sequence databases. Their records are derived from laboratory work undertaken by different individuals, by different teams, with a range of technologies and assumptions, and over a period of decades. As a consequence, they contain a great many duplicates, redundancies, and inconsistencies, but neither the prevalence nor the characteristics of various types of duplicates have been rigorously assessed. Existing duplicate detection methods in bioinformatics only address specific duplicate types, with inconsistent assumptions; and the impact of duplicates in bioinformatics databases has not been carefully assessed, making it difficult to judge the value of such methods. Our goal is to assess the scale, kinds, and impact of duplicates in bioinformatics databases, through a retrospective analysis of merged groups in INSDC databases. Our outcomes are threefold: (1) We analyse a benchmark dataset consisting of duplicates manually identified in INSDC, a dataset of 67,888 merged groups with 111,823 duplicate pairs across 21 organisms from INSDC databases in terms of the prevalence, types, and impacts of duplicates. (2) We categorise duplicates at both sequence and annotation level, with supporting quantitative statistics, showing that different organisms have different prevalence of distinct kinds of duplicate. (3) We show that the presence of duplicates has practical impact via a simple case study on duplicates, in terms of GC content and melting temperature. We demonstrate that duplicates not only introduce redundancy, but can lead to inconsistent results for certain tasks. Our findings lead to a better understanding of the problem of duplication in biological databases."}, {"title": "Benchmarks for Measurement of Duplicate Detection Methods in Nucleotide Databases", "url": "https://www.biorxiv.org/content/early/2016/11/03/085324", "tag": "Bioinformatics", "abstract": "Duplication of information in databases is a major data quality challenge. The presence of duplicates, implying either redundancy or inconsistency, can have a range of impacts on the quality of analyses that use the data. To provide a sound basis for research on this issue in databases of nucleotide sequences, we have developed new, large-scale validated collections of duplicates, which can be used to test the effectiveness of duplicate detection methods. Previous collections were either designed primarily to test efficiency, or contained only a limited number of duplicates of limited kinds. To date, duplicate detection methods have been evaluated on separate, inconsistent benchmarks, leading to results that cannot be compared and, due to limitations of the benchmarks, of questionable generality. In this study we present three nucleotide sequence database benchmarks, based on information drawn from a range of resources, including information derived from mapping to Swiss-Prot and TrEMBL. Each benchmark has distinct characteristics. We quantify these characteristics and argue for their complementary value in evaluation. The benchmarks collectively contain a vast number of validated biological duplicates; the largest has nearly half a billion duplicate pairs (although this is probably only a tiny fraction of the total that is present). They are also the first benchmarks targeting the primary nucleotide databases. The records include the 21 most heavily studied organisms in molecular biology research. Our quantitative analysis shows that duplicates in the different benchmarks, and in different organisms, have different characteristics. It is thus unreliable to evaluate duplicate detection methods against any single benchmark. For example, the benchmark derived from Swiss-Prot mappings identifies more diverse types of duplicates, showing the importance of expert curation, but is limited to coding sequences. Overall, these benchmarks form a resource that we believe will be of great value for development and evaluation of the duplicate detection methods that are required to help maintain these essential resources."}, {"title": "PBrowse: A web-based platform for real-time collaborative exploration of genomic data", "url": "https://www.biorxiv.org/content/early/2016/11/03/068049", "tag": "Bioinformatics", "abstract": "Summary: The central task of a genome browser is to enable easy visual exploration of large genomic data to gain biological insight. Most existing genome browsers were designed for data exploration by individual users, while a few allow some limited forms of collaboration among multiple users, such as file sharing and wiki-style collaborative editing of gene annotations. Our work's premise is that allowing sharing of genome browser views instantaneously in real-time enables the exchange of ideas and insight in a collaborative project, thus harnessing the wisdom of the crowd. PBrowse is a parallel-access real-time collaborative web-based genome browser that provides both an integrated, real-time collaborative platform and a comprehensive file sharing system. PBrowse also allows real-time track comment and has integrated group chat to facilitate interactive discussion among multiple users. Through the Distributed Annotation Server protocol, PBrowse can easily access a wide range of publicly available genomic data, such as the ENCODE data sets. We argue that PBrowse, with the re-designed user management, data management and novel collaborative layer based on Biodalliance, represents a paradigm shift from seeing genome browser merely as a tool of data visualisation to a tool that enables real-time human-human interaction and knowledge exchange in a collaborative setting. Availability: PBrowse is available at http://pbrowse.victorchang.edu.au, and its source code is available via the open source BSD 3 license at http://github.com/VCCRI/PBrowse. Supplementary Information: Supplementary video demonstrating collaborative feature of PBrowse is available in https://www.youtube.com/watch?v=ROvKXZoXiIc."}, {"title": "Tissue specificity of in vitro drug sensitivity", "url": "https://www.biorxiv.org/content/early/2016/11/03/085357", "tag": "Bioinformatics", "abstract": "Research in oncology traditionally focuses on specific tissue type from which the cancer develops. However, advances in high-throughput molecular profiling technologies have enabled the comprehensive characterization of molecular aberrations in multiple cancer types. It was hoped that these large-scale datasets would provide the foundation for a paradigm shift in oncology which would see tumors being classified by their molecular profiles rather than tissue types, but tumors with similar genomic aberrations may respond differently to targeted therapies depending on their tissue of origin. There is therefore a need to reassess the potential association between pharmacological response and tissue of origin for therapeutic drugs, and to test how these associations translate from preclinical to clinical settings. In this paper, we investigate the tissue specificity of drug sensitivities in large-scale pharmacological studies and compare these associations to those found in clinical trial descriptions. Our meta-analysis of the four largest in vitro drug screening datasets indicates that tissue of origin is strongly associated with drug response. We identify novel tissue-drug associations, which may present exciting new avenues for drug repurposing. One caveat is that the vast majority of the significant associations found in preclinical settings do not concur with clinical observations. Accordingly, our results call for more testing to find the root cause of the discrepancies between preclinical and clinical observations."}, {"title": "Automating Morphological Profiling with Generic Deep Convolutional Networks", "url": "https://www.biorxiv.org/content/early/2016/11/02/085118", "tag": "Bioinformatics", "abstract": "Morphological profiling aims to create signatures of genes, chemicals and diseases from microscopy images. Current approaches use classical computer vision-based segmentation and feature extraction. Deep learning models achieve state-of-the-art performance in many computer vision tasks such as classification and segmentation. We propose to transfer activation features of generic deep convolutional networks to extract features for morphological profiling. Our approach surpasses currently used methods in terms of accuracy and processing speed. Furthermore, it enables fully automated processing of microscopy images without need for single cell identification."}, {"title": "Predicting Enhancer-Promoter Interaction from Genomic Sequence with Deep Neural Networks", "url": "https://www.biorxiv.org/content/early/2016/11/02/085241", "tag": "Bioinformatics", "abstract": "In the human genome, distal enhancers are involved in regulating target genes through proximal promoters by forming enhancer-promoter interactions. However, although recently developed high-throughput experimental approaches have allowed us to recognize potential enhancer-promoter interactions genome-wide, it is still largely unknown whether there are sequence-level instructions encoded in our genome that help govern such interactions. Here we report a new computational method (named \"SPEID\") using deep learning models to predict enhancer-promoter interactions based on sequence-based features only, when the locations of putative enhancers and promoters in a particular cell type are given. Our results across six different cell types demonstrate that SPEID is effective in predicting enhancer-promoter interactions as compared to state-of-the-art methods that use non-sequence features from functional genomic signals. This work shows for the first time that sequence-based features alone can reliably predict enhancer-promoter interactions genome-wide, which provides important insights into the sequence determinants for long-range gene regulation."}, {"title": "Inference of epistatic effects leading to entrenchment and drug resistance in HIV-1 protease", "url": "https://www.biorxiv.org/content/early/2016/11/02/063750", "tag": "Bioinformatics", "abstract": "Understanding the complex mutation patterns that give rise to drug resistant viral strains provides a foundation for developing more effective treatment strategies for HIV/AIDS. Multiple sequence alignments of drug-experienced HIV-1 protease sequences contain networks of many pair correlations which can be used to build a (Potts) Hamiltonian model of these mutation patterns. Using this Hamiltonian model we translate HIV protease sequence covariation data into quantitative predictions for the probability of observing specific mutation patterns which are in agreement with the observed sequence statistics. We find that the statistical energies of the Potts model are correlated with the fitness of individual proteins containing therapy-associated mutations as estimated by in vitro measurements of protein stability and viral infectivity. We show that the penalty for acquiring primary resistance mutations depends on the epistatic interactions with the sequence background. Primary mutations which lead to drug resistance can become highly advantageous (or entrenched) by the complex mutation patterns which arise in response to drug therapy despite being destabilizing in the wildtype background. Anticipating epistatic effects is important for the design of future protease inhibitor therapies."}, {"title": "Dynamic Modeling, Parameter Estimation and Uncertainty Analysis in R", "url": "https://www.biorxiv.org/content/early/2016/11/02/085001", "tag": "Bioinformatics", "abstract": "In a wide variety of research fields, dynamic modeling is employed as an instrument to learn and understand complex systems. The differential equations involved in this process are usually non-linear and depend on many parameters whose values decide upon the characteristics of the emergent system. The inverse problem, i.e. the inference or estimation of parameter values from observed data, is of interest from two points of view. First, the existence point of view, dealing with the question whether the system is able to reproduce the observed dynamics for any parameter values. Second, the identifiability point of view, investigating invariance of the prediction under change of parameter values, as well as the quantification of parameter uncertainty. In this paper, we present the R package dMod providing a framework for dealing with the inverse problem in dynamic systems. The particularity of the approach taken by dMod is to provide and propagate accurate derivatives computed from symbolic expressions wherever possible. This derivative information highly supports the convergence of optimization routines and enhances their numerical stability, a requirement for the applicability of sofisticated uncertainty analysis methods. Computational efficiency is achieved by automatic generation and execution of C code. The framework is object oriented (S3) and provides a variety of functions to set up dynamic models, observation functions and parameter transformations for multi-conditional parameter estimation. The key elements of the framework and the methodology implemented in dMod are highlighted by an application on a three-compartment transporter model."}, {"title": "PopPAnTe: population and pedigree association testing for quantitative data", "url": "https://www.biorxiv.org/content/early/2016/11/02/084871", "tag": "Bioinformatics", "abstract": "Family-based designs, from twin studies to isolated populations with their complex genealogical data, are a valuable resource for genetic studies of heritable molecular biomarkers. Existing software for family-based studies have mainly focused on facilitating association between response phenotypes and genetic markers, and no user-friendly tools are at present available to straightforwardly extend association studies in related samples to large datasets of generic quantitative data, as those generated by current -omics technologies. We developed PopPAnTe, a user-friendly Java program, which evaluates the association of quantitative data in related samples. Additionally, PopPAnTe implements data pre and post processing, region based testing, and empirical assessment of associations. PopPAnTe is an integrated and flexible framework for pairwise association testing in related samples with a large number of predictors and response variables. It works either with family data of any size and complexity, or, when the genealogical information is unknown, it uses genetic similarity information between individuals as those inferred from genome-wide genetic data. It can therefore be particularly useful in facilitating usage of biobank data collections from population isolates when extensive genealogical information is missing."}, {"title": "PREDetector 2.0: Online and Enhanced Version of the Prokaryotic Regulatory Elements Detector Tool", "url": "https://www.biorxiv.org/content/early/2016/11/01/084780", "tag": "Bioinformatics", "abstract": "In the era that huge numbers of microbial genomes are being released in the databases, it becomes increasingly important to rapidly mine genes as well as predict the regulatory networks that control their expression. To this end, we have developed an improved and online version of the PREDetector software aimed at identifying putative transcription factor-binding sites (TFBS) in bacterial genomes. The original philosophy of PREDetector 1.0 is maintained, i.e. to allow users to freely fix the DNA-motif screening parameters, and to provide a statistical means to estimate the reliability of the prediction output. This new version offers an interactive table as well as graphics to dynamically alter the main screening parameters with automatic update of the list of identified putative TFBS. PREDetector 2.0 also has the following additional options: (i) access to genome sequences from different databases, (ii) access to weight matrices from public repositories, (iii) visualization of the predicted hits in their genomic context, (iv) grouping of hits identified in the same upstream region, (v) possibility to store the performed jobs, and (vi) automated export of the results in various formats. PREDetector 2.0 is available at http://predetector.fsc.ulg.ac.be/."}, {"title": "CMPyMOL: A Tool for Protein Contact-Map Analysis", "url": "https://www.biorxiv.org/content/early/2016/11/01/084269", "tag": "Bioinformatics", "abstract": "Contact-maps are reduced 2D representation of the 3D spatial configuration of a protein. Many valuable structural features like secondary structures, inter and intra-protein interactions, interacting domains, etc., can be readily identified from these maps. However, it is not straightforward and intuitive to reckon the spatial organization of the contact regions from reduced representation. The CMPyMOL software attempts to bridge this gap as an interactive graphical tool for protein contact-maps that interfaces with PyMOL for 3D visualization. Importantly, CMPyMOL helps understand the functional importance of contacts by providing visual overlays of various structural and biochemical properties of a protein on top of its contact-map."}, {"title": "Pavian: Interactive analysis of metagenomics data for microbiomics and pathogen identification", "url": "https://www.biorxiv.org/content/early/2016/10/31/084715", "tag": "Bioinformatics", "abstract": "Pavian is a web application for exploring metagenomics classification results, with a special focus on infectious disease diagnosis. Pinpointing pathogens in metagenomics classification results is often complicated by host and laboratory contaminants as well as many non-pathogenic microbiota. With Pavian, researchers can analyze, display and transform results from the Kraken, Centrifuge and MetaPhlAn using interactive tables and plots. Pavian further provides an alignment viewer for validation of matches to a particular genome."}, {"title": "bModelTest: Bayesian phylogenetic site model averaging and model comparison", "url": "https://www.biorxiv.org/content/early/2016/10/31/020792", "tag": "Bioinformatics", "abstract": "Background: Reconstructing phylogenies through Bayesian methods has many benefits, which include providing a mathematically sound framework, providing realistic estimates of uncertainty and being able to incorporate different sources of information based on formal principles. Bayesian phylogenetic analyses are popular for interpreting nucleotide sequence data, however for such studies one needs to specify a site model and associated substitution model. Often, the parameters of the site model is of no interest and an ad-hoc or additional likelihood based analysis is used to select a single site model. Results: bModelTest allows for a Bayesian approach to inferring and marginalizing site models in a phylogenetic analysis. It is based on trans-dimensional Markov chain Monte Carlo (MCMC) proposals that allow switching between substitution models as well as estimating the posterior probability for gamma-distributed rate heterogeneity, a proportion of invariable sites and unequal base frequencies. The model can be used with the full set of time-reversible models on nucleotides, but we also introduce and demonstrate the use of two subsets of time-reversible substitution models. Conclusion: With the new method the site model can be inferred (and marginalized) during the MCMC analysis and does not need to be pre-determined, as is now often the case in practice, by likelihood-based methods. The method is implemented in the bModelTest package of the popular BEAST 2 software, which is open source, licensed under the GNU Lesser General Public License and allows joint site model and tree inference under a wide range of models."}, {"title": "Phenopolis: an open platform for harmonization and analysis of sequencing and phenotype data", "url": "https://www.biorxiv.org/content/early/2016/10/31/084582", "tag": "Bioinformatics", "abstract": "Summary: Phenopolis is an open-source web server which provides an intuitive interface to genetic and phenotypic databases. It integrates analysis tools which include variant filtering and gene prioritisation based on phenotype. The Phenopolis platform will accelerate clinical diagnosis, gene discovery and encourage wider adoption of the Human Phenotype Ontology in the study of rare disease. Availability and Implementation: A demo of the website is available at http://phenopolis.github.io (username: demo, password: demo123). If you wish to install a local copy, souce code and installation instruction are available at https://github.com/pontikos/phenopolis. The software is implemented using Python, MongoDB, HTML/Javascript and various bash shell scripts. Contact: n.pontikos@ucl.ac.uk Supplementary information: http://phenopolis.github.io"}, {"title": "EuMicrobedbLite: A lightweight genomic resource and analytic platform for draft oomycete genomes", "url": "https://www.biorxiv.org/content/early/2016/10/31/084483", "tag": "Bioinformatics", "abstract": "We have developed EuMicrobedbLite A light weight comprehensive genome resource and sequence analysis platform for oomycete organisms. EuMicrobedbLite is a successor of the VBI Microbial Database (VMD) that was built using the Genome Unified Schema (GUS). In this version, the GUS schema has been greatly simplified with removal of many obsolete modules and redesign of others to incorporate contemporary data. Several dependencies such as perl object layers used for data loading in VMD have been replaced with independent light weight scripts. EumicrobedbLite now runs on a powerful annotation engine developed at our lab called Genome Annotator Lite. Currently this database has 26 publicly available genomes and 10 EST datasets of oomycete organisms. The browser page has dynamic tracks presenting comparative genomics analyses, coding and non-coding data, tRNA genes, repeats and EST alignments. In addition, we have defined 44,777 core conserved proteins from twelve oomycete organisms that form 2974 clusters. Synteny viewing is enabled by incorporation of the Genome Synteny Viewer (GSV) tool. The user interface has undergone major changes for ease of browsing. Queryable comparative genomics information, conserved orthologous genes and pathways are among the new key features updated in this database. The browser has been upgraded to enable user upload of GFF files for quick view of genome annotation comparisons. The toolkit page integrates the EMBOSS package and has a gene prediction tool. Annotations for the organisms are updated once every six months to ensure quality. The database resource is available at www.eumicrobedb.org."}, {"title": "Automatic genome segmentation with HMM-ANN hybrid models", "url": "https://www.biorxiv.org/content/early/2016/10/29/034579", "tag": "Bioinformatics", "abstract": "We consider the problem of automatic genome segmentation (AGS) that aims to assign discrete labels to all genomic regions based on multiple ChIP-seq samples. We propose to use a hybrid model that combines a hidden Markov model (HMM) with an artificial neural network (ANN) to overcome the weaknesses of a standard HMM. Our contributions are threefold: first, we benchmark two approaches to generate targets for ANN training on an example dataset; second, we investigate many different ANN models to identify the ones with best predictions on chromatin states; third, we test different hyper-parameters and discuss how they affect the machine learning algorithms' performance. We find our best performing models to beat two pervious state-of-the-art methods for AGS by large margins."}, {"title": "Accurate prediction of human essential genes using only nucleotide composition and association information", "url": "https://www.biorxiv.org/content/early/2016/10/28/084129", "tag": "Bioinformatics", "abstract": "Three groups recently identified essential genes in human cancer cell lines using wet experiments, and these genes are of high values. Herein, we improved the widely used Z curve method by creating a \u03bb-interval Z curve, which considered interval association information. With this method and recursive feature elimination technology, a computational model was developed to predict human gene essentiality. The 5-fold cross-validation test based on our benchmark dataset obtained an area under the receiver operating characteristic curve (AUC) of 0.8814. For the rigorous jackknife test, the AUC score was 0.8854. These results demonstrated that the essentiality of human genes could be reliably reflected by only sequence information. However, previous classifiers in three eukaryotes can gave satisfactory prediction only combining sequence with other features. It is also demonstrated that although the information contributed by interval association is less than adjacent nucleotides, this information can still play an independent role. Integrating the interval information into adjacent ones can significantly improve our classifier's prediction capacity. We re-predicted the benchmark negative dataset by Pheg server (http://cefg.uestc.edu.cn/Pheg), and 118 genes were additionally predicted as essential. Among them, 21 were found to be homologues in mouse essential genes, indicating that at least a part of the 118 genes were indeed essential, however previous experiments overlooked them. As the first available server, Pheg could predict essentiality for anonymous gene sequences of human. It is also hoped the \u03bb-interval Z curve method could be effectively extended to classification issues of other DNA elements."}, {"title": "Gist: an ensemble approach to the taxonomic classification of metatranscriptomic sequence data.", "url": "https://www.biorxiv.org/content/early/2016/10/28/081026", "tag": "Bioinformatics", "abstract": "The study of whole microbial communities through RNA-seq, or metatranscriptomics, offers a unique view of the relative levels of activity for different genes across a large number of species simultaneously. To make sense of these sequencing data, it is necessary to be able to assign both taxonomic and functional identities to each sequenced read. High-quality identifications are important not only for community profiling, but to also ensure that functional assignments of sequence reads are correctly attributed to their source taxa. Such assignments allow biochemical pathways to be appropriately allocated to discrete species, enabling the capture of cross-species interactions. Typically read annotation is performed by a single alignment-based search tool such as BLAST. However, due to the vast extent of bacterial diversity, these approaches tend to be highly error prone, particularly for taxonomic assignments. Here we introduce a novel program for generating taxonomic assignments, called Gist, which integrates information from a number of machine learning methods and the Burrows-Wheeler Aligner. Uniquely Gist establishes the most appropriate weightings of methods for individual genomes, facilitating high classification accuracy on next-generation sequencing reads. We validate our approach using a synthetic metatranscriptome generator based on Flux Simulator, termed Genepuddle. Further, unlike previous taxonomic classifiers, we demonstrate the capacity of composition-based techniques to accurately inform on taxonomic origin without resorting to longer scanning windows that mimic alignment-based methods. Gist is made freely available under the terms of the GNU General Public License at compsysbio.org/gist."}, {"title": "PatternMarkers & GWCoGAPS for novel data-driven biomarkers via whole transcriptome NMF", "url": "https://www.biorxiv.org/content/early/2016/10/28/083717", "tag": "Bioinformatics", "abstract": "Non-negative Matrix Factorization (NMF) algorithms associate gene expression with biological processes (e.g., time-course dynamics or disease subtypes). Compared with univariate associations, the relative weights of NMF solutions can obscure biomarkers. Therefore, we developed a novel PatternMarkers statistic to extract genes for biological validation and enhanced visualization of NMF results. Finding novel and unbiased gene markers with PatternMarkers requires whole-genome data. However, NMF algorithms typically do not converge for the tens of thousands of genes in genome-wide profiling. Therefore, we also developed Genome-Wide CoGAPS Analysis in Parallel Sets (GWCoGAPS), the first robust whole genome Bayesian NMF using the sparse, MCMC algorithm, CoGAPS. This software contains analytic and visualization tools including a Shiny web application, patternMatcher, which are generalized for any NMF. Using these tools, we find granular brain-region and cell-type specific signatures with corresponding biomarkers in GTex data, illustrating GWCoGAPS and patternMarkers ascertainment of data-driven biomarkers from whole-genome data. Availability: PatternMarkers & GWCoGAPS are in the CoGAPS Bioconductor package (3.5) under the GPL license."}, {"title": "Mocap: Large-scale inference of transcription factor binding sites from chromatin accessibility", "url": "https://www.biorxiv.org/content/early/2016/10/27/083998", "tag": "Bioinformatics", "abstract": "Differential binding of transcription factors (TFs) at cis-regulatory loci drives the differentiation and function of diverse cellular lineages. Understanding the regulatory interactions that underlie cell fate decisions requires characterizing TF binding sites (TFBS) across multiple cell types and conditions. Techniques, e.g. ChIP-Seq can reveal genome-wide patterns of TF binding, but typically requires laborious and costly experiments for each TF-cell-type (TFCT) condition of interest. Chromosomal accessibility assays can connect accessible chromatin in one cell type to many TFs through sequence motif mapping. Such methods, however, rarely take into account that the genomic context preferred by each factor differs from TF to TF, and from cell type to cell type. To address the differences in TF behaviors, we developed Mocap, a method that integrates chromatin accessibility, motif scores, TF footprints, CpG/GC content, evolutionary conservation and other factors in an ensemble of TFCT-specific classifiers. We show that integration of genomic features, such as CpG islands improves TFBS prediction in some TFCT. Further, we describe a method for mapping new TFCT, for which no ChIP-seq data exists, onto our ensemble of classifiers and show that our cross-sample TFBS prediction method outperforms several previously described methods."}, {"title": "Falco: A quick and flexible single-cell RNA-seq processing framework on the cloud", "url": "https://www.biorxiv.org/content/early/2016/10/27/064006", "tag": "Bioinformatics", "abstract": "Summary: Single-cell RNA-seq (scRNA-seq) is increasingly used in a range of biomedical studies. Nonetheless, current RNA-seq analysis tools are not specifically designed to efficiently process scRNA-seq data due to their limited scalability. Here we introduce Falco, a cloud-based framework to enable paralellisation of existing RNA-seq processing pipelines using big data technologies of Apache Hadoop and Apache Spark for performing massively parallel analysis of large scale transcriptomic data. Using two public scRNA-seq data sets and two popular RNA-seq alignment/feature quantification pipelines, we show that the same processing pipeline runs 2.6 - 145.4 times faster using Falco than running on a highly optimised single node analysis. Falco also allows user to the utilise low-cost spot instances of Amazon Web Services (AWS), providing a 65% reduction in cost of analysis. Availability: Falco is available via a GNU General Public License at https://github.com/VCCRI/Falco/"}, {"title": "HBP: an integrative and flexible pipeline for the interaction analysis of Hi-C dataset", "url": "https://www.biorxiv.org/content/early/2016/10/26/083576", "tag": "Bioinformatics", "abstract": "Background: The spatial organization of interphase chromatin in the nucleus play an important role in gene expression regulation and function. With the rapid development of revolutionized chromosome conformation capture technology and its genome-wide derivatives such as Hi-C, investigation of the genome folding becomes more efficient and convenient. How to robustly deal with these massive datasets and infer accurate 3D model and within-nucleus compartmentalization of chromosomes becomes a new challenge. Result: The implemented pipeline HBP (Hi-C BED file analysis Pipeline) integrates existing pipelines focusing on individual steps of Hi-C data processing into an all-in-one package with adjustable parameters to infer the consensus 3D structure of genome from raw Hi-C sequencing data. What is more, HBP could assign statistical confidence estimation for chromatin interactions, and clustering interaction loci according to enrichment tracks or topological structure automatically. Conclusion: The freely available HBP is an optimized and flexible pipeline for analyzing the folding of whole chromosome and interactions between some specific sites from the Hi-C raw sequencing reads to the partially processed datasets. The other complex genetic and epigenetic datasets from public sources such as GWAS, ENCODE consortiums etc. will also easily be integrated into HBP, hence the final output results of HBP could provide a comprehensive in-depth understanding for the specific chromatin interactions, potential molecular mechanisms and biological significance. We believe that HBP is a reliable tool for the rapidly analysis of Hi-C data and will be very useful for a wide range of researchers, particularly those who lack of background in computational biology. HBP is freely accessible at https://github.com/hechao0407/HBP/blob/master/HBP_1.0.tar.gz."}, {"title": "Connecting tumor genomics with therapeutics through multi-dimensional network modules", "url": "https://www.biorxiv.org/content/early/2016/10/25/083410", "tag": "Bioinformatics", "abstract": "Recent efforts have catalogued genomic, transcriptomic, epigenetic and proteomic changes in tumors, but connecting these data with effective therapeutics remains a challenge. In contrast, cancer cell lines can model therapeutic responses but only partially reflect tumor biology. Bridging this gap requires new methods of data integration to identify a common set of pathways and molecular events. Using MAGNETIC, a new method to integrate molecular profiling data using functional networks, we identify 219 gene modules in TCGA breast cancers that capture recurrent alterations, reveal new roles for H3K27 tri-methylation and accurately quantitate various cell types within the tumor microenvironment. We show that a significant portion of gene expression and methylation in tumors is poorly reproduced in cell lines due to differences in biology and microenvironment and MAGNETIC identifies therapeutic biomarkers that are robust to these differences. This work addresses a fundamental challenge in pharmacogenomics that can only be overcome by the joint analysis of patient and cell line data."}, {"title": "An Optimized Approach for Annotation of Large Eukaryotic Genomic Sequences using Genetic Algorithm", "url": "https://www.biorxiv.org/content/early/2016/10/25/083238", "tag": "Bioinformatics", "abstract": "Detection of important functional and/or structural elements and identifying their positions in a large eukaryotic genome is an active research area. Gene is an important functional and structural unit of DNA. The computation of gene prediction is essential for detailed genome annotation. In this paper, we propose a new gene prediction technique based on Genetic Algorithm (GA) for determining the optimal positions of exons of a gene in a chromosome or genome. The correct identification of the coding and non-coding regions are difficult and computationally demanding. The proposed genetic-based method, named Gene Prediction with Genetic Algorithm (GPGA), reduces this problem by searching only one exon at a time instead of all exons along with its introns. The advantage of this representation is that it can break the entire gene-finding problem into a number of smaller subspaces and thereby reducing the computational complexity. We tested the performance of the GPGA with some benchmark datasets and compared the results with the well-known and relevant techniques. The comparison shows the better or comparable performance of the proposed method (GPGA). We also used GPGA for annotating the human chromosome 21 (HS21) using cross species comparison with the mouse orthologs."}, {"title": "normR: Regime enrichment calling for ChIP-seq data", "url": "https://www.biorxiv.org/content/early/2016/10/25/082263", "tag": "Bioinformatics", "abstract": "ChIP-seq probes genome-wide localization of DNA-associated proteins. To mitigate technical biases ChIP-seq read densities are normalized to read densities obtained by a control. Our statistical framework \"normR\" achieves a sensitive normalization by accounting for the effect of putative protein-bound regions on the overall read statistics. Here, we demonstrate normR's suitability in three studies: (i) calling enrichment for high (H3K4me3) and low (H3K36me3) signal-to-ratio data; (ii) identifying two previously undescribed H3K27me3 and H3K9me3 heterochromatic regimes of broad and peak enrichment; and (iii) calling differential H3K4me3 or H3K27me3-enrichment between HepG2 hepatocarcinoma cells and primary human Hepatocytes. normR is readily available on http://bioconductor.org/packages/normr"}, {"title": "MathIOmica: An Integrative Platform for Dynamic Omics", "url": "https://www.biorxiv.org/content/early/2016/10/25/074260", "tag": "Bioinformatics", "abstract": "Multiple omics data are rapidly becoming available, necessitating the use of new methods to integrate different technologies and interpret the results arising from multimodal assaying. The MathIOmica package for Mathematica provides one of the first extensive introductions to the use of the Wolfram Language to tackle such problems in bioinformatics. The package particularly addresses the necessity to integrate multiple omics information arising from dynamic profiling in a personalized medicine approach. It provides multiple tools to facilitate bioinformatics analysis, including importing data, annotating datasets, tracking missing values, normalizing data, clustering and visualizing the classification of data, carrying out annotation and enumeration of ontology memberships and pathway analysis. We anticipate MathIOmica to not only help in the creation of new bioinformatics tools, but also in promoting interdisciplinary investigations, particularly from researchers in mathematical, physical science and engineering fields transitioning into genomics, bioinformatics and omics data integration."}, {"title": "Towards an ontology-based recommender system for relevant bioinformatics workflows", "url": "https://www.biorxiv.org/content/early/2016/10/24/082776", "tag": "Bioinformatics", "abstract": "With the large and diverse type of biological data, bioinformatic solutions are being more complex and computationally intensive. New specialized data skills need to be acquired by researchers in order to follow this development. Workflow Management Systems rise as an efficient way to automate tasks through abstract models in order to assist users during their problem solving tasks. However, current solutions could have several problems in reusing the developed models for given tasks. The large amount of heterogenous data and the lack of knowledge in using bioinformatics tools could mislead the users during their analyses. To tackle this issue, we propose an ontology-based workflow-mining framework generating semantic models of bioinformatic best practices in order to assist scientists. To this end, concrete workflows are extracted from scientific articles and then mined using a rich domain ontology. Discovered frequent workflows would help to build a robust pattern-based recommender system using relevant concepts and relations of practices. In this study, we explore the specific topics of phylogenetic analyses. We annotated more than 300 recent articles using different ontological concepts and relations. Relative supports (frequencies) of discovered workflow components in texts show interesting results of relevant resources currently used in the different phylogenetic analysis steps. Mining concrete workflows from texts lead us to discover abstract but relevant patterns of the best combinations of tools, parameters and input data for specific phylogenetic problems. Extracted patterns would make workflows more intuitive and easy to be reused in similar situations. This could provide a stepping-stone into the identification of best practices and pave the road to a recommender system."}, {"title": "LR-DNase: Predicting TF binding from DNase-seq data", "url": "https://www.biorxiv.org/content/early/2016/10/24/082594", "tag": "Bioinformatics", "abstract": "Transcription factors play a key role in the regulation of gene expression. Hypersensitivity to DNase I cleavage has long been used to gauge the accessibility of genomic DNA for transcription factor binding and as an indicator of regulatory genomic locations. An increasing amount of ChIP-seq data on a large number of TFs is being generated, mostly in a small number of cell types. DNase-seq data are being produced for hundreds of cell types. We aimed to develop a computational method that could combine ChIP-seq and DNase-seq data to predict TF binding sites in a wide variety of cell types. We trained and tested a logistic regression model, called LR-DNase, to predict binding sites for a specific TF using seven features derived from DNase-seq and genomic sequence. We calculated the area under the precision-recall curve at a false discovery rate cutoff of 0.5 for the LR-DNase model, a number of logistic regression models with fewer features, and several existing state-of-the-art TF binding prediction methods. The LR-DNase model outperformed existing unsupervised and supervised methods. Additionally, for many TFs, a model that uses only two features, DNase-seq reads and motif score, was sufficient to match the performance of the best existing methods."}, {"title": "GSKB: A gene set database for pathway analysis in mouse", "url": "https://www.biorxiv.org/content/early/2016/10/24/082511", "tag": "Bioinformatics", "abstract": "Interpretation of high-throughput genomics data based on biological pathways constitutes a constant challenge, partly because of the lack of supporting pathway database. In this study, we created a functional genomics knowledgebase in mouse, which includes 33,261 pathways and gene sets compiled from 40 sources such as Gene Ontology, KEGG, GeneSetDB, PANTHER, microRNA and transcription factor target genes, etc. In addition, we also manually collected and curated 8,747 lists of differentially expressed genes from 2,526 published gene expression studies to enable the detection of similarity to previously reported gene expression signatures. These two types of data constitute a Gene Set Knowledgebase (GSKB), which can be readily used by various pathway analysis software such as gene set enrichment analysis (GSEA). Using our knowledgebase, we were able to detect the correct microRNA (miR-29) pathway that was suppressed using antisense oligonucleotides and confirmed its role in inhibiting fibrogenesis, which might involve upregulation of transcription factor SMAD3. The knowledgebase can be queried as a source of published gene lists for further meta-analysis. Through meta-analysis of 56 published gene lists related to retina cells, we revealed two fundamentally different types of gene expression changes. One is related to stress and inflammatory response blamed for causing blindness in many diseases; the other associated with visual perception by normal retina cells. GSKB is available online at http://ge-lab.org/gs/, and also as a Bioconductor package (gskb, https://bioconductor.org/packages/gskb/). This database enables in-depth interpretation of mouse genomics data both in terms of known pathways and the context of thousands of published expression signatures."}, {"title": "Ontology-based workflow extraction from texts using word sense disambiguation", "url": "https://www.biorxiv.org/content/early/2016/10/24/082784", "tag": "Bioinformatics", "abstract": "This paper introduces a method for automatic workflow extraction from texts using Process-Oriented Case-Based Reasoning (POCBR). While the current workflow management systems implement mostly different complicated graphical tasks based on advanced distributed solutions e.g. cloud computing and grid computation), workflow knowledge acquisition from texts using case-based reasoning represents more expressive and semantic cases representations. We propose in this context, an ontology-based workflow extraction framework to acquire processual knowledge from texts. Our methodology extends classic NLP techniques to extract and disambiguate tasks in texts. Using a graph-based representation of workflows and a domain ontology, our extraction process uses a context-based approach to recognize workflow components: data and control flows. We applied our framework in a technical domain in bioinformatics: i.e. phylogenetic analyses. An evaluation based on workflow semantic similarities on a gold standard proves that our approach provides promising results in the process extraction domain. Both data and implementation of our framework are available http://labo.bioinfo.uqam.ca/tgrowler."}, {"title": "The presence or absence alone of miRNA isoforms (isomiRs) successfully discriminate amongst the 32 TCGA cancer types", "url": "https://www.biorxiv.org/content/early/2016/10/24/082685", "tag": "Bioinformatics", "abstract": "Previously, we demonstrated that miRNA isoforms (isomiRs) are constitutive and their expression profiles depend on tissue, tissue state, and disease subtype. We have now extended our isomiR studies to The Cancer Genome Atlas (TCGA) repository. Specifically, we studied whether isomiR profiles can distinguish amongst the 32 cancers. We analyzed 10,271 datasets from 32 cancers and found 7,466 isomiRs from 807 miRNA hairpin-arms to be expressed above threshold. Using the top 20% most abundant isomiRs, we built a classifier that relied on \"binary\" isomiR profiles: isomiRs were simply represented as \"present\" or \"absent\" and, unlike previous methods, all knowledge about their expression levels was ignored. The classifier could label tumor samples with an average sensitivity of 93% and a False Discovery Rate of 3%. Notably, its ability to classify well persisted even when we reduced the set of used features (=isomiRs) by a factor of 10. A counterintuitive finding of our analysis is that the isomiRs and miRNA loci with the highest ability to classify tumors are not the ones that have been attracting the most research attention in the miRNA field. Our results provide a framework in which to study cancer-type-specific isomiRs and explore their potential uses as cancer biomarkers."}, {"title": "QuickStitch for seamless stitching of confocal mosaics through high-pass filtering and recursive normalization", "url": "https://www.biorxiv.org/content/early/2016/10/24/075440", "tag": "Bioinformatics", "abstract": "Fluorescence micrographs naturally exhibit darkening around their edges (vignetting), which makes seamless stitching challenging. If vignetting is not corrected for, a stitched image will have visible seams where the individual images (tiles) overlap, introducing a systematic error into any quantitative analysis of the image. Although multiple vignetting correction methods exist, there remains no open-source tool that robustly handles large 2D immunofluorescence-based mosaic images. Here, we develop and validate QuickStitch, a tool that applies a recursive normalization algorithm to stitch large-scale immunofluorescence-based mosaics without incurring vignetting seams. We demonstrate how the tool works successfully for tissues of differing size, morphology, and fluorescence intensity. QuickStitch requires no specific information about the imaging system. It is provided as an open-source tool that is both user friendly and extensible, allowing straightforward incorporation into existing image processing pipelines. This enables studies that require accurate segmentation and analysis of high-resolution datasets when parameters of interest include both cellular-level phenomena and larger tissue-level regions of interest."}, {"title": "CASTOR: A machine learning platform for reproducible viral genome classification", "url": "https://www.biorxiv.org/content/early/2016/10/24/082768", "tag": "Bioinformatics", "abstract": "Motivation: Advances in cloning and sequencing technology yielded a massive number of genome of virus strains. The classification and annotation of these genomes constitute important assets in the discovery of genomic variability, taxonomic characteristics and disease mechanisms. Existing classification methods are often designed for a well-studied virus. Thus, the viral comparative genomic studies could benefit from more generic, fast and accurate tools for classifying and typing newly sequenced strains of diverse virus families. Results: Here, we introduce a fast, accurate and generic virus classification platform, CASTOR, based on a machine learning approach. CASTOR is inspired by a well-known technique in molecular biology: Restriction Fragment Length Polymorphism (RFLP). It simulates the restriction digestion of genomic material by different enzymes into fragments in-silico. It uses two metrics to construct feature vectors for machine learning algorithms in the classification step. We benchmark CASTOR for the classification of distinct datasets of Human Papillomaviruses (HPV), Hepatitis B Viruses (HBV) and Human Immunodeficiency viruses (HIV). Results reveal true positive rates of 99%, 99% and 98% for HPV Alpha species, HBV genotyping and HIV M group subtyping respectively. Furthermore, CASTOR shows a competitive performance compare to well-known HIV-specific classifier REGA and COMET on whole genome and pol fragments. With such prediction rates, genericity and robustness, as well as rapidity, such approach could constitute a reference in large-scale virus studies. Finally, we developed the CASTOR web platform for open access and reproducible viral machine learning classifiers. Availability: http://castor.bioinfo.uqam.ca"}, {"title": "Selection Corrected Statistical Inference for Region Detection with High-throughput Assays", "url": "https://www.biorxiv.org/content/early/2016/10/23/082321", "tag": "Bioinformatics", "abstract": "Scientists use high-dimensional measurement assays to detect and prioritize regions of strong signal in a spatially organized domain. Examples include finding methylation enriched genomic regions using microarrays and identifying active cortical areas using brain-imaging. The most common procedure for detecting potential regions is to group together neighboring sites where the signal passed a threshold. However, one needs to account for the selection bias induced by this opportunistic procedure to avoid diminishing effects when generalizing to a population. In this paper, we present a model and a method that permit population inference for these detected regions. In particular, we provide non-asymptotic point and confidence interval estimates for mean effect in the region, which account for the local selection bias and the non-stationary covariance that is typical of these data. Such summaries allow researchers to better compare regions of different sizes and different correlation structures. Inference is provided within a conditional one-parameter exponential family for each region, with truncations that match the constraints of selection. A secondary screening-and-adjustment step allows pruning the set of detected regions, while con- trolling the false-coverage rate for the set of regions that are reported. We illustrate the benefits of the method by applying it to detected genomic regions with differing DNA-methylation rates across tissue types. Our method is shown to provide superior power compared to non-parametric approaches."}, {"title": "Rabix: an open-source workflow executor supporting recomputability and interoperability of workflow descriptions", "url": "https://www.biorxiv.org/content/early/2016/10/23/074708", "tag": "Bioinformatics", "abstract": "As biomedical data has become increasingly easy to generate in large quantities, the methods used to analyze it have proliferated rapidly. Reproducible and reusable methods are required to learn from large volumes of data reliably. To address this issue, numerous groups have developed workflow specifications or execution engines, which provide a framework with which to perform a sequence of analyses. One such specification is the Common Workflow Language, an emerging standard which provides a robust and flexible framework for describing data analysis tools and workflows. In addition, reproducibility can be furthered by executors or workflow engines which interpret the specification and enable additional features, such as error logging, file organization, optimizations to computation and job scheduling, and allow for easy computing on large volumes of data. To this end, we have developed the Rabix Executora, an open-source workflow engine for the purposes of improving reproducibility through reusability and interoperability of workflow descriptions."}, {"title": "Methods for detecting co-mutated pathways in cancer samples to inform treatment selection", "url": "https://www.biorxiv.org/content/early/2016/10/22/082552", "tag": "Bioinformatics", "abstract": "Tumor genomes evolve through a selection of mutations. These mutations may complement each other to promote tumorigenesis. To better understand the functional interactions of different processes in cancer, we studied mutation data of a set of tumors and identified significantly co-mutated pathways. Fisher's exact test is a standard approach that can be used to assess the significance of the joint dysregulation of pathways pairs across a patient population. We developed a robust test to identify co-occurrence using DNA mutations, which overcomes deficiencies of the Fisher's exact test by taking into account the large variability in overall mutation load and sequencing depth. Applying our method to a study of six common cancer types, we identify enrichment of co-mutated signal transduction pathways such as IP3 synthesis and PI3K and pairs of co-mutated pathways involving other processes such as immunity and development. We observed enrichment of clonal co-mutation of the proteasome and apoptosis pathways in colorectal cancer, which suggests potential mechanisms for immune evasion."}, {"title": "Binary Relation Extraction from Biomedical Literature using Dependency Trees and SVMs", "url": "https://www.biorxiv.org/content/early/2016/10/21/082479", "tag": "Bioinformatics", "abstract": "The goal of Biomedical relation extraction is to uncover high-quality relations from life science literature with diverse applications in the fields of Biology and Medicine. In the last decade, several methods can be found in published literature ranging from binary to complex relation extraction. In this work, we present a binary relation extraction system that relies on sentence level dependency features. We use a novel approach to map dependency tree based rules to feature vectors that can be used to train a classifier. We build a SVM classifier using these feature vectors and our experimental results show that it outperforms simple co-occurrence and rule-based systems. Through our experiments, using two 'real-world' examples, we quantify the positive impact of improved relation extraction on Literature Based Discovery."}, {"title": "Transformation and model choice for RNA-seq co-expression analysis", "url": "https://www.biorxiv.org/content/early/2016/10/21/065607", "tag": "Bioinformatics", "abstract": "Although a large number of clustering algorithms have been proposed to identify groups of co-expressed genes from microarray data, the question of if and how such methods may be applied to RNA-seq data remains unaddressed. In this work, we investigate the use of data transformations in conjunction with Gaussian mixture models for RNA-seq co-expression analyses, as well as a penalized model selection criterion to select both an appropriate transformation and number of clusters present in the data. This approach has the advantage of accounting for per-cluster correlation structures among samples, which can be quite strong in real RNA-seq data. In addition, it provides a rigorous statistical framework for parameter estimation, an objective assessment of data transformations and number of clusters, and the possibility of performing diagnostic checks on the quality and homogeneity of the identified clusters. We analyze four varied RNA-seq datasets to illustrate the use of transformations and model selection in conjunction with Gaussian mixture models. Finally, we propose an R package coseq (co-expression of RNA-seq data) to facilitate implementation and visualization of the recommended RNA-seq co-expression analyses."}, {"title": "Lowest expressing microRNAs capture indispensable information - identifying cancer types", "url": "https://www.biorxiv.org/content/early/2016/10/20/082081", "tag": "Bioinformatics", "abstract": "The primary function of microRNAs (miRNAs) is to maintain cell homeostasis. In cancerous tissues miRNAs expression undergo drastic alterations. In this study, we used miRNA expression profiles from The Cancer Genome Atlas (TCGA) of 24 cancer types and 3 healthy tissues, collected from >8500 samples. We seek to classify the cancers origin and tissue identification using the expression from 1046 reported miRNAs. Despite an apparent uniform appearance of miRNAs among cancerous samples, we recover indispensable information from lowly expressed miRNAs regarding the cancer/tissue types. Multiclass support vector machine classification yields an average recall of 58% in identifying the correct tissue and tumor types. Data discretization has led to substantial improvement reaching an average recall of 91% (95% median). We propose a straightforward protocol as a crucial step in classifying tumors of unknown primary origin. Our counter-intuitive conclusion is that in almost all cancer types, highly expressing miRNAs mask the significant signal that lower expressed miRNAs provide."}, {"title": "Ribbon: Visualizing complex genome alignments and structural variation", "url": "https://www.biorxiv.org/content/early/2016/10/20/082123", "tag": "Bioinformatics", "abstract": "Visualization has played an extremely important role in the current genomic revolution to inspect and understand variants, expression patterns, evolutionary changes, and a number of other relationships. However, most of the information in read-to-reference or genome-genome alignments is lost for structural variations in the one-dimensional views of most genome browsers showing only reference coordinates. Instead, structural variations captured by long reads or assembled contigs often need more context to understand, including alignments and other genomic information from multiple chromosomes. We have addressed this problem by creating Ribbon (genomeribbon.com) an interactive online visualization tool that displays alignments along both reference and query sequences, along with any associated variant calls in the sample. This way Ribbon shows patterns in alignments of many reads across multiple chromosomes, while allowing detailed inspection of individual reads (Supplementary Note 1). For example, here we show a gene fusion in the SK-BR-3 breast cancer cell line linking the genes CYTH1 and EIF3H. While it has been found in the transcriptome previously, genome sequencing did not identify a direct chromosomal fusion between these two genes. After SMRT sequencing, Ribbon shows that there are indeed long reads that span from one gene to the other, going through not one but two variants, for the first time showing the genomic link between these two genes (Figure 1a). More gene fusions of this cancer cell line are investigated in Supplementary Note 2. Figure 1b shows another complex event in this sample made simple in Ribbon: the translocation of a 4.4 kb sequence deleted from chr19 and inserted into chr16 (Figure 1b). Thus, Ribbon enables understanding of complex variants, and it may also help in the detection of sequencing and sample preparation issues, testing of aligners and variant-callers, and rapid curation of structural variant candidates (Supplementary Note 3). In addition to SAM and BAM files with long, short, or paired-end reads, Ribbon can also load coordinate files from whole genome aligners such as MUMmer. Therefore, Ribbon can be used to test assembly algorithms or inspect the similarity between species. Supplementary Note 4 shows a comparison of gorilla and human genomes using Ribbon, highlighting major structural differences. In conclusion, Ribbon is a powerful interactive web tool for viewing complex genomic alignments."}, {"title": "Tissue-aware RNA-Seq processing and normalization for heterogeneous and sparse data", "url": "https://www.biorxiv.org/content/early/2016/10/20/081802", "tag": "Bioinformatics", "abstract": "Although ultrahigh-throughput RNA-Sequencing has become the dominant technology for genome-wide transcriptional profiling, the vast majority of RNA-Seq studies typically profile only tens of samples, and most analytical pipelines are optimized for these smaller studies. However, projects are generating ever-larger data sets comprising RNA-Seq data from hundreds or thousands of samples, often collected at multiple centers and from diverse tissues. These complex data sets present significant analytical challenges due to batch and tissue effects, but provide the opportunity to revisit the assumptions and methods that we use to preprocess, normalize, and filter RNA-Seq data - critical first steps for any subsequent analysis. We find analysis of large RNA-Seq data sets requires both careful quality control and that one account for sparsity due to the heterogeneity intrinsic in multi-group studies. An R package instantiating our method for large-scale RNA-Seq normalization and preprocessing, YARN, is available at bioconductor.org/packages/yarn."}, {"title": "Portable and Error-Free DNA-Based Data Storage", "url": "https://www.biorxiv.org/content/early/2016/10/19/079442", "tag": "Bioinformatics", "abstract": "DNA-based data storage is an emerging nonvolatile memory technology of potentially unprecedented density, durability, and replication efficiency. The basic system implementation steps include synthesizing DNA strings that contain user information and subsequently reading them via high-throughput sequencing technologies. All existing architectures enable reading and writing, while some also allow for editing and elementary sequencing error correction. However, none of the current architectures offers error-free and random-access readouts from a portable device. Here we show through experimental and theoretical verification that such a platform may be easily implemented in practice using MinION sequencers. The gist of the approach is to design an integrated pipeline that encodes data to avoid synthesis and sequencing errors, enables random access through addressing, and leverages efficient portable nanopore sequencing via new anchored iterative alignment and insertion/deletion error-correcting codes. Our work represents the only known random access DNA-based data storage system that uses error-prone MinION sequencers and produces error-free readouts with the highest reported information rate and density."}, {"title": "Combining transcription factor binding affinities with open-chromatin data for accurate gene expression prediction", "url": "https://www.biorxiv.org/content/early/2016/10/19/081935", "tag": "Bioinformatics", "abstract": "The binding and contribution of transcription factors (TF) to cell specific gene expression is often deduced from open-chromatin measurements to avoid costly TF ChIP-seq assays. Thus, it is important to develop computational methods for accurate TF binding prediction in open-chromatin regions (OCRs). Here, we report a novel segmentation-based method, TEPIC, to predict TF binding by combining sets of OCRs with position weight matrices. TEPIC can be applied to various open-chromatin data, e.g. DNaseI-seq and NOMe-seq. Additionally, Histone-Marks (HMs) can be used to identify candidate TF binding sites. TEPIC computes TF affinities and uses open-chromatin/HM signal intensity as quantitative measures of TF binding strength. Using machine learning, we find low affinity binding sites to improve our ability to explain gene expression variability compared to the standard presence/absence classification of binding sites. Further, we show that both footprints and peaks capture essential TF binding events and lead to a good prediction performance. In our application, gene-based scores computed by TEPIC with one open-chromatin assay nearly reach the quality of several TF ChIP-seq datasets. Finally, these scores correctly predict known transcriptional regulators as illustrated by the application to novel DNaseI-seq and NOMe-seq data for primary human hepatocytes and CD4+ T-cells, respectively."}, {"title": "False Negatives Are a Significant Feature of Next Generation Sequencing Callsets", "url": "https://www.biorxiv.org/content/early/2016/10/18/066043", "tag": "Bioinformatics", "abstract": "Short-read, next-generation sequencing (NGS) is now broadly used to identify rare or de novo mutations in population samples and disease cohorts. However, NGS data is known to be error-prone and post-processing pipelines have primarily focused on the removal of spurious mutations or \"false positives\" for downstream genome datasets. Less attention has been paid to characterizing the fraction of missing mutations or \"false negatives\" (FN). Here we interrogate several publically available human NGS autosomal variant datasets using corresponding Sanger sequencing as a truth-set. We examine both low-coverage Illumina and high-coverage Complete Genomics genomes. We show that the FN rate varies between 3%-18% and that false-positive rates are considerably lower (<3%) for publically available human genome callsets like 1000 Genomes. The FN rate is strongly dependent on calling pipeline parameters, as well as read coverage. Our results demonstrate that missing mutations are a significant feature of genomic datasets and imply additional fine-tuning of bioinformatics pipelines is needed. To address this, we design a phylogeny-aware tool [PhyloFaN] which can be used to quantify the FN rate for haploid genomic experiments, without additional generation of validation data. Using PhyloFaN on ultra-high coverage NGS data from both Illumina HiSeq and Complete Genomics platforms derived from the 1000 Genomes Project, we characterize the false negative rate in human mtDNA genomes. The false negative rate for the publically available mtDNA callsets is 17-20%, even for extremely high coverage haploid data."}, {"title": "Semi-Automated Identification of Ontological Labels in the Biomedical Literature with goldi", "url": "https://www.biorxiv.org/content/early/2016/10/18/073460", "tag": "Bioinformatics", "abstract": "Recent growth in both the scale and the scope of large publicly available ontologies has spurred the development of computational methodologies which can leverage structured information to answer important questions. However, ontological labels, or \"terms\" have thus far proved difficult to use in practice; text mining, one crucial aspect of electronically understanding and parsing the biomedical literature, has historically had difficulty identifying terms in literature. In this article, we present goldi, an open source R package whose goal it is to identify terms of variable length in free form text. It is available at https://github.com/Chris1221/goldi. The algorithm works through identifying words or synonyms of words present in individual terms and comparing the number of present words to an acceptance function for decision making. In this article we present the theoretical rationale behind the algorithm, as well as practical advice for its usage applied to Gene Ontology term identification and quantification. We additionally detail the options available and describe their respective computational efficiencies."}, {"title": "Overlapping long sequence reads: Current innovations and challenges in developing sensitive, specific and scalable algorithms", "url": "https://www.biorxiv.org/content/early/2016/10/17/081596", "tag": "Bioinformatics", "abstract": "Identifying overlaps between error-prone long reads, specifically those from Oxford Nanopore Technologies (ONT) and Pacific Biosciences (PB), is essential for certain downstream applications, including error correction and de novo assembly. Though akin to the read-to-reference alignment problem, read-to-read overlap detection is a distinct problem that can benefit from specialized algorithms that perform efficiently and robustly on high error rate long reads. Here, we review the current state-of-the-art read-to-read overlap tools for error-prone long reads, including BLASR, DALIGNER, MHAP, GraphMap, and Minimap. These specialized bioinformatics tools differ not just in their algorithmic designs and methodology, but also in their robustness of performance on a variety of datasets, time and memory efficiency, and scalability. We highlight the algorithmic features of these tools, as well as their potential issues and biases when utilizing any particular method. We benchmarked these tools, tracking their resource needs and computational performance, and assessed the specificity and precision of each. The concepts surveyed may apply to future sequencing technologies, as scalability is becoming more relevant with increased sequencing throughput."}, {"title": "FIDDLE: An integrative deep learning framework for functional genomic data inference", "url": "https://www.biorxiv.org/content/early/2016/10/17/081380", "tag": "Bioinformatics", "abstract": "Numerous advances in sequencing technologies have revolutionized genomics through generating many types of genomic functional data. Statistical tools have been developed to analyze individual data types, but there lack strategies to integrate disparate datasets under a unified framework. Moreover, most analysis techniques heavily rely on feature selection and data preprocessing which increase the difficulty of addressing biological questions through the integration of multiple datasets. Here, we introduce FIDDLE (Flexible Integration of Data with Deep LEarning) an open source data-agnostic flexible integrative framework that learns a unified representation from multiple data types to infer another data type. As a case study, we use multiple Saccharomyces cerevisiae genomic datasets to predict global transcription start sites (TSS) through the simulation of TSS-seq data. We demonstrate that a type of data can be inferred from other sources of data types without manually specifying the relevant features and preprocessing. We show that models built from multiple genome-wide datasets perform profoundly better than models built from individual datasets. Thus FIDDLE learns the complex synergistic relationship within individual datasets and, importantly, across datasets."}, {"title": "Automatic time-series phenotyping using massive feature extraction", "url": "https://www.biorxiv.org/content/early/2016/10/17/081463", "tag": "Bioinformatics", "abstract": "Phenotype measurements frequently take the form of time series, but we currently lack a systematic method for relating these complex data streams to scientifically meaningful outcomes, such as relating the movement dynamics of a model organism to their genotype, or measurements of brain dynamics of a patient to their disease diagnosis. Here we report a new tool, hctsa, that automatically selects interpretable and useful properties of time series by comparing over 7700 time-series features drawn from diverse scientific literatures. Using exemplar applications to high throughput phenotyping experiments, we show how hctsa allows researchers to leverage decades of time-series research to understand and quantify informative structure in time-series data."}, {"title": "Semi-automated genome annotation using epigenomic data and Segway", "url": "https://www.biorxiv.org/content/early/2016/10/17/080382", "tag": "Bioinformatics", "abstract": "Biochemical techniques measure many individual properties of chromatin along the genome. These properties include DNA accessibility (measured by DNase-seq) and the presence of individual transcription factors and histone modifications (measured by ChIP-seq). Segway is software that transforms multiple datasets on chromatin properties into a single annotation of the genome that a biologist can more easily interpret. This protocol describes how to use Segway to annotate the genome, starting with reads from a ChIP-seq experiment. It includes pre-processing of data, training the Segway model, annotating the genome, assigning biological meanings to labels, and visualizing the annotation in a genome browser."}, {"title": "Identification of new bacterial type III secreted effectors with a recursive Hidden Markov Model profile-alignment strategy", "url": "https://www.biorxiv.org/content/early/2016/10/16/081265", "tag": "Bioinformatics", "abstract": "To identify new bacterial type III secreted effectors is computationally a big challenge. At least a dozen machine learning algorithms have been developed, but so far have only achieved limited success. Sequence similarity appears important for biologists but is frequently neglected by algorithm developers for effector prediction, although large success was achieved in the field with this strategy a decade ago. In this study, we propose a recursive sequence alignment strategy with Hidden Markov Models, to comprehensively find homologs of known YopJ/P full-length proteins, effector domains and N-terminal signal sequences. Using this method, we identified 155 different YopJ/P-family effectors and 59 proteins with YopJ/P N-terminal signal sequences from 27 genera and more than 70 species. Among these genera, we also identified one type III secretion system (T3SS) from Uliginosibacterium and two T3SSs from Rhizobacter for the first time. Higher conservation of effector domains, N-terminal fusion of signal sequences to other effectors, and the exchange of N-terminal signal sequences between different effector proteins were frequently observed for YopJ/P-family proteins. This made it feasible to identify new effectors based on separate similarity screening for the N-terminal signal peptides and the effector domains of known effectors. This method can also be applied to search for homologues of other known T3SS effectors."}, {"title": "UNOISE2: improved error-correction for Illumina 16S and ITS amplicon sequencing", "url": "https://www.biorxiv.org/content/early/2016/10/15/081257", "tag": "Bioinformatics", "abstract": "Amplicon sequencing of tags such as 16S and ITS ribosomal RNA is a popular method for investigating microbial populations. In such experiments, sequence errors caused by PCR and sequencing are difficult to distinguish from true biological variation. I describe UNOISE2, an updated version of the UNOISE algorithm for denoising (error-correcting) Illumina amplicon reads and show that it has comparable or better accuracy than DADA2."}, {"title": "ALTRE: workflow for defining ALTered Regulatory Elements using chromatin accessibility data", "url": "https://www.biorxiv.org/content/early/2016/10/14/080564", "tag": "Bioinformatics", "abstract": "Regulatory elements regulate gene transcription, and their location and accessibility is cell-type specific, particularly for enhancers. Mapping and comparing chromatin accessibility between different cell types may identify mechanisms involved in cellular development and disease progression. To streamline and simplify differential analysis of regulatory elements genome-wide using chromatin accessibility data (e.g. DNase-seq, ATAC-seq), we developed ALTRE (ALTered Regulatory Elements), an R-package and associated R Shiny web app. ALTRE makes such analysis accessible to a wide range of users - from novice to practiced computational biologists."}, {"title": "Benchmarking substrate-based kinase activity inference using phosphoproteomic data", "url": "https://www.biorxiv.org/content/early/2016/10/14/080978", "tag": "Bioinformatics", "abstract": "Motivation: Phosphoproteomic experiments are increasingly used to study the changes in signalling occurring across different conditions. It has been proposed that changes in phosphorylation of kinase target sites can be used to infer when a kinase activity is under regulation. However, these approaches have not yet been benchmarked due to a lack of appropriate benchmarking strategies. Results: We curated public phosphoproteomic experiments to identify a gold standard dataset containing a total of 184 kinase-condition pairs where regulation is expected to occur. A list of kinase substrates was compiled and used to estimate changes in kinase activities using the following methods: Z-test, Kolmogorov Smirnov test, Wilcoxon rank sum test, gene set enrichment analysis (GSEA), and a multiple linear regression model (MLR). We also tested weighted variants of the Z-test, and GSEA that include information on kinase sequence specificity as proxy for affinity. Finally, we tested how the number of known substrates and the type of evidence (in vivo, in vitro or in silico) supporting these influence the predictions. Conclusions: Most models performed well with the Z-test and the GSEA performing best as determined by the area under the ROC curve (Mean AUC=0.722). Weighting kinase targets by the kinase target sequence preference improves the results only marginally. However, the number of known substrates and the evidence supporting the interactions has a strong effect on the predictions."}, {"title": "AGFusion: annotate and visualize gene fusions", "url": "https://www.biorxiv.org/content/early/2016/10/14/080903", "tag": "Bioinformatics", "abstract": "The discovery of novel gene fusions in tumor samples has rapidly accelerated with the rise of next-generation sequencing. A growing number of tools enable discovery of gene fusions from RNA-seq data. However it is likely that not all gene fusions are driving tumors. Assessing the potential functional consequences of a fusion is critical to understand their driver role. It is also challenging as gene fusions are described by chromosomal breakpoint coordinates that need to be translated into an actual amino acid fusion sequence and predicted domain architecture of the fusion proteins. Currently there are no easy-to-use tools that can automatically reconstruct and visualize fusion proteins from genomic breakpoints. To facilitate the functional interpretation of gene fusions, we developed AGFusion, available as an online web tool that can be readily used by non-computational researchers as well as a python package that can be built into computational pipelines. With minimal input from the user, AGFusion predicts the cDNA, CDS, and protein sequences of all gene fusion products based on all combinations of gene isoforms. For protein coding fusions, AGFusion can annotate and visualize the protein domain architecture. AGFusion currently supports Homo sapiens (genome builds GRCh37 and GRCh38) and Mus musculus (genome build GRCm38) and new genomes can easily be added."}, {"title": "Zika infection of neural progenitor cells perturbs transcription in neurodevelopmental pathways", "url": "https://www.biorxiv.org/content/early/2016/10/14/072439", "tag": "Bioinformatics", "abstract": "Background A recent study of the gene expression patterns of Zika virus (ZIKV) infected human neural progenitor cells (hNPCs) revealed transcriptional dysregulation and identified cell-cycle-related pathways that are affected by infection. However deeper exploration of the information present in the RNA-Seq data can be used to further elucidate the manner in which Zika infection of hNPCs affects the transcriptome, refining pathway predictions and revealing isoform-specific dynamics. Methodology/Principal Findings We analyzed data published by Tang et al. using state-of-the-art tools for transcriptome analysis. By accounting for the experimental design and estimation of technical and inferential variance we were able to pinpoint Zika infection affected pathways that highlight Zika's neural tropism. The examination of differential genes reveals cases of isoform divergence. Conclusions/Significance Transcriptome analysis of Zika infected hNPCs has the potential to identify the molecular signatures of Zika infected neural cells. These signatures may be useful for diagnostics and for the resolution of infection pathways that can be used to harvest specific targets for further study."}, {"title": "Proteobacteria drive significant functional variability in the human gut microbiome", "url": "https://www.biorxiv.org/content/early/2016/10/13/056614", "tag": "Bioinformatics", "abstract": "While human gut microbiomes vary significantly in taxonomic composition, biological pathway abundance is surprisingly invariable across hosts. We hypothesized that healthy microbiomes appear functionally redundant due to factors that obscure differences in gene abundance across hosts. To account for these biases, we developed a powerful test of gene variability, applicable to shotgun metagenomes from any environment. Our analysis of healthy stool metagenomes reveals thousands of genes whose abundance differs significantly between people consistently across studies, including glycolytic enzymes, lipopolysaccharide biosynthetic genes, and secretion systems. Even housekeeping pathways contain a mix of variable and invariable genes, though most deeply conserved genes are significantly invariable. Variable genes tend to be associated with Proteobacteria, as opposed to taxa used to define enterotypes or the dominant phyla Bacteroidetes and Firmicutes. These results establish limits on functional redundancy and predict specific genes and taxa that may drive physiological differences between gut microbiomes."}, {"title": "iCARE: An R Package to Build and Apply Absolute Risk Models", "url": "https://www.biorxiv.org/content/early/2016/10/12/079954", "tag": "Bioinformatics", "abstract": "This report describes an R package, called the Individualized Coherent Absolute Risk Estimation (iCARE) tool, which allows researchers to quickly build models for absolute risk, and apply them to estimate an individual's risk of developing disease during a specified time interval, based on a set of user defined input parameters. An attractive feature of the software is that it gives users flexibility to update models rapidly based on new knowledge of risk factors and tailor models to different populations. The tool requires three input arguments be specified: (1) a model for relative risk (2) an age-specific disease incidence rate and (3) the distribution of risk factors for the population of interest. The tool handles missing risk factor information for individuals for whom risks are to be predicted using a coherent approach where all estimates are derived from a single model after appropriate model averaging. The software allows single nucleotide polymorphisms (SNPs) to be incorporated into the model using published odds ratios and allele frequencies. We discuss the statistical framework, handling of missing data and genetic factors, and provide real data examples that demonstrate the utility of iCARE for building and applying absolute risk models, using breast cancer as an example."}, {"title": "deSPI: efficient classification of metagenomic reads with lightweight de Bruijn graph-based reference indexing", "url": "https://www.biorxiv.org/content/early/2016/10/12/080200", "tag": "Bioinformatics", "abstract": "In metagenomic studies, fast and effective tools are on wide demand to implement taxonomy classification for upto billions of reads. Herein, we propose deSPI, a novel read classification method that classifies reads by recognizing and analyzing the matches between reads and reference with de Bruijn graph-based lightweight reference indexing. deSPI has faster speed with relatively small memory footprint, meanwhile, it can also achieve higher or similar sensitivity and accuracy."}, {"title": "ssHMM: Extracting intuitive sequence-structure motifs from high-throughput RNA-binding protein data", "url": "https://www.biorxiv.org/content/early/2016/10/11/076034", "tag": "Bioinformatics", "abstract": "RNA-binding proteins (RBPs) play an important role in RNA post-transcriptional regulation and recognize target RNAs via sequence-structure motifs. To which extent RNA structure influences protein binding in the presence or absence of a sequence motif is still poorly understood. Existing RNA motif finders either take the structure of the RNA only partially into account, or produce models which are not directly interpretable as sequence-structure motifs. Thus, a tool which produces informative motifs and at the same time captures the relationship between RNA primary sequence and secondary structure is missing. We developed ssHMM, an RNA motif finder that combines a hidden Markov model (HMM) with Gibbs sampling to learn the joint sequence and structure binding preferences of RBPs from high-throughput data, such as CLIP-Seq sequences, and intuitively visualizes them as a graph. Evaluations on synthetic data showed that ssHMM reliably recovers fuzzy sequence motifs in 80 to 100% of the cases, outperforming state-of-the-art methods designed for a similar task. On real data, it produces motifs with higher information content than existing tools. Additionally, ssHMM is considerably faster than other methods on large data sets. We also discuss examples of novel sequence-structure motifs for uncharacterized RBPs which could be identified by ssHMM. ssHMM is freely available on Github."}, {"title": "RIblast: An ultrafast RNA-RNA interaction prediction system for comprehensive lncRNA interaction analysis", "url": "https://www.biorxiv.org/content/early/2016/10/11/077271", "tag": "Bioinformatics", "abstract": "Long non-coding RNAs (lncRNAs) play important roles in various biological processes. Although more than 58,000 human lncRNA genes have been discovered, most known lncRNAs are still poorly characterised. One approach to understanding the functions of lncRNAs is the detection of the interacting RNA target of each lncRNA. Because experimental detection of comprehensive lncRNA-RNA interactions are difficult, computational prediction of lncRNA-RNA interactions is an indispensable technique. However, the high computational costs of existing RNA-RNA interaction prediction tools prevents their application to large-scale lncRNA datasets. Here, we present RIblast, an ultrafast RNA-RNA interaction prediction method based on the seed-and-extension approach. RIblast discovers seed regions using suffix arrays and subsequently extends seed regions based on an RNA secondary structure energy model. Computational experiments indicate that RIblast achieves a level of prediction accuracy similar to those of existing programs, but at speeds over 64 times faster than existing programs."}, {"title": "Tradict enables accurate prediction of eukaryotic transcriptional states from 100 marker genes", "url": "https://www.biorxiv.org/content/early/2016/10/11/060111", "tag": "Bioinformatics", "abstract": "Transcript levels are a critical determinant of the proteome and hence cellular function. Because the transcriptome is an outcome of the interactions between genes and their products, it may be accurately represented by a subset of transcript abundances. We developed a method, Tradict (transcriptome predict), capable of learning and using the expression measurements of a small subset of 100 marker genes to predict transcriptome-wide gene abundances and the expression of a comprehensive, but interpretable list of transcriptional programs that represent the major biological processes and pathways of the cell. By analyzing over 23,000 publicly available RNA-Seq datasets, we show that Tradict is robust to noise and accurate. Coupled with targeted RNA sequencing, Tradict may therefore enable simultaneous transcriptome-wide screening and mechanistic investigation at large scales."}, {"title": "Netpredictor: R and Shiny package to perform Drug-Target Bipartite network analysis and prediction of missing links.", "url": "https://www.biorxiv.org/content/early/2016/10/10/080036", "tag": "Bioinformatics", "abstract": "Netpredictor is an R package for prediction of missing links in any given bipartite network. The package provides utilities to compute missing links in a bipartite and well as unipartite networks using Random Walk with Restart and Network inference algorithm. The package also allows computation of Bipartite network properties, visualization of communities for two different sets of nodes, and calculation of significant interactions between two sets of nodes using permutation based testing. The R standalone package (including detailed introductory vignettes) and associated R Shiny web application is available under the GPL-2 Open Source license and is freely available to download from github Netpredictor repository and Shiny Netpredictor repository respectively."}, {"title": "Examining the interaction between TGFBR3 and ESRRA in ovarian cancer prognosis", "url": "https://www.biorxiv.org/content/early/2016/10/10/080101", "tag": "Bioinformatics", "abstract": "The authors found that ESRRA when co-expressed at high levels with TGF\u03b2R3 may be prognostic for serous ovarian cancer overall survival in data from the Cancer Genome Atlas; however, this result was not validated in further datasets. A cell line was also identified in this study for future functional investigation of interactions between ESRRA and TGF\u03b2R3 in the context of oestrogen signaling in order to further elucidate their potential roles as prognostic biomarkers for serous ovarian cancer."}, {"title": "RNAModR: Functional analysis of mRNA modifications in R", "url": "https://www.biorxiv.org/content/early/2016/10/10/080051", "tag": "Bioinformatics", "abstract": "Motivation: Research in the emerging field of epitranscriptomics is increasingly generating comprehensive maps of chemical modifications in messenger RNAs (mRNAs). A computational framework allowing a reproducible and standardised analysis of these mRNA modification data is missing, but will be crucial for reliable functional meta-gene analyses and cross-study comparisons. Results: We have developed RNAModR, an open-source and R-based set of methods, to analyse and visualise the transcriptome-wide distribution of mRNA modifications. RNAModR allows the statistical evaluation of the mRNA modification site distribution relative to null sites on a meta-gene level, providing insight into the functional role of these mRNA modifications on e.g. mRNA structure and stability. Availability and implementation: RNAModR is available under the GNU General Public License (GPL) as an R-package from https://github.com/mevers/RNAModR. Contact: maurits.evers@anu.edu.au"}, {"title": "Using predictive specificity to determine when gene set analysis is biologically meaningful", "url": "https://www.biorxiv.org/content/early/2016/10/10/080127", "tag": "Bioinformatics", "abstract": "Gene set analysis, which translates gene lists into enriched functions, is among the most common bioinformatic methods. Yet few would advocate taking the results at face value. Not only is there no agreement on the algorithms themselves, there is no agreement on how to benchmark them. In this paper, we evaluate the robustness and uniqueness of enrichment results as a means of assessing methods even where correctness is unknown. We show that heavily annotated (\"multifunctional\") genes are likely to appear in genomics study results and drive the generation of biologically non-specific enrichment results as well as highly fragile significances. By providing a means of determining where enrichment analyses report non-specific and non-robust findings, we are able to assess where we can be confident in their use. We find significant progress in recent bias correction methods for enrichment and provide our own software implementation. Our approach can be readily adapted to any pre-existing package."}, {"title": "Exploratory bioinformatics analysis reveals importance of \"junk\" DNA in early embryo development", "url": "https://www.biorxiv.org/content/early/2016/10/09/079921", "tag": "Bioinformatics", "abstract": "Background: Instead of testing predefined hypotheses, the goal of exploratory data analysis (EDA) is to find what data can tell us. Following this strategy, we re-analyzed a large body of genomic data to investigate how the early mouse embryos develop from fertilized eggs through a complex, poorly understood process. Results: Starting with a single-cell RNA-seq dataset of 259 mouse embryonic cells from zygote to blastocyst stages, we reconstructed the temporal and spatial dynamics of gene expression. Our analyses revealed similarities in the expression patterns of regular genes and those of retrotransposons, and the enrichment of transposable elements in the promoters of corresponding genes. Long Terminal Repeats (LTRs) are associated with transient, strong induction of many nearby genes at the 2-4 cell stages, probably by providing binding sites for Obox and other homeobox factors. The presence of B1 and B2 SINEs (Short Interspersed Nuclear Elements) in promoters is highly correlated with broad upregulation of intracellular genes in a dosage- and distance-dependent manner. Such enhancer-like effects are also found for human Alu and bovine tRNA SINEs. Promoters for genes specifically expressed in embryonic stem cells (ESCs) are rich in B1 and B2 SINEs, but low in CpG islands. Conclusions: Our results provide evidence that transposable elements may play a significant role in establishing the expression landscape in early embryos and stem cells. This study also demonstrates that open-ended, exploratory analysis aimed at a broad understanding of a complex process can pinpoint specific mechanisms for further study."}, {"title": "GW-CALL: Accurate Genome-Wide Variant Caller", "url": "https://www.biorxiv.org/content/early/2016/10/09/079905", "tag": "Bioinformatics", "abstract": "The main challenge in reliable variant calling using DNA reads is to extract information from reads mappable to multiple locations on the reference genome. Conventional approaches ignore these reads and rely on reads mappable uniquely to the reference genome. These approaches fail to perform satisfactorily in variant calling within repeat regions which are abundant in many species including homo sapiens. This, in turn, lowers the reliability of any downstream analysis including poor performance in genome-wide association studies. GW-CALL, a fast and accurate variant caller, is proposed. GW-CALL exploits information of all reads in a genome-wide decision making process. In particular, it partitions the genome into several independent regions called clusters and incorporates an efficient algorithm to use all reads belonging to a cluster in calling variants within that cluster. Availability: GW-CALL is implemented in C++ and is freely avail- able at URL: brl.ce.sharif.edu/gwcall."}, {"title": "The Identification of a 1916 Irish Rebel: New Approach for Estimating Relatedness From Low Coverage Homozygous Genomes", "url": "https://www.biorxiv.org/content/early/2016/10/08/076992", "tag": "Bioinformatics", "abstract": "Thomas Kent was an Irish rebel who was executed by British forces in the aftermath of the Easter Rising armed insurrection of 1916 and buried in a shallow grave on Cork prison's grounds. In 2015, ninety-nine years after his death, a state funeral was offered to his living family to honour his role in the struggle for Irish independence. However, inaccuracies in record keeping did not allow the bodily remains that supposedly belonged to Kent to be identified with absolute certainty. Using a novel approach based on homozygous single nucleotide polymorphisms, we identified these remains to be those of Kent by comparing his genetic data to that of two known living relatives. As the DNA degradation found on Kent's DNA, characteristic of ancient DNA, rendered traditional methods of relatedness estimation unusable, we forced all loci homozygous, in a process we refer to as ''forced homozygote approach''. The results were confirmed using simulated data for different relatedness classes. We argue that this method provides a necessary alternative for relatedness estimations, not only in forensic analysis, but also in ancient DNA studies, where reduced amounts of genetic information can limit the application of traditional methods."}, {"title": "Accurate contact predictions for thousands of protein families using PconsC3", "url": "https://www.biorxiv.org/content/early/2016/10/07/079673", "tag": "Bioinformatics", "abstract": "Protein structure prediction was for decades one of the grand unsolved challenges in bioinformatics. A few years ago it was shown that by using a maximum entropy approach to describe couplings between columns in a multiple sequence alignment it was possible to significantly increase the accuracy of residue contact predictions. For very large protein families with more than 1000 effective sequences the accuracy is sufficient to produce accurate models of proteins as well as complexes. Today, for about half of all Pfam domain families no structure is known, but unfortunately most of these families have at most a few hundred members, i.e. are too small for existing contact prediction methods. To extend accurate contact predictions to the thousands of smaller protein families we present PconsC3, an improved method for protein contact predictions that can be used for families with as little as 100 effective sequence members. We estimate that PconsC3 provides accurate contact predictions for up to 4646 Pfam domain families. In addition, PconsC3 outperforms previous methods significantly independent on family size, secondary structure content, contact range, or the number of selected contacts. This improvement translates into improved de-novo prediction of three-dimensional structures. PconsC3 is available as a web server and downloadable version at http://c3.pcons.net. The downloadable version is free for all to use and licensed under the GNU General Public License, version 2."}, {"title": "Considerations and complications of mapping small RNA libraries to transposable elements", "url": "https://www.biorxiv.org/content/early/2016/10/07/079749", "tag": "Bioinformatics", "abstract": "The advent of high-throughput sequencing (HTS) has revolutionized the way in which epigenetic research is conducted. Often coupled with the availability of fully sequenced genomes, millions of small RNA (sRNA) reads are mapped to regions of interest and the results scrutinized for clues about epigenetic mechanisms. However, this approach requires careful consideration in regards to experimental design, especially when one investigates repetitive parts of genomes such as transposable elements (TEs), and especially when such genomes are large as is often the case in plants. Here, to shed light on the challenges of mapping sRNAs to TEs, we focus on the 2,300Mb maize genome, of which >85% is derived from TEs. We compare various methodological strategies that are commonly employed in TE studies. These include choices for the reference dataset, the normalization of multiple mapping sRNAs, and the selection among different types of sRNA metrics. We further examine how these choices influence the relationship between sRNAs and the critical feature of TE age, and explore and contrast their effect on low copy regions (exons) and other popular HTS data (RNA-seq). Finally, based on our analysis, we share a series of take-home messages to help guide TE epigenetic studies specifically, but our conclusions may also apply to any work that involves mapping and analysis of HTS data."}, {"title": "PTRNAmark: an all-atomic distance-dependent knowledge-based potential for 3D RNA structure evaluation", "url": "https://www.biorxiv.org/content/early/2016/10/07/076000", "tag": "Bioinformatics", "abstract": "RNA molecules play vital biological roles, and understanding their structures gives us crucial insights into their biological functions. Model evaluation is a necessary step for better prediction and design of 3D RNA structures. Knowledge-based statistical potential has been proved to be a powerful approach for evaluating models of protein tertiary structures. In present, several knowledge-based potentials have also been proposed to assess models of RNA 3D structures. However, further amelioration is required to rank near-native structures and pick out the native structure from near-native structures, which is crucial in the prediction of RNA tertiary structures. In this work, we built a novel RNA knowledge-based potential:PTRNAmark, which not only combines mutual and self energies but also fully considers the specificity of every RNA. The benchmarks on different testing data sets all show that PTRNAmark are more efficient than existing evaluation methods in recognizing native state from a pool of near-native states of RNAs as well as in ranking near-native states of RNA models."}, {"title": "Genetic Information Relationship Network (GIRN): A Force-directed graphing tool for gene expression analysis", "url": "https://www.biorxiv.org/content/early/2016/10/07/079590", "tag": "Bioinformatics", "abstract": "We present a web-based tool, Genetic Information Relationship Network (GIRN), for mapping genes to a global protein interaction network for humans, and for further annotating this map with gene ontology, pathway, disease, and user-generated terms. These annotations can be adjusted according to enrichment within a subset of genes. Additionally, drugs that interact with genes in the subset can be added to the graph. The maps are force-directed graphs in which highly connected nodes tend toward the center, and highly interconnected nodes tend toward each other. Icons of different shapes and colors are employed to indicate whether the node represents a protein, a gene, or one of the various types of annotation. Coordinated interaction of genes and functional inference can be identified by visual inspection. Each node in the graph is associated with a menu of links to external data sources. Collectively, these tools provide an efficient portal to gene-associated public data for any group of genes specified by the user. The site can be found at www.voxvill.org/relnet."}, {"title": "SCORPIUS improves trajectory inference and identifies novel modules in dendritic cell development", "url": "https://www.biorxiv.org/content/early/2016/10/07/079509", "tag": "Bioinformatics", "abstract": "Recent advances in RNA sequencing enable the generation of genome-wide expression data at the single-cell level, opening up new avenues for transcriptomics and systems biology. A new application of single-cell whole-transcriptomics is the unbiased ordering of cells according to their progression along a dynamic process of interest. We introduce SCORPIUS, a method which can effectively reconstruct an ordering of individual cells without any prior information about the dynamic process. Comprehensive evaluation using ten scRNA-seq datasets shows that SCORPIUS consistently outperforms state-of-the-art techniques. We used SCORPIUS to generate novel hypotheses regarding dendritic cell development, which were subsequently validated in vivo. This work enables data-driven investigation and characterization of dynamic processes and lays the foundation for objective benchmarking of future trajectory inference methods."}, {"title": "KAT: A K-mer Analysis Toolkit to quality control NGS datasets and genome assemblies", "url": "https://www.biorxiv.org/content/early/2016/10/07/064733", "tag": "Bioinformatics", "abstract": "ABSTRACT Motivation: De novo assembly of whole genome shotgun (WGS) next-generation sequencing (NGS) data benefits from high-quality input with high coverage. However, in practice, determining the quality and quantity of useful reads quickly and in a reference-free manner is not trivial. Gaining a better understanding of the WGS data, and how that data is utilised by assemblers, provides useful insights that can inform the assembly process and result in better assemblies. Results: We present the K-mer Analysis Toolkit (KAT): a multi-purpose software toolkit for reference-free quality control (QC) of WGS reads and de novo genome assemblies, primarily via their k-mer frequencies and GC composition. KAT enables users to assess levels of errors, bias and contamination at various stages of the assembly process. In this paper we highlight KAT's ability to provide valuable insights into assembly composition and quality of genome assemblies through pairwise comparison of k-mers present in both input reads and the assemblies. Availability: KAT is available under the GPLv3 license at: https://github.com/TGAC/KAT."}, {"title": "Assessing Pathogens for Natural versus Laboratory Origins Using Genomic Data and Machine Learning", "url": "https://www.biorxiv.org/content/early/2016/10/06/079541", "tag": "Bioinformatics", "abstract": "Pathogen genomic data is increasingly important in investigations of infectious disease outbreaks. The objective of this study is to develop methods for using large-scale genomic data to determine the type of the environment an outbreak pathogen came from. Specifically, this study focuses on assessing whether an outbreak strain came from a natural environment or experienced substantial laboratory culturing. The approach uses phylogenetic analyses and machine learning to identify DNA changes that are characteristic of laboratory culturing. The analysis methods include parallelized sequence read alignment, variant identification, phylogenetic tree construction, ancestral state reconstruction, semi-supervised classification, and random forests. These methods were applied to 902 Salmonella enterica serovar Typhimurium genomes from the NCBI Sequence Read Archive database. The analyses identified candidate signatures of laboratory culturing that are highly consistent with genes identified in published laboratory passage studies. In particular, the analysis identified mutations in rpoS, hfq, rfb genes, acrB, and rbsR as strong signatures of laboratory culturing. In leave-one-out cross-validation, the classifier had an area under the receiver operating characteristic (ROC) curve of 0.89 for strains from two laboratory reference sets collected in the 1940s and 1980s. The classifier was also used to assess laboratory culturing in foodborne and laboratory acquired outbreak strains closely related to laboratory reference strain serovar Typhimurium 14028. The classifier detected some evidence of laboratory culturing on the phylogeny branch leading to this clade, suggesting all of these strains may have a common ancestor that experienced laboratory culturing. Together, these results suggest that phylogenetic analysis and machine learning could be used to assess whether pathogens collected from patients are naturally occurring or have been extensively cultured in laboratories. The data analysis methods can be applied to any bacterial pathogen species, and could be adapted to assess viral pathogens and other types of source environments."}, {"title": "kWIP: The k-mer Weighted Inner Product, a de novo Estimator of Genetic Similarity", "url": "https://www.biorxiv.org/content/early/2016/10/04/075481", "tag": "Bioinformatics", "abstract": "Modern genomics techniques generate overwhelming quantities of data. Extracting population genetic variation demands computationally efficient methods to determine genetic relatedness between individuals or samples in an unbiased manner, preferably de novo. The rapid and unbiased estimation of genetic relatedness has the potential to overcome reference genome bias, to detect mix-ups early, and to verify that biological replicates belong to the same genetic lineage before conclusions are drawn using mislabelled, or misidentified samples. We present the k-mer Weighted Inner Product (kWIP), an assembly-, and alignment-free estimator of genetic similarity. kWIP combines a probabilistic data structure with a novel metric, the weighted inner product (WIP), to efficiently calculate pairwise similarity between sequencing runs from their \\k-mer counts. It produces a distance matrix, which can then be further analysed and visualised. Our method does not require prior knowledge of the underlying genomes and applications include detecting sample identity and mix-up, non-obvious genomic variation, and population structure. We show that kWIP can reconstruct the true relatedness between samples from simulated populations. By re-analysing several published datasets we show that our results are consistent with marker-based analyses. kWIP is written in C++, licensed under the GNU GPL, and is available from https://github.com/kdmurray91/kwip."}, {"title": "Tracing co-regulatory network dynamics in noisy, single-cell transcriptome trajectories", "url": "https://www.biorxiv.org/content/early/2016/10/04/070151", "tag": "Bioinformatics", "abstract": "The availability of gene expression data at the single cell level makes it possible to probe the molecular underpinnings of complex biological processes such as differentiation and oncogenesis. Promising new methods have emerged for reconstructing a progression trajectory from static single-cell transcriptome measurements. However, it remains unclear how to adequately model the appreciable level of noise in these data to elucidate gene regulatory network rewiring. Here, we present a framework called Single Cell Inference of MorphIng Trajectories and their Associated Regulation (SCIMITAR) that infers progressions from static single-cell transcriptomes by employing a continuous parametrization of Gaussian mixtures in high-dimensional curves. SCIMITAR yields rich models from the data that highlight genes with expression and co-expression patterns that are associated with the inferred progression. Further, SCIMITAR extracts regulatory states from the implicated trajectory-evolving co-expression networks. We benchmark the method on simulated data to show that it yields accurate cell ordering and gene network inferences. Applied to the interpretation of a single-cell human fetal neuron dataset, SCIMITAR finds progression-associated genes in cornerstone neural differentiation pathways missed by standard differential expression tests. Finally, by leveraging the rewiring of gene-gene co-expression relations across the progression, the method reveals the rise and fall of co-regulatory states and trajectory-dependent gene modules. These analyses implicate new transcription factors in neural differentiation including putative co-factors for the multi-functional NFAT pathway."}, {"title": "Computational prediction shines light on type III secretion origins", "url": "https://www.biorxiv.org/content/early/2016/10/04/036251", "tag": "Bioinformatics", "abstract": "Type III secretion system is a key bacterial symbiosis and pathogenicity mechanism responsible for a variety of infectious diseases, ranging from food borne illnesses to the bubonic plague. In many Gram negative bacteria, the type III secretion system transports effector proteins into host cells, converting resources to bacterial advantage. Here we introduce a computational method that identifies type III effectors by combining homology based inference with de novo predictions, reaching up to 3fold higher performance than existing tools. Our work reveals that signals for recognition and transport of effectors are distributed over the entire protein sequence instead of being confined to the N-terminus, as was previously thought. Our scan of hundreds of prokaryotic genomes identified previously unknown effectors, suggesting that type III secretion may have evolved prior to the archaea/bacteria split. Crucially, our method performs well for short sequence fragments, facilitating evaluation of microbial communities and rapid identification of bacterial pathogenicity, no genome assembly required. pEffect and its data sets are available at http://services.bromberglab.org/peffect."}, {"title": "Computational prediction of the tolerance to amino-acid deletion in green-fluorescent protein", "url": "https://www.biorxiv.org/content/early/2016/10/04/079061", "tag": "Bioinformatics", "abstract": "Proteins evolve through two primary mechanisms: substitution, where mutations alter a protein's amino-acid sequence, and insertions and deletions (indels), where amino acids are either added to or removed from the sequence. Protein structure has been shown to influence the rate at which substitutions accumulate across sites in proteins, but whether structure similarly constrains the occurrence of indels has not been rigorously studied. Here, we investigate the extent to which structural properties known to covary with protein evolutionary rates might also predict protein tolerance to indels. Specifically, we analyze a publicly available dataset of single-amino-acid deletion mutations in enhanced green fluorescent protein (eGFP) to assess how well the functional effect of deletions can be predicted from protein structure. We find that weighted contact number (WCN), which measures how densely packed a residue is within the protein's three-dimensional structure, provides the best single predictor for whether eGFP will tolerate a given deletion. We additionally find that using protein design to explicitly model deletions results in improved predictions of functional status when combined with other structural predictors. Our work suggests that structure plays fundamental role in constraining deletions at sites in proteins, and further that similar biophysical constraints influence both substitutions and deletions. This study therefore provides a solid foundation for future work to examine how protein structure influences tolerance of more complex indel events, such as insertions or large deletions."}, {"title": "Adaptive Somatic Mutations Calls with Deep Learning and Semi-Simulated Data", "url": "https://www.biorxiv.org/content/early/2016/10/04/079087", "tag": "Bioinformatics", "abstract": "A number of approaches have been developed to call somatic variation in high-throughput sequencing data. Here, we present an adaptive approach to calling somatic variations. Our approach trains a deep feed-forward neural network with semi-simulated data. Semi-simulated datasets are constructed by planting somatic mutations in real datasets where no mutations are expected. Using semi-simulated data makes it possible to train the models with millions of training examples, a usual requirement for successfully training deep learning models. We initially focus on calling variations in RNA-Seq data. We derive semi-simulated datasets from real RNA-Seq data, which offer a good representation of the data the models will be applied to. We test the models on independent semi-simulated data as well as pure simulations. On independent semi-simulated data, models achieve an AUC of 0.973. When tested on semi-simulated exome DNA datasets, we find that the models trained on RNA-Seq data remain predictive (sens ~0.4 & spec ~0.9 at cutoff of P>=0.9), albeit with lower overall performance (AUC=0.737). Interestingly, while the models generalize across assay, training on RNA-Seq data lowers the confidence for a group of mutations. Haloplex exome specific training was also performed, demonstrating that the approach can produce probabilistic models tuned for specific assays and protocols. We found that the method adapts to the characteristics of experimental protocol. We further illustrate these points by training a model for a trio somatic experimental design when germline DNA of both parents is available in addition to data about the individual. These models are distributed with Goby (http://goby.campagnelab.org)."}, {"title": "Quantifying the regulatory effect size of cis-acting genetic variation using allelic fold change", "url": "https://www.biorxiv.org/content/early/2016/09/30/078717", "tag": "Bioinformatics", "abstract": "Mapping cis-acting expression quantitative trait loci (cis-eQTL) has become a popular approach for characterizing proximal genetic regulatory variants. However, measures used for quantifying the effect size of cis-eQTLs have been inconsistent and poorly defined. In this paper, we describe log allelic fold change (aFC) as a biologically interpretable and mathematically convenient unit that represents the magnitude of expression change associated with a given genetic variant. This measure is mathematically independent from expression level and allele frequency, applicable to multi-allelic variants, and generalizable to multiple independent variants. We provide tools and guidelines for estimating aFC from eQTL and allelic expression data sets, and apply it to GTEx data. We show that aFC estimates independently derived from eQTL and allelic expression data are highly consistent, and identify technical and biological correlates of eQTL effect size. We generalize aFC to analyze genes with two eQTLs in GTEx, and show that in nearly all cases these eQTLs are independent in their regulatory activity. In summary, aFC is a solid measure of cis-regulatory effect size that allows quantitative interpretation of cellular regulatory events from population data, and it is a valuable approach for investigating novel aspects of eQTL data sets."}, {"title": "Real-time tagging of biomedical entities", "url": "https://www.biorxiv.org/content/early/2016/09/30/078469", "tag": "Bioinformatics", "abstract": "Automatic annotation of text is an important complement to manual annotation, because the latter is highly labor intensive. We have developed a fast dictionary-based named entity recognition system, which is used for both real-time and bulk processing of text in a variety of biomedical web resources. We propose to adapt the system to make it interoperable with the PubAnnotation and Open Annotation standards."}, {"title": "Comparing multi- and single-sample variant calls to improve variant call sets from deep coverage whole-genome sequencing data", "url": "https://www.biorxiv.org/content/early/2016/09/30/078642", "tag": "Bioinformatics", "abstract": "Motivation: Variant calling from next-generation sequencing (NGS) data is susceptible to false positive calls due to sequencing, mapping and other errors. To better distinguish true from false positive calls, we present a method that uses genotype array data from the sequenced samples, rather than public data such as HapMap or dbSNP, to train an accurate classifier using Random Forests. We demonstrate our method on a set of variant calls obtained from 642 African-ancestry genomes from the The Consortium on Asthma among African-ancestry Populations in the Americas (CAAPA), sequenced to high depth (~30X). Results: We have applied our classifier to compare call sets generated with different calling methods, including both single-sample and multi-sample callers. At a False Positive Rate of 5%, our method determines true positive rates of 97.5%, 95% and 99% on variant calls obtained using Illumina's single-sample caller CASAVA, Real Time Genomics' multisample variant caller, and the GATK Unified Genotyper, respectively. Since most NGS sequencing data is accompanied by genotype data for the same samples, our method can be trained on each dataset to provide a more accurate computational validation of site calls compared to generic methods. Moreover, our method allows for adjustment based on allele frequency (e.g., a different set of criteria to determine quality for rare vs. common variants) and thereby provides insight into sequencing characteristics that indicate data quality for variants of different frequencies. Availability: Code will be made available prior to publication on Github."}, {"title": "A Machine Learning-based Framework to Identify Type 2 Diabetes through Electronic Health Records", "url": "https://www.biorxiv.org/content/early/2016/09/30/078634", "tag": "Bioinformatics", "abstract": "Objective: To discover diverse genotype-phenotype associations affiliated with Type 2 Diabetes Mellitus (T2DM) via genome-wide association study (GWAS) and phenome-wide association study (PheWAS), more cases (T2DM subjects) and controls (subjects without T2DM) are required to be identified (e.g., via Electronic Health Records (EHR)). However, existing expert based identification algorithms often suffer in a low recall rate and could miss a large number of valuable samples under conservative filtering standards. The goal of this work is to develop a semi-automated framework based on machine learning as a pilot study to liberalize filtering criteria to improve recall rate with a keeping of low false positive rate. Materials and Methods: We propose a data informed framework for identifying subjects with and without T2DM from EHR via feature engineering and machine learning. We evaluate and contrast the identification performance of widely-used machine learning models within our framework, including k-Nearest-Neighbors, Naive Bayes, Decision Tree, Random Forest, Support Vector Machine and Logistic Regression. Our framework was conducted on 300 patient samples (161 cases, 60 controls and 79 unconfirmed subjects), randomly selected from 23,281 diabetes related cohort retrieved from a regional distributed EHR repository ranging from 2012 to 2014. Results: We apply top-performing machine learning algorithms on the engineered features. We benchmark and contrast the accuracy, precision, AUC, sensitivity and specificity of classification models against the state-of-the-art expert algorithm for identification of T2DM subjects. Our results indicate that the framework achieved high identification performances (~0.98 in average AUC), which are much higher than the state-of-the-art algorithm (0.71 in AUC). Discussion: Expert algorithm-based identification of T2DM subjects from EHR is often hampered by the high missing rates due to their conservative selection criteria. Our framework leverages machine learning and feature engineering to loosen such selection criteria to achieve a high identification rate of cases and controls. Conclusions: Our proposed framework demonstrates a more accurate and efficient approach for identifying subjects with and without T2DM from EHR."}, {"title": "Estimating the timing of multiple admixture events using 3-locus Linkage Disequilibrium", "url": "https://www.biorxiv.org/content/early/2016/09/30/078378", "tag": "Bioinformatics", "abstract": "Estimating admixture histories is crucial for understanding the genetic diversity we see in present-day populations. Existing allele frequency or phylogeny-based methods are excellent for inferring the existence of admixture or its proportions, but have less power for estimating admixture times. Recently introduced approaches for estimating these times use spatial information from admixed chromosomes, such as the local ancestry or the decay of admixture linkage disequilibrium (ALD). One popular method, implemented in the programs ALDER and ROLLOFF, uses two-locus ALD to infer the time of a single admixture event, but is only able to estimate the time of the most recent admixture event based on this summary statistic. We derive analytical expressions for the expected ALD in a three-locus system and provide a new statistical method based on these results that is able to resolve more complicated admixture histories. Using simulations, we show how this new statistic behaves on a range of admixture histories. As an example, we also apply our method to the Colombian and Mexican samples from the 1000 Genomes project."}, {"title": "PyVar: An Extensible Framework for Variant Annotator Comparison", "url": "https://www.biorxiv.org/content/early/2016/09/30/078386", "tag": "Bioinformatics", "abstract": "Modern genomics projects are generating millions of variant calls that must be annotated for predicted functional consequences at the level of gene expression and protein function. Many of these variants are of interest owing to their potential clinical significance. Unfortunately, state-of-the-art methods do not always agree on downstream effects for any given variant. Here we present a readily extensible python framework (PyVar) for comparing the output of variant annotator methods in order to aid the research community in quickly assessing differences between methods and benchmarking new methods as they are developed. We also apply our framework to assess the annotation performance of ANNOVAR, VEP, and SnpEff when annotating 81 million variants from the \"1000 Genomes Project\" against both RefSeq and Ensembl human transcript sets."}, {"title": "Predicting Protein Thermostability Upon Mutation Using Molecular Dynamics Timeseries Data", "url": "https://www.biorxiv.org/content/early/2016/09/28/078246", "tag": "Bioinformatics", "abstract": "A large number of human diseases result from disruptions to protein structure and function caused by missense mutations. Computational methods are frequently employed to assist in the prediction of protein stability upon mutation. These methods utilize a combination of protein sequence data, protein structure data, empirical energy functions, and physicochemical properties of amino acids. In this work, we present the first use of dynamic protein structural features in order to improve stability predictions upon mutation. This is achieved through the use of a set of timeseries extracted from microsecond timescale atomistic molecular dynamics simulations of proteins. Standard machine learning algorithms using mean, variance, and histograms of these timeseries were found to be 60-70% accurate in stability classification based on experimental \u0394\u0394G or protein-chaperone interaction measurements. A recurrent neural network with full treatment of timeseries data was found to be 80% accurate according the F1 score. The performance of our models was found to be equal or better than two recently developed machine learning methods for binary classification as well as two industry-standard stability prediction algorithms. In addition to classification, understanding the molecular basis of protein stability disruption due to disease-causing mutations is a significant challenge that impedes the development of drugs and therapies that may be used treat genetic diseases. The use of dynamic structural features allows for novel insight into the molecular basis of protein disruption by mutation in a diverse set of soluble proteins. To assist in the interpretation of machine learning results, we present a technique for determining the importance of features to a recurrent neural network using Garson's method. We propose a novel extension of neural interpretation diagrams by implementing Garson's method to scale each node in the neural interpretation diagram according to its relative importance to the network."}, {"title": "Case study: Paralog diverged features may help reduce off-target effects of drugs", "url": "https://www.biorxiv.org/content/early/2016/09/28/078063", "tag": "Bioinformatics", "abstract": "Side effects from targeted drugs is a serious concern. One reason is the nonselective binding of a drug to unintended proteins such as its paralogs, which are highly homologous in sequences and exhibit similar structures and drug-binding pockets. In this study, we analyzed amino acid residues with type-II functional divergence, i.e., sites that are conserved in sequence constraints but differ in physicochemical properties between paralogs, to identify targetable differences between two paralogs. We analyzed paralogous protein receptors in the glucagon-like subfamily, glucagon receptor (GCGR) and glucagon-like peptide-1 receptor (GLP-1R), which are clinically validated drug targets in patients with type 2 diabetes and exhibit divergence in ligands, showing opposing roles in regulating glucose homeostasis. We identified 8 residues related to type-II functional divergence, which are conserved in functional constraints but differ in physicochemical properties between GCGR and GLP-1R. We detected significant enrichment of predicted residues in binding sites of the antagonist MK-0893 to GCGR. We also identified a type-II functional divergence-related residue involved in ligand-specific effects that was critical for agonist-mediated activation of GLP-1R. We describe the important role of type-II functional divergence-related sites in paralog discrimination, enabling the identification of binding sites to reduce undesirable side effects and increase the target specificity of drugs."}, {"title": "Variant Set Enrichment: An R package to Identify Dis-ease-Associated Functional Genomic Regions", "url": "https://www.biorxiv.org/content/early/2016/09/28/077990", "tag": "Bioinformatics", "abstract": "Summary: Genetic predispositions to diseases populate the noncoding regions of the human genome. Delineating their functional basis can inform on the mechanisms contributing to disease development. However, this remains a challenge due to the poor characterization of the noncoding genome. Variant Set Enrichment (VSE) is a fast method to calculate the enrichment of a set of disease-associated variants across functionally annotated genomic regions, consequently highlighting the mechanisms important in the etiology of the disease studied. VSE is implemented as an R package and can easily be implemented in any system with R."}, {"title": "State aggregation for fast likelihood computations in molecular evolution", "url": "https://www.biorxiv.org/content/early/2016/09/28/035063", "tag": "Bioinformatics", "abstract": "Motivation: Codon models are widely used to identify the signature of selection at the molecular level and to test for changes in selective pressure during the evolution of genes encoding proteins. The large size of the state space of the Markov processes used to model codon evolution makes it difficult to use these models with large biological datasets. We propose here to use state aggregation to reduce the state space of codon models and, thus, improve the computational performance of likelihood estimation on these models. Results: We show that this heuristic speeds up the computations of the M0 and branch-site models up to 6.8 times. We also show through simulations that state aggregation does not introduce a detectable bias. We analysed a real dataset and show that aggregation provides highly correlated predictions compared to the full likelihood computations. Finally, state aggregation is a very general approach and can be applied to any continuous-time Markov process-based model with large state space, such as amino acid and coevolution models. We therefore discuss different ways to apply state aggregation to Markov models used in phylogenetics. Availability: The heuristic is implemented in the godon package (https://bitbucket.org/Davydov/godon) and in a version of FastCodeML (https://gitlab.isb-sib.ch/phylo/fastcodeml)."}, {"title": "Glutton: large-scale integration of non-model organism transcriptome data for comparative analysis", "url": "https://www.biorxiv.org/content/early/2016/09/26/077511", "tag": "Bioinformatics", "abstract": "High-throughput RNA-seq data has become ubiquitous in the study of non-model organisms, but its use in comparative analysis remains a challenge. Without a reference genome for mapping, sequence data has to be de novo assembled, producing large numbers of short, highly redundant contigs. Preparing these assemblies for comparative analyses requires the removal of redundant isoforms, assignment of orthologs and converting fragmented transcripts into gene alignments. In this article we present Glutton, a novel tool to process transcriptome assemblies for downstream evolutionary analyses. Glutton takes as input a set of fragmented, possibly erroneous transcriptome assemblies. Utilising phylogeny-aware alignment and reference data from a closely related species, it reconstructs one transcript per gene, finds orthologous sequences and produces accurate multiple alignments of coding sequences. We present a comprehensive analysis of Glutton's performance across a wide range of divergence times between study and reference species. We demonstrate the impact choice of assembler has on both the number of alignments and the correctness of ortholog assignment and show substantial improvements over heuristic methods, without sacrificing correctness. Finally, using inference of Darwinian selection as an example of downstream analysis, we show that Glutton-processed RNA-seq data give results comparable to those obtained from full length gene sequences even with distantly related reference species. Glutton is available from http://wasabiapp.org/software/glutton/ and is licensed under the GPLv3."}, {"title": "Co-estimation of Phylogeny-aware Alignment and Phylogenetic Tree", "url": "https://www.biorxiv.org/content/early/2016/09/26/077503", "tag": "Bioinformatics", "abstract": "The phylogeny-aware alignment algorithm implemented in both PRANK and PAGAN has been found to produce highly accurate alignments for comparative sequence analysis. However, the algorithm's reliance on a guide tree during the alignment process can bias the resulting alignment rendering it unsuitable for phylogenetic inference. To overcome these issues, we have developed a new tool, Canopy, for parallelized iterative search of optimal alignment. Using Canopy, we studied the impact of the guide tree as well as the number and relative divergence of sequences on the accuracy of the alignment and inferred phylogeny. We find that PAGAN is the more robust of the two phylogeny-aware alignment methods to errors in the guide tree, but Canopy largely resolves the guide tree-related biases in PRANK. We demonstrate that, for all experimental settings tested, Canopy produces the most accurate sequence alignments and, further, that the inferred phylogenetic trees are of comparable accuracy to those obtained with the leading alternative method, SAT\u00e9. Our analyses also show that, unlike traditional alignment algorithms, the phylogeny-aware algorithm effectively uses the information from denser sequence sampling and produces more accurate alignments when additional closely-related sequences are included. All methods are available for download at http://wasabiapp.org/software."}, {"title": "Enrich2: a statistical framework for analyzing deep mutational scanning data", "url": "https://www.biorxiv.org/content/early/2016/09/25/075150", "tag": "Bioinformatics", "abstract": "Measuring the functional consequences of protein variants can reveal how a protein works or help unlock the meaning of an individual's genome. Deep mutational scanning is a widely used method for multiplex measurement of the functional consequences of protein variants. A major limitation of this method has been the lack of a common analysis framework. We developed a statistical model for estimating variant scores that can be applied to many experimental designs. Our method generates an error estimate for each score that captures both sampling error and consistency between replicates. We apply our model to one novel and five published datasets comprising 243,732 variants and demonstrate its superiority, particularly for removing noisy variants, detecting variants of small effect, and conducting hypothesis testing. We implemented our model in easy-to-use software, Enrich2, that can empower researchers analyzing deep mutational scanning data."}, {"title": "Zipper plot: visualizing transcriptional activity of genomic regions", "url": "https://www.biorxiv.org/content/early/2016/09/23/077073", "tag": "Bioinformatics", "abstract": "Summary: Reconstructing transcript models from RNA-sequencing (RNA-seq) data and establishing these as independent transcriptional units can be a challenging task. The Zipper plot is an application that enables users to interrogate putative transcription start sites (TSSs) in relation to various features that are indicative for transcriptional activity. These features are obtained from publicly available datasets including CAGE-sequencing (CAGE-seq), ChIP-sequencing (ChIP-seq) for histone marks and DNase-sequencing (DNase-seq). The Zipper plot application requires three input fields (chromosome, genomic coordinate (hg19) of the TSS and strand) and generates a report that includes a detailed summary table, a Zipper plot and several statistics derived from this plot. Availability and Implementation: The Zipper plot is implemented using the statistical programming language R and is freely available at http://zipperplot.cmgg.be Contact: Pieter.Mestdagh@UGent.be; Katleen.DePreter@UGent.be; Francisco.AvilaCobos@UGent.be Supplementary information: Supplementary Methods available online."}, {"title": "dbOTU3: A new implementation of distribution-based OTU calling", "url": "https://www.biorxiv.org/content/early/2016/09/22/076927", "tag": "Bioinformatics", "abstract": "Distribution-based operational taxonomic unit-calling (dbOTU) improves on other approaches by incorporating information about the input sequences' distribution across samples. Previous implementations of dbOTU presented challenges for users. Here we introduce and evaluate a new implementation of dbOTU that is faster and more user-friendly. We show that this new implementation has theoretical and practical improvements over previous implementations of dbOTU, making the algorithm more accessible to microbial ecology and biomedical researchers."}, {"title": "Enhancing pre-defined workflows with ad hoc analytics using Galaxy, Docker and Jupyter", "url": "https://www.biorxiv.org/content/early/2016/09/22/075457", "tag": "Bioinformatics", "abstract": "What does it take to convert a heap of sequencing data into a publishable result? First, common tools are employed to reduce primary data (sequencing reads) to a form suitable for further analyses (i.e., list of variable sites). The subsequent exploratory stage is much more ad hoc and requires development of custom scripts making it problematic for biomedical researchers. Here we describe a hybrid platform combining common analysis pathways with exploratory environments. It aims at fully encompassing and simplifying the \u201craw data-to-publication\u201d pathway and making it reproducible."}, {"title": "Probabilistic inference of bifurcations in single-cell data using a hierarchical mixture of factor analysers", "url": "https://www.biorxiv.org/content/early/2016/09/21/076547", "tag": "Bioinformatics", "abstract": "Modelling bifurcations in single-cell transcriptomics data has become an increasingly popular field of research. Several methods have been proposed to infer bifurcation structure from such data but all rely on heuristic non-probabilistic inference. Here we propose the first generative, fully probabilistic model for such inference based on a Bayesian hierarchical mixture of factor analysers. Our model exhibits competitive performance on large datasets despite implementing full MCMC sampling and its unique hierarchical prior structure enables automatic determination of genes driving the bifurcation process."}, {"title": "Data aggregation at the level of molecular pathways improves stability of experimental transcriptomic and proteomic data", "url": "https://www.biorxiv.org/content/early/2016/09/21/076620", "tag": "Bioinformatics", "abstract": "High throughput technologies opened a new era in biomedicine by enabling massive analysis of gene expression at both RNA and protein levels. Unfortunately, expression data obtained in different experiments are often poorly compatible, even for the same biological samples. Here, using experimental and bioinformatic investigation of major experimental platforms, we show that aggregation of gene expression data at the level of molecular pathways helps to diminish cross- and intra-platform bias otherwise clearly seen at the level of individual genes. We created a mathematical model of cumulative suppression of data variation that predicts the ideal parameters and the optimal size of a molecular pathway. We compared the abilities to aggregate experimental molecular data for the five alternative methods, also evaluated by their capacity to retain meaningful features of biological samples. The bioinformatic method OncoFinder showed optimal performance in both tests and should be very useful for future cross-platform data analyses."}, {"title": "MINT: A multivariate integrative method to identify reproducible molecular signatures across independent experiments and platforms", "url": "https://www.biorxiv.org/content/early/2016/09/20/070813", "tag": "Bioinformatics", "abstract": "Molecular signatures identified from high-throughput transcriptomic studies often have poor reliability and fail to reproduce across studies. One solution is to combine independent studies into a single integrative analysis, additionally increasing sample size. However, the different protocols and technological platforms across transcriptomic studies produce unwanted systematic variation that strongly confounds the integrative analysis results. When studies aim to discriminate an outcome of interest, the common approach is a sequential two-step procedure; unwanted systematic variation removal techniques are applied prior to classification methods. To limit the risk of overfitting and over-optimistic results of a two-step procedure, we developed a novel multivariate integration method, MINT, that simultaneously accounts for unwanted systematic variation and identifies predictive gene signatures with greater reproducibility and accuracy. In two biological examples on the classification of three human cell types and four subtypes of breast cancer, we combined high-dimensional microarray and RNA-seq data sets and MINT identified highly reproducible and relevant gene signatures predictive of a given phenotype. MINT led to superior classification and prediction accuracy compared to the existing sequential two-step procedures. MINT is a powerful approach and the first of its kind to solve the integrative classification framework in a single step by combining multiple independent studies. MINT is computationally fast as part of the mixOmics R CRAN package, available at http://www.mixOmics.org/mixMINT/ and http://cran.r-project.org/web/packages/mixOmics/"}, {"title": "DynOmics to identify delays and co-expression patterns across time course experiments", "url": "https://www.biorxiv.org/content/early/2016/09/20/076257", "tag": "Bioinformatics", "abstract": "Dynamic changes in biological systems can be captured by measuring molecular expression from different levels (e.g., genes and proteins) across time. Integration of such data aims to identify molecules that show similar expression changes over time; such molecules may be co-regulated and thus involved in similar biological processes. Combining data sources presents a systematic approach to study molecular behaviour. It can compensate for missing data in one source, and can reduce false positives when multiple sources highlight the same pathways. However, integrative approaches must accommodate the challenges inherent in \u2018omics\u2019 data, including high-dimensionality, noise, and timing differences in expression. As current methods for identification of co-expression cannot cope with this level of complexity, we developed a novel algorithm called DynOmics. DynOmics is based on the fast Fourier transform, from which the difference in expression initiation between trajectories can be estimated. This delay can then be used to realign the trajectories and identify those which show a high degree of correlation. Through extensive simulations, we demonstrate that DynOmics is efficient and accurate compared to existing approaches. We consider two case studies highlighting its application, identifying regulatory relationships across \u2018omics\u2019 data within an organism and for comparative gene expression analysis across organisms."}, {"title": "High throughput estimation of functional cell activities reveals disease mechanisms and predicts relevant clinical outcomes", "url": "https://www.biorxiv.org/content/early/2016/09/19/076083", "tag": "Bioinformatics", "abstract": "Understanding the aspects of the cell functionality that account for disease or drug action mechanisms is a main challenge for precision medicine. Here we propose a new method that models cell signaling using biological knowledge on signal transduction. The method recodes individual gene expression values (and/or gene mutations) into accurate measurements of changes in the activity of signaling circuits, which ultimately constitute high-throughput estimations of cell functionalities caused by gene activity within the pathway. Moreover, such estimations can be obtained either at cohort-level, in case/control comparisons, or personalized for individual patients. The accuracy of the method is demonstrated in an extensive analysis involving 5640 patients from 12 different cancer types. Circuit activity measurements not only have a high diagnostic value but also can be related to relevant disease outcomes such as survival, and can be used to assess therapeutic interventions."}, {"title": "Towards an Open Data Framework for Body Sensor Networks Supporting Bluetooth Low Energy", "url": "https://www.biorxiv.org/content/early/2016/09/19/076166", "tag": "Bioinformatics", "abstract": "Major companies, healthcare professionals, the military, and other scientists and innovators are now sensing that fitness and health data from wearable biosensors will likely provide new discoveries and insights into physiological, cognitive, and emotional health status of an individual. Having the ability to collect, process, and correlate data simultaneously from a set of heterogonous biosensor sources may be a key factor in informing the development of new technologies for reducing health risks, improving health status, and possibly preventing and predicting disease. The challenge in achieving this is getting easy access to heterogeneous data from a set of disparate sensors in a single, integrated wearable monitoring system. Often times, the data recorded by commercial biosensing devices are contained within each manufacturer's proprietary platform. Summary data is available for some devices as free downloads or included only in annual premium memberships. Access to raw measurements is generally unavailable, especially from a custom developed application that may include prototype biosensors. In this paper, we explore key ideas on how to leverage the design features of Bluetooth Low Energy to ease the integration of disparate biosensors at the sensor communication layer. This component is intended to fit into a larger, multi-layered, open data framework that can provide additional data management and analytics capabilities for consumers and scientists alike at all the layers of a data access model which is typically employed in a body sensor network system."}, {"title": "De novo transcriptome assembly for the spiny mouse (Acomys cahirinus)", "url": "https://www.biorxiv.org/content/early/2016/09/19/076067", "tag": "Bioinformatics", "abstract": "Background: Spiny mice of the genus Acomys are small desert-dwelling rodents that display physiological characteristics not typically found in rodents. Recent investigations have reported a menstrual cycle and scar free-wound healing in this species; characteristics that are exceedingly rare in mammals, and of considerable interest to the scientific community. These unique physiological traits, and the potential for spiny mice to accurately model human diseases, are driving increased use of this genus in biomedical research. However, little genetic information is currently available for Acomys, limiting the application of some modern investigative techniques. This project aimed to generate a reference transcriptome assembly for the common spiny mouse (Acomys cahirinus). Results: Illumina RNA sequencing of male and female spiny mice produced 451 million, 150bp paired-end reads from 15 organ types. An extensive survey of de novo transcriptome assembly approaches of high-quality reads using Trinity, SOAPdenovo-Trans, and Velvet/Oases at multiple kmer lengths was conducted with 49 single-kmer assemblies generated from this dataset, with and without in silico normalization and probabilistic error correction. Merging transcripts from 49 individual single-kmer assemblies into a single meta-assembly of non-redundant transcripts using the EvidentialGene 'tr2aacds' pipeline produced the highest quality transcriptome assembly, comprised of 880,080 contigs, of which 189,925 transcripts were annotated using the SwissProt/Uniprot database. Conclusions: This study provides the first detailed characterization of the spiny mouse transcriptome. It validates the application of the EvidentialGene 'tr2aacds' pipeline to generate a high-quality reference transcriptome assembly in a mammalian species, and provides a valuable scientific resource for further investigation into the unique physiological characteristics inherent in the genus Acomys."}, {"title": "Genome\u2011wide Identification and Characterization of Transcription Factors of Basic Leucine Zipper Family in Malus domestica", "url": "https://www.biorxiv.org/content/early/2016/09/19/075994", "tag": "Bioinformatics", "abstract": "Basic leucine zipper proteins (bZIP) contain a basic DNA-binding region and a leucine zipper region, acting as transcriptional factors in regulation of gene expression exclusively in eukaryotes. In this investigation, total 116 bZIP members were identified in apple genome and mapped on all 17 chromosomes with various densities as M.bZIPs. All these members were divided into six groups according to the phylogenetic relationship combining with bZIPs from rice and Arabidopsis. Investigating gene structure of M.bZIPs, five splicing patterns of intron were found in the DNA-binging region with no splicing position and splicing positions at different nucleotide of codons or different positions. Analyzing of protein structure of M.bZIPs, twenty-five motifs were identified with certain characteristic in different phylogenetic groups. To predict dimerization of leucine zipper region, the key positions of amino acid in heptad(s) were investigated. The results showed that most M.bZIPs may form hetero-dimer or homo-dimer and some M.bZIPs may form both. Expression experiment revealed that M.bZIP genes have organ-specific expression and widely expressed in flowers, leaves, and fruits. To investigate the response of M.bZIPs to abiotic stresses, the promoter sequences of randomly selected M.bZIP genes were analyzed. Cis-acting elements related to multiple stresses were found existing widely in promoter sequences. Quantitative real-time PCR results further demonstrated that the expression of some M.bZIP genes were quite sensitive to exogenous abscisic acid and osmotic treatments."}, {"title": "Assessing Prediction Performance of Neoadjuvant Chemotherapy Response in Bladder Cancer", "url": "https://www.biorxiv.org/content/early/2016/09/16/075705", "tag": "Bioinformatics", "abstract": "Neoadjuvant chemotherapy is a treatment routinely prescribed to patients diagnosed with muscle-invasive bladder cancer. Unfortunately, not all patients are responsive to this treatment and would greatly benefit from an accurate prediction of their expected response to chemotherapy. In this project, I attempt to develop a model that will predict response using tumour microarray data. I show that using my dataset, every method is insufficient at accurately classifying responders and non-responders."}, {"title": "Identification of outcome-related driver mutations in cancer using conditional co-occurrence distributions", "url": "https://www.biorxiv.org/content/early/2016/09/16/075408", "tag": "Bioinformatics", "abstract": "The methods proposed for the detection of cancer driver mutations are based on the estimation of background mutation rate, impact on protein function, or network influence. Instead, we focus on those influencing patient survival. For this, an approximation of the log-rank test has been systematically applied even though it assumes a large and similar number of patients in both risk groups, which is violated in cancer genomics. Here, we propose VALORATE, a novel algorithm for the estimation of the null distribution for the log-rank test independently of the number of mutations. VALORATE is based on conditional distributions of the co-occurrences between events and mutations. The results using simulations, comparisons with other methods, TCGA and ICGC cancer datasets, and validations, suggests that VALORATE is accurate, fast, and can identify known and novel gene mutations. Our proposal and results may have important implications in cancer biology, in bioinformatics analyses, and ultimately in precision medicine."}, {"title": "Construction of the third generation Zea mays haplotype map", "url": "https://www.biorxiv.org/content/early/2016/09/16/026963", "tag": "Bioinformatics", "abstract": "Characterization of genetic variations in maize has been challenging, mainly due to deterioration of collinearity between individual genomes in the species. An international consortium of maize research groups combined resources to develop the maize haplotype version 3 (HapMap 3), built from whole genome sequencing data from 1,218 maize lines, covering pre-domestication and domesticated Zea mays varieties across the world.A new computational pipeline was set up to process over 12 trillion bp of sequencing data, and a set of population genetics filters were applied to identify over 83 million variant sites. We identified polymorphisms in regions where collinearity is largely preserved in the maize species. However, the fact that the B73 genome used as the reference only represents a fraction of all haplotypes is still an important limiting factor."}, {"title": "HydDB: A web tool for hydrogenase classification and analysis", "url": "https://www.biorxiv.org/content/early/2016/09/16/061994", "tag": "Bioinformatics", "abstract": "H2 metabolism is proposed to be the most ancient and diverse mechanism of energy-conservation. The metalloenzymes mediating this metabolism, hydrogenases, are encoded by over 60 microbial phyla and are present in all major ecosystems. We developed a classification system and web tool, HydDB, for the structural and functional analysis of these enzymes. We show that hydrogenase function can be predicted by primary sequence alone using an expanded classification scheme (comprising 29 [NiFe], 8 [FeFe], and 1 [Fe] hydrogenase classes) that defines 11 new classes with distinct biological functions. Using this scheme, we built a web tool that rapidly and reliably classifies hydrogenase primary sequences using a combination of k-nearest neighbors' algorithms and CDD referencing. Demonstrating its capacity, the tool reliably predicted hydrogenase content and function in 12 newly-sequenced bacteria, archaea, and eukaryotes. HydDB provides the capacity to browse the amino acid sequences of 3248 annotated hydrogenase catalytic subunits and also contains a detailed repository of physiological, biochemical, and structural information about the 38 hydrogenase classes defined here. The database and classifier are freely and publicly available at http://services.birc.au.dk/hyddb/"}, {"title": "MCbiclust: a novel algorithm to discover large-scale functionally related gene sets from massive transcriptomics data collections", "url": "https://www.biorxiv.org/content/early/2016/09/15/075374", "tag": "Bioinformatics", "abstract": "The potential to understand fundamental biological processes from gene expression data has grown parallel with the recent explosion of the size of data collections. However, to exploit this potential, novel analytical methods are required, capable of handling massive data matrices. We found current methods limited in the size of correlated gene sets they could discover within biologically heterogeneous data collections, hampering the identification of multi-gene controlled fundamental cellular processes such as energy metabolism, organelle biogenesis and stress responses. Here we describe a novel biclustering algorithm called Massively Correlated Biclustering (MCbiclust) that selects samples and genes from large datasets with maximal correlated gene expression, allowing regulation of complex pathway to be examined. The method has been evaluated using synthetic data and applied to large bacterial and cancer cell datasets. We show that the large biclusters discovered, so far elusive to identification by existing techniques, are biologically relevant and thus MCbiclust has great potential use in the analysis of transcriptomics data to identify large scale unknown effects hidden within the data. The identified massive biclusters can be used to develop improved transcriptomics based diagnosis tools for diseases caused by altered gene expression, or used for further network analysis to understand genotype-phenotype correlations."}, {"title": "The Drosophila Gene Expression Tool (DGET) for expression analyses", "url": "https://www.biorxiv.org/content/early/2016/09/15/075358", "tag": "Bioinformatics", "abstract": "Background: Next-generation sequencing technologies have greatly increased our ability to identify gene expression levels, including at specific developmental stages and in specific tissues. Gene expression data can help researchers understand the diverse functions of genes and gene networks, as well as help in the design of specific and efficient functional studies, such as by helping researchers choose the most appropriate tissue for a study of a group of genes, or conversely, by limiting a long list of gene candidates to the subset that are normally expressed at a given stage or in a given tissue. Results: We report a Drosophila Gene Expression Tool (DGET, www.flyrnai.org/tools/dget/web/), which stores and facilitates search of RNA-Seq based expression profiles available from the modENCODE consortium and other public data sets. Using DGET, researchers are able to look up gene expression profiles, filter results based on threshold expression values, and compare expression data across different developmental stages, tissues and treatments. In addition, at DGET a researcher can analyze tissue or stage-specific enrichment for an inputted list of genes (e.g. hits from a screen) and search for additional genes with similar expression patterns. We performed a number of analyses to demonstrate the quality and robustness of the resource. In particular, we show that evolutionary conserved genes expressed at high or moderate levels in both fly and human tend to be expressed in similar tissues. Using DGET, we compared whole tissue profile and sub-region/cell-type specific datasets and estimated the potential cause of false positives in one dataset. We also demonstrated the usefulness of DGET for synexpression studies by querying genes with similar expression profile to the mesodermal master regulator Twist. Conclusion: Altogether, DGET provides a flexible tool for expression data retrieval and analysis with short or long lists of Drosophila genes, which can help scientists to design stage- or tissue-specific in vivo studies and do other subsequent analyses."}, {"title": "Solving the influence maximization problem reveals regulatory organization of the yeast cell cycle.", "url": "https://www.biorxiv.org/content/early/2016/09/14/075069", "tag": "Bioinformatics", "abstract": "The Influence Maximization Problem (IMP) aims to discover the set of nodes with the greatest influence on network dynamics. The problem has previously been applied in epidemiology and social network analysis. Here, we demonstrate the application to cell cycle regulatory network analysis of Saccharomyces cerevisiae. Fundamentally, gene regulation is linked to the flow of information. Therefore, our implementation of the IMP was framed as an information theoretic problem on a diffusion network. Utilizing all regulatory edges from YeastMine, gene expression dynamics were encoded as edge weights using a variant of time lagged transfer entropy, a method for quantifying information transfer between variables. Influence, for a particular number of sources, was measured using a diffusion model based on Markov chains with absorbing states. By maximizing over different numbers of sources, an influence ranking on genes was produced. The influence ranking was compared to other metrics of network centrality. Although 'top genes' from each centrality ranking contained well-known cell cycle regulators, there was little agreement and no clear winner. However, it was found that influential genes tend to directly regulate or sit upstream of genes ranked by other centrality measures. This is quantified by computing node reachability between gene sets; on average, 59% of central genes can be reached when starting from the influential set, compared to 7% of influential genes when starting at another centrality measure. The influential nodes act as critical sources of information flow, potentially having a large impact on the state of the network. Biological events that affect influential nodes and thereby affect information flow could have a strong effect on network dynamics, potentially leading to disease. Code and example data can be found at: https://github.com/Gibbsdavidl/miergolf"}, {"title": "Numericware i: Identical in state matrix calculator", "url": "https://www.biorxiv.org/content/early/2016/09/14/075267", "tag": "Bioinformatics", "abstract": "Herein we introduce software, Numericware i to compute a matrix consisting of all pairwise identical in state (IIS) coefficients from genotypic data. Since the emergence of high throughput technology for genotyping, calculating an IIS matrix between many pairs of entities has required large computer memory and lengthy processing times. Numericware i addresses these limitations with two algorithmic methods: multithreading and forward chopping. The multithreading feature allows computational routines to concurrently run on multiple CPU processors. The forward chopping addresses memory limitations by dividing the genotypic data into appropriately sized subsets. Numericware i allows researchers who need to estimate an IIS matrix for big genotypes to use typical laptop/desktop computers. For comparison with different software, we calculated genetic relationship matrices using Numericware i, SPAGeDi and TASSEL with the same small-sized data set. Numericware i measured kinship coefficients between zero and two, while the matrices from SPAGeDi and TASSEL produced different ranges of values, including negative values. The Pearson correlation coefficient between the matrices from Numericware i and TASSEL was high at 0.993, while SPAGeDi rarely showed correlation with Numericware i (0.088) and TASSEL (0.087). To compare the capacity with high dimensional data, we applied the three software to a simulated data set consisted of 500 entities by 1,000,000 SNPs. Numericware i spent 71 minutes using seven CPU cores on a laptop (DELL LATITUDE E6540), while SPAGeDi and TASSEL failed to start. Numericware i is freely available for Windows and Linux under CC-BY license at https://figshare.com/s/f100f33a8857131eb2db."}, {"title": "3D chromatin structure estimation through a constraint-enhanced score function", "url": "https://www.biorxiv.org/content/early/2016/09/14/075184", "tag": "Bioinformatics", "abstract": "Based on experimental techniques of the type Chromosome Conformation Capture (3c), several methods have been proposed in the literature to estimate the structure of the nuclear dna in homogeneous populations of cells. Many of these methods transform contact frequencies into Euclidean distances between pairs of chromatin fragments, and then reconstruct the structure by solving a distance-to-geometry problem. To avoid the drawbacks of this strategy, we propose to abandon the frequency-distance translation and adopt a recursive multiscale procedure, where the chromatin fibre is modelled by a new kind of modified bead chain, the data are suitably partitioned at each scale, and the resulting partial structures are estimated independently of each other and then connected again to rebuild the whole chain. We propose a new score function to generate the solution space: it includes a data-fit part that does not require target distances, and a penalty part, which enforces soft geometric constraints on the solution, coherent with known physical and biological constraints. The relative weights of the two parts are balanced automatically at each scale and each subchain treated. Since it is reasonable to expect that many different structures fit any 3c-type data set, we sample the solution space by simulated annealing, with no search for an absolute optimum. A set of different solutions with similar scores is thus generated. The procedure can be managed through a minimum set of parameters, independent of both the scale and the particular genomic segment being treated. The user is thus allowed to control the solutions easily and effectively. The partition of the fibre, along with several intrinsically parallel parts, make this method computationally efficient. We report some results obtained with the new method and code, tested against real data, that support the reliability of our method and the biological plausibility of our solutions."}, {"title": "Sequence biases in CLIP experimental data are incorporated in protein RNA-binding models", "url": "https://www.biorxiv.org/content/early/2016/09/14/075259", "tag": "Bioinformatics", "abstract": "We report a newly-identified bias in CLIP data that results from cleaving enzyme specificity. This bias is inadvertently incorporated into standard peak calling methods, which identify the most likely locations where proteins bind RNA. We further show how, in downstream analysis, this bias is incorporated into models inferred by the state-of-the-art GraphProt method to predict protein RNA-binding. We call for both experimental controls to measure enzyme specificities and algorithms to identify unbiased CLIP binding sites."}, {"title": "HDP-Align: Hierarchical Dirichlet Process Clustering for Multiple Peak Alignment of Liquid Chromatography Mass Spectrometry Data.", "url": "https://www.biorxiv.org/content/early/2016/09/13/074831", "tag": "Bioinformatics", "abstract": "Matching peak features across multiple LC-MS runs (alignment) is an integral part of all LC-MS data processing pipelines. Alignment is challenging due to variations in the retention time of peak features across runs and the large number of peak features produced by a single compound in the analyte. In this paper, we propose a Bayesian non-parametric model that aligns peaks via a hierarchical cluster model using both peak mass and retention time. Crucially, this method provides confidence values in the form of posterior probabilities allowing the user to distinguish between aligned peaksets of high and low confidence. The results from our experiments on a diverse set of proteomic, glycomic and metabolomic data show that the proposed model is able to produce alignment results competitive to other widely-used benchmark methods, while at the same time, provide a probabilistic measure of confidence in the alignment results, thus allowing the possibility to trade precision and recall."}, {"title": "Phylogenetic factorization of compositional data", "url": "https://www.biorxiv.org/content/early/2016/09/12/074112", "tag": "Bioinformatics", "abstract": "Marker gene sequencing of microbial communities has generated big datasets of microbial relative abundances varying across environmental conditions, sample sites and treatments. These data often come with putative phylogenies, providing unique opportunities to investigate how shared evolutionary history affects microbial abundance patterns. Here, we present a method to identify the phylogenetic factors driving patterns in microbial community composition. We use the method, \"phylofactorization\", to re-analyze datasets from human body and soil microbial communities, demonstrating how phylofactorization can be a dimensionality-reducing tool, an ordination-visualization tool, and also mass-produce inferences on the edges in the phylogeny in which meaningful differences arose."}, {"title": "Annotation and differential analysis of alternative splicing using de novo assembly of RNAseq data", "url": "https://www.biorxiv.org/content/early/2016/09/12/074807", "tag": "Bioinformatics", "abstract": "Genome-wide analyses reveal that more than 90% of multi exonic human genes produce at least two transcripts through alternative splicing (AS). Various bioinformatics methods are available to analyze AS from RNAseq data. Most methods start by mapping the reads to an annotated reference genome, but some start by a de novo assembly of the reads. In this paper, we present a systematic comparison of a mapping-first approach (FaRLine) and an assembly-first approach (KisSplice). These two approaches are event-based, as they focus on the regions of the transcripts that vary in their exon content. We applied these methods to an RNAseq dataset from a neuroblastoma SK-N-SH cell line (ENCODE) differentiated or not using retinoic acid. We found that the predictions of the two pipelines overlapped (70% of exon skipping events were common), but with noticeable differences. The assembly-first approach allowed to find more novel variants, including novel unannotated exons and splice sites. It also predicted AS in families of paralog genes. The mapping-first approach allowed to find more lowly expressed splicing variants, and was better in predicting exons overlapping repeated elements. This work demonstrates that annotating AS with a single approach leads to missing a large number of candidates. We further show that these candidates cannot be neglected, since many of them are differentially regulated across conditions, and can be validated experimentally. We therefore advocate for the combine use of both mapping-first and assembly-first approaches for the annotation and differential analysis of AS from RNAseq data."}, {"title": "UCHIME2: improved chimera prediction for amplicon sequencing", "url": "https://www.biorxiv.org/content/early/2016/09/09/074252", "tag": "Bioinformatics", "abstract": "Amplicon sequencing generates chimeric reads which can cause spurious inferences of biological variation. I describe UCHIME2, an update of the popular UCHIME chimera detection algorithm with new modes optimized for high-resolution biological sequence reconstruction (\"denoising\") and other applications. I show that chimera frequency correlates inversely with divergence, that error-free chimera prediction from sequence is impossible in principle, and that UCHIME2 achieves higher detection accuracy than previous methods."}, {"title": "Revealing complex ecological dynamics via symbolic regression", "url": "https://www.biorxiv.org/content/early/2016/09/11/074617", "tag": "Bioinformatics", "abstract": "Complex ecosystems, from food webs to our gut microbiota, are essential to human life. Understanding the dynamics of those ecosystems can help us better maintain or control them. Yet, reverse-engineering complex ecosystems (i.e., extracting their dynamic models) directly from measured temporal data has not been very successful so far. Here we propose to close this gap via symbolic regression. We validate our method using both synthetic and real data. We firstly show this method allows reverse engineering two-species ecosystems, inferring both the structure and the parameters of ordinary differential equation models that reveal the mechanisms behind the system dynamics. We find that as the size of the ecosystem increases or the complexity of the inter-species interactions grow, using a dictionary of known functional responses (either previously reported or reverse-engineered from small ecosystems using symbolic regression) opens the door to correctly reverse-engineer large ecosystems."}, {"title": "Ultrasensitive detection of TCR hypervariable region in solid-tissue RNA-seq data", "url": "https://www.biorxiv.org/content/early/2016/09/09/073395", "tag": "Bioinformatics", "abstract": "Characterization of tissue-infiltrating T cell repertoire is critical to understanding tumor-immune interactions and autoimmune disease etiology. We present TRUST, an open source algorithm for calling the TCR transcript hypervariable CDR3 regions using unselected RNA-seq data profiled from solid tissues. TRUST achieved high sensitivity in CDR3 calling even for samples with low sequencing depth and has demonstrated utilities in its application to large tumor cohorts."}, {"title": "SINTAX: a simple non-Bayesian taxonomy classifier for 16S and ITS sequences", "url": "https://www.biorxiv.org/content/early/2016/09/09/074161", "tag": "Bioinformatics", "abstract": "Metagenomics experiments often characterize microbial communities by sequencing the ribosomal 16S and ITS regions. Taxonomy prediction is a fundamental step in such studies. The SINTAX algorithm predicts taxonomy by using k-mer similarity to identify the top hit in a reference database and provides bootstrap confidence for all ranks in the prediction. SINTAX achieves comparable or better accuracy to the RDP Naive Bayesian Classifier with a simpler algorithm that does not require training. Most tested methods are shown to have high rates of over-classification errors where novel taxa are incorrectly predicted to have known names."}, {"title": "A framework for RNA quality correction in differential expression analysis", "url": "https://www.biorxiv.org/content/early/2016/09/09/074245", "tag": "Bioinformatics", "abstract": "RNA sequencing (RNA-seq) is a powerful approach for measuring gene expression levels in cells and tissues, but it relies on high-quality RNA. We demonstrate here that statistical adjustment employing existing quality measures largely fails to remove the effects of RNA degradation when RNA quality associates with the outcome of interest. Using RNA-seq data from a molecular degradation experiment of human brain tissue, we introduce the quality surrogate variable (qSVA) analysis framework for estimating and removing the confounding effect of RNA quality in differential expression analysis. We show this approach results in greatly improved replication rates (>3x) across two large independent postmortem human brain studies of schizophrenia. Finally, we explored public datasets to demonstrate potential RNA quality confounding when comparing expression levels of different brain regions and diagnostic groups beyond schizophrenia. Our approach can therefore improve the interpretation of differential expression analysis of transcriptomic data from the human brain."}, {"title": "Optimal Point Process Filtering and Estimation of the Coalescent Process", "url": "https://www.biorxiv.org/content/early/2016/09/09/024737", "tag": "Bioinformatics", "abstract": "The coalescent process is an important and widely used model for inferring the dynamics of biological populations from samples of genetic diversity. Coalescent analysis typically involves applying statistical methods to either samples of genetic sequences or an estimated genealogy in order to estimate the demographic history of the population from which the samples originated. Several parametric and non-parametric estimation techniques, employing diverse methods, such as Gaussian processes and Monte Carlo particle filtering, already exist. However, these techniques often trade estimation accuracy and sophistication for methodological flexibility and ease of use. Thus, there is room for new coalescent estimation techniques that can be easily implemented for a range of inference problems while still maintaining some sense of statistical optimality. Here we introduce the Bayesian Snyder filter as a natural, easily implementable and flexible minimum mean square error estimator for parametric demographic functions. By reinterpreting the coalescent as a self-correcting inhomogeneous Poisson process, we show that the Snyder filter can be applied to both isochronous (sampled at one time point) and heterochronous (serially sampled) estimation problems. We test the estimation performance of the filter on both standard, simulated demographic models and on a well-studied empirical dataset comprising hepatitis C virus sequences from Egypt. Additionally, we provide some analytical insight into the relationship between the Snyder filter and popular maximum likelihood and skyline plot techniques for coalescent inference. The Snyder filter is an exact and direct Bayesian estimation method that provides optimal mean square error estimates. It has the potential to become as a useful, alternative technique for coalescent inference."}, {"title": "MTQuant: \"Seeing\" Beyond the Diffraction Limit in Fluorescence Images to Quantify Neuronal Microtubule Organization", "url": "https://www.biorxiv.org/content/early/2016/09/08/074047", "tag": "Bioinformatics", "abstract": "Motivation: Microtubules (MTs) are polarized polymers that are critical for cell structure and axonal transport. They form a bundle in neurons, but beyond that, their organization is relatively unstudied. Results: We present MTQuant, a method for quantifying MT organization using light microscopy, which distills three parameters from MT images: the spacing of MT minus-ends, their average length, and the average number of MTs in a cross-section of the bundle. This method allows for robust and rapid in vivo analysis of MTs, rendering it more practical and more widely applicable than commonly-used electron microscopy reconstructions. MTQuant was successfully validated with three ground truth data sets and applied to over 3000 images of MTs in a C. elegans motor neuron."}, {"title": "Secondary-structure prediction revisited: P\u03b2 and Pc represent structures of amyloids and aid elucidating phenomena in interspecies transmissions of prion.", "url": "https://www.biorxiv.org/content/early/2016/09/08/073668", "tag": "Bioinformatics", "abstract": "Prion is a unique infectious agent which consists solely of abnormally-folded prion protein (PrPSc) but possesses virus-like features, e.g. existence of strain diversity, adaptation to new hosts and evolutionary changes. These biological phenomena were attributed to the structural properties of PrPSc due to lack of genetic material of prion. Therefore, regardless of incompatibility with high-resolution structural analysis, many structural models of PrPSc have been hypothesized based on limited structural information and, recently models consisting solely of \u03b2-sheets and intervening loops/kinks have been suggested, i.e. parallel in-register \u03b2-sheet models and \u03b2-solenoid model. Given the relatively simple structural models of PrPSc, we utilized values of theoretical \u03b2-sheet or random-coil propensity (P\u03b2 or Pc, respectively) calculated by secondary structure prediction with a neural network to analyze interspecies transmissions of prion, because numerical conversion of the primary structures would enable quantitative comparison of PrP with distinct primary structures. Reviewing experiments in the literature, we ascertained biological relevance of P\u03b2 and Pc and demonstrated how those parameters could aid interpretation and explain phenomena in interspecies transmissions. Our approach can lead to development of a versatile tool for investigation of not only prion but also other amyloids."}, {"title": "Overcoming confounding plate effects in differential expression analyses of single-cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2016/09/08/073973", "tag": "Bioinformatics", "abstract": "An increasing number of studies are using single-cell RNA-sequencing (scRNA-seq) to characterize the gene expression profiles of individual cells. One common analysis applied to scRNA-seq data involves detecting differentially expressed (DE) genes between cells in different biological groups. However, many experiments are designed such that the cells to be compared are processed in separate plates or chips, meaning that the groupings are confounded with systematic plate effects. This confounding aspect is frequently ignored in DE analyses of scRNA-seq data. In this article, we demonstrate that failing to consider plate effects in the statistical model results in loss of type I error control. A solution is proposed whereby counts are summed from all cells in each plate and the count sums for all plates are used in the DE analysis. This restores type I error control in the presence of plate effects without compromising detection power in simulated data. Summation is also robust to varying numbers and library sizes of cells on each plate. Similar results are observed in DE analyses of real data where the use of count sums instead of single-cell counts improves specificity and the ranking of relevant genes. This suggests that summation can assist in maintaining statistical rigour in DE analyses of scRNA-seq data with plate effects."}, {"title": "De novo extraction of microbial strains from metagenomes reveals intra-species niche partitioning", "url": "https://www.biorxiv.org/content/early/2016/09/06/073825", "tag": "Bioinformatics", "abstract": "Background: We introduce DESMAN for De novo Extraction of Strains from MetAgeNomes. Metagenome sequencing generates short reads from throughout the genomes of a microbial community. Increasingly large, multi-sample metagenomes, stratified in space and time are being generated from communities with thousands of species. Repeats result in fragmentary co-assemblies with potentially millions of contigs. Contigs can be binned into metagenome assembled genomes (MAGs) but strain level variation will remain. DESMAN identifies variants on core genes, then uses co-occurrence across samples to link variants into strain sequences and abundance profiles. These strain profiles are then searched for on non-core genes to determine the accessory genes present in each strain. Results: We validated DESMAN on a synthetic twenty genome community with 64 samples. We could resolve the five E. coli strains present with 99.58% accuracy across core gene variable sites and their gene complement with 95.7% accuracy. Similarly, on real fecal metagenomes from the 2011 E. coli (STEC) O104:H4 outbreak, the outbreak strain was reconstructed with 99.8% core sequence accuracy. Application to an anaerobic digester metagenome time series reveals that strain level variation is endemic with 16 out of 26 MAGs (61.5%) examined exhibiting two strains. In almost all cases the strain proportions were not statistically different between replicate reactors, suggesting intra-species niche partitioning. The only exception being when the two strains had almost identical gene complement and, hence, functional capability. Conclusions: DESMAN will provide a provide a powerful tool for de novo resolution of fine-scale variation in microbial communities. It is available as open source software from https://github.com/chrisquince/DESMAN."}, {"title": "SVScore: An Impact Prediction Tool For Structural Variation", "url": "https://www.biorxiv.org/content/early/2016/09/06/073833", "tag": "Bioinformatics", "abstract": "Motivation: Structural variation (SV) is an important and diverse source of human genome variation. Over the past several years, much progress has been made in the area of SV detection, but predict-ing the functional impact of SVs discovered in whole genome sequencing (WGS) studies remains extremely challenging. Accurate SV impact prediction is especially important for WGS-based rare variant association studies and studies of rare disease. Results: Here we present SVScore, a computational tool for in silico SV impact prediction. SVScore aggregates existing per-base single nucleotide polymorphism pathogenicity scores across relevant genomic intervals for each SV in a manner that considers variant type, gene features, and uncertainty in breakpoint location. We show that in a Finnish cohort, the allele frequency spectrum of SVs with high impact scores is strongly skewed toward lower frequencies, suggesting that these variants are under purifying selection. We further show that SVScore identifies deleterious variants more effectively than naive alternative methods. Finally, our results indicate that high-scoring tandem duplications may be under surprisingly strong selection relative to high-scoring deletions, suggesting that duplications may be more deleterious than previously thought. In conclusion, SVScore provides pathogenicity prediction for SVs that is both informative and meaningful for understanding their functional role in disease. Availability: SVScore is implemented in Perl and available freely at {{http://www.github.com/lganel/SVScore}} for use under the MIT license. Contact: ihall@wustl.edu"}, {"title": "A Common Class of Transcripts with 5'-Intron Depletion, Distinct Early Coding Sequence Features, and N1-Methyladenosine Modification", "url": "https://www.biorxiv.org/content/early/2016/09/06/057455", "tag": "Bioinformatics", "abstract": "Introns are found in 5' untranslated regions (5'UTRs) for 35% of all human transcripts. These 5'UTR introns are not randomly distributed: genes that encode secreted, membrane-bound and mitochondrial proteins are less likely to have them. Curiously, transcripts lacking 5'UTR introns tend to harbor specific RNA sequence elements in their early coding regions. To model and understand the connection between coding-region sequence and 5'UTR intron status, we developed a classifier that can predict 5'UTR intron status with >80% accuracy using only sequence features in the early coding region. Thus, the classifier identifies transcripts with 5' proximal-intron-minus-like-coding regions (\"5IM\" transcripts). Unexpectedly, we found that the early coding sequence features defining 5IM transcripts are widespread, appearing in 21% of all human RefSeq transcripts. The 5IM class of transcripts is enriched for non-AUG start codons, more extensive secondary structure both preceding the start codon and near the 5' cap, greater dependence on eIF4E for translation, and association with ER-proximal ribosomes. 5IM transcripts are bound by the Exon Junction Complex (EJC) at non-canonical 5' proximal positions. Finally, N1-methyladenosines are specifically enriched in the early coding regions of 5IM transcripts. Taken together, our analyses point to the existence of a distinct 5IM class comprising ~20% of human transcripts. This class is defined by depletion of 5' proximal introns, presence of specific RNA sequence features associated with low translation efficiency, N1-methyladenosines in the early coding region, and enrichment for non-canonical binding by the Exon Junction Complex."}, {"title": "LoRTE: Detecting transposon-induced genomic variants using low coverage PacBio long read sequences", "url": "https://www.biorxiv.org/content/early/2016/09/05/073551", "tag": "Bioinformatics", "abstract": "Motivation: Population genomic analysis of transposable elements has greatly benefited from recent advances of sequencing technologies. However, the propensity of transposable elements to nest in highly repeated regions of genomes limits the efficiency of bioinformatic tools when short read sequences technology is used. Results: LoRTE is the first tool able to use PacBio long read sequences to identify transposon deletions and insertions between a reference genome and genomes of different strains or populations. Tested against Drosophila melanogaster PacBio datasets, LoRTE appears to be a reliable and broadly applicable tools to study the dynamic and evolutionary impact of transposable elements using low coverage, long read sequences. Availability and Implementation: LoRTE is available at http://www.egce.cnrs-gif.fr/?p=6422. It is written in Python 2.7 and only requires the NCBI BLAST + package. LoRTE can be used on standard computer with limited RAM resources and reasonable running time even with large datasets."}, {"title": "A Scalable Algorithm for Structure Identification of Complex Gene Regulatory Network from Temporal Expression Data", "url": "https://www.biorxiv.org/content/early/2016/09/04/073296", "tag": "Bioinformatics", "abstract": "Motivation: Gene regulatory interactions are of fundamental importance to various biological functions and processes. However, only a few previous computational studies have claimed success in revealing genome-wide regulatory landscapes from temporal gene expression data, especially for complex eukaryotes like human. Moreover, recent work suggests that these methods still suffer from the curse of dimensionality if network size increases to 100 or higher. Result: We present a novel scalable algorithm for identifying genome-wide regulatory network structures. The highlight of our method is that its superior performance does not degenerate even for a network size on the order of $10^4$, and is thus readily applicable to large-scale complex networks. Such a breakthrough is achieved by considering both prior biological knowledge and multiple topological properties (i.e., sparsity and hub gene structure) of complex networks in the regularized formulation. We also illustrate the application of our algorithm in practice using the time-course expression data from an influenza infection study in respiratory epithelial cells. Availability and Implementation: The algorithm described in this article is implemented in MATLAB\u00ae. The source code is freely available from https://github.com/Hongyu-Miao/DMI.git. Contact: jliu@cs.rochester.edu; hongyu.miao@uth.tmc.edu Supplementary information: Supplementary data are available online."}, {"title": "Pitfalls in Inferring Human Microbial Dynamics from Temporal Metagenomics Data", "url": "https://www.biorxiv.org/content/early/2016/09/04/073254", "tag": "Bioinformatics", "abstract": "Human gut microbiota is a very complex and dynamic ecosystem that plays a crucial role in our health and well-being. Inferring microbial community structure and dynamics directly from time-resolved metagenomics data is key to understanding the community ecology and predicting its temporal behavior. Many methods have been proposed to perform the inference. Yet, we point out that there are several pitfalls along the way, from uninformative temporal measurements to the compositional nature of the relative abundance data, focusing on highly abundant species by ignoring or grouping low-abundance species, and implicit assumptions in various regularization methods. These issues have to be seriously considered in ecological modeling of human gut microbiota."}, {"title": "A Novel Clustering Method for Patient Stratification", "url": "https://www.biorxiv.org/content/early/2016/09/03/073189", "tag": "Bioinformatics", "abstract": "Patient stratification or disease subtyping is crucial for precision medicine and personalized treatment of complex diseases. The increasing availability of high-throughput molecular data provides a great opportunity for patient stratification. In particular, many clustering methods have been employed to tackle this problem in a purely data-driven manner. Yet, existing methods leveraging high-throughput molecular data often suffers from various limitations, e.g., noise, data heterogeneity, high dimensionality or poor interpretability. Here we introduced an Entropy-based Consensus Clustering (ECC) method that overcomes those limitations all together. Our ECC method employs an entropy-based utility function to fuse many basic partitions to a consensus one that agrees with the basic ones as much as possible. Maximizing the utility function in ECC has a much more meaningful interpretation than any other consensus clustering methods. Moreover, we exactly map the complex utility maximization problem to the classic K-means clustering problem with a modified distance function, which can then be efficiently solved with linear time and space complexity. Our ECC method can also naturally integrate multiple molecular data types measured from the same set of subjects, and easily handle missing values without any imputation. We applied ECC to both synthetic and real data, including 35 cancer gene expression benchmark datasets and 13 cancer types with four molecular data types from The Cancer Genome Atlas. We found that ECC shows superior performance against existing clustering methods. Our results clearly demonstrate the power of ECC in clinically relevant patient stratification."}, {"title": "SC3 - consensus clustering of single-cell RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2016/09/02/036558", "tag": "Bioinformatics", "abstract": "Using single-cell RNA-seq (scRNA-seq), the full transcriptome of individual cells can be acquired, enabling a quantitative cell-type characterisation based on expression profiles. However, due to the large variability in gene expression, identifying cell types based on the transcriptome remains challenging. We present Single-Cell Consensus Clustering (SC3), a tool for unsupervised clustering of scRNA-seq data. SC3 achieves high accuracy and robustness by consistently integrating different clustering solutions through a consensus approach. Tests on twelve published datasets show that SC3 outperforms five existing methods while remaining scalable, as shown by the analysis of a large dataset containing 44,808 cells. Moreover, an interactive graphical implementation makes SC3 accessible to a wide audience of users, and SC3 aids biological interpretation by identifying marker genes, differentially expressed genes and outlier cells. We illustrate the capabilities of SC3 by characterising newly obtained transcriptomes from subclones of neoplastic cells collected from patients."}, {"title": "Reductive Analytics on Big MS Data leads to tremendous reduction in time for peptide deduction", "url": "https://www.biorxiv.org/content/early/2016/09/02/073064", "tag": "Bioinformatics", "abstract": "In this paper we present a feasibility of using a data-reductive strategy for analyzing big MS data. The proposed method utilizes our reduction algorithm MS-REDUCE and peptide deduction is accomplished using Tide with hiXcorr. Using this approach we were able to process 1 million spectra in under 3 hours. Our results showed that running peptide deduction with smaller amount of selected peaks made the computations much faster and scalable with increasing resolution of MS data. Quality assessment experiments performed on experimentally generated datasets showed good quality peptide matches can be made using the reduced datasets. We anticipate that the proteomics and systems biology community will widely adopt our reductive strategy due to its efficacy and reduced time for analysis."}, {"title": "Contig annotation tool CAT robustly classifies assembled metagenomic contigs and long sequences", "url": "https://www.biorxiv.org/content/early/2016/09/01/072868", "tag": "Bioinformatics", "abstract": "In modern-day metagenomics, there is an increasing need for robust taxonomic annotation of long DNA sequences from unknown micro-organisms. Long metagenomic sequences may be derived from assembly of short-read metagenomes, or from long-read single molecule sequencing. Here we introduce CAT, a pipeline for robust taxonomic classification of long DNA sequences. We show that CAT correctly classifies contigs at different taxonomic levels, even in simulated metagenomic datasets that are very distantly related from the sequences in the database. CAT is implemented in Python and the required scripts can be freely downloaded from Github."}, {"title": "Predictive computational phenotyping and biomarker discovery using reference-free genome comparisons", "url": "https://www.biorxiv.org/content/early/2016/08/31/045153", "tag": "Bioinformatics", "abstract": "Background: The identification of genomic biomarkers is a key step towards improving diagnostic tests and therapies. We present a reference-free method for this task that relies on a k-mer representation of genomes and a machine learning algorithm that produces intelligible models. The method is computationally scalable and well-suited for whole genome sequencing studies. Results: The method was validated by generating models that predict the antibiotic resistance of C. difficile, M. tuberculosis, P. aeruginosa, and S. pneumoniae for 17 antibiotics. The obtained models are accurate, faithful to the biological pathways targeted by the antibiotics, and they provide insight into the process of resistance acquisition. Moreover, a theoretical analysis of the method revealed tight statistical guarantees on the accuracy of the obtained models, supporting its relevance for genomic biomarker discovery. Conclusions: Our method allows the generation of accurate and interpretable predictive models of phenotypes, which rely on a small set of genomic variations. The method is not limited to predicting antibiotic resistance in bacteria and is applicable to a variety of organisms and phenotypes. Kover, an efficient implementation of our method, is open-source and should guide biological efforts to understand a plethora of phenotypes (http://github.com/aldro61/kover/)."}, {"title": "Targeted reconstruction of T cell receptor sequence from single cell RNA-sequencing links CDR3 length to T cell differentiation state", "url": "https://www.biorxiv.org/content/early/2016/08/31/072744", "tag": "Bioinformatics", "abstract": "The T cell compartment must contain diversity in both TCR repertoire and cell state to provide effective immunity against pathogens. However, it remains unclear how differences in the TCR contribute to heterogeneity in T cell state at the single cell level because most analysis of the TCR repertoire has, to date, aggregated information from populations of cells. Single cell RNA-sequencing (scRNA-seq) can allow simultaneous measurement of TCR sequence and global transcriptional profile from single cells. However, current protocols to directly sequence the TCR require the use of long sequencing reads, increasing the cost and decreasing the number of cells that can be feasibly analyzed. Here we present a tool that can efficiently extract TCR sequence information from standard, short-read scRNA-seq libraries of T cells: TCR Reconstruction Algorithm for Paired-End Single cell (TRAPeS). We apply it to investigate heterogeneity in the CD8+ T cell response in humans and mice, and show that it is accurate and more sensitive than previous approaches. We applied TRAPeS to single cell RNA-seq of CD8+ T cells specific for a single epitope from Yellow Fever Virus. We show that the recently-described \"naive-like\" memory population of YFV-specific CD8+ T cells have significantly longer CDR3 regions and greater divergence from germline sequence than do effector-memory phenotype CD8+ T cells specific for YFV. This suggests that TCR usage contributes to heterogeneity in the differentiation state of the CD8+ T cell response to YFV. TRAPeS is publicly available, and can be readily used to investigate the relationship between the TCR repertoire and cellular phenotype."}, {"title": "A phylogenetic transform enhances analysis of compositional microbiota data", "url": "https://www.biorxiv.org/content/early/2016/08/31/072413", "tag": "Bioinformatics", "abstract": "High-throughput DNA sequencing technologies have revolutionized the study of microbial communities (microbiota) and have revealed their importance in both human health and disease. However, due to technical limitations, data from microbiota surveys reflect the relative abundance of bacterial taxa and not their absolute levels. It is well known that applying common statistical methods, such as correlation or hypothesis testing, to relative abundance data can lead to spurious results. Here, we introduce the PhILR transform, a data transform that utilizes microbial phylogenetic information. This transform enables off-the-shelf statistical tools to be applied to microbiota surveys free from artifacts usually associated with analysis of relative abundance data. Using environmental and human-associated microbial community datasets as benchmarks, we find that the PhILR transform significantly improves the performance of distance-based and machine learning-based statistics, boosting the accuracy of widely used algorithms on reference benchmarks by 90%. Because the PhILR transform relies on bacterial phylogenies, statistics applied in the PhILR coordinate system are also framed within an evolutionary perspective. Regression on PhILR transformed human microbiota data identified evolutionarily neighboring bacterial clades that may have differentiated to adapt to distinct body sites. Variance statistics showed that the degree of covariation of bacterial clades across human body sites tended to increase with phylogenetic relatedness between clades. These findings support the hypothesis that environmental selection, not competition between bacteria, plays a dominant role in structuring human-associated microbial communities."}, {"title": "Do Read Errors Matter for Genome Assembly?", "url": "https://www.biorxiv.org/content/early/2016/08/31/014399", "tag": "Bioinformatics", "abstract": "While most current high-throughput DNA sequencing technologies generate short reads with low error rates, emerging sequencing technologies generate long reads with high error rates. A basic question of interest is the tradeoff between read length and error rate in terms of the information needed for the perfect assembly of the genome. Using an adversarial erasure error model, we make progress on this problem by establishing a critical read length, as a function of the genome and the error rate, above which perfect assembly is guaranteed. For several real genomes, including those from the GAGE dataset, we verify that this critical read length is not significantly greater than the read length required for perfect assembly from reads without errors."}, {"title": "Simera: Modelling the PCR Process to Simulate Realistic Chimera Formation", "url": "https://www.biorxiv.org/content/early/2016/08/31/072447", "tag": "Bioinformatics", "abstract": "Polymerase Chain Reaction (PCR) is the principal method of amplifying target DNA regions and, as such, is of great importance when performing microbial diversity studies. An unfortunate side effect of PCR is the formation of unwanted byproducts such as chimeras. The main goal of the work covered in this article is the development of an algorithm that simulates realistic chimeras for use in the evaluation of chimera detection software and for investigations into the accuracy of community structure analyses. Experimental data has helped to identify factors which may cause the formation of chimeras and has provided evidence of how influential these factors can be. This article makes use of some of this evidence in order to build a model with which to simulate the PCR process. This model helps to better explain the formation of chimeras and is therefore able to provide aid to future studies that intend to use PCR."}, {"title": "Detecting consistent patterns of directional adaptation using differential selection codon models", "url": "https://www.biorxiv.org/content/early/2016/08/30/072405", "tag": "Bioinformatics", "abstract": "Background: Phylogenetic codon models are often used to characterize the selective regimes acting on protein coding sequences. Recent methodological developments have led to models explicitly accounting for the interplay between mutation and selection, by explicitly modelling the amino acid fitness landscape along the sequence. However, thus far, most of these models have assumed that the fitness landscape is constant over time. Fluctuations of the fitness landscape may often be random or depend on complex and unknown factors. However, some organisms may be subject to systematic changes in selective pressure, resulting in reproducible molecular adaptations across independent lineages subject to similar conditions. Results: Here, we developed a codon-based differential selection model, which aims to detect and quantify the fine-grained consistent patterns of adaptation at the protein-coding level, as a function of external conditions experienced by the organism under investigation. The model parameterizes the global mutational pressure, as well as the site- and condition-specific amino acid selective preferences. This phylogenetic model is implemented in a Bayesian MCMC framework. After validation with simulations, we applied our method to a dataset of HIV sequences from patients with known HLA genetic background. Our differential selection model detects and characterizes differentially selected coding positions specifically associated with two different HLA alleles. Conclusion: our differential selection model is able to identify consistent molecular adaptations as a function of repeated changes in the environment of the organism. These models can be applied to many other problems, ranging from viral adaptation to evolution of life-history strategies in plants or animals."}, {"title": "GAVIN - Gene-Aware Variant INterpretation for medical sequencing", "url": "https://www.biorxiv.org/content/early/2016/08/30/072330", "tag": "Bioinformatics", "abstract": "Here, we present GAVIN, a new method that delivers accurate classification of variants for next-generation sequencing molecular diagnostics. It is based on gene-specific calibrations of allele frequencies (from the ExAC database), effect impact (using SnpEff) and estimated deleteriousness (CADD scores) for >3,000 genes. In a benchmark on 18 clinical gene sets, we achieved a sensitivity of 91.6%, with a specificity of 78.2%. This accuracy was unmatched by 12 other tools we tested. We provide GAVIN as an online MOLGENIS service to annotate VCF files, and as open source executable for use in bioinformatic pipelines. It can be found at http://molgenis.org/gavin."}, {"title": "microRNA-mRNA interaction identification in Wilms tumor using principalcomponent analysis based unsupervised feature extraction", "url": "https://www.biorxiv.org/content/early/2016/08/30/059295", "tag": "Bioinformatics", "abstract": "Wilms tumor is one of lethal child renal cancers, for which no known disease causing mechanisms exist. In this paper, we tried to identify possible disease causing microRNA(miRNA)-mRNA pairs (interactions) by analyzing (partially matched) miRNA/mRNA gene expression profiles with the recently proposed principal component analysis based unsupervised feature extraction. It successfully identified multiple miRNA-mRNA pairs whose biological natures are convincing. Correlation coefficients between miRNA and mRNA expression in matched parts of profiles turned out to be significantly negative. Constructed miRNA-mRNA network will be a key to understand Wilms tumor causing mechanisms."}, {"title": "Increased taxon sampling reveals thousands of hidden orthologs in flatworms", "url": "https://www.biorxiv.org/content/early/2016/08/30/050724", "tag": "Bioinformatics", "abstract": "Gains and losses shape the gene complement of animal lineages and are a fundamental aspect of genomic evolution. Acquiring a comprehensive view of the evolution of gene repertoires is limited by the intrinsic limitations of common sequence similarity searches and available databases. Thus, a subset of the complement of an organism consists of hidden orthologs, those with no apparent homology with common sequenced animal lineages - mistakenly considered new genes - but actually representing rapidly evolving orthologs or undetected paralogs. Here, we describe Leapfrog, a simple automated BLAST pipeline that leverages increased taxon sampling to overcome long evolutionary distances and identify hidden orthologs in large transcriptomic databases. As a case study, we used 35 transcriptomes of 29 flatworm lineages to recover 3,427 hidden orthologs, some of them not identified by OrthoFinder, a common orthogroup inference algorithm. Unexpectedly, we do not observe a correlation between the number of hidden orthologs in a lineage and its \"average\" evolutionary rate. Hidden orthologs do not show unusual sequence composition biases (e.g. GC content, average length, domain composition) that might account for systematic errors in sequence similarity searches. Instead, gene duplication and divergence of one paralog and weak positive selection appear to underlie hidden orthology in Platyhelminthes. By using Leapfrog, we identify key centrosome-related genes and homeodomain classes previously reported as absent in free-living flatworms, e.g. planarians. Altogether, our findings demonstrate that hidden orthologs comprise a significant proportion of the gene repertoire in flatworms, qualifying the impact of gene losses and gains in gene complement evolution."}, {"title": "Salmon provides accurate, fast, and bias-aware transcript expression estimates using dual-phase inference", "url": "https://www.biorxiv.org/content/early/2016/08/30/021592", "tag": "Bioinformatics", "abstract": "We introduce Salmon, a new method for quantifying transcript abundance from RNA-seq reads that is highly-accurate and very fast. Salmon is the first transcriptome-wide quantifier to model and correct for fragment GC content bias, which we demonstrate substantially improves the accuracy of abundance estimates and the reliability of subsequent differential expression analysis compared to existing methods that do not account for these biases. Salmon achieves its speed and accuracy by combining a new dual-phase parallel inference algorithm and feature-rich bias models with an ultra-fast read mapping procedure. These innovations yield both exceptional accuracy and order-of-magnitude speed benefits over alignment-based methods."}, {"title": "The Phenoscape Knowledgebase: tools and APIs for computing across phenotypes from evolutionary diversity and model organisms", "url": "https://www.biorxiv.org/content/early/2016/08/29/071951", "tag": "Bioinformatics", "abstract": "The Phenoscape Knowledgebase (KB) is an ontology-driven database that combines existing phenotype annotations from model organism databases with new phenotype annotations from the evolutionary literature. Phenoscape curators have created phenotype annotations for more than 5,000 species and higher taxa, by defining computable phenotype concepts for more than 20,000 character states from over 160 published phylogenetic studies. These phenotype concepts are in the form of Entity-Quality (EQ) compositions which incorporate terms from the Uberon anatomy ontology, the Biospatial Ontology (BSPO), and the Phenotype and Trait Ontology (PATO). Taxonomic concepts are drawn from the Vertebrate Taxonomy Ontology (VTO). This knowledge of comparative biodiversity is linked to potentially relevant developmental genetic mechanisms by importing associations of genes to phenotypic effects and gene expression locations from zebrafish (ZFIN), mouse (MGI), Xenopus (Xenbase), and human (Human Phenotype Ontology project). Thus far, the Phenoscape KB has been used to identify candidate genes for evolutionary phenotypes, to match profiles of ancestral evolutionary variation with gene phenotype profiles, and to combine data across many evolutionary studies by inferring indirectly asserted values within synthetic supermatrices. Here we describe the software architecture of the Phenoscape KB, including data ingestion, integration of OWL reasoning, web service interface, and application features."}, {"title": "Perturbation-response genes reveal signaling footprints in cancer gene expression", "url": "https://www.biorxiv.org/content/early/2016/08/28/065672", "tag": "Bioinformatics", "abstract": "Aberrant cell signaling is known to cause cancer and many other diseases, as well as a focus of treatment. A common approach is to infer its activity on the level of pathways using gene expression. However, mapping gene expression to pathway components disregards the effect of post-translational modifications, and downstream signatures represent very specific experimental conditions. Here we present PROGENy, a method that overcomes both limitations by leveraging a large compendium of publicly available perturbation experiments to yield a common core of Pathway RespOnsive GENes. Unlike existing methods, PROGENy can (i) recover the effect of known driver mutations, (ii) provide or improve strong markers for drug indications, and (iii) distinguish between oncogenic and tumor suppressor pathways for patient survival. Collectively, these results show that PROGENy more accurately infers pathway activity from gene expression than other methods."}, {"title": "Integrative networks illuminate biological factors underlying gene-disease associations", "url": "https://www.biorxiv.org/content/early/2016/08/28/062695", "tag": "Bioinformatics", "abstract": "Integrative networks combine multiple layers of biological data into a model of how genes work together to carry out cellular processes. Such networks become more valuable as they become more context specific, for example, by capturing how genes work together in a certain tissue or cell type. We discuss the applications of these networks to the study of human disease. Once constructed, these networks provide the means to identify broad biological patterns underlying genes associated with complex traits and diseases. We cover the different types of integrative networks that currently exist and how such networks that encompass multiple biological layers are constructed. We highlight how specificity can be incorporated into the reconstruction of different types of biomolecular interactions between genes, using tissue-specificity as a motivating example. We discuss examples of cases where networks have been applied to study human diseases and opportunities for new applications. Integrative networks with specificity to tissue or other biological features provide new capabilities to researchers engaged in the study of human disease. We expect improved data and algorithms to continue to improve such networks, allowing them to provide more detailed and mechanistic predictions into the context-specific genetic etiology of common diseases"}, {"title": "DNAmod: the DNA modification database", "url": "https://www.biorxiv.org/content/early/2016/08/26/071712", "tag": "Bioinformatics", "abstract": "Covalent DNA modifications, such as 5-methylcytosine (5mC), are increasingly the focus of numerous research programs. In eukaryotes, both 5mC and 5-hydroxymethylcytosine are now recognized as stable epigenetic marks, with diverse functions. Bacteria, archaea, and viruses contain various modified DNA nucleobases, including several in which one base is largely or entirely replaced by a particular covalent modification. Numerous databases describe RNA and histone modifications, but no database specifically catalogues DNA modifications, despite their broad importance as an element of epigenetic regulation. To address this need, we have developed DNAmod: the DNA modification database. DNAmod is an open-source database (http://dnamod.hoffmanlab.org) that catalogues DNA modifications and provides a single source to learn about their properties. DNAmod provides a web interface to easily browse and search through its modifications. The database annotates the chemical properties and structures of all curated modified DNA bases, and a much larger list of candidate chemical entities. DNAmod includes manual annotations of available sequencing methods, descriptions of their occurrence in nature, and provides existing and suggested nomenclature. DNAmod enables researchers to rapidly review previous work, select mapping techniques, and track recent developments concerning modified bases of interest."}, {"title": "Metavisitor, a suite of Galaxy tools for simple and rapid detection and discovery of viruses in deep sequence data", "url": "https://www.biorxiv.org/content/early/2016/08/26/048983", "tag": "Bioinformatics", "abstract": "We present user-friendly and adaptable software to provide biologists, clinical researchers and possibly diagnostic clinicians with the ability to robustly detect and reconstruct viral genomes from complex deep sequence datasets. A set of modular bioinformatic tools and workflows was implemented as the Metavisitor package in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules. Metavisitor can be used on our Mississippi server, or can be installed on any Galaxy server instance and a pre-configured Metavisitor server image is provided. Metavisitor works with DNA, RNA or small RNA sequencing data over a range of read lengths and can use a combination of de novo and guided approaches to assemble genomes from sequencing reads. We show that the software has the potential for quick diagnosis as well as discovery of viruses from a vast array of organisms. Importantly, we provide here executable Metavisitor use cases, which increase the accessibility and transparency of the software, ultimately enabling biologists or clinicians to focus on biological or medical questions."}, {"title": "Empowering Multi-Cohort Gene Expression Analysis to Increase Reproducibility", "url": "https://www.biorxiv.org/content/early/2016/08/25/071514", "tag": "Bioinformatics", "abstract": "A major contributor to the scientific reproducibility crisis has been that the results from homogeneous, single-center studies do not generalize to heterogeneous, real world populations. Multi-cohort gene expression analysis has helped to increase reproducibility by aggregating data from diverse populations into a single analysis. To make the multi-cohort analysis process more feasible, we have assembled an analysis pipeline which implements rigorously studied meta-analysis best practices. We have compiled and made publicly available the results of our own multi-cohort gene expression analysis of 103 diseases, spanning 615 studies and 36,915 samples, through a novel and interactive web application. As a result, we have made both the process of and the results from multi-cohort gene expression analysis more approachable for non-technical users."}, {"title": "HLA-MA: Simple yet powerful matching of samples using HLA typing results", "url": "https://www.biorxiv.org/content/early/2016/08/24/066548", "tag": "Bioinformatics", "abstract": "We propose the simple method HLA-MA for consistency checking in pipelines operating on human HTS data. The method is based on the HLA typing result of the state-of-the-art method OptiType. Provided that there is sufficient coverage of the HLA loci, comparing HLA types allows for simple, fast, and robust matching of samples from whole genome, exome, and RNA-seq data. This approach is reliable for sample re-identification even for samples with high mutational loads, e.g., caused by microsatellite instability or POLE1 defects."}, {"title": "Edlib: A C/C++ library for fast, exact sequence alignment using edit distance", "url": "https://www.biorxiv.org/content/early/2016/08/23/070649", "tag": "Bioinformatics", "abstract": "We present Edlib, an open-source C/C++ library for exact pairwise sequence alignment using edit distance. We compare Edlib to other libraries and show that it is the fastest while not lacking in functionality, and can also easily handle very large sequences. Being easy to use, flexible, fast and low on memory usage, we expect it to be a cornerstone for many future bioinformatics tools. Source code, installation instructions and test data are freely available for download at https://github.com/Martinsos/edlib, implemented in C/C++ and supported on Linux, MS Windows, and Mac OS."}, {"title": "Ohana, a tool set for population genetic analyses of admixture components", "url": "https://www.biorxiv.org/content/early/2016/08/23/071233", "tag": "Bioinformatics", "abstract": "Motivation: Structure methods are highly used population genetic methods for classifying individuals in a sample fractionally into discrete ancestry components. Contribution: We introduce a new optimization algorithm of the classical Structure model in a maximum likelihood framework. Using analyses of real data we show that the new optimization algorithm finds higher likelihood values than the state-of-the-art method in the same computational time. We also present a new method for estimating population trees from ancestry components using a Gaussian approximation. Using coalescence simulations modeling populations evolving in a tree-like fashion, we explore the adequacy of the Structure model and the Gaussian assumption for identifying ancestry components correctly and for inferring the correct tree. In most cases, ancestry components are inferred correctly, although sample sizes and times since admixture can influence the inferences. Similarly, the popular Gaussian approximation tends to perform poorly when branch lengths are long, although the tree topology is correctly inferred in all scenarios explored. The new methods are implemented together with appropriate visualization tools in the computer package Ohana. Availability: Ohana is publicly available at: https://github.com/jade-cheng/ohana. Besides its source code and installation instructions, we also provide example workflows in the project wiki site."}, {"title": "Machine learning as a tool for predicting insincere effort in power grips", "url": "https://www.biorxiv.org/content/early/2016/08/23/068494", "tag": "Bioinformatics", "abstract": "Background It was not possible to detect the common problem of insincere grip effort in grip strength evaluation until now. The usually used JAMAR dynamometer has low sensitivity and specificity in distinguishing between maximal and submaximal effort. The manugraphy system may give additional information to the dynamometer measurements used to assess grip force, as it also measures the load distribution of the hand while it grips a cylinder. Until now, the data of load distribution evaluation were analyzed by comparing discrete variables (e.g., load values of a defined area). From another point of view, the results of manugraphy measurements form a pattern. Analyzing patterns is a typical domain of machine learning. Methods We used data from several studies that assessed load distribution with maximal and submaximal effort. They consisted of 2016 total observations, including 324 patterns of submaximal effort. The rest were from grips with maximal effort. After preparation and feature selection, XGBoost machine learning was used for classification of the patterns. Findings After applying machine learning to the given data, we were able to predict submaximal grip effort based on the inherent pattern with a sensitivity of 94% and a specificity of 100%. Interpretation Using techniques from applied predictive modeling, submaximal effort in grip strength testing could be detected with high accuracy through load distribution analysis. Machine learning is a suitable method for recognizing altered grip patterns."}, {"title": "hdnom: Building Nomograms for Penalized Cox Models with High-Dimensional Survival Data", "url": "https://www.biorxiv.org/content/early/2016/08/23/065524", "tag": "Bioinformatics", "abstract": "Summary: We developed hdnom, an R package for survival modeling with high-dimensional data. The package is the first free and open-source software package that streamlines the workflow of penalized Cox model building, validation, calibration, comparison, and nomogram visualization, with nine types of penalized Cox regression methods fully supported. A web application and an online prediction tool maker are offered to enhance interac- tivity and flexibility in high-dimensional survival analysis. Availability: The hdnom R package is available from CRAN: https://cran.r-project.org/package=hdnom under GPL. The hdnom web application can be accessed at http://hdnom.io. The web application maker is available from http://hdnom.org/appmaker. The hdnom project website: http://hdnom.org."}, {"title": "ResistoMap - online visualization of human gut microbiota antibiotic resistome", "url": "https://www.biorxiv.org/content/early/2016/08/23/070714", "tag": "Bioinformatics", "abstract": "Summary: We created ResistoMap - a Web-based interactive visualization of the presence of genetic determinants conferring resistance to antibiotics, biocides and heavy metals in human gut microbiota. ResistoMap displays the data about more than 1600 published gut metagenomes of the world populations including both healthy subjects and patients. Multiparameter display filters allow visual assessment of the associations between the meta-data and proportions of resistome. The geographic map navigation layer allows to state hypotheses regarding the global trends of antibiotic resistance and correlate the gut resistome variations with the national clinical guidelines on antibiotics application. Availability and implementation: ResistoMap is publicly available at http://resistomap.datalaboratory.ru."}, {"title": "OrchID: a Generalized Framework for Taxonomic Classification of Images Using Evolved Artificial Neural Networks", "url": "https://www.biorxiv.org/content/early/2016/08/22/070904", "tag": "Bioinformatics", "abstract": "Taxonomic expertise for the identification of species is rare and costly. On-going advances in computer vision and machine learning have led to the development of numerous semi- and fully automated species identification systems. However, these systems are rarely agnostic to specific morphology, rarely can perform taxonomic \"approximation\" (by which we mean partial identification at least to higher taxonomic level if not to species), and frequently rely on costly scientific imaging technologies. We present a generic, hierarchical identification system for automated taxonomic approximation of organisms from images. We assessed the effectiveness of this system using photographs of slipper orchids (Cypripedioideae), for which we implemented image pre-processing, segmentation, and colour and shape feature extraction algorithms to obtain digital phenotypes for 116 species. The identification system trained on these digital phenotypes uses a nested hierarchy of artificial neural networks for pattern recognition and automated classification that mirrors the Linnean taxonomy, such that user-submitted photos can be assigned a genus, section, and species classification by traversing this hierarchy. Performance of the identification system varied depending on photo quality, number of species included for training, and desired taxonomic level for identification. High quality photos were scarce for some taxa and were under-represented in the training set, resulting in imbalanced network training. The image features used for training were sufficient to reliably identify photos to the correct genus but less so to the correct section and species. The outcomes of this project include a library of feature extraction algorithms called ImgPheno, a collection of scripts for neural network training called NBClassify, a library for evolutionary optimization of artificial neural network construction called AI::FANN::Evolving and a planned web application called OrchID for identification of user-submitted images. All project outcomes are open source and freely available."}, {"title": "ChimPipe: Accurate detection of fusion genes and transcription-induced chimeras from RNA-seq data", "url": "https://www.biorxiv.org/content/early/2016/08/22/070888", "tag": "Bioinformatics", "abstract": "Background: Chimeric transcripts are commonly defined as transcripts linking two or more different genes in the genome, and can be explained by various biological mechanisms such as genomic rearrangement, read-through or trans-splicing, but also by technical or biological artefacts. Several studies have shown their importance in cancer, cell pluripotency and motility. Many programs have recently been developed to identify chimeras from Illumina RNA-seq data (mostly fusion genes in cancer). However outputs of different programs on the same dataset can be widely inconsistent, and tend to include many false positives. Other issues relate to simulated datasets restricted to fusion genes, real datasets with limited numbers of validated cases, result inconsistencies between simulated and real datasets, and gene rather than junction level assessment. Results: Here we present ChimPipe, a modular and easy-to-use method to reliably identify chimeras from paired-end Illumina RNA-seq data. We have also produced realistic simulated datasets for three different read lengths, and enhanced two gold-standard cancer datasets by associating exact junction points to validated gene fusions. Benchmarking ChimPipe together with four other state-of-the-art tools on this data showed ChimPipe to be the top program at identifying exact junction coordinates for both kinds of datasets, and the one showing the best trade-off between sensitivity and precision. Applied to 106 ENCODE human RNA-seq datasets, ChimPipe identified 137 high confidence chimeras connecting the protein coding sequence of their parent genes. In subsequent experiments, three out of four predicted chimeras, two of which recurrently expressed in a large majority of the samples, could be validated. Cloning and sequencing of the three cases revealed several new chimeric transcript structures, 3 of which with the potential to encode a chimeric protein for which we hypothesized a new role. Conclusions: ChimPipe combines spanning and paired end RNA-seq reads to detect any kind of chimeras, including read-throughs, and shows an excellent trade-off between sensitivity and precision. The chimeras found by ChimPipe can be validated in-vitro with high accuracy."}, {"title": "Quantifying uncertainty of taxonomic placement in DNA barcoding and metabarcoding", "url": "https://www.biorxiv.org/content/early/2016/08/19/070573", "tag": "Bioinformatics", "abstract": "1. A crucial step in the use of DNA markers for biodiversity surveys is the assignment of Linnaean taxonomies (species, genus, etc.) to sequence reads. This allows the use of all the information known based on the taxonomic names. Taxonomic placement of DNA barcoding sequences is inherently probabilistic because DNA sequences contain errors, because there is natural variation among sequences within a species, and because reference databases are incomplete and can have false annotations. However, most existing bioinformatics methods for taxonomic placement either exclude uncertainty, or quantify it using metrics other than probability. 2. In this paper we evaluate the performance of a recently proposed probabilistic taxonomic placement method PROTAX by applying it to both annotated reference sequence data as well as unknown environmental data. Our four case studies include contrasting taxonomic groups (fungi, bacteria, mammals, and insects), variation in the length and quality of the barcoding sequences (from individually Sanger-sequenced sequences to short Illumina reads), variation in the structures and sizes of the taxonomies (from 800 to 130 000 species), and variation in the completeness of the reference databases (representing 15% to 100% of the species). 3. Our results demonstrate that PROTAX yields essentially unbiased assessment of probabilities of taxonomic placement, and thus that its quantification of species identification uncertainty is reliable. As expected, the accuracy of taxonomic placement increases with increasing coverage of taxonomic and reference sequence databases, and with increasing ratio of genetic variation among taxonomic levels over within taxonomic levels. 4. Our results show that reliable species-level identification from environmental samples is still challenging, and thus neglecting identification uncertainty can lead to spurious inference. A key aim for future research is the completion and pruning of taxonomic and reference sequence databases, and making these two types of data compatible."}, {"title": "chromstaR: Tracking combinatorial chromatin state dynamics in space and time", "url": "https://www.biorxiv.org/content/early/2016/08/18/038612", "tag": "Bioinformatics", "abstract": "Background: Post-translational modifications of histone residue tails are an important component of genome regulation. It is becoming increasingly clear that the combinatorial presence and absence of various modifications define discrete chromatin states which determine the functional properties of a locus. An emerging experimental goal is to track changes in chromatin state maps across different conditions, such as experimental treatments, cell-types or developmental time points. Results: Here we present chromstaR, an algorithm for the computational inference of combinatorial chromatin state dynamics across an arbitrary number of conditions. ChromstaR uses a multivariate Hidden Markov Model to determine the number of discrete combinatorial chromatin states using multiple ChIP-seq experiments as input and assigns every genomic region to a state based on the presence/absence of each modification in every condition. We demonstrate the advantages of chromstaR in the context of three common experimental data scenarios. First, we study how different histone modifications combine to form combinatorial chromatin states in a single tissue. Second, we infer genome-wide patterns of combinatorial state differences between two cell types or conditions. Finally, we study the dynamics of combinatorial chromatin states during tissue differentiation involving up to six differentiation points. Our findings reveal a striking sparcity in the combinatorial organization and temporal dynamics of chromatin state maps. Conclusions: chromstaR is a versatile computational tool that facilitates a deeper biological understanding of chromatin organization and dynamics. The algorithm is implemented as an R-package and freely available from http://bioconductor.org/packages/chromstaR/."}, {"title": "A novel quantile regression approach for eQTL discovery", "url": "https://www.biorxiv.org/content/early/2016/08/17/070052", "tag": "Bioinformatics", "abstract": "Over the past decade, there has been a remarkable improvement in our understanding of the role of genetic variation in complex human diseases, especially via genome-wide association studies. However, the underlying molecular mechanisms are still poorly characterized, impending the development of therapeutic interventions. Identifying genetic variants that influence the expression level of a gene, i.e. expression quantitative trait loci (eQTLs), can help us understand how genetic variants influence traits at the molecular level. While most eQTL studies focus on identifying mean effects on gene expression using linear regression, evidence suggests that genetic variation can impact the entire distribution of the expression level. Indeed, several studies have already investigated higher order associations with a special focus on detecting heteroskedasticity. In this paper, we develop a Quantile Rank-score Based Test (QRBT) to identify eQTLs that are associated with the conditional quantile functions of gene expression. We have applied the proposed QRBT to the Genotype-Tissue Expression project, an international tissue bank for studying the relationship between genetic variation and gene expression in human tissues, and found that the proposed QRBT complements the existing methods, and identifies new eQTLs with heterogeneous effects across different quantile levels. Notably, we show that the eQTLs identified by QRBT but missed by linear regression are more likely to be tissue specific, and also associated with greater enrichment in genome-wide significant SNPs from the GWAS catalog. An R package implementing QRBT is available on our website."}, {"title": "DEAR-O: Differential Expression Analysis based on RNA-seq data - Online", "url": "https://www.biorxiv.org/content/early/2016/08/16/069807", "tag": "Bioinformatics", "abstract": "Differential expression analysis using high-throughput RNA sequencing (RNA-seq) data is widely applied in transcriptomic studies and many software tools have been developed for this purpose. Active development of existing popular tools, together with emergence of new tools means that studies comparing the performance of differential expression analysis methods become rapidly out-of-date. In order to enable researchers to evaluate new and updated software in a timely manner, we developed DEAR-O, a user-friendly platform for performance evaluation of differential expression analysis based on RNA-seq data. The platform currently includes four of the most popular tools: DESeq, DESeq2, edgeR and Cuffdiff2. Based on the DEAR-O platform, researchers can evaluate the performance of different tools, or the same tool with different versions, with a customised number of biological replicates using already curated RNA-seq datasets. We also initiated an online forum for discussion of RNA-seq differential expression analysis. Through this forum, new useful tools and benchmarking datasets can be introduced. Our platform will be actively maintained to ensure new major versions of existing tools and new popular tools are included. DEAR-O will serve the community by providing timely evaluations of tools, versions and number of replicates for RNA-seq differential expression analysis."}, {"title": "BinSanity: Unsupervised Clustering of Environmental Microbial Assemblies Using Coverage and Affinity Propagation", "url": "https://www.biorxiv.org/content/early/2016/08/16/069567", "tag": "Bioinformatics", "abstract": "Metagenomics has become an integral part of defining microbial diversity in various environments. Many ecosystems have characteristically low biomass and few cultured representatives. Linking potential metabolisms to phylogeny in environmental microorganisms is important for interpreting microbial community functions and the impacts these communities have on geochemical cycles. However, with metagenomic studies there is the computational hurdle of binning contigs into phylogenetically related units or putative genomes. Binning methods have been implemented with varying approaches such as k-means clustering, Gaussian mixture models, hierarchical clustering, neural networks, and two-way clustering; however, many of these suffer from biases against low coverage/abundance organisms and closely related taxa/strains. We are introducing a new binning method, BinSanity, that utilizes the clustering algorithm affinity propagation (AP), to cluster assemblies using coverage alone, removing potential composition based biases in clustering contigs, but requires a minimum of two samples. To increase fidelity, a refinement script was developed that uses composition data (tetranucleotide frequency and %G+C content) to refine bins containing multiple source organisms. This separation of composition and coverage based signatures reduces clustering bias for closely related taxa. BinSanity was developed and tested on artificial metagenomes varying in size and complexity. Results indicate that this implementation of AP lead to a higher precision, recall, and Adjusted Rand Index over five commonly implemented methods. When tested on a previously published infant gut metagenome, BinSanity generated high completion and low redundancy bins corresponding with the published metagenome-assembled genomes."}, {"title": "PubData: search engine for bioinformatics databases worldwide", "url": "https://www.biorxiv.org/content/early/2016/08/16/069575", "tag": "Bioinformatics", "abstract": "We propose a search engine and file retrieval system for all bioinformatics databases worldwide. PubData searches biomedical data in a user-friendly fashion similar to how PubMed searches biomedical literature. PubData is built on novel network programming, natural language processing, and artificial intelligence algorithms that can patch into the file transfer protocol servers of any user-specified bioinformatics database, query its contents, retrieve files for download, and adapt to the user's search preferences. PubData is hosted as a user-friendly, cross-platform graphical user interface program developed using PyQt : http://www.pubdata.bio. The methods are implemented in Python, and are available as part of the PubData project at: https://github.com/Bohdan-Khomtchouk/PubData."}, {"title": "HySA: A Hybrid Structural variant Assembly approach using next generation and single-molecule sequencing technologies", "url": "https://www.biorxiv.org/content/early/2016/08/16/069815", "tag": "Bioinformatics", "abstract": "Achieving complete, accurate and cost-effective assembly of human genome is of great importance for realizing the promises of precision medicine. The abundance of repeats and genetic variations in human genome and the limitations of existing sequencing technologies call for the development of novel assembly methods that could leverage the complementary strengths of multiple technologies. We propose a Hybrid Structural variant Assembly (HySA) approach that integrates sequencing reads from next generation sequencing (NGS) and single-molecule sequencing (SMS) technologies to accurately assemble and detect structural variations (SV) in human genome. By identifying homologous SV-containing reads from different technologies through a bipartite-graph-based clustering algorithm, our approach turns a whole genome assembly problem into a set of independent SV assembly problems, each of which can be effectively solved to enhance assembly of structurally altered regions in human genome. In testing our approach using data generated from a haploid hydatidiform mole genome (CHM1) and a diploid human genome (NA12878), we found that our approach substantially improved the detection of many types of SVs, particularly novel large insertions, small INDELs (10-50bp) and short tandem repeat expansions and contractions over existing approaches with a low false discovery rate. Our work highlights the strengths and limitations of current approaches and provides an effective solution for extending the power of existing sequencing technologies for SV discovery."}, {"title": "scater: pre-processing, quality control, normalisation and visualisation of single-cell RNA-seq data in R", "url": "https://www.biorxiv.org/content/early/2016/08/15/069633", "tag": "Bioinformatics", "abstract": "Motivation: Single-cell RNA sequencing (scRNA-seq) is increasingly used to study gene expression at the level of individual cells. However, preparing raw sequence data for further analysis is not a straightforward process. Biases, artifacts, and other sources of unwanted variation are present in the data, requiring substantial time and effort to be spent on pre-processing, quality control (QC) and normalisation. Results: We have developed the R/Bioconductor package scater to facilitate rigorous pre-processing, quality control, normalisation and visualisation of scRNA-seq data. The package provides a convenient, flexible workflow to process raw sequencing reads into a high-quality expression dataset ready for downstream analysis. scater provides a rich suite of plotting tools for single-cell data and a flexible data structure that is compatible with existing tools and can be used as infrastructure for future software development. Availability: The open-source code, along with installation instructions, vignettes and case studies, is available through Bioconductor at http://bioconductor.org/packages/scater. Supplementary information: Supplementary material is available online at bioRxiv accompanying this manuscript, and all materials required to reproduce the results presented in this paper are available at dx.doi.org/10.5281/zenodo.60139."}, {"title": "Statistical Association Mapping of Population-Structured Genetic Data", "url": "https://www.biorxiv.org/content/early/2016/08/15/069658", "tag": "Bioinformatics", "abstract": "Association mapping of genetic diseases has attracted extensive research interest during the recent years. However, most of the methodologies introduced so far suffer from spurious inference of the disease-causing sites due to population inhomogeneities. In this paper, we introduce a statistical framework to compensate for this shortcoming by equipping the current methodologies with a state-of-the-art clustering algorithm being widely used in population genetics applications. The proposed framework jointly infers the disease causal factors and the hidden population structures. In this regard, a Markov Chain-Monte Carlo (MCMC) procedure has been employed to assess the posterior probability distribution of the model parameters. We have implemented our proposed framework on a software package whose performance is extensively evaluated on a number of synthetic datasets, and compared to some of the well-known existing methods such as STRUCTURE. It has been shown that in extreme scenarios, up to 10-15% of improvement in the inference accuracy is achieved with a moderate increase in computational complexity."}, {"title": "OncoSimulR: genetic simulation of cancer progression with arbitrary epistasis and mutator genes", "url": "https://www.biorxiv.org/content/early/2016/08/14/069500", "tag": "Bioinformatics", "abstract": "OncoSimulR implements forward-in-time genetic simulations of diallelic loci in asexual populations with special focus on cancer progression. Fitness can be defined as an arbitrary function of genetic interactions between multiple genes or modules of genes, including epistasis, restrictions in the order of accumulation of mutations, and order effects. Mutation rates can be made to differ between genes, and can be affected by (anti)mutator genes. Also available are sampling from single or multiple simulations, including single-cell sampling, plotting the parent-child relationships of the clones and generating and plotting random fitness landscapes. Availability and implementation: Implemented in R and C++, freely available from BioConductor for Linux, Mac, and Windows under the GNU GPL license. Version 2.3.12 or higher available from: http://www.bioconductor.org/packages/devel/bioc/html/OncoSimulR.html. GitHub repository at: https://github.com/rdiaz02/OncoSimul."}, {"title": "Peptide partitions and protein identification: a computational analysis", "url": "https://www.biorxiv.org/content/early/2016/08/14/069526", "tag": "Bioinformatics", "abstract": "Peptide sequences from a proteome can be partitioned into N mutually exclusive sets and used to identify their parent proteins in a sequence database. This is illustrated with the human proteome (http://www.uniprot.org; id UP000005640), which is partitioned into eight subsets KZ*R, KZ*D, KZ*E, KZ*, Z*R, Z*D, Z*E, and Z*, where Z \u03f5 {A, N, C, Q, G, H, I, L, M, F, P, S, T, W, Y, V} and Z* \u2261 0 or more occurrences of Z. If the full peptide sequence is known then over 98% of the proteins in the proteome can be identified from such sequences. The rate exceeds 78% if the positions of four internal residue types are known. When the standard set of 20 amino acids is replaced with an alphabet of size four based on residue volume the identification rate exceeds 96%. In an information-theoretic sense this last result suggests that protein sequences effectively carry nearly the same amount of information as the exon sequences in the genome that code for them using an alphabet of size four. An appendix discusses possible in vitro methods to create peptide partitions and potential ways to sequence partitioned peptides."}, {"title": "Higher classification sensitivity of short metagenomic reads with CLARK-S", "url": "https://www.biorxiv.org/content/early/2016/08/13/053462", "tag": "Bioinformatics", "abstract": "The growing number of metagenomic studies in medicine and environmental sciences is creating increasing demands on the computational infrastructure designed to analyze these very large datasets. Often, the construction of ultra-fast and precise taxonomic classifiers can compromise on their sensitivity (i.e., the number of reads correctly classified). Here we introduce CLARK-S, a new software tool that can classify short reads with high precision, high sensitivity and high speed at the same time."}, {"title": "phyC: Clustering cancer evolutionary trees", "url": "https://www.biorxiv.org/content/early/2016/08/12/069302", "tag": "Bioinformatics", "abstract": "Motivation: Multi-regional sequencing provides new opportunities to investigate genetic heterogeneity within or between common tumors from an evolutionary perspective. Several state-of-the-art methods have been proposed for reconstructing cancer sub-clonal evolutionary trees based on multi-regional sequencing data to develop models of cancer evolution. However, the methods developed thus far are not sufficient to characterize and interpret the diversity of cancer sub-clonal evolutionary trees. Results: We propose a clustering method (phyC) for cancer sub-clonal evolutionary trees, in which sub-groups of the trees are identified based on topology and edge length attributes. For interpretation, we also propose a method for evaluating the diversity of trees in the clusters, which provides insight into the acceleration of sub-clonal expansion. Simulation showed that the proposed method can detect true clusters with sufficient accuracy. Application of the method to actual multi-regional sequencing data of clear cell renal carcinoma and non-small cell lung cancer allowed for the detection of clusters related to cancer type or phenotype. Availability: phyC is implemented with R(>=3.2.2) and is available from https://github.com/ymatts/phyC"}, {"title": "Gene expression markers of Tumor Infiltrating Leukocytes", "url": "https://www.biorxiv.org/content/early/2016/08/11/068940", "tag": "Bioinformatics", "abstract": "Background: Assays of the abundance of immune cell populations in the tumor microenvironment promise to inform immune oncology research and the choice of immunotherapy for individual patients. We propose to measure the intratumoral abundance of various immune cells populations with gene expression. In contrast to IHC and flow cytometry, gene expression assays yield high information content from a clinically practical workflow. Previous studies of gene expression in purified immune cells have reported hundreds of genes showing enrichment in a single cell type, but the utility of these genes in tumor samples is unknown. We describe a novel statistical method for using co-expression patterns in large tumor gene expression datasets to validate previously reported candidate cell type marker genes, and we use this method to winnow previously published gene lists down to a subset of high confidence marker genes. Methods: We use co-expression patterns in 9986 samples from The Cancer Genome Atlas (TCGA) to validate previously reported cell type marker genes. We compare immune cell scores derived from these genes to measurements from flow cytometry and immunohistochemistry. We characterize the reproducibility of our cell scores in replicate runs of RNA extracted from FFPE tumor tissue. Results: We identify a list of 60 marker genes whose expression levels quantify 14 immune cell populations. Cell type scores calculated from these genes are concordant with flow cytometry and IHC readings, show high reproducibility in replicate RNA samples from FFPE tissue, and reveal an intricate picture of the immune infiltrate in TCGA. Most genes previously reported to be enriched in a single cell type have co-expression patterns inconsistent with cell type specificity. Conclusions: Due to their concise gene set, computational simplicity and utility in tumor samples, these cell type gene signatures may be useful in future discovery research and clinical trials to understand how tumors and therapeutic intervention shape the immune response."}, {"title": "Reproducible Computational Workflows with Continuous Analysis", "url": "https://www.biorxiv.org/content/early/2016/08/11/056473", "tag": "Bioinformatics", "abstract": "Reproducing experiments is vital to science. Being able to replicate, validate and extend previous work also speeds new research projects. Reproducing computational biology experiments, which are scripted, should be straightforward. But reproducing such work remains challenging and time consuming. In the ideal world we would be able to quickly and easily rewind to the precise computing environment where results were generated. We would then be able to reproduce the original analysis or perform new analyses. We introduce a process termed \"continuous analysis\" which provides inherent reproducibility to computational research at a minimal cost to the researcher. Continuous analysis combines Docker, a container service similar to virtual machines, with continuous integration, a popular software development technique, to automatically re-run computational analysis whenever relevant changes are made to the source code. This allows results to be reproduced quickly, accurately and without needing to contact the original authors. Continuous analysis also provides an audit trail for analyses that use data with sharing restrictions. This allows reviewers, editors, and readers to verify reproducibility without manually downloading and rerunning any code. Example configurations are available at our online repository (https://github.com/greenelab/continuous_analysis)."}, {"title": "tHapMix: simulating tumour samples through haplotype mixtures", "url": "https://www.biorxiv.org/content/early/2016/08/11/057414", "tag": "Bioinformatics", "abstract": "Motivation: Large-scale rearrangements and copy number changes combined with different modes of clonal evolution create extensive somatic genome diversity, making it difficult to develop versatile and scalable variant calling tools and create well-calibrated benchmarks. Results: We developed a new simulation framework tHapMix that enables the creation of tumour sam-ples with different ploidy, purity and polyclonality features. It easily scales to simulation of hundreds of somatic genomes, while re-use of real read data preserves noise and biases present in sequencing platforms. We further demonstrate tHapMix utility by creating a simulated set of 140 somatic genomes and showing how it can be used in training and testing of somatic copy number variant calling tools. Availability and implementation: tHapMix is distributed under an open source license and can be downloaded from https://github.com/Illumina/tHapMix ."}, {"title": "WEVOTE: Weighted Voting Taxonomic Identification Method of Microbial Sequences", "url": "https://www.biorxiv.org/content/early/2016/08/11/054205", "tag": "Bioinformatics", "abstract": "Metagenome shotgun sequencing presents opportunities to identify organisms that may prevent or promote disease. The analysis of sample diversity is achieved by taxonomic identification of metagenomic reads followed by generating an abundance profile. Numerous tools have been developed based on different design principles. Tools achieving high precision can lack sensitivity in some applications. Conversely, tools with high sensitivity can suffer from low precision and require long computation time. In this paper, we present WEVOTE (WEighted VOting Taxonomic idEntification), a method that classifies metagenome shotgun sequencing DNA reads based on an ensemble of existing methods using k-mer-based, marker-based, and naive-similarity based approaches. Our evaluation on fourteen benchmarking datasets shows that WEVOTE improves the classification precision by reducing false positive annotations while preserving a high level of sensitivity. WEVOTE is an efficient and automated tool that combines multiple individual taxonomic identification methods to produce more precise and sensitive microbial profiles. WEVOTE is developed primarily to identify reads generated by MetaGenome Shotgun sequencing. It is expandable and has the potential to incorporate additional tools to produce a more accurate taxonomic profile. WEVOTE was implemented using C++ and shell scripting and is available at www.bitbucket.org/ametwally/wevote"}, {"title": "AKT: Ancestry and Kinship Toolkit", "url": "https://www.biorxiv.org/content/early/2016/08/11/047829", "tag": "Bioinformatics", "abstract": "Ancestry and Kinship Toolkit(AKT) is a statistical genetics tool for analysing large cohorts of whole-genome sequenced samples. It can rapidly detect related samples, characterise sample ancestry, calculate correlation between variants, check Mendel consistency and perform data clustering. AKT brings together the functionality of many state-of-the-art methods, with a focus on speed and a unified interface. We believe it will be an invaluable tool for the curation of large WGS data-sets."}, {"title": "Bartender: an ultrafast and accurate clustering algorithm to count barcode and amplicon reads", "url": "https://www.biorxiv.org/content/early/2016/08/10/068916", "tag": "Bioinformatics", "abstract": "Barcode sequencing (bar-seq) is a high-throughput, and cost effective method to assay large numbers of lineages or genotypes in complex cell pools. Because of its advantages, applications for bar-seq are quickly growing -- from using neutral random barcodes to study the evolution of microbes or cancer, to using pseudo-barcodes, such as shRNAs, sgRNAs, or transposon insertion libraries, to simultaneously screen large numbers of cell perturbations. However, the computational pipelines for bar-seq have not been well developed. Available methods, which use prior information and/or simple brute-force comparisons, are slow and often result in over-clustering artifacts that group distinct barcodes together. Here, we developed Bartender: an ultrafast and accurate clustering algorithm to detect barcodes and their abundances from raw next-generation sequencing data. To improve speed and reduce unnecessary pairwise comparisons, Bartender employs a divide-and-conquer strategy that intelligently sorts barcode reads into distinct bins before performing comparisons. To improve accuracy and reduce over-clustering artifacts, Bartender employs a modified two-sample proportion test that uses information on both the cluster sequence distances and cluster sizes to make merging decisions. Additionally, Bartender includes a \"multiple time point\" mode, which matches barcode clusters between different clustering runs for seamless handling of time course data. For both simulated and real data, Bartender clusters millions of unique barcodes in a few minutes at high accuracy (>99.9%), and is ~100-fold faster than previous methods. Bartender is a set of simple-to-use command line tools that can be performed on a laptop."}, {"title": "Scale-Free Exponents of Resting State are Biomarkers of Neuro-Typical and Atypical Brain Activity", "url": "https://www.biorxiv.org/content/early/2016/08/10/068841", "tag": "Bioinformatics", "abstract": "Scale-free networks (SFN) arise from simple growth processes, which can encourage efficient, centralized and fault tolerant communication (1). Recently its been shown that stable network hub structure is governed by a phase transition at exponents (>2.0) causing a dramatic change in network structure including a loss of global connectivity, an increasing minimum dominating node set, and a shift towards increasing connectivity growth compared to node growth. Is this SFN shift identifiable in atypical brain activity? The Pareto Distribution (P(D)~D^-\u03b2) on the hub Degree (D) is a signature of scale-free networks. During resting-state, we assess Degree exponents across a large range of neurotypical and atypical subjects. We use graph complexity theory to provide a predictive theory of the brain network structure. Results. We show that neurotypical resting-state fMRI brain activity possess scale-free Pareto exponents (1.8 se .01) in a single individual scanned over 66 days as well as in 60 different individuals (1.8 se .02). We also show that 60 individuals with Autistic Spectrum Disorder, and 60 individuals with Schizophrenia have significantly higher (>2.0) scale-free exponents ( 2.4 se .03, 2.3 se .04), indicating more fractionated and less controllable dynamics in the brain networks revealed in resting state. Finally we show that the exponent values vary with phenotypic measures of atypical disease severity indicating that the global topology of the network itself can provide specific diagnostic biomarkers for atypical brain activity."}, {"title": "Evaluating the genetic diagnostic power of exome sequencing: Identifying missing data.", "url": "https://www.biorxiv.org/content/early/2016/08/10/068825", "tag": "Bioinformatics", "abstract": "A hurdle of exome sequencing is its limited capacity to represent the entire exome. To ascertain the diagnostic power of this approach we determined the extent of coverage per individual sample. Using alignment data (BAM files) from 15 exome samples, sequences of any length that were below a determined sequencing depth coverage (DP) were detected and annotated with the Ensembl exon database using MIST, a novel software tool. Samples sequenced at 50X mean coverage had, on average, up to 50% of the Ensembl annotated exons with at least one nucleotide (L=1) with a DP<20, improving to 35% at 100X mean coverage. In addition, almost 15% of annotated exons were never sequenced (L=50, DP<1) at 50x mean coverage, reaching down to 5% at 100x. The diagnostic utility of this approach was tested for hypertrophic cardiomyopathy, a genetically heterogeneous disease, where exome sequencing covered as much as 80% of all candidate genes exons at DP\u226520. This report stresses the value of identifying, precisely, which sequences are below a specific depth in an individual exome and provides a useful tool to assess the potential and pitfalls of exome sequencing in a diagnostic or gene discovery setting."}, {"title": "A complete tool set for molecular QTL discovery and analysis", "url": "https://www.biorxiv.org/content/early/2016/08/10/068635", "tag": "Bioinformatics", "abstract": "Population scale studies combining genetic information with molecular phenotypes (e.g. gene expression) become a standard to dissect the effects of genetic variants onto organismal phenotypes. This kind of datasets requires powerful, fast and versatile methods able to discover molecular Quantitative Trait Loci (molQTL). Here we propose such a solution, QTLtools, a modular framework that contains multiple methods to prepare the data, to discover proximal and distal molQTLs and to finally integrate them with GWAS variants and functional annotations of the genome. We demonstrate its utility by performing a complete expression QTL study in a few and easy-to-perform steps. QTLtools is open source and available at https://qtltools.github.io/qtltools/."}, {"title": "ABySS 2.0: Resource-Efficient Assembly of Large Genomes using a Bloom Filter", "url": "https://www.biorxiv.org/content/early/2016/08/07/068338", "tag": "Bioinformatics", "abstract": "The assembly of DNA sequences de novo is fundamental to genomics research. It is the first of many steps towards elucidating and characterizing whole genomes. Downstream applications, including analysis of genomic variation between species, between or within individuals critically depends on robustly assembled sequences. In the span of a single decade, the sequence throughput of leading DNA sequencing instruments has increased drastically, and coupled with established and planned large-scale, personalized medicine initiatives to sequence genomes in the thousands and even millions, the development of efficient, scalable and accurate bioinformatics tools for producing high-quality reference draft genomes is timely. With ABySS 1.0, we originally showed that assembling the human genome using short 50 bp sequencing reads was possible by aggregating the half terabyte of compute memory needed over several computers using a standardized message-passing system (MPI). We present here its re-design, which departs from MPI and instead implements algorithms that employ a Bloom filter, a probabilistic data structure, to represent a de Bruijn graph and reduce memory requirements. We present assembly benchmarks of human Genome in a Bottle 250 bp Illumina paired-end and 6 kbp mate-pair libraries from a single individual, yielding a NG50 (NGA50) scaffold contiguity of 3.5 (3.0) Mbp using less than 35 GB of RAM, a modest memory requirement by today\u2019s standard that is often available on a single computer. We also investigate the use of BioNano Genomics and 10x Genomics\u2019 Chromium data to further improve the scaffold contiguity of this assembly to 42 (15) Mbp."}, {"title": "in silico Whole Genome Sequencer & Analyzer (iWGS): a computational pipeline to guide the design and analysis of de novo genome sequencing studies", "url": "https://www.biorxiv.org/content/early/2016/08/07/028134", "tag": "Bioinformatics", "abstract": "The availability of genomes across the tree of life is highly biased toward vertebrates, pathogens, human disease models, and organisms with relatively small and simple genomes. Recent progress in genomics has enabled the de novo decoding of the genome of virtually any organism, greatly expanding its potential for understanding the biology and evolution of the full spectrum of biodiversity. The increasing diversity of sequencing technologies, assays, and de novo assembly algorithms have augmented the complexity of de novo genome sequencing projects in non-model organisms. To reduce the costs and challenges in de novo genome sequencing projects and streamline their experimental design and analysis, we developed iWGS (in silico Whole Genome Sequencer and Analyzer), an automated pipeline for guiding the choice of appropriate sequencing strategy and assembly protocols. iWGS seamlessly integrates the four key steps of a de novo genome sequencing project: data generation (through simulation), data quality control, de novo assembly, and assembly evaluation and validation. The last three steps can also be applied to the analysis of real data. iWGS is designed to enable the user to have great flexibility in testing the range of experimental designs available for genome sequencing projects, and supports all major sequencing technologies and popular assembly tools. Three case studies illustrate how iWGS can guide the design of de novo genome sequencing projects and evaluate the performance of a wide variety of user-specified sequencing strategies and assembly protocols on genomes of differing architectures. iWGS, along with a detailed documentation, is freely available at https://github.com/zhouxiaofan1983/iWGS."}, {"title": "A qualitative model of the rod photoreceptor in natural context", "url": "https://www.biorxiv.org/content/early/2016/08/05/050823", "tag": "Bioinformatics", "abstract": "Given the intricacies of the retinal neural circuit, which bears a striking resemblance to that of the brain, it is proposed that retinal function goes beyond mere spatiotemporal prefiltering. We hypothesise that aspects related to motion detection and discrimination, anticipation and adaptation to environmental and contextual conditions, which have traditionally been ascribed to the brain, may be supported by neurons in the retina. Such early computations may be dependent on compensative and adaptive mechanisms that stem from qualities intrinsic to the retinal neural circuit and its interaction with the environment (neural transduction time, connectivity patterns, regularities in the input signal, temporal dynamics and light variations). With a view to investigating the contribution of the photoreceptor population to the processing performed by the retina in natural scotopic conditions, we present a continuous model of the rod photoreceptor. Our model permits the reproduction and exploration of a set of qualitative features displayed in vitro, such as excitation-dependent activation level and time-to-membrane current integration. We captured qualitative aspects of key features selected for their presumed importance in early visual function. Further, we subjected our model to extensive parameter sensitivity analyses, aiming to provide a visual representation of their contribution to the observed qualitative behavior."}, {"title": "Structural and Functional Analyses of PolyProline-II helices in Globular Proteins", "url": "https://www.biorxiv.org/content/early/2016/08/05/068098", "tag": "Bioinformatics", "abstract": "PolyProline-II (PPII) helices are defined as a continuous stretch of a protein chain in which the constituent residues have the backbone torsion angle (\u03c6,\u03c8) values of (-75\u00b0, 145\u00b0) and take up extended left handed conformation, lacking any intra-helical hydrogen bonds. They are found to occur very frequently in protein structures with their number exceeding that of \u03c0-helices, though it is considerably less than that of \u03b1-helices and \u03b2-strands. A relatively new procedure, ASSP, for the identification of regular secondary structures using C\u03b1 trace identifies 3597 PPII helices in 3582 protein chains, solved at resolution \u2264 2.5\u00c5. Taking advantage of this significantly expanded database of PPII-helices, we have analyzed the functional and structural roles of PPII helices as well as determined the amino acid propensity within and around them. Though Pro residues are highly preferred, it is not a mandatory condition for the formation of PPII-helices, since ~40% PPII-helices were found to contain no Proline residues. Aromatic amino acids are avoided within this helix, while Gly, Asn and Asp residues are preferred in the proximal flanking regions. These helices range from 3 to 13 residues in length with the average twist and rise being -121.2\u00b0\u00b19.2\u00b0 and 3.0\u00c5\u00b10.1\u00c5 respectively. A majority (~72%) of PPII-helices were found to occur in conjunction with \u03b1-helices and \u03b2-strands, and serve as linkers as well. The analysis of various intra-helical non-bonded interactions revealed frequent presence of C-H...O H-bonds. PPII-helices participate in maintaining the three-dimensional structure of proteins and are important constituents of binding motifs involved in various biological functions."}, {"title": "Fast and accurate de novo genome assembly from long uncorrected reads", "url": "https://www.biorxiv.org/content/early/2016/08/05/068122", "tag": "Bioinformatics", "abstract": "The assembly of long reads from Pacific Biosciences and Oxford Nanopore Technologies typically requires resource intensive error correction and consensus generation steps to obtain high quality assemblies. We show that the error correction step can be omitted and high quality consensus sequences can be generated efficiently with a SIMD accelerated, partial order alignment based stand-alone consensus module called Racon. Based on tests with PacBio and Oxford Nanopore datasets we show that Racon coupled with Miniasm enables consensus genomes with similar or better quality than state-of-the-art methods while being an order of magnitude faster."}, {"title": "Conservation of co-evolving protein interfaces bridges prokaryote-eukaryote homologies in the twilight zone", "url": "https://www.biorxiv.org/content/early/2016/08/05/067587", "tag": "Bioinformatics", "abstract": "Protein-protein interactions are fundamental for the proper functioning of the cell. As a result, protein interaction surfaces are subject to strong evolutionary constraints. Recent developments have shown that residue co-evolution provides accurate predictions of heterodimeric protein interfaces from sequence information. So far these approaches have been limited to the analysis of families of prokaryotic complexes for which large multiple sequence alignments of homologous sequences can be compiled. We explore the hypothesis that co-evolution points to structurally conserved contacts at protein-protein interfaces, which can be reliably projected to homologous complexes with distantly related sequences. We introduce a novel domain-centred protocol to study the interplay between residue co-evolution and structural conservation of protein-protein interfaces. We show that sequence-based co-evolutionary analysis systematically identifies residue contacts at prokaryotic interfaces that are structurally conserved at the interface of their eukaryotic counterparts. In turn, this allows the prediction of conserved contacts at eukaryotic protein-protein interfaces with high confidence using solely mutational patterns extracted from prokaryotic genomes. Even in the context of high divergence in sequence (the twilight zone), where standard homology modelling of protein complexes is unreliable, our approach provides sequence-based accurate information about specific details of protein interactions at the residues level. Selected examples of the application of prokaryotic co-evolutionary analysis to the prediction of eukaryotic interfaces further illustrates the potential of this novel approach."}, {"title": "Interactive online brain shape visualization", "url": "https://www.biorxiv.org/content/early/2016/08/05/067678", "tag": "Bioinformatics", "abstract": "The Mindboggle project was initiated to improve the labeling as well as morphometry of brain imaging data, and to promote open science by making all data, software, and documentation freely and openly available. An interface for interactive visualization is essential for assessing issues in brain image processing and analysis, including surface reconstruction, labeling, and morphometry. We completed an initial version of our browser-based interactive visualization tool; a left hemisphere of a human brain is available at http://roygbiv.mindboggle.info."}, {"title": "A Scalable Data Access Layer to Manage Structured Heterogeneous Biomedical Data", "url": "https://www.biorxiv.org/content/early/2016/08/04/067371", "tag": "Bioinformatics", "abstract": "This work presents a scalable data access layer, called PyEHR, intended for building data management systems for secondary use of structured heterogeneous biomedical and clinical data. PyEHR adopts openEHR formalisms to guarantee the decoupling of data descriptions from implementation details and exploits structures indexing to speed up searches. The persistence is guarantee by a driver layer with a common driver interface. Presently, are implemented the interfaces with two NoSQL DBMS: MongoDB and Elasticsearch. The scalability of PyEHR has been evaluated experimentally through two types of tests, namely constant load and constant number of records, with queries of increasing complexity on a two synthetic datasets of ten millions records each, containing very complex openEHR archetype structures, distributed on up to ten working nodes."}, {"title": "BayesFM: a software program to fine-map multiple causative variants in GWAS identified risk loci.", "url": "https://www.biorxiv.org/content/early/2016/08/04/067801", "tag": "Bioinformatics", "abstract": "We herein describe a new method to fine-map GWAS-identified risk loci based on the Bayesian Least Absolute Shrinkage Selection Operator (LASSO) combined with a Monte Carlo Markov Chain (MCMC) approach, and corresponding software package (BayesFM). We characterize the performances of BayesFM using simulated data, showing that it outperforms standard forward selection both in terms of sensitivity and specificity. We apply the method to the NOD2 locus, a well-established risk locus for Crohn's disease, in which we identify 13 putative independent signals."}, {"title": "DIABLO - an integrative, multi-omics, multivariate method for multi-group classification", "url": "https://www.biorxiv.org/content/early/2016/08/03/067611", "tag": "Bioinformatics", "abstract": "Rapid advances in technology have led to a wealth of large-scale molecular omics datasets. Integrating such data offers an unprecedented opportunity to assess molecular interactions at multiple functional levels and provide a more comprehensive understanding of the biological pathways involved in different diseases subgroups. However, multiple omics data integration is a challenging task due to the heterogeneity in the different platforms used. There is a need to address the complex and correlated nature of different data-types, in order to identify a robust and reliable multi-omics signature that can predict a phenotype of interest. We introduce a novel multivariate dimension reduction method for multiple omics integration, classification and identification of a multi-omics molecular signature. DIABLO - Data Integration Analysis for Biomarker discovery using a Latent component method for Omics studies, models the correlation structure between omics datasets, resulting in an improved ability to associate biomarkers across multiple functional levels to phenotypes of interest. We demonstrate the capabilities of DIABLO using simulated data and studies of breast cancer and asthma, integrating up to four types of omics datasets to identify relevant biomarkers, while still retaining competitive classification and predictive performance compared to existing methods. Our statistical integrative framework can benefit a diverse range of research areas with varying types of study designs, as well as enabling module-based analyses. Importantly, graphical outputs of our method assist in the interpretation of such complex analyses and provide significant biological insights."}, {"title": "Monitoring the circadian clock in human blood using personalized machine learning", "url": "https://www.biorxiv.org/content/early/2016/08/03/066126", "tag": "Bioinformatics", "abstract": "The circadian clock and the rhythms it produces are crucial for human health, but frequently perturbed by the modern environment. At the same time, circadian rhythms may influence the efficacy and toxicity of therapeutics and the metabolic response to food intake. Measuring the body's response to treatments for circadian dysfunction, as well as optimizing the daily timing of treatments for other health conditions, requires a simple and accurate method for monitoring the circadian clock. Here we used a recently developed method called ZeitZeiger to predict circadian time (CT, time of day according to the circadian clock) from genome-wide gene expression in human blood. In cross-validation on 498 samples from 60 individuals across three publicly available datasets, ZeitZeiger predicted CT in single samples with a median absolute error of 2.1 h. The predictor trained on all 498 samples used 15 genes, only two of which are part of the core circadian clock. We then extended ZeitZeiger to make predictions for groups of samples, and developed a general framework to personalize predictions using samples from only the respective individual. Each of these strategies improved prediction of CT by ~20%. Our results are an important step towards precision circadian medicine."}, {"title": "A novel independence test for somatic alterations in cancer shows that biology drives mutual exclusivity but chance explains co-occurrence", "url": "https://www.biorxiv.org/content/early/2016/08/03/052803", "tag": "Bioinformatics", "abstract": "Just like recurrent somatic alterations characterize cancer genes, mutually exclusive or co-occurring alterations across genes suggest functional interactions. Identifying such patterns in large cancer studies thus helps the discovery of unknown interactions. Many studies use Fisher's exact test or simple permutation procedures for this purpose. These tests assume identical gene alteration probabilities across tumors, which is not true for cancer. We show that violating this assumption yields many spurious co-occurrences and misses many mutual exclusivities. We present DISCOVER, a novel statistical test that addresses the limitations of existing tests. In a comparison with six published mutual exclusivity tests, DISCOVER is more sensitive while controlling its false positive rate. A pan-cancer analysis using DISCOVER finds no evidence for widespread co-occurrence. Most co-occurrences previously detected do not exceed expectation by chance. In contrast, many mutual exclusivities are identified. These cover well known genes involved in the cell cycle and growth factor signaling. Interestingly, also lesser known regulators of the cell cycle and Hedgehog signaling are identified. Availability: R and Python implementations of DISCOVER, as well as Jupyter notebooks for reproducing all results and figures from this paper can be found at http://ccb.nki.nl/software/discover ."}, {"title": "A reference dataset of 5.4 million phased human variants validated by genetic inheritance from sequencing a three-generation 17-member pedigree", "url": "https://www.biorxiv.org/content/early/2016/08/02/055541", "tag": "Bioinformatics", "abstract": "Improvement of variant calling in next-generation sequence data requires a comprehensive, genome-wide catalogue of high-confidence variants called in a set of genomes for use as a benchmark. We generated deep, whole-genome sequence data of seventeen individuals in a three-generation pedigree and called variants in each genome using a range of currently available algorithms. We used haplotype transmission information to create a phased \"platinum\" variant catalogue of 4.7 million single nucleotide variants (SNVs) plus 0.7 million small (1-50bp) insertions and deletions (indels) that are consistent with the pattern of inheritance in the parents and eleven children of this pedigree. Platinum genotypes are highly concordant with the current catalogue of the National Institute of Standards and Technology for both SNVs (>99.99%) and indels (99.92%), and add a validated truth catalogue that has 26% more SNVs and 45% more indels. Analysis of 334,652 SNVs that were consistent between informatics pipelines yet inconsistent with haplotype transmission (\"non-platinum\") revealed that the majority of these variants are de novo and cell-line mutations or reside within previously unidentified duplications and deletions. The reference materials from this study are a resource for objective assessment of the accuracy of variant calls throughout genomes."}, {"title": "High-Quality Assembly of an Individual of Yoruban Descent", "url": "https://www.biorxiv.org/content/early/2016/08/02/067447", "tag": "Bioinformatics", "abstract": "De novo assembly of human genomes is now a tractable effort due in part to advances in sequencing and mapping technologies. We use PacBio single-molecule, real-time (SMRT) sequencing and BioNano genomic maps to construct the first de novo assembly of NA19240, a Yoruban individual from Africa. This chromosome-scaffolded assembly of 3.08 Gb with a contig N50 of 7.25 Mb and a scaffold N50 of 78.6 Mb represents one of the most contiguous high-quality human genomes. We utilize a BAC library derived from NA19240 DNA and novel haplotype-resolving sequencing technologies and algorithms to characterize regions of complex genomic architecture that are normally lost due to compression to a linear haploid assembly. Our results demonstrate that multiple technologies are still necessary for complete genomic representation, particularly in regions of highly identical segmental duplications. Additionally, we show that diploid assembly has utility in improving the quality of de novo human genome assemblies."}, {"title": "Spherical: an iterative workflow for assembling metagenomic datasets", "url": "https://www.biorxiv.org/content/early/2016/08/02/067256", "tag": "Bioinformatics", "abstract": "The consensus emerging from microbiome studies is that they are far more complex than previously thought, requiring deep sequencing. As deep sequenced datasets provide greater coverage than previous datasets, recovering a higher proportion of reads to the assembly is still a challenge. To tackle this issue, we set of to identify if multiple iterations of assembly would allow for otherwise lost contigs to be formed and studied and if so, how successful is such an avenue at improving the current methodology. A simulated metagenomic dataset was initially used to identify if multiple iterations of assembly produce useable contigs or mis-assembled artefacts were produced. Once we had confirmed that the secondary iterations were producing both accurate contigs without a reduction in contig quality we applied this methodology in the form of Spherical to 3 metagenomic studies. The additional contigs produced by Spherical increased the number of reads aligning to an identified gene by 11-109% compared to the initial iterations assembly. As the size of the dataset increased, as did the amount of data multiple iterations were able to add."}, {"title": "One tagger, many uses - Illustrating the power of ontologies in dictionary-based named entity recognition", "url": "https://www.biorxiv.org/content/early/2016/08/02/067132", "tag": "Bioinformatics", "abstract": "Automatic annotation of text is an important complement to manual annotation, because the latter is highly labour intensive. We have developed a fast dictionary-based named entity recognition (NER) system and addressed a wide variety of biomedical problems by applied it to text from many different sources. We have used this tagger both in real-time tools to support curation efforts and in pipelines for populating databases through bulk processing of entire Medline, the open-access subset of PubMed Central, NIH grant abstracts, FDA drug labels, electronic health records, and the Encyclopedia of Life. Despite the simplicity of the approach, it typically achieves 80-90% precision and 70-80% recall. Many of the underlying dictionaries were built from open biomedical ontologies, which further facilitate integration of the text-mining results with evidence from other sources."}, {"title": "Improved long read correction for de novo assembly using an FM-index", "url": "https://www.biorxiv.org/content/early/2016/08/02/067272", "tag": "Bioinformatics", "abstract": "Long read sequencing is changing the landscape of genomic research, especially de novo assembly. Despite the high error rate inherent to long read technologies, increased read lengths dramatically improve the continuity and accuracy of genome assemblies. However, the cost and throughput of these technologies limits their application to complex genomes. One solution is to decrease the cost and time to assemble novel genomes by leveraging \u201chybrid\u201d assemblies that use long reads for scaffolding and short reads for accuracy. To this end, we describe a novel application of a multi-string Burrows-Wheeler transform with auxiliary FM-index to correct errors in long read sequences using a set of complementary short reads. We show that our method efficiently produces significantly higher quality corrected sequence than existing hybrid error-correction methods. We demonstrate the effectiveness of our method compared to state-of-the-art hybrid and long-read only de novo assembly methods."}, {"title": "Advances in the recovery of haplotypes from the metagenome", "url": "https://www.biorxiv.org/content/early/2016/08/02/067215", "tag": "Bioinformatics", "abstract": "High-throughput DNA sequencing has enabled us to look beyond consensus reference sequences to the variation observed in sequences within organisms; their haplotypes. Recovery, or assembly of haplotypes has proved computationally difficult and there exist many probabilistic heuristics that attempt to recover the original haplotypes for a single organism of known ploidy. However, existing approaches make simplifications or assumptions that are easily violated when investigating sequence variation within a metagenome. We propose the \"metahaplome\" as the set of haplotypes for any particular genomic region of interest within a metagenomic data set and present Hansel and Gretel, a data structure and algorithm that together provide a proof of concept framework for the recovery of true haplotypes from a metagenomic data set. The algorithm performs incremental haplotype recovery, using smoothed Naive Bayes - a simple, efficient and effective method. Hansel and Gretel pose several advantages over existing solutions: the framework is capable of recovering haplotypes from metagenomes, does not require a priori knowledge about the input data, makes no assumptions regarding the distribution of alleles at variant sites, is robust to error, and uses all available evidence from aligned reads, without altering or discarding observed variation. We evaluate our approach using synthetic metahaplomes constructed from sets of real genes and show that up to 99% of SNPs on a haplotype can be correctly recovered from short reads that originate from a metagenomic data set."}, {"title": "PathScore: a web tool for identifying altered pathways in cancer data", "url": "https://www.biorxiv.org/content/early/2016/08/01/067090", "tag": "Bioinformatics", "abstract": "PathScore quantifies the level of enrichment of somatic mutations within curated pathways, applying a novel approach that identifies pathways enriched across patients. The application provides several user-friendly, interactive graphic interfaces for data exploration, including tools for comparing pathway effect sizes, significance, gene-set overlap and enrichment differences between projects. Availability and Implementation: Web application available at http://pathscore.publichealth.yale.edu. Site implemented in Python and MySQL, with all major browsers supported. Source code available at http://github.com/sggaffney/pathscore with a GPLv3 license. Supplementary Information: Additional documentation can be found at http://pathscore.publichealth.yale.edu/faq."}, {"title": "GutCyc: a Multi-Study Collection of Human Gut Microbiome Metabolic Models", "url": "https://www.biorxiv.org/content/early/2016/07/31/055574", "tag": "Bioinformatics", "abstract": "Advances in high-throughput sequencing are reshaping how we perceive microbial communities inhabiting the human body, with implications for therapeutic interventions. Several large-scale datasets derived from hundreds of human microbiome samples sourced from multiple studies are now publicly available. However, idiosyncratic data processing methods between studies introduce systematic differences that confound comparative analyses. To overcome these challenges, we developed GutCyc, a compendium of environmental pathway genome databases constructed from 418 assembled human microbiome datasets using MetaPathways, enabling reproducible functional metagenomic annotation. We also generated metabolic network reconstructions for each metagenome using the Pathway Tools software, empowering researchers and clinicians interested in visualizing and interpreting metabolic pathways encoded by the human gut microbiome. For the first time, GutCyc provides consistent annotations and metabolic pathway predictions, making possible comparative community analyses between health and disease states in inflammatory bowel disease, Crohn's disease, and type 2 diabetes. GutCyc data products are searchable online, or may be downloaded and explored locally using MetaPathways and Pathway Tools."}, {"title": "Deep Learning-based Pipeline to Recognize Alzheimer\u2032s Disease using fMRI Data", "url": "https://www.biorxiv.org/content/early/2016/07/31/066910", "tag": "Bioinformatics", "abstract": "Over the past decade, machine learning techniques and in particular predictive modeling and pattern recognition in biomedical sciences, from drug delivery systems to medical imaging, have become one of the most important methods of assisting researchers in gaining a deeper understanding of issues in their entirety and solving complex medical problems. Deep learning is a powerful machine learning algorithm in classification that extracts low- to high-level features. In this paper, we employ a convolutional neural network to distinguish an Alzheimer\u2032s brain from a normal, healthy brain. The importance of classifying this type of medical data lies in its potential to develop a predictive model or system in order to recognize the symptoms of Alzheimer\u2032s disease when compared with normal subjects and to estimate the stages of the disease. Classification of clinical data for medical conditions such as Alzheimer\u2032s disease has always been challenging, and the most problematic aspect has always been selecting the strongest discriminative features. Using the Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified functional MRI data of Alzheimer\u2032s subjects from normal controls, where the accuracy of testing data reached 96.85%. This experiment suggests that the shift and scale invariant features extracted by CNN followed by deep learning classification represents the most powerful method of distinguishing clinical data from healthy data in fMRI. This approach also allows for expansion of the methodology to predict more complicated systems."}, {"title": "Characteristic arrangement of nucleosomes is predictive of chromatin interactions at kilobase resolution", "url": "https://www.biorxiv.org/content/early/2016/07/31/060327", "tag": "Bioinformatics", "abstract": "High-throughput chromosome conformation capture technologies, such as Hi-C, have made it possible to survey 3D genome structure. However, the ability to obtain 3D profiles at kilobase resolution at low cost remains a major challenge. Therefore, we herein report a computational method to precisely identify chromatin interaction sites at kilobase resolution from MNase-seq data, termed chromatin interaction site detector (CISD), and a CISD-based chromatin loop predictor (CISD_loop) that predicts chromatin-chromatin interaction (CCI) from low-resolution Hi-C data. The methods are built on a hypothesis that CCIs result in a characteristic nucleosome arrangement pattern flanking the interaction sites. Accordingly, we show that the predictions of CISD and CISD_loop overlap closely with chromatin interaction analysis by paired-end tag sequencing (ChIA-PET) anchors and loops, respectively. Moreover, the methods trained in one cell type can be applied to other cell types with high accuracy. The validity of the methods was further supported by chromosome conformation capture (3C) experiments at 5kb resolution. Finally, we demonstrate that only modest amounts of MNase-seq and Hi-C data are sufficient to achieve ultrahigh resolution CCI map. The predictive power of CISD/CISD_loop supports the hypothesis that CCIs induce local nucleosome rearrangement and that the pattern may serve as probes for 3D dynamics of the genome. Thus, our method will facilitate precise and systematic investigations of the interactions between distal regulatory elements on a larger scale than hitherto have been possible."}, {"title": "MethFlowVM: a virtual machine for the integral analysis of bisulfite sequencing data", "url": "https://www.biorxiv.org/content/early/2016/07/31/066795", "tag": "Bioinformatics", "abstract": "The analysis of whole genome DNA methylation patterns is an important first step towards the understanding on how DNA methylation is involved in the regulation of gene expression and genome stability. Previously, we published MethylExtract, a program for DNA methylation profiling and genotyping from the same sample. Over the last years we developed it further into a methylation analysis pipeline that allows to take full advantage of novel genome assembly models. The result is a new pipeline termed MethFlow which permits both, profiling of methylation levels and differential methylation analysis. Frequently DNA methylation research is carried out in the biomedical field, where privacy issues play an important role. Therefore we implemented the pipeline into a virtual machine termed MethFlowVM which shares with a web-server its user-friendliness however, the decisive advantage is that the sequencing data does not leave the user desktop or server and therefore no privacy issues do exist. The virtual machine is available at: http://bioinfo2.ugr.es:8080/MethFlow/"}, {"title": "Perception Enhancement using Visual Attributes in Sequence Motif Visualization", "url": "https://www.biorxiv.org/content/early/2016/07/31/066928", "tag": "Bioinformatics", "abstract": "Sequence logo is a well-accepted scientific method to visualize the conservation characteristics of biological sequence motifs. Previous studies found that using sequence logo graphical representation for scientific evidence reports or arguments could seriously cause biases and misinterpretation by users. This study investigates on the visual attributes performance of a sequence logo in helping users to perceive and interpret the information based on preattentive theories and Gestalt principles of perception. A survey was conducted to gather users' opinion after being presented with several alternative design details to perform selected tasks on motif analysis. Analysis of results showed that there are improvements needed on the use of colour, negative space, size, and arrangement of the nucleotides, richness of information and interactivity aspect in a sequence logo visualization. These improvements can alleviate biases and misinterpretation of the results in sequence logo visualization."}, {"title": "SnoVault and encodeD: A novel object-based storage system and applications to ENCODE metadata", "url": "https://www.biorxiv.org/content/early/2016/07/29/044578", "tag": "Bioinformatics", "abstract": "The Encyclopedia of DNA elements (ENCODE) project is an ongoing collaborative effort[1-6] to create a comprehensive catalog of functional elements initiated shortly after the completion of the Human Genome Project[7][1]. The current database exceeds 6500 experiments across more than 450 cell lines and tissues using a wide array of experimental techniques to study the chromatin structure, regulatory and transcriptional landscape of the H. sapiens and M. musculus genomes. All ENCODE experimental data, metadata, and associated computational analyses are submitted to the ENCODE Data Coordination Center (DCC) for validation, tracking, storage, unified processing, and distribution to community resources and the scientific community. As the volume of data increases, the identification and organization of experimental details becomes increasingly intricate and demands careful curation. The ENCODE DCC[8-10] has created a general purpose software system, known as SnoVault, that supports metadata and file submission, a database used for metadata storage, web pages for displaying the metadata and a robust API for querying the metadata. The software is fully open-source, code and installation instructions can be found at: http://github.com/ENCODE-DCC/snovault/ (for the generic database) and http://github.com/ENCODE-DCC/encoded/ to store genomic data in the manner of ENCODE. The core database engine, SnoVault (which is completely independent of ENCODE, genomic data, or bioinformatic data) has been released as a separate Python package."}, {"title": "de novo assembly and population genomic survey of natural yeast isolates with the Oxford Nanopore MinION sequencer", "url": "https://www.biorxiv.org/content/early/2016/07/28/066613", "tag": "Bioinformatics", "abstract": "Oxford Nanopore Technologies Ltd (Oxford, UK) have recently commercialized MinION, a small and low-cost single-molecule nanopore sequencer, that offers the possibility of sequencing long DNA fragments. The Oxford Nanopore technology is truly disruptive and can sequence small genomes in a matter of seconds. It has the potential to revolutionize genomic applications due to its portability, low-cost, and ease of use compared with existing long reads sequencing technologies. The MinION sequencer enables the rapid sequencing of small eukaryotic genomes, such as the yeast genome. Combined with existing assembler algorithms, near complete genome assemblies can be generated and comprehensive population genomic analyses can be performed. Here, we resequenced the genome of the Saccharomyces cerevisiae S288C strain to evaluate the performance of nanopore-only assemblers. Then we de novo sequenced and assembled the genomes of 21 isolates representative of the S. cerevisiae genetic diversity using the MinION platform. The contiguity of our assemblies was 14 times higher than the Illumina-only assemblies and we obtained one or two long contigs for 65% of the chromosomes. This high continuity allowed us to accurately detect large structural variations across the 21 studied genomes. Moreover, because of the high completeness of the nanopore assemblies, we were able to produce a complete cartography of transposable elements insertions and inspect structural variants that are generally missed using a short-read sequencing strategy."}, {"title": "ADRes: a computational pipeline for detecting molecular markers of Anti-malarial Drug Resistance, from Sanger sequencing data", "url": "https://www.biorxiv.org/content/early/2016/07/27/066183", "tag": "Bioinformatics", "abstract": "Background: Malaria control efforts are stifled by the emergence and dispersal of parasite strains resistant to available anti-malarials. Amino acid changes in specific positions of proteins encoded by Plasmodium falciparum genes pfcrt, dhps, dhfr, and pfmdr1 are used as molecular markers of resistance to antimalarials such as chloroquine, sulphadoxine-pyrimethamine, as well as artemisinin derivatives. However, a challenge to the detection of single nucleotide polymorphisms (SNPs) in codons responsible for these amino acid changes, in several samples, is the scarcity of automated computational pipelines for molecular biologists to; rapidly analyze ABI (Applied Biosystems) Sanger sequencing data spanning the codons of interest in order to characterize these codons and detect these molecular markers of drug resistance. The pipeline described here is an attempt to address this need. Method: This pipeline is a combination of existing tools, notably SAMtools and Burrows Wheeler Aligner (BWA), as well as custom Python and BASH scripts. It is designed to run on the UNIX shell, a command line interpreter. To characterize the codons associated with anti-malarial drug resistance (ADR) in a particular gene using this pipeline, the following options are required; a path to reference coding sequence of the gene in FASTA format, gene symbol (pfcrt, pfmdr1, dhps or dhfr), and a path to the directory of ABI sequencing trace files for the samples. With these inputs, the pipeline performs base calling and trimming, sequence alignment, and alignment parsing. Results: The output of the pipeline is a CSV (Comma-separated values) file of sample names, codons and their corresponding encoded amino acids. The data generated can be readily analyzed using widely available statistical or spreadsheet software, to determine the frequency of molecular markers of resistance to anti-malarials such as chloroquine, sulphadoxine-pyrimethamine and artemisinin derivatives. Conclusions: ADRes is a quick and effective pipeline for detecting common molecular markers of anti-malarial drug resistance, and could be a useful tool for surveillance. The code, description, and instructions for using this pipeline are publicly available at http://setfelix.github.io/ADRes."}, {"title": "Structure, interaction and post-translational modification study of arsenic reduction system in Bifidobacterium longum", "url": "https://www.biorxiv.org/content/early/2016/07/27/066324", "tag": "Bioinformatics", "abstract": "Microbial metabolism contributes to degradation of organoarsenicals, where arsenic reductases (glutaredoxins) play pivotal role in bacterial resistance to arsenic. Ars operon studies have revealed reduction of arsenate As(V) to arsenite As(III) by respiratory-chain-linked reductase enzyme complexes. Although structure of some bacterial arsenate reductases has been solved but not attempted for Bifidobacterium longum DJO10A colonizing the human gastrointestinal tract. Here it has been endeavoured to analyze and understand the structure, properties, interaction, evolution and action mechanism of this enzyme (arsC1) and its accessory interactors (arsB1, arsB2 and arsR). A systematic bioinformatic based analysis was carried out using a battery of tools and web servers for this purpose. Arsenic resistance gene cluster of gram-positive Bifidobacterium obtained from STRING database illustrated contiguous arsC and arsB genes and absence of arsA gene. ArsC1 was determined to be a cytoplasmic small-molecular-mass protein (~15 kDa) related to a class of tyrosine phosphatases mediating the reduction of As(V) to As(III). ArsC1 was found to be involved in dephosphorylation of arsR, arsB1 and arsB2, indicating its role in post translational modification (PTM) of interacting proteins. 3D structure analysis revealed that it was composed of 1 sheet,1 beta alpha beta unit, 4 strands, 5 helices, 3 helix-helix interacs, 13 beta turns and 1 gamma turn. All proteins in the cluster exhibited hydrophobic interactions. Explicit protein-protein hydrogen, ionic, aromatic and cation-pi interactions in arsenate reducing operon of Bifidobacterium longum DJO10A further aided structural understanding of arsenate reduction process."}, {"title": "Evaluating approaches to find exon chains corresponding to long reads", "url": "https://www.biorxiv.org/content/early/2016/07/27/066241", "tag": "Bioinformatics", "abstract": "Transcript prediction can be modelled as a graph problem where exons are modelled as nodes and reads spanning two or more exons are modelled as exon chains. PacBio third-generation sequencing technology produces significantly longer reads than earlier second-generation sequencing technologies, which gives valuable information about longer exon chains in a graph. However, with the high error rates of third-generation sequencing, aligning long reads correctly around the splice sites is a challenging task. Incorrect alignments lead to spurious nodes and arcs in the graph, which in turn lead to incorrect transcript predictions. We survey several approaches to find the exon chains corresponding to long reads in a splicing graph, and experimentally study the performance of these methods using simulated data to allow for sensitivity / precision analysis. Our experiments show that short reads from second-generation sequencing can be used to significantly improve exon chain correctness either by error-correcting the long reads before splicing graph creation, or by using them to create a splicing graph on which the long read alignments are then projected. We also study the memory and time consumption of various modules, and show that accurate exon chains lead to significantly increased transcript prediction accuracy."}, {"title": "Hybrid assembly of the large and highly repetitive genome of Aegilops tauschii, a progenitor of bread wheat, with the mega-reads algorithm", "url": "https://www.biorxiv.org/content/early/2016/07/26/066100", "tag": "Bioinformatics", "abstract": "Long sequencing reads generated by single-molecule sequencing technology offer the possibility of dramatically improving the contiguity of genome assemblies. The biggest challenge today is that long reads have relatively high error rates, currently around 15%. The high error rates make it difficult to use this data alone, particularly with highly repetitive plant genomes. Errors in the raw data can lead to insertion or deletion errors (indels) in the consensus genome sequence, which in turn create significant problems for downstream analysis; for example, a single indel may shift the reading frame and incorrectly truncate a protein sequence. Here we describe an algorithm that solves the high error rate problem by combining long, high-error reads with shorter but much more accurate Illumina sequencing reads, whose error rates average <1%. Our hybrid assembly algorithm combines these two types of reads to construct mega-reads, which are both long and accurate, and then assembles the mega-reads using the CABOG assembler, which was designed for long reads. We apply this technique to a large data set of Illumina and PacBio sequences from the species Aegilops tauschii, a large and highly repetitive plant genome that has resisted previous attempts at assembly. We show that the resulting assembled contigs are far larger than in any previous assembly, with an N50 contig size of 486,807. We compare the contigs to independently produced optical maps to evaluate their large-scale accuracy, and to a set of high-quality bacterial artificial chromosome (BAC)-based assemblies to evaluate base-level accuracy."}, {"title": "Phosphoproteomics-based Profiling of Kinase Activities in Cancer Cells", "url": "https://www.biorxiv.org/content/early/2016/07/26/066019", "tag": "Bioinformatics", "abstract": "Cellular signaling, predominantly mediated by phosphorylation through protein kinases, is found to be deregulated in most cancers. Accordingly, protein kinases have been subject to intense investigations in cancer research, to understand their role in oncogenesis and to discover new therapeutic targets. Despite great advances, an understanding of kinase dysfunctioning in cancer is far from complete. A powerful tool to investigate phosphorylation is mass-spectrometry (MS)-based phosphoproteomics, which enables the identification of thousands of phosphorylated peptides in a single experiment. Since every phosphorylation event results from the activity of a protein kinase, high-coverage phosphoproteomics data should indirectly contain comprehensive information about the activity of protein kinases. In this chapter, we discuss the use of computational methods to predict kinase activity scores from MS-based phosphoproteomics data. We start with a short explanation of the fundamental features of the phosphoproteomics data acquisition process from the perspective of the computational analysis. Next, we briefly review the existing databases with experimentally verified kinase-substrate relationships and present a set of bioinformatic tools to discover novel kinase targets. We then introduce different methods to infer kinase activities from phosphoproteomics data and these kinase-substrate relationships. We illustrate their application with a detailed protocol of one of the methods, KSEA (Kinase Substrate Enrichment Analysis). This method is implemented in Python within the framework of the open-source Kinase Activity Toolbox (kinact), which is freely available at http://github.com/saezlab/kinact/."}, {"title": "An Empirical Biomarker-based Calculator for Autosomal Recessive Polycystic Kidney Disease The Nieto-Narayan Formula", "url": "https://www.biorxiv.org/content/early/2016/07/26/064139", "tag": "Bioinformatics", "abstract": "Autosomal polycystic kidney disease (ARPKD) is associated with progressive enlargement of the kidneys fuelled by the formation and expansion of fluid-filled cysts. The disease is congenital and children that do not succumb to it during the neonatal period will, by age 10 years, more often than not, require nephrectomy+renal replacement therapy for management of both pain and renal insufficiency. Since increasing cystic index (CI; percent of kidney occupied by cysts) drives both renal expansion and organ dysfunction, management of these patients, including decisions such as elective nephrectomy and prioritization on the transplant waitlist, could clearly benefit from serial determination of CI. So also, clinical trials in ARPKD evaluating efficacy of novel drug candidates could benefit from serial determination of CI. Although ultrasound is currently the imaging modality of choice for diagnosis of ARPKD, its utilization for assessing disease progression is highly limited. Magnetic resonance imaging or computed tomography, although more reliable for determination of CI, are expensive, time-consuming and somewhat impractical in the pediatric population. Using a well-established mammalian model of ARPKD, we undertook a big data-like analysis of minimally- or non-invasive serum and urine biomarkers of renal injury/dysfunction to derive a family of equations for estimating CI. We then applied a signal averaging protocol to distil these equations to a single empirical formula for calculation of CI. Such a formula will eventually find use in identifying and monitoring patients at high risk for progressing to end-stage renal disease and aid in the conduct of clinical trials."}, {"title": "From genomes to phenotypes: Traitar, the microbial trait analyzer", "url": "https://www.biorxiv.org/content/early/2016/07/26/043315", "tag": "Bioinformatics", "abstract": "The number of sequenced genomes is growing exponentially, profoundly shifting the bottleneck from data generation to genome interpretation. Traits are often used to characterize and distinguish bacteria, and are likely a driving factor in microbial community composition, yet little is known about the traits of most microbes. We describe Traitar, the microbial trait analyzer, which is a fully automated software package for deriving phenotypes from the genome sequence. Traitar provides phenotype classifiers to predict 67 traits related to the use of various substrates as carbon and energy sources, oxygen requirement, morphology, antibiotic susceptibility, proteolysis and enzymatic activities. Furthermore, it suggests protein families associated with the presence of particular phenotypes. Our method uses L1-regularized L2-loss support vector machines for phenotype assignments based on phyletic patterns of protein families and their evolutionary histories across a diverse set of microbial species. We demonstrate reliable phenotype assignment for Traitar to bacterial genomes from 572 species of 8 phyla, also based on incomplete single-cell genomes and simulated draft genomes. We also showcase its application in metagenomics by verifying and complementing a manual metabolic reconstruction of two novel Clostridiales species based on draft genomes recovered from commercial biogas reactors. Traitar is available at https://github.com/hzi-bifo/traitar."}, {"title": "Semi-Supervised Learning of the Electronic Health Record for Phenotype Stratification", "url": "https://www.biorxiv.org/content/early/2016/07/26/039800", "tag": "Bioinformatics", "abstract": "Patient interactions with health care providers result in entries to electronic health records (EHRs). EHRs were built for clinical and billing purposes but contain many data points about an individual. Mining these records provides opportunities to extract electronic phenotypes, which can be paired with genetic data to identify genes underlying common human diseases. This task remains challenging: high quality phenotyping is costly and requires physician review; many fields in the records are sparsely filled; and our definitions of diseases are continuing to improve over time. Here we develop and evaluate a semi-supervised learning method for EHR phenotype extraction using denoising autoencoders for phenotype stratification. By combining denoising autoencoders with random forests we find classification improvements across multiple simulation models and improved survival prediction in ALS clinical trial data. This is particularly evident in cases where only a small number of patients have high quality phenotypes, a common scenario in EHR-based research. Denoising autoencoders perform dimensionality reduction enabling visualization and clustering for the discovery of new subtypes of disease. This method represents a promising approach to clarify disease subtypes and improve genotype-phenotype association studies that leverage EHRs."}, {"title": "A natural encoding of genetic variation in a Burrows-Wheeler Transform to enable mapping and genome inference", "url": "https://www.biorxiv.org/content/early/2016/07/25/059170", "tag": "Bioinformatics", "abstract": "We show how positional markers can be used to encode genetic variation within a Burrows-Wheeler Transform (BWT), and use this to construct a generalisation of the traditional 'reference genome', incorporating known variation within a species. Our goal is to support the inference of the closest mosaic of previously known sequences to the genome(s) under analysis. Our scheme results in an increased alphabet size, and by using a wavelet tree encoding of the BWT we reduce the performance impact on rank operations. We give a specialised form of the backward search that allows variation-aware exact matching. We implement this, and demonstrate the cost of constructing an index of the whole human genome with 8 million genetic variants is 25GB of RAM. We also show that inferring a closer reference can close large kilobase-scale coverage gaps in P. falciparum."}, {"title": "TelomereHunter: telomere content estimation and characterization from whole genome sequencing data", "url": "https://www.biorxiv.org/content/early/2016/07/23/065532", "tag": "Bioinformatics", "abstract": "Abstract Summary: Telomere shortening plays an important role in cellular aging and tumor suppression. The availability of large next-generation sequencing cohorts of matched tumor and control samples enables a computational high-throughput analysis of changes in telomere content and composition in cancer. Here we describe a novel software tool specifically tailored for the processing of large data collections. Availability and Implementation: TelomereHunter is implemented as a python package. It is freely available online at: www.dkfz.de/en/applied-bioinformatics/telomerehunter/telomerehunter.html."}, {"title": "Preprocessing, normalization and integration of the Illumina HumanMethylationEPIC array", "url": "https://www.biorxiv.org/content/early/2016/07/23/065490", "tag": "Bioinformatics", "abstract": "The minfi package is widely used for analyzing Illumina DNA methylation array data. Here we describe modifications to the minfi package required to support the HumanMethylationEPIC (\"EPIC\") array from Illumina. We discuss methods for the joint analysis and normalization of data from the HumanMethylation450 (\"450k\") and EPIC platforms. We also introduce the single-sample Noob method, a normalization procedure suitable for incremental preprocessing of individual HumanMethylation arrays. Our results recommend the single sample Noob method when integrating data from multiple generations of Infinium methylation arrays. Finally, we show how to use reference 450k datasets to estimate cell type composition of samples on EPIC arrays. The cumulative effect of these updates is to ensure that minfi provides the tools to best integrate existing and forthcoming Illumina methylation array data."}, {"title": "Calculating biological module enrichment or depletion and visualizing data on large-scale molecular maps with ACSNMineR and RNaviCell R packages", "url": "https://www.biorxiv.org/content/early/2016/07/22/064469", "tag": "Bioinformatics", "abstract": "Biological pathways or modules represent sets of interactions or functional relationships occurring at the molecular level in living cells. A large body of knowledge on pathways is organized in public databases such as the KEGG, Reactome, or in more specialized repositories, such as the Atlas of Cancer Signaling Network (ACSN). All these open biological databases facilitate analyses, improving our understanding of cellular systems. We hereby describe the R package ACSNMineR for calculation of enrichment or depletion of lists of genes of interest in biological pathways. ACSNMineR integrates ACSN molecular pathways, but can use any molecular pathway encoded as a GMT file, for instance sets of genes available in the Molecular Signatures Database (MSigDB). We also present the R package RNaviCell, that can be used in conjunction with ACSNMineR to visualize different data types on web-based, interactive ACSN maps. We illustrate the functionalities of the two packages with biological data taken from large-scale cancer datasets."}, {"title": "Short Tandem Repeat stutter model inferred from direct measurement of in vitro stutter noise", "url": "https://www.biorxiv.org/content/early/2016/07/21/065110", "tag": "Bioinformatics", "abstract": "Short tandem repeats (STRs) are polymorphic genomic loci valuable for various applications such as research, diagnostics and forensics. However, their polymorphic nature acts as a double-edged sword, as during in vitro amplification STRs undergo mutational processes that cause stutter noise, especially in the shorter, more mutable, repeat types. Although it is possible to overcome stutter noise by using amplification-free library preparation, such protocols are presently incompatible with single cell analysis and with known targeted-enrichment protocols. To address this challenge, we have designed a method for direct measurement of in vitro noise. Using a synthetic STR sequencing library, we have calibrated a proposed Markov model for the prediction of stutter patterns at any amplification cycle. By employing this model, we have managed to genotype accurately even cases of severe amplification noise, where as little as 3% of the reads accurately reflect the original STR size."}, {"title": "Avoidance of stochastic RNA interactions can be harnessed to control protein expression levels in bacteria and archaea", "url": "https://www.biorxiv.org/content/early/2016/07/21/033613", "tag": "Bioinformatics", "abstract": "A critical assumption of gene expression analysis is that mRNA abundances broadly correlate with protein abundance, but these two are often imperfectly correlated. Some of the discrepancy can be accounted for by two important mRNA features: codon usage and mRNA secondary structure. We present a new global factor, called mRNA:ncRNA avoidance, and provide evidence that avoidance increases translational efficiency. We also demonstrate a strong selection for avoidance of stochastic mRNA:ncRNA interactions across prokaryotes, and that these have a greater impact on protein abundance than mRNA structure or codon usage. By generating synonymously variant green fluorescent protein (GFP) mRNAs with different potential for mRNA:ncRNA interactions, we demonstrate that GFP levels correlate well with interaction avoidance. Therefore, taking stochastic mRNA:ncRNA interactions into account enables precise modulation of protein abundance."}, {"title": "Comparison of aggregation methods for multiphenotype exomic variant prioritization", "url": "https://www.biorxiv.org/content/early/2016/07/20/064899", "tag": "Bioinformatics", "abstract": "The identification of disease-causing genes in Mendelian disorders has been facilitated by the detection of rare disease-causing variation through exome sequencing experiments. These studies rely on population databases to filter a majority of the putatively neutral variation in the genome and additional filtering steps using either cohorts of diseased individuals or familial information to narrow down the list of candidate variants. Recently, new computational methods have been proposed to prioritize variants by scoring them not only based on their potential impact on protein function but also on their relevance given the available information on the disease under study. Usually these diseases comprise several phenotypic presentations, which are separately prioritized and then aggregated into a global score. In this study we compare several simple (e.g. maximum and mean score) and more complex aggregation methods (e.g. order statistics, parametric modeling) in order to obtain the best possible prioritization performance. We show that all methods perform reasonably well (median rank below 20 out of more than 8000 variants) and that the selection of an optimal aggregation method depends strongly on the fraction of uninformative phenotypes. Finally, we propose guidelines as to how to select an appropriate aggregation method based on knowledge of the phenotype under study."}, {"title": "sCNAphase: using haplotype resolved read depth to genotype somatic copy number alterations from low cellularity aneuploid tumors", "url": "https://www.biorxiv.org/content/early/2016/07/19/038828", "tag": "Bioinformatics", "abstract": "Accurate identification of copy number alterations is an essential step in understanding the events driving tumor progression. While a variety of algorithms have been developed to use high-throughput sequencing data to profile copy number changes, no tool is able to reliably characterize ploidy and genotype absolute copy number from tumor samples which contain less than 40% tumor cells. To increase our power to resolve the copy number profile from low-cellularity tumor samples, we developed a novel approach which pre-phases heterozygote germline SNPs in order to replace the commonly used 'B-allele frequency' with a more powerful 'parental-haplotype frequency'. We apply our tool - sCNAphase - to characterize the copy number and loss-of-heterozygosity profiles of four publicly available breast cancer cell-lines. Comparisons to previous spectral karyotyping and microarray studies revealed that sCNAphase reliably identified overall ploidy as well as the individual copy number mutations from each cell-line. Analysis of artificial cell-line mixtures demonstrated the capacity of this method to determine the level of tumor cellularity, consistently identify sCNAs and characterize ploidy in samples with as little as 10% tumor cells. This novel methodology has the potential to bring sCNA profiling to low-cellularity tumors, a form of cancer unable to be accurately studied by current methods."}, {"title": "Whose sample is it anyway? Widespread misannotation of samples in transcriptomics studies", "url": "https://www.biorxiv.org/content/early/2016/07/19/064626", "tag": "Bioinformatics", "abstract": "Concern about the reproducibility and reliability of biomedical research has been rising. An understudied issue is the prevalence of sample mislabeling, one impact of which would be invalid comparisons. We studied this issue in a corpus of human genomics studies by comparing the provided annotations of sex to the expression levels of sex-specific genes. We identified apparent mislabeled samples in 46% of the datasets studied, yielding a 99% confidence lower-bound estimate for all studies of 33%. In a separate analysis of a set of datasets concerning a single cohort of subjects, 2/4 had mislabelled samples, indicating laboratory mix-ups rather than data recording errors. While the number of mixed-up samples per study was generally small, because our method can only identify a subset of potential mix-ups, our estimate is conservative for the breadth of the problem. Our findings emphasize the need for more stringent sample tracking, and that re-users of published data must be alert to the possibility of annotation and labelling errors."}, {"title": "Resolution and reconciliation of non-binary gene trees with transfers, duplications and losses", "url": "https://www.biorxiv.org/content/early/2016/07/19/064675", "tag": "Bioinformatics", "abstract": "Motivation: Gene trees reconstructed from sequence alignments contain poorly supported branches when the phylogenetic signal in the sequences is weak. When a species tree is available, the signal of gains and losses of genes can be used to correctly resolve the unsupported parts of the gene history. Unfortunately, finding the best (i.e. most parsimonious) resolution has been shown to be NP-hard if transfers are considered as possible gene scale events, in addition to gene originations, duplications and losses. Results: We propose an exact, parameterized algorithm solving this problem in single-exponential time, where the parameter is the number of connected branches of the gene tree that show low support from the sequence alignment or, equivalently, the maximum number of children of any node of the gene tree once the low-support branches have been collapsed. We propose a way to choose among optimal solutions based on the available information. We show the usability of this principle on several simulated and biological data sets. The results show a comparable or better quality than several other tested methods having similar goals, but with a lower running time and a guarantee on the optimality of the solution. Availability: Our algorithm has been integrated into the ecceTERA phylogeny package, available at http://mbb.univ-montp2.fr/MBB/download_sources/16__ecceTERA and which can be run online at http://mbb.univ-montp2.fr/MBB/subsection/softExec.php?soft=eccetera."}, {"title": "CLIMB (the Cloud Infrastructure for Microbial Bioinformatics): an online resource for the medical microbiology community", "url": "https://www.biorxiv.org/content/early/2016/07/19/064451", "tag": "Bioinformatics", "abstract": "The increasing availability and decreasing cost of high-throughput sequencing has transformed academic medical microbiology, delivering an explosion in available genomes while also driving advances in bioinformatics. However, many microbiologists are unable to exploit the resulting large genomics datasets because they do not have access to relevant computational resources and to an appropriate bioinformatics infrastructure. Here, we present the Cloud Infrastructure for Microbial Bioinformatics (CLIMB) facility, a shared computing infrastructure that has been designed from the ground up to provide an environment where microbiologists can share and reuse methods and data."}, {"title": "A profile-based method for identifying functional divergence of orthologous genes in bacterial genomes", "url": "https://www.biorxiv.org/content/early/2016/07/19/022616", "tag": "Bioinformatics", "abstract": "Motivation: Next generation sequencing technologies have provided us with a wealth of information on genetic variation, but predicting the functional significance of this variation is a difficult task. While many comparative genomics studies have focused on gene flux and large scale changes, relatively little attention has been paid to quantifying the effects of single nucleotide polymorphisms and indels on protein function, particularly in bacterial genomics. Results: We present a hidden Markov model based approach we call delta-bitscore (DBS) for identifying orthologous proteins that have diverged at the amino acid sequence level in a way that is likely to impact biological function. We benchmark this approach with several widely used datasets and apply it to a proof-of-concept study of orthologous proteomes in an investigation of host adaptation in Salmonella enterica. We highlight the value of the method in identifying functional divergence of genes, and suggest that this tool may be a better approach than the commonly used dN/dS metric for identifying functionally significant genetic changes occurring in recently diverged organisms. Availability and Implementation: A program implementing DBS for pairwise genome comparisons is freely available at: https://github.com/UCanCompBio/deltaBS. Contact: nicole.wheeler@pg.canterbury.ac.nz, lars.barquist@uni-wuzberg.de"}, {"title": "Resolving microsatellite genotype ambiguity in populations of allopolyploid and diploidized autopolyploid organisms using negative correlations between allelic variables", "url": "https://www.biorxiv.org/content/early/2016/07/19/020610", "tag": "Bioinformatics", "abstract": "A major limitation in the analysis of genetic marker data from polyploid organisms is non-Mendelian segregation, particularly when a single marker yields allelic signals from multiple, independently segregating loci (isoloci). However, with markers such as microsatellites that detect more than two alleles, it is sometimes possible to deduce which alleles belong to which isoloci. Here we describe a novel mathematical property of codominant marker data when it is recoded as binary (presence/absence) allelic variables: under random mating in an infinite population, two allelic variables will be negatively correlated if they belong to the same locus, but uncorrelated if they belong to different loci. We present an algorithm to take advantage of this mathematical property, sorting alleles into isoloci based on correlations, then refining the allele assignments after checking for consistency with individual genotypes. We demonstrate the utility of our method on simulated data, as well as a real microsatellite dataset from a natural population of octoploid white sturgeon Acipenser transmontanus). Our methodology is implemented in the R package polysat version 1.5."}, {"title": "FEELnc: A tool for Long non-coding RNAs annotation and its application to the dog transcriptome", "url": "https://www.biorxiv.org/content/early/2016/07/18/064436", "tag": "Bioinformatics", "abstract": "Whole transcriptome sequencing (RNA-seq) has become a standard for cataloguing and monitoring RNA populations. Among the plethora of reconstructed transcripts, one of the main bottlenecks consists in correctly identifying the different classes of RNAs, particularly those that will be translated (mRNAs) from the class of long non-coding RNAs (lncRNAs). Here, we present FEELnc (FlExible Extraction of LncRNAs), an alignment-free program which accurately annotates lncRNAs based on a Random Forest model trained with general features such as multi k-mer frequencies and relaxed open reading frames. Benchmarking versus five state-of-art tools shows that FEELnc achieves similar or better classification performance on GENCODE and NONCODE datasets. The program also provides several specific modules that enable to fine-tune classification accuracy, to formalize the annotation of lncRNA classes and to annotate lncRNAs even in the absence of training set of noncoding RNAs. We used FEELnc on a real dataset comprising 20 new canine RNA-seq samples produced in the frame of the European LUPA consortium to expand the canine genome annotation and classified 10,374 novel lncRNAs and 58,640 new mRNA transcripts. FEELnc represents a standardized protocol for identifying and annotating lncRNAs and is freely accessible at https://github.com/tderrien/FEELnc."}, {"title": "Assessment of single cell RNA-seq normalization methods", "url": "https://www.biorxiv.org/content/early/2016/07/17/064329", "tag": "Bioinformatics", "abstract": "We have assessed the performance of seven normalization methods for single cell RNA-seq using data generated from dilution of RNA samples. Our analyses showed that methods considering spike-in ERCC RNA molecules significantly outperformed those not considering ERCCs. This work provides a guidance of selecting normalization methods to remove technical noise in single cell RNA-seq data."}, {"title": "Influence of rotational nucleosome positioning on transcription start site selection in animals promoters", "url": "https://www.biorxiv.org/content/early/2016/07/15/064113", "tag": "Bioinformatics", "abstract": "The recruitment of RNA PolII to the transcription start site (TSS) is an important step in gene regulation in all organisms. Core promoter elements (CPE) are conserved sequence motifs that guide PolII to the TSS by interacting with specific transcription factors (TFs). However, only a minority of animal promoters contains CPEs. It is still unknown how PolII selects the TSS in their absence. Here we present a comparative analysis of promoters' sequence composition and chromatin architecture in five eukaryotic model organisms, which shows the presence of common and unique DNA encoded features used to organize chromatin. Analysis of Pol II initiation patterns uncovers that, in the absence of certain CPEs, there is a strong correlation between the spread of initiation and the intensity of the 10 bp periodic signal in the nearest downstream nucleosome. Moreover, promoters' primary and secondary initiation sites show a characteristic 10 bp periodicity in the absence of CPEs. We also show that DNA natural variants in the region immediately downstream the TSS are able to affect both the nucleosome DNA affinity and Pol-II initiation pattern. These findings support the notion that, in addition to CPEs mediated selection, sequence induced nucleosome positioning could be a common and conserved mechanism of TSS selection in animals."}, {"title": "MIGS: Methylation Interpolated Gene Signatures Determine Associations Between Differential Methylation and Gene Expression", "url": "https://www.biorxiv.org/content/early/2016/07/15/063941", "tag": "Bioinformatics", "abstract": "A large number of genomic studies are underway to determine which genes are abnormally regulated by methylation in disease. However, our understanding of how disease-specific methylation changes potentially affect expression is poorly understood. We need better tools to explain specific variation in methylation that potentially affects gene expression in clinical sequencing. We have developed a model, Methylation Interpolated Gene Signatures (MIGS), that captures the complexity of DNA methylation changes around a gene promoter. Using data from the Roadmap Epigenomics Project, we show that MIGS significantly outperforms current methods to use methylation data to predict differential expression. We find that methylation changes at the TSS and downstream ~2kb are most predictive of expression change. MIGS will be an invaluable tool to analyze genome-wide methylation data as MIGS produces a longer and more accurate list of genes with methylation-associated expression changes."}, {"title": "Annotation Regression for Genome-Wide Association Studies with an Application to Psychiatric Genomic Consortium Data", "url": "https://www.biorxiv.org/content/early/2016/07/14/049932", "tag": "Bioinformatics", "abstract": "Although genome-wide association studies (GWAS) have been successful at finding thousands of disease-associated genetic variants (GVs), identifying causal variants and elucidating the mechanisms by which genotypes influence phenotypes are critical open questions. A key challenge is that a large percentage of disease-associated GVs are potential regulatory variants located in noncoding regions, making them difficult to interpret. Recent research efforts focus on going beyond annotating GVs by integrating functional annotation data with GWAS to prioritize GVs. However, applicability of these approaches is challenged by high dimensionality and heterogeneity of functional annotation data. Furthermore, existing methods often assume global associations of GVs with annotation data. This strong assumption is susceptible to violations for GVs involved in many complex diseases. To address these issues, we develop a general regression framework, named Annotation Regression for GWAS (ARoG). ARoG is based on finite mixture of linear regression models where GWAS association measures are viewed as responses and functional annotations as predictors. This mixture framework addresses heterogeneity of effects of GVs by grouping them into clusters and high dimensionality of the functional annotations by enabling annotation selection within each cluster. ARoG further employs permutation testing to evaluate the significance of selected annotations. Computational experiments indicate that ARoG can discover distinct associations between disease risk and functional annotations. Application of ARoG to autism and schizophrenia data from Psychiatric Genomics Consortium led to identification of GVs that significantly affect interactions of several transcription factors with DNA as potential mechanisms contributing to these disorders."}, {"title": "Multi-rate Poisson Tree Processes for single-locus species delimitation under Maximum Likelihood and Markov Chain Monte Carlo.", "url": "https://www.biorxiv.org/content/early/2016/07/14/063875", "tag": "Bioinformatics", "abstract": "In recent years, molecular species delimitation has become a routine approach for quantifying and classifying biodiversity. Barcoding methods are of particular importance in large-scale surveys as they promote fast species discovery and biodiversity estimates. Among those, distance-based methods are the most common choice as they scale well with large datasets; however, they are sensitive to similarity threshold parameters and they ignore evolutionary relationships. The recently introduced 'Poisson Tree Processes' (PTP) method is a phylogeny-aware approach that does not rely on such thresholds. Yet, two weaknesses of PTP impact its accuracy and practicality when applied to large datasets; it does not account for divergent intraspecific variation and is slow for a large number of sequences. We introduce the multi-rate PTP (mPTP), an improved method that alleviates the theoretical and technical shortcomings of PTP. It incorporates different levels of intraspecific genetic diversity deriving from differences in either the evolutionary history or sampling of each species. Results on empirical data suggest that mPTP is superior to PTP and popular distance-based methods as it, consistently, yields more accurate delimitations with respect to the taxonomy (i.e., identifies more taxonomic species, infers species numbers closer to the taxonomy). Moreover, mPTP does not require any similarity threshold as input. The novel dynamic programming algorithm attains a speedup of at least five orders of magnitude compared to PTP, allowing it to delimit species in large (meta-) barcoding data. In addition, Markov Chain Monte Carlo sampling provides a comprehensive evaluation of the inferred delimitation in just a few seconds for millions of steps, independently of tree size. mPTP is implemented in C and is available for download at http://github.com/Pas-Kapli/mptp under the GNU Affero 3 license. A web-service is available at http://mptp.h-its.org"}, {"title": "DiscoMark: Nuclear marker discovery from orthologous sequences using draft genome data", "url": "https://www.biorxiv.org/content/early/2016/07/14/047282", "tag": "Bioinformatics", "abstract": "High-throughput sequencing has laid the foundation for fast and cost-effective development of phylogenetic markers. Here we present the program DISCOMARK, which streamlines the development of nuclear DNA (nDNA) markers from whole-genome (or whole-transcriptome) sequencing data, combining local alignment, alignment trimming, reference mapping and primer design based on multiple sequence alignments in order to design primer pairs from input orthologous sequences. In order to demonstrate the suitability of DISCOMARK we designed markers for two groups of species, one consisting of closely related species and one group of distantly related species. For the closely related members of the species complex of Cloeon dipterum s.l. (Insecta, Ephemeroptera), the program discovered a total of 78 markers. Among these, we selected eight markers for amplification and Sanger sequencing. The exon sequence alignments (2,526 base pairs (bp)) were used to reconstruct a well supported phylogeny and to infer clearly structured haplotype networks. For the distantly related species we designed primers for several families in the insect order Ephemeroptera, using available genomic data from four sequenced species. We developed primer pairs for 23 markers that are designed to amplify across several families. The DISCOMARK program will enhance the development of new nDNA markers by providing a streamlined, automated approach to perform genome-scale scans for phylogenetic markers. The program is written in Python, released under a public license (GNU GPL v2), and together with a manual and example data set available at: https://github.com/hdetering/discomark."}, {"title": "ADPriboDB: The Database of ADP-ribosylated Proteins", "url": "https://www.biorxiv.org/content/early/2016/07/13/063768", "tag": "Bioinformatics", "abstract": "ADP-ribosylation refers to the addition of one or more ADP-ribose units onto proteins post-translationally. This protein modification is often added by ADP-ribosyltransferases, commonly known as PARPs, but it can also be added by other enzymes, including sirtuins or bacterial toxins. While past literature has utilized a variety of methods to identify ADP-ribosylated proteins, recent proteomics studies bring the power of mass spectrometry to determine sites of the modification. To appreciate the diverse roles of ADP-ribosylation across the proteome, we have created ADPriboDB, a database of ADP-ribosylated proteins (http://ADPriboDB.leunglab.org). Each entry of ADPriboDB is annotated manually by at least two independent curators from the literature between January 1975 and July 2015. The current database includes over 12,400 protein entries from 459 publications, identifying 2,389 unique proteins. Here we describe the structure and the current state of ADPriboDB as well as the criteria for entry inclusion. Using this aggregate data, we identified a statistically significant enrichment of ADP-ribosylated proteins in non-membranous RNA granules. To our knowledge, ADPriboDB is the first publicly available database encapsulating ADP-ribosylated proteins identified from the past 40 years, with a hope to facilitate the research of both basic scientists and clinicians to better understand ADP-ribosylation at the molecular level."}, {"title": "A cloud-based workflow to quantify transcript-expression levels in public cancer compendia", "url": "https://www.biorxiv.org/content/early/2016/07/12/063552", "tag": "Bioinformatics", "abstract": "Public compendia of raw sequencing data are now measured in petabytes. Accordingly, it is becoming infeasible for individual researchers to transfer these data to local computers. Recently, the National Cancer Institute funded an initiative to explore opportunities and challenges of working with molecular data in cloud-computing environments. With data in the cloud, it becomes possible for scientists to take their tools to the data and thereby avoid large data transfers. It also becomes feasible to scale computing resources to the needs of a given analysis. To evaluate this concept, we quantified transcript-expression levels for 12,307 RNA-Sequencing samples from the Cancer Cell Line Encyclopedia and The Cancer Genome Atlas. We used two cloud-based configurations to process the data and examined the performance and cost profiles of each configuration. Using \"preemptible virtual machines,\" we processed the samples for as little as $0.09 (USD) per sample. In total, we processed the TCGA samples (n=11,373) for only $1,065.49 and simultaneously processed thousands of samples at a time. As the samples were being processed, we collected detailed performance metrics, which helped us to track the duration of each processing step and to identify computational resources used at different stages of sample processing. Although the computational demands of reference alignment and expression quantification have decreased considerably, there remains a critical need for researchers to optimize preprocessing steps (e.g., sorting, converting, and trimming sequencing reads). We have created open-source Docker containers that include all the software and scripts necessary to process such data in the cloud and to collect performance metrics. The processed data are available in tabular format and in Google's BigQuery database (see https://osf.io/gqrz9)."}, {"title": "Fast genotyping of known SNPs through approximate k-mer matching", "url": "https://www.biorxiv.org/content/early/2016/07/12/063446", "tag": "Bioinformatics", "abstract": "Motivation: As the volume of next-generation sequencing (NGS) data increases, faster algorithms become necessary. Although speeding up individual components of a sequence analysis pipeline (e.g. read mapping) can reduce the computational cost of analysis, such approaches do not take full advantage of the particulars of a given problem. One problem of great interest, genotyping a known set of variants (e.g. dbSNP or Affymetrix SNPs), is important for characterization of known genetic traits and causative disease variants within an individual, as well as the initial stage of many ancestral and population genomic pipelines (e.g. GWAS). Results: We introduce LAVA (Lightweight Assignment of Variant Alleles), an NGS-based genotyping algorithm for a given set of SNP loci, which takes advantage of the fact that approximate matching of mid-size k-mers (with k = 32) can typically uniquely identify loci in the human genome without full read alignment. LAVA accurately calls the vast majority of SNPs in dbSNP and Affymetrix's Genome-Wide Human SNP Array 6.0 up to about an order of magnitude faster than standard NGS genotyping pipelines. For Affymetrix SNPs, LAVA has significantly higher SNP calling accuracy than existing pipelines while using as low as \u223c5GB of RAM. As such, LAVA represents a scalable computational method for population-level genotyping studies as well as a flexible NGS-based replacement for SNP arrays. Availability: LAVA software is available at http://lava.csail.mit.edu."}, {"title": "Coordinates and Intervals in Graph-based Reference Genomes", "url": "https://www.biorxiv.org/content/early/2016/07/11/063206", "tag": "Bioinformatics", "abstract": "Motivation: It has been proposed that future reference genomes should be graph structures in order to better represent the sequence diversity present in a species. However, there is currently no standard method to represent genomic intervals, such as positions of genes, on graph-based reference genomes. Results: We formalize offset-based coordinate systems on graph-based reference genomes and introduce a method for representing intervals on these reference structures. We show the advantage of our method by representing genes on a graph-based representation of the GRCh38 version of the human genome and its alternative loci for regions that are highly variable. Conclusion: More complex reference genomes, containing alternative loci, require methods to represent genomic data on these structures. Our proposed notation for genomic intervals makes it possible to fully utilize the alternative loci of GRCh38 and potential future graph-based reference genomes. We illustrate our notation for genomic intervals, as well as the offset-based coordinate systems, through a web tool at: https://github.com/uio-cels/gen-graph-coords."}, {"title": "High-throughput pipeline for de-novo assembly and drug resistance mutations identification from Next-Generation Sequencing viral data of residual diagnostic samples", "url": "https://www.biorxiv.org/content/early/2016/07/11/035154", "tag": "Bioinformatics", "abstract": "Motivation: The underlying genomic variation of a large number of pathogenic viruses can give rise to drug resistant mutations resulting in treatment failure. Next generation sequencing (NGS) enables the identification of viral quasi-species and the quantification of minority variants in clinical samples; therefore, it can be of direct benefit by detecting drug resistant mutations and devising optimal treatment strategies for individual patients. Results: The ICONIC (InfeCtion respONse through vIrus genomiCs) project has developed an automated, portable and customisable high-throughput computational pipeline to assemble de novo whole viral genomes, either segmented or non-segmented, and quantify minority variants using residual diagnostic samples. The pipeline has been benchmarked on a dedicated High-Performance Computing cluster using paired-end reads from RSV and Influenza clinical samples. The median length of generated genomes was 96% for the RSV dataset and 100% for each Influenza segment. The analysis of each set lasted less than 12 hours; each sample took around 3 hours and required a maximum memory of 10 GB. The pipeline can be easily ported to a dedicated server or cluster through either an installation script or a docker image. As it enables the subtyping of viral samples and the detection of relevant drug resistance mutations within three days of sample collection, our pipeline could operate within existing clinical reporting time frames and potentially be used as a decision support tool towards more effective personalised patient treatments. Availability: The software and its documentation are available from https://github.com/ICONIC-UCL/pipeline"}, {"title": "Deconvolution model for cytometric microbial subgroups along a freshwater hydrologic continuum", "url": "https://www.biorxiv.org/content/early/2016/07/11/063164", "tag": "Bioinformatics", "abstract": "Flow cytometry is suitable to discriminate and quantify aquatic microbial cells within a spectrum of fluorescence and light scatter signals. Using fixed operational and gating settings, a mixture model, coupled to Laplacian operator and Nelder-Mead optimization algorithm, allowed deconvolving bivariate cytometric profiles into single cell subgroups. This procedure was applied to outline recurrent patterns and quantitative changes of the aquatic microbial community along a river hydrologic continuum. We found five major persistent subgroups within each of the commonly retrieved populations of cells with Low and High content of Nucleic Acids (namely, LNA and HNA cells). Moreover, we assessed changes of the cytometric community profile over-imposed by water inputs from a wastewater treatment plant. Our approach for multiparametric data deconvolution confirmed that flow cytometry could represent a prime candidate technology for assessing microbial community patterns in flowing waters."}, {"title": "Boosting alignment accuracy through adaptive local realignment", "url": "https://www.biorxiv.org/content/early/2016/07/10/063131", "tag": "Bioinformatics", "abstract": "Motivation: While mutation rates can vary across the residues of a protein, when computing alignments of protein sequences the same setting of values for substitution score and gap penalty parameters is typically used across their entire length. We provide for the first time a new method called adaptive local realignment that automatically uses diverse parameter settings in different regions of the input sequences when computing multiple sequence alignments. This allows parameter settings to adapt to more closely match the local mutation rate across a protein. Method: Our method builds on our prior work on global alignment parameter advising with the Facet alignment accuracy estimator. Given a computed alignment, in each region that has low estimated accuracy, a collection of candidate realignments is generated using a precomputed set of alternate parameter settings. If one of these alternate realignments has higher estimated accuracy than the original subalignment, the region is replaced with the new realignment, and the concatenation of these realigned regions forms the final alignment that is output. Results: Adaptive local realignment significantly improves the quality of alignments over using the single best default parameter setting. In particular, this new method of local advising, when combined with prior methods for global advising, boosts alignment accuracy by as much as 26% over the best default setting on hard-to-align benchmarks (and by 6.4% over using global advising alone). Availability: A new version of the Opal multiple sequence aligner that incorporates adaptive local realignment using Facet for parameter advising, is available free for non-commercial use at http://facet.cs.arizona.edu."}, {"title": "Reversible Polymorphism-Aware Phylogenetic Models and their Application to Tree Inference", "url": "https://www.biorxiv.org/content/early/2016/07/10/048496", "tag": "Bioinformatics", "abstract": "We present a reversible Polymorphism-Aware Phylogenetic Model (revPoMo) for species tree estimation from genome-wide data. revPoMo enables the reconstruction of large scale species trees for many within-species samples. It expands the alphabet of DNA substitution models to include polymorphic states, thereby, naturally accounting for incomplete lineage sorting. We implemented revPoMo in the maximum likelihood software IQ-TREE. A simulation study and an application to great apes data show that the runtimes of our approach and standard substitution models are comparable but that revPoMo has much better accuracy in estimating trees, divergence times and mutation rates. The advantage of revPoMo is that an increase of sample size per species improves estimations but does not increase runtime. Therefore, revPoMo is a valuable tool with several applications, from speciation dating to species tree reconstruction."}, {"title": "Combining multiple tools outperforms individual methods in gene set enrichment analyses", "url": "https://www.biorxiv.org/content/early/2016/07/08/042580", "tag": "Bioinformatics", "abstract": "Gene set enrichment (GSE) analysis allows researchers to efficiently extract biological insight from long lists of differentially expressed genes by interrogating them at a systems level. In recent years, there has been a proliferation of GSE analysis methods and hence it has become increasingly difficult for researchers to select an optimal GSE tool based on their particular data set. Moreover, the majority of GSE analysis methods do not allow researchers to simultaneously compare gene set level results between multiple experimental conditions. Results: The ensemble of genes set enrichment analyses (EGSEA) is a method developed for RNA-sequencing data that combines results from twelve algorithms and calculates collective gene set scores to improve the biological relevance of the highest ranked gene sets. redEGSEA's gene set database contains around 25,000 gene sets from sixteen collections. It has multiple visualization capabilities that allow researchers to view gene sets at various levels of granularity. EGSEA has been tested on simulated data and on a number of human and mouse data sets and, based on biologists' feedback, consistently outperforms the individual tools that have been combined. Our evaluation demonstrates the superiority of the ensemble approach for GSE analysis, and its utility to effectively and efficiently extrapolate biological functions and potential involvement in disease processes from lists of differentially regulated genes."}, {"title": "Rapid and efficient analysis of 20,000 RNA-seq samples with Toil", "url": "https://www.biorxiv.org/content/early/2016/07/07/062497", "tag": "Bioinformatics", "abstract": "Toil is portable, open-source workflow software that supports contemporary workflow definition languages and can be used to securely and reproducibly run scientific workflows efficiently at large-scale. To demonstrate Toil, we processed over 20,000 RNA-seq samples to create a consistent meta-analysis of five datasets free of computational batch effects that we make freely available. Nearly all the samples were analysed in under four days using a commercial cloud cluster of 32,000 preemptable cores."}, {"title": "Assocplots: a python package for static and interactive visualization of multiple-group GWAS results", "url": "https://www.biorxiv.org/content/early/2016/07/07/062737", "tag": "Bioinformatics", "abstract": "Summary: Over the last decade, genome-wide association studies (GWAS) have generated vast amounts of analysis results, requiring development of novel tools for data visualization. Quantile-quantile plots and Manhattan plots are classical tools which have been utilized to visually summarize GWAS results and identify genetic variants significantly associated with traits of interest. However, static visualizations are limiting in the information that can be shown. Here we present Assocplots, a python package for viewing and exploring GWAS results not only using classic static Manhattan and quantile-quantile plots, but also through a dynamic extension which allows to visualize data interactively, and to visualize the relationships between GWAS results from multiple cohorts or studies. Availability: The Assocplots package is open source and distributed under the MIT license via GitHub (https://github.com/khramts/assocplots) along with examples, documentation and installation instructions. Contact: ekhramts@medicine.bsd.uchicago.edu, bstranger@medicine.bsd.uchicago.edu"}, {"title": "Modelling the transcription factor DNA-binding affinity using genome-wide ChIP-based data", "url": "https://www.biorxiv.org/content/early/2016/07/07/061978", "tag": "Bioinformatics", "abstract": "Understanding protein-DNA binding affinity is still a mystery for many transcription factors (TFs). Although several approaches have been proposed in the literature to model the DNA-binding specificity of TFs, they still have some limitations. Most of the methods require a cut-off threshold in order to classify a K-mer as a binding site (BS) and finding such a threshold is usually done by handcraft rather than a science. Some other approaches use a prior knowledge on the biological context of regulatory elements in the genome along with machine learning algorithms to build classifier models for TFBSs. Noticeably, these methods deliberately select the training and testing datasets so that they are very separable. Hence, the current methods do not actually capture the TF-DNA binding relationship. In this paper, we present a threshold-free framework based on a novel ensemble learning algorithm in order to locate TFBSs in DNA sequences. Our proposed approach creates TF-specific classifier models using genome-wide DNA-binding experiments and a prior biological knowledge on DNA sequences and TF binding preferences. Systematic background filtering algorithms are utilized to remove non-functional K-mers from training and testing datasets. To reduce the complexity of classifier models, a fast feature selection algorithm is employed. Finally, the created classifier models are used to scan new DNA sequences and identify potential binding sites. The analysis results show that our proposed approach is able to identify novel binding sites in the Saccharomyces cerevisiae genome."}, {"title": "An Empirical Analysis of Topic Modeling for Mining Cancer Clinical Notes", "url": "https://www.biorxiv.org/content/early/2016/07/06/062307", "tag": "Bioinformatics", "abstract": "Using a variety of techniques including Topic Modeling, Principal Component Analysis and Bi-clustering, we explore electronic patient records in the form of unstructured clinical notes and genetic mutation test results. Our ultimate goal is to gain insight into a unique body of clinical data, specifically regarding the topics discussed within the note content and relationships between patient clinical notes and their underlying genetics."}, {"title": "cy3sabiork: A Cytoscape app for visualizing kinetic data from SABIO-RK", "url": "https://www.biorxiv.org/content/early/2016/07/05/062091", "tag": "Bioinformatics", "abstract": "Kinetic data of biochemical reactions are essential for the creation of kinetic models of biochemical networks. One of the main resources of such information is SABIO-RK, a curated database for kinetic data of biochemical reactions and their related information. Despite the importance for computational modelling there has been no simple solution to visualize the kinetic data from SABIO-RK. In this work, I present cy3sabiork, an app for querying and visualization of kinetic data from SABIO-RK in Cytoscape. The kinetic information is accessible via a combination of graph structure and annotations of nodes, with provided information consisting of: (I) reaction details, enzyme and organism; (II) kinetic law, formula, parameters; (III) experimental conditions; (IV) publication; (V) additional annotations. cy3sabiork creates an intuitive visualization of kinetic entries in form of a species-reaction-kinetics graph, which reflects the reaction-centered approach of SABIO-RK. Kinetic entries can be imported in SBML format from either the SABIO-RK web interface or via web service queries. The app allows for easy comparison of kinetic data, visual inspection of the elements involved in the kinetic record and simple access to the annotation information of the kinetic record. I applied cy3sabiork in the computational modelling of galactose metabolism in the Human liver."}, {"title": "Cis/transgene optimization: systematic discovery of some key gene expression elements integrating bioinformatics and computational biology", "url": "https://www.biorxiv.org/content/early/2016/07/05/061945", "tag": "Bioinformatics", "abstract": "In recombinant protein production, quantity and quality are the major challenges particularly for large scale and high-throughput production systems. The present study mainly focused on computational analysis and in silico systematic discovery of some key functional gene expression elements in microalgae Dunaliella salina as a case study which there is no or poor information in this regard. Among the key factors, we took a shot at matrix attachment regions (MARs), translation initiation sites (TIS), signal peptide (SP) sequences, gene optimization and transformation system. Computational analysis of MARs sequences provided enough information about the structure of these sequences and led us to design an artificial MAR sequence considering the essential motifs and underlain rules. As the consensus TIS, we revealed that A-3, G-6C-5C-4 and G+1C+2G+3 arrange the specific context in this microalgae which help in locating the ribosome at the correct reading frame. Bioinformatics studies unveiled the sequence of MASTRAPLLALLALLCAGSARA with the highest signal score as the specific SP for secretion systems. A multi-criteria optimization procedure was performed to redesign the coding sequence of the BAR selectable marker gene. The optimized version of the gene mainly covered the host codon preference, the less structured mRNA and exposure of TIS. As the intragenic factors, we selected an efficient promoter, a 5\u02c8-UTR and an intron from the closely related species (Chlamydomonas Sp.) to construct the specific expression vectors. The expression cassettes containing optimized genetic elements could be delivered into the microalgae cells and conferred the resistance to the transformants for at least 90 generations. The findings indicated that the MARs flanking the expression cassette along with the optimized expression elements particularly codon adaptation could potentially improve transformation efficiency and stability. The findings can be efficiently deployed as an empirical model for systematic discovery of the key expression elements and optimization of the cis/transgenes."}, {"title": "Tempus et Locus: a tool for extracting precisely dated viral sequences from GenBank, and its application to the phylogenetics of primate erythroparvovirus 1 (B19V)", "url": "https://www.biorxiv.org/content/early/2016/07/04/061697", "tag": "Bioinformatics", "abstract": "The presence of data in the collection_date field of a GenBank sequence record is of great assistance in the use of that sequence for Bayesian phylogenetics using tip-dating. We present Tempus et Locus (TeL), a tool for extracting such sequences from a GenBank-formatted sequence database. TeL shows that 60% of viral sequences in GenBank have collection date fields, but that this varies considerably between species. Primate erythroparvovirus 1 (human parvovirus B19 or B19V) has only 40% of its sequences dated, of which only 112 are of more than 4 kb. 100 of these are from B19V sub-genotype 1a and were collected from a mere 6 studies conducted in 5 countries between 2002 and 2013. Nevertheless, Bayesian phylogenetic analysis of this limited set gives a date for the common ancestor of sub-genotype 1a in 1990 (95% HPD 1981-1996) which is in reasonable agreement with estimates of previous studies where collection dates have been assembled by more laborious methods of literature search and direct enquiries to sequence submitters. We conclude that although collection dates should become standard for all future GenBank submissions of virus sequences, accurate dating of ancestors is possible with even a small number of sequences if sampling information is high quality."}, {"title": "MicroScope: ChIP-seq and RNA-seq software analysis suite for gene expression heatmaps", "url": "https://www.biorxiv.org/content/early/2016/07/04/034694", "tag": "Bioinformatics", "abstract": "We propose a user-friendly ChIP-seq and RNA-seq software suite for the interactive visualization and analysis of genomic data, including integrated features to support differential expression analysis, interactive heatmap production, principal component analysis, gene ontology analysis, and dynamic network analysis. MicroScope is hosted online as an R Shiny web application based on the D3 JavaScript library: http://microscopebioinformatics.org/. The methods are implemented in R, and are available as part of the MicroScope project at: https://github.com/Bohdan-Khomtchouk/Microscope."}, {"title": "Cross-species genome-wide identification of evolutionary conserved microProteins", "url": "https://www.biorxiv.org/content/early/2016/07/01/061655", "tag": "Bioinformatics", "abstract": "MicroProteins are small single domain proteins that act by engaging their targets into non-productive protein complexes. In order to identify novel microProteins in any sequenced genome of interest, we have developed miPFinder, a program that identifies and classifies potential microProteins. In the past years, several microProteins have been discovered in plants where they are mainly involved in the regulation of development. The miPFinder algorithm identifies all up to date known plant microProteins and extends the microProtein concept to other protein families. Here, we reveal potential microProtein candidates in several plant and animal reference genomes. A large number of these microProteins are species-specific while others evolved early and are evolutionary highly conserved. Most known microProtein genes originated from large ancestral genes by gene duplication, mutation and subsequent degradation. Gene ontology analysis shows that putative microProtein ancestors are often located in the nucleus, and involved in DNA binding and formation of protein complexes. Additionally, microProtein candidates act in plant transcriptional regulation, signal transduction and anatomical structure development. MiPFinder is freely available to find microProteins in any genome and will aid in the identification of novel microProteins in plants and animals."}, {"title": "Detection of statistically significant network changes in complex biological networks", "url": "https://www.biorxiv.org/content/early/2016/06/30/061515.1", "tag": "Bioinformatics", "abstract": "Biological networks contribute effectively to unveil the complex structure of molecular interactions and to discover driver genes especially in cancer context. It can happen that due to gene mutations, as for example when cancer progresses, the gene expression network undergoes some amount of localised re-wiring. The ability to detect statistical relevant changes in the interaction patterns induced by the progression of the disease can lead to discovery of novel relevant signatures. Several procedures have been recently proposed to detect sub-network differences in pairwise labeled weighted networks. In this paper, we propose an improvement over the state-of-the-art based on the Generalized Hamming Distance adopted for evaluating the topological difference between two networks and estimating its statistical significance. The proposed procedure exploits a more effective model selection criteria to generate p-values for statistical significance and is more efficient in terms of computational time and prediction accuracy than literature methods. Moreover, the structure of the proposed algorithm allows for a faster parallelized implementation. In the case of dense random geometric networks the proposed approach is 10-15x faster and achieves 5-10% higher AUC, Precision/Recall, and Kappa value than the state-of-the-art. We also report the application of the method to dissect the difference between the regulatory networks of IDH-mutant versus IDH-wild-type glioma cancer. In such a case our method is able to identify some recently reported master regulators as well as novel important candidates. The scripts implementing the proposed algorithms are available in R at https://sites.google.com/site/raghvendramallmlresearcher/codes"}, {"title": "iCAVE: an open source tool for immersive 3D visualization of complex biomolecular interaction networks", "url": "https://www.biorxiv.org/content/early/2016/06/29/061374", "tag": "Bioinformatics", "abstract": "Visualizations of biomolecular networks assist in systems-level data exploration in myriad cellular processes in health and disease. While these networks are increasingly informed by data generated from high-throughout (HT) experiments, current tools do not adequately scale with concomitant increase in their size and complexity. We present an open-source software platform, interactome-CAVE, (iCAVE), that leverages stereoscopic (3D) immersive display technologies for visualizing complex biomolecular interaction networks. Users can explore networks (i) in 3D in any computer and (ii) in immersive 3D in any computer with an appropriate graphics card as well as in CAVE environments. iCAVE includes new 3D network layout algorithms in addition to extensions of known 2D network layout, clustering and edge-bundling algorithms to the 3D space, to assist in understanding the underlying structures in large, dense, layered or clustered networks. Users can perform simultaneous queries of several databases within iCAVE or visualize their own networks (e.g. disease, drug, protein, metabolite, phenotype, genotype) utilizing directionality, weight or other properties by using different property settings. iCAVE has modular structure to allow rapid development by the addition of algorithms, datasets or features without affecting other parts of the code. Overall, iCAVE is a freely available open source tool to help gain novel insights from complex HT datasets."}, {"title": "Characterization of kinase gene expression and splicing profile in prostate cancer with RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2016/06/29/061085", "tag": "Bioinformatics", "abstract": "Background: Alternative splicing is a ubiquitous post-transcriptional process in most eukaryotic genes. Aberrant splicing isoforms and abnormal isoform ratios can contribute to cancer development. Kinase genes are key regulators of various cellular processes. Many kinases are found to be oncogenic and have been intensively investigated in the study of cancer and drugs. RNA-Seq provides a powerful technology for genome-wide study of alternative splicing in cancer besides the conventional gene expression profiling. But this potential has not been fully demonstrated yet. Methods: Here we characterized the transcriptome profile of prostate cancer using RNA-Seq data from viewpoints of both differential expression and differential splicing, with an emphasis on kinase genes and their splicing variations. We built up a pipeline to conduct differential expression and differential splicing analysis. Further functional enrichment analysis was performed to explore functional interpretation of the genes. With focus on kinase genes, we performed kinase domain analysis to identify the functionally important candidate kinase gene in prostate cancer. We further calculated the expression level of isoforms to explore the function of isoform switching of kinase genes in prostate cancer. Results: We identified distinct gene groups from differential expression and splicing analysis, which suggested that alternative splicing adds another level to gene expression regulation. Enriched GO terms of differentially expressed and spliced kinase genes were found to play different roles in regulation of cellular metabolism. Function analysis on differentially spliced kinase genes showed that differentially spliced exons of these genes are significantly enriched in protein kinase domains. Among them, we found that gene CDK5 has isoform switching between prostate cancer and benign tissues, which may affect cancer development by changing androgen receptor (AR) phosphorylation. The observation was validated in another RNA-Seq dataset of prostate cancer cell lines. Conclusions: Our work characterized the expression and splicing profile of kinase genes in prostate cancer and proposed a hypothetical model on isoform switching of CDK5 and AR phosphorylation in prostate cancer. These findings bring new understanding to the role of alternatively spliced kinases in prostate cancer and demonstrate the use of RNA-Seq data in studying alternative splicing in cancer."}, {"title": "Locating the ligand binding sites for the G-protein coupled estrogen receptor (GPER) using combined information from docking and sequence conservation", "url": "https://www.biorxiv.org/content/early/2016/06/29/061051", "tag": "Bioinformatics", "abstract": "High concentrations of estrogenic compounds can overstimulate estrogen receptors and potentially lead to breast, ovarian, and cervical cancers. Recently, a G-protein coupled estrogen receptor (GPER/GPR30) was discovered that has no structural similarity to the well-characterized, classical estrogen receptor ER\u03b1. The crystal structure of GPER has not yet been determined, and the ligand binding sites have not yet been experimentally identified. The recent explosion of GPCR crystal structures now allow homology modeling with unprecedented reliability. We create, validate, and describe a homology model for GPER. We describe and apply ConDock, the first hybrid scoring function to use information from protein surface conservation and ligand docking, to predict binding sites on GPER for four ligands, estradiol, G1, G15, and tamoxifen. ConDock is a simple product function of sequence conservation and binding energy scores. ConDock predicts that all four ligands bind to the same location on GPER, centered on L119, H307, and N310; this site is deeper in the receptor cleft than are ligand binding sites predicted by previous studies. We compare the sites predicted by ConDock and traditional methods analyzing surface geometry, surface conservation, and ligand chemical interactions. Incorporating sequence conservation information in ConDock avoids errors resulting from physics-based scoring functions and modeling."}, {"title": "Simulations reveal increased fluctuations in estrogen receptor-alpha conformation upon antagonist binding", "url": "https://www.biorxiv.org/content/early/2016/06/29/061069", "tag": "Bioinformatics", "abstract": "Molecular dynamics (MD) simulations have been used to model dynamic fluctuations in the structure of estrogen receptor-alpha (ER-\u03b1) upon binding to the natural agonist 17\u03b2-estradiol (E2) and to the active metabolite of the breast cancer drug and antagonist, 4-hydroxytamoxifen (OHT). We present the most extensive MD simulations to date of ER-\u03b1; with over 1 \u03bcs of combined simulations for the monomer and dimer forms. Simulations reveal that the antagonist-bound complex includes significant fluctuations while the agonist-bound complex is tightly restrained. OHT increases dynamic disorder in the loops located to either side of the tail H12 helix; H12 has been associated with the activation status of ER-\u03b1. We also report that fluctuations near H12 lead to greater conformational variation in the binding mode of the ethylamine tail of OHT. Both the agonist and antagonist conformations are stable throughout the 240 ns simulations, supporting the hypothesis that there are no transitions between these two states or into intermediate states. The stable position of H12 in the OHT-bound conformation suggests that OHT stabilizes a well-defined antagonist conformational ensemble rather than merely blocking the agonist-driven activation of ER-\u03b1. Simultaneously, the increased dynamic properties of the OHT-bound complex is a potential source of binding entropy."}, {"title": "fCCAC: functional canonical correlation analysis to evaluate covariance between nucleic acid sequencing datasets", "url": "https://www.biorxiv.org/content/early/2016/06/27/060780", "tag": "Bioinformatics", "abstract": "Computational evaluation of variability across DNA or RNA sequencing datasets is a crucial step in genomic science, as it allows both to evaluate the reproducibility across biological or technical replicates, and to compare different datasets to identify their potential correlations. Here I present fCCAC, an application of functional canonical correlation analysis to assess covariance of nucleic acid sequencing datasets such as chromatin immunoprecipitation followed by deep sequencing (ChIP-seq). I exemplify how this method can reveal shared covariance between histone modifications and DNA binding proteins, such as the relationship between the H3K4me3 chromatin mark and its epigenetic writers and readers. R code is publicly available at http://github.com/pmb59/fCCAC/."}, {"title": "Varapp: A reactive web-application for variants filtering", "url": "https://www.biorxiv.org/content/early/2016/06/27/060806", "tag": "Bioinformatics", "abstract": "Summary: Varapp is an open-source web application to filter variants from large sets of exome data stored in a relational database. Varapp offers a reactive graphical user interface, very fast data processing, security and facility to save, reproduce and share results. Typically, a few seconds suffice to apply non-trivial filters to a set of half a million variants and extract a handful of potential clinically relevant targets. Varapp implements different scenarios for Mendelian diseases (dominant, recessive, de novo, X-linked, and compound heterozygous), and allows searching for variants in genes or chromosomal regions of interest. Availability: The application is made of a Javascript front-end and a Python back-end. Its source code is hosted at https://github.com/varapp. A demo version is available at https://varapp-demo.vital-it.ch. The full documentation can be found at https://varapp-demo.vital-it.ch/docs."}, {"title": "Comparative distribution of antisense-RNA regulated toxin-antitoxin systems", "url": "https://www.biorxiv.org/content/early/2016/06/27/060863", "tag": "Bioinformatics", "abstract": "Toxin-antitoxin (TA) systems are gene modules that appear to be widely horizontally mobile. It has been proposed that type I TA systems, with an antisense RNA-antitoxin, are less mobile than other TAs but no direct comparisons have been made. We searched for type I, II and III toxin families on chromosomes, plasmids and phages across bacterial phyla. The distribution of type I TA systems were more narrow than most type II and III system families, though this was less true of more recently discovered families. We discuss how the function and phenotypes of type I TA systems as well as biases in our databases and discovery of these modules may account for differences in their distribution."}, {"title": "16SpeB: Towards defining bacterial species boundaries by intra-species gene sequence identity", "url": "https://www.biorxiv.org/content/early/2016/06/24/060657", "tag": "Bioinformatics", "abstract": "Summary: 16SpeB (16S rRNA-based Species Boundary) is a package of Perl programs that evaluates total sequence variation of a bacterial species at the levels of the whole 16S rRNA sequences or single hypervariable (V) regions, using publicly-available sequences. The 16SpeB pipelines filter sequences from duplicated strains and of low quality, extracts a V region of interest using general primer sequences, and calculates sequence percentage identity (%ID) through all possible pairwise alignments. Results: The minimum %ID of 16S rRNA gene sequences for 15 clinically-important bacterial species, as determined by 16SpeB, ranged from 82.6% to 99.8%. The relationship between minimum %ID of V2/V6 regions and full-gene sequences varied among species, indicating that %ID species limits should be resolved independently for each region of the 16S rRNA gene and bacterial species. Availability: 16SpeB and user manual are freely available for download from: https://github.com/pnpnpn/16SpeB. A video tutorial is available at: https://youtu.be/Vd6YmMhyBiA"}, {"title": "Bottom-up ecology of the human microbiome: from metagenomes to metabolomes", "url": "https://www.biorxiv.org/content/early/2016/06/24/060673", "tag": "Bioinformatics", "abstract": "The environmental metabolome is a dominant and essential factor shaping microbial communities. Thus, we hypothesized that metagenomic datasets could reveal the quantitative metabolic status of a given sample. Using a newly developed bottom-up ecology algorithm, we predicted high-resolution metabolomes of hundreds of metagenomic datasets from the human microbiome, revealing body-site specific metabolomes consistent with known metabolomics data, and suggesting that common cosmetics ingredients are some of the major metabolites shaping the human skin microbiome."}, {"title": "Qudaich: A smart sequence aligner", "url": "https://www.biorxiv.org/content/early/2016/06/24/060509", "tag": "Bioinformatics", "abstract": "Next generation sequencing (NGS) technology produces massive amounts of data in a reasonable time and low cost. Analyzing and annotating these data requires sequence alignments to compare them with genes, proteins and genomes in different databases. Sequence alignment is the first step in metagenomics analysis, and pairwise comparisons of sequence reads provide a measure of similarity between environments. Most of the current aligners focus on aligning NGS datasets against long reference sequences rather than comparing between datasets. As the number of metagenomes and other genomic data increases each year, there is a demand for more sophisticated, faster sequence alignment algorithms. Here, we introduce a novel sequence aligner, Qudaich, which can efficiently process large volumes of data and is suited to de novo comparisons of next generation reads datasets. Qudaich can handle both DNA and protein sequences and attempts to provide the best possible alignment for each query sequence. Qudaich can produce more useful alignments quicker than other contemporary alignment algorithms."}, {"title": "Is voice a marker for autism spectrum disorder? A systematic review and meta-analysis", "url": "https://www.biorxiv.org/content/early/2016/06/24/046565", "tag": "Bioinformatics", "abstract": "Individuals with Autism Spectrum Disorder (ASD) tend to show distinctive, atypical acoustic patterns of speech. These behaviours affect social interactions and social development and could represent a non-invasive marker for ASD. We systematically reviewed the literature quantifying acoustic patterns in ASD. Search terms were: (prosody OR intonation OR inflection OR intensity OR pitch OR fundamental frequency OR speech rate OR voice quality OR acoustic) AND (autis* OR Asperger). Results were filtered to include only: empirical studies quantifying acoustic features of vocal production in ASD, with a sample size > 2, and the inclusion of a neurotypical comparison group and/or correlations between acoustic measures and severity of clinical features. We identified 34 articles, including 30 univariate studies and 15 multivariate machine-learning studies. We performed meta-analyses of the univariate studies, identifying significant differences in mean pitch and pitch range between individuals with ASD and comparison participants (Cohen's d of 0.4-0.5 and discriminatory accuracy of about 61-64%). The multivariate studies reported higher accuracies than the univariate studies (63-96%). However, the methods used and the acoustic features investigated were too diverse for performing meta-analysis. We conclude that multivariate studies of acoustic patterns are a promising but yet unsystematic avenue for establishing ASD markers. We outline three recommendations for future studies: open data, open methods, and theory-driven research."}, {"title": "Evaluating the Evaluation of Cancer Driver Genes", "url": "https://www.biorxiv.org/content/early/2016/06/23/060426", "tag": "Bioinformatics", "abstract": "Sequencing has identified millions of somatic mutations in human cancers, but distinguishing cancer driver genes remains a major challenge. Numerous methods have been developed to identify driver genes, but evaluation of the performance of these methods is hindered by the lack of a gold standard, i.e., bona fide driver gene mutations. Here, we establish an evaluation framework that can be applied when a gold standard is not available. We used this framework to compare the performance of eight driver gene prediction methods. One of these methods, newly described here, incorporated a machine learning-based ratiometric approach. We show that the driver genes predicted by each of these eight methods vary widely. Moreover, the p-values reported by several of the methods were inconsistent with the uniform values expected, thus calling into question the assumptions that were used to generate them. Finally, we evaluated the potential effects of unexplained variability in mutation rates on false positive driver gene predictions. Our analysis points to the strengths and weaknesses of each of the currently available methods and offers guidance for improving them in the future."}, {"title": "Breaking Lander-Waterman's Coverage Bound", "url": "https://www.biorxiv.org/content/early/2016/06/23/060384", "tag": "Bioinformatics", "abstract": "Lander-Waterman's coverage bound establishes the total number of reads required to cover the whole genome of size G bases. In fact, their bound is a direct consequence of the well-known solution to the coupon collector's problem which proves that for such genome, the total number of bases to be sequenced should be O (G ln G). Although the result leads to a tight bound, it is based on a tacit assumption that the set of reads are first collected through a sequencing process and then are processed through a computation process, i.e., there are two different machines: one for sequencing and one for processing. In this paper, we present a significant improvement compared to Lander-Waterman's result and prove that by combining the sequencing and computing processes, one can re-sequence the whole genome with as low as O(G) sequenced bases in total. Our approach also dramatically reduces the required computational power for the combined process. Simulation results are performed on real genomes with different sequencing error rates. The results support our theory predicting the log G improvement on coverage bound and corresponding reduction in the total number of bases required to be sequenced."}, {"title": "Meta-aligner: Long-read alignment based on genome statistics", "url": "https://www.biorxiv.org/content/early/2016/06/21/060129", "tag": "Bioinformatics", "abstract": "Fast and accurate alignment of long-reads plays an important role in reducing the overall cost of long-read sequencing. In this paper, we propose Meta-aligner, an efficient and accurate long-read aligner that exploits the statistics of reference genome to improve performance in terms of reducing time complexity and achieving significantly higher recall for very noisy and long reads. The first step of algorithm adopts well-known short-read aligners in order to rapidly align a large fraction of reads through a progressive process of aligning read fragments to the reference genome. In the second phase, the remaining reads are handled by simultaneous alignment of all read fragments and a decision making process which exploits the overall information provided by the corresponding mapped fragments. By using this procedure, significant performance improvement is attained in comparison with traditional schemes in the case of PacBio long-reads."}, {"title": "An algorithm for fast preranked gene set enrichment analysis using cumulative statistic calculation", "url": "https://www.biorxiv.org/content/early/2016/06/20/060012", "tag": "Bioinformatics", "abstract": "Gene set enrichment analysis is a widely used tool for analyzing gene expression data. However, current implementations are slow due to a large number of required samples for the analysis to have a good statistical power. In this paper we present a novel algorithm, that efficiently reuses one sample multiple times and thus speeds up the analysis. We show that it is possible to make hundreds of thousands permutations in a few minutes, which leads to very accurate p-values. This, in turn, allows applying standard FDR correction procedures, which are more accurate than the ones currently used. The method is implemented in a form of an R package and is freely available at \\url{https://github.com/ctlab/fgsea}."}, {"title": "MPAthic: quantitative modeling of sequence-function relationships for massively parallel assays", "url": "https://www.biorxiv.org/content/early/2016/06/20/054676", "tag": "Bioinformatics", "abstract": "Massively parallel assays (MPAs) are being rapidly adopted for studying a wide range of DNA, RNA, and protein sequence-function relationships. However, the software available for quantitatively modeling these relationships is severely limited. Here we describe MPAthic, a software package that enables the rapid inference of such models from a variety of MPA datasets. Using both simulated and previously published data, we show that the modeling capabilities of MPAthic greatly improve on those of existing software. In particular, only MPAthic can accurately quantify the strength of epistatic interactions. These capabilities address a major need in the analysis of MPA data."}, {"title": "SourceData - a semantic platform for curating and searching figures", "url": "https://www.biorxiv.org/content/early/2016/06/20/058529", "tag": "Bioinformatics", "abstract": "In molecular and cell biology, most of the data presented in published papers are not available in accessible formats that would allow for analysis and systematic mining. Here we present SourceData (http://sourcedata.embo.org), a platform that allows researchers and publishers to share scientific figures and, when available, the underlying source data in a way that is machine-readable and findable. SourceData has therefore developed tools to generate machine-readable descriptive metadata from figures in published manuscripts. Experimentally tested hypotheses are represented as directed relationships between standardized biological entities, which can be connected into a searchable data-oriented \u2032knowledge graph\u2032. SourceData focuses on the core of scientific evidence - data presented in figures - and makes papers searchable based on their data content. By coupling data availability to improved discoverability, SourceData aims at establishing a self-reinforcing data \u2032ecosystem\u2032 that bridges the conventional visual and narrative description of research findings with a machine-readable representation of data and hypotheses."}, {"title": "BlastPhyMe: A toolkit for rapid generation and analysis of protein-coding sequence datasets", "url": "https://www.biorxiv.org/content/early/2016/06/19/059881", "tag": "Bioinformatics", "abstract": "SUMMARY: We present BlastPhyMe (BLAST, Phylogenies, and Molecular Evolution) a new application to facilitate the fast and easy generation and analysis of protein-coding sequence datasets. The application uses a portable database framework to manage and organize sequences along with a graphical user interface (GUI) that makes the application extremely easy to use. BlastPhyMe utilizes several existing services and applications in a unique way that save researchers considerable time when building and analyzing protein-coding datasets. The application consists of two modules that can be used separately or together. The first module enables the assembly of coding sequence datasets. BLAST searches can be used to obtain all related sequences of interest from NCBI. Full GenBank records are saved within the database and coding sequences are automatically extracted. A feature of particular note is that sequences can be sorted based on NCBI taxonomic hierarchy before export to MEGA for visualization. The application provides GUIs for automatic alignment of sequences with the popular tools MUSCLE and PRANK, as well as for reconstructing phylogenetic trees using PhyML. The second module incorporates selection analyses using codon-based likelihood methods. The alignments and phylogenetic trees generated with the dataset module, or those generated elsewhere, can be used to run the models implemented in the codeml PAML package. A GUI allows easy selection of models and parameters. Importantly, replicate analyses with different parameter starting values can be automatically performed in order to ensure selection of the best-fitting model. Multiple analyses can be run simultaneously based on the number of processor cores available, while additional analyses will be run iteratively until completed. Results are saved within the database and can be exported to publication-ready Excel tables, which further automatically compute the appropriate likelihood ratio test between models in order to determine statistical significance. Future updates will add additional options for phylogenetic reconstruction (eg, MrBayes) and selection analyses (eg, HYPHY). BlastPhyMe saves researches of all bioinformatics experience levels considerable time by automating the numerous tasks required for the generation and analysis of protein-coding sequence datasets using a straightforward graphical interface. AVAILABILITY: Installation package and source code available from http://blastphyme.codeplex.com CONTACT: ryan.schott@utoronto.ca"}, {"title": "Voodoo Machine Learning for Clinical Predictions", "url": "https://www.biorxiv.org/content/early/2016/06/19/059774", "tag": "Bioinformatics", "abstract": "The availability of smartphone and wearable sensor technology is leading to a rapid accumulation of human subject data, and machine learning is emerging as a technique to map that data into clinical predictions. As machine learning algorithms are increasingly used to support clinical decision making, it is important to reliably quantify their prediction accuracy. Cross-validation is the standard approach for evaluating the accuracy of such algorithms; however, several cross-validations methods exist and only some of them are statistically meaningful. Here we compared two popular cross-validation methods: record-wise and subject-wise. Using both a publicly available dataset and a simulation, we found that record-wise cross-validation often massively overestimates the prediction accuracy of the algorithms. We also found that this erroneous method is used by almost half of the retrieved studies that used accelerometers, wearable sensors, or smartphones to predict clinical outcomes. As we move towards an era of machine learning based diagnosis and treatment, using proper methods to evaluate their accuracy is crucial, as erroneous results can mislead both clinicians and data scientists."}, {"title": "RSQ: a statistical method for quantification of isoform-specific structurome using transcriptome-wide structural profiling data", "url": "https://www.biorxiv.org/content/early/2016/06/18/043232", "tag": "Bioinformatics", "abstract": "The structure of RNA, which is considered to be a second layer of information alongside the genetic code, provides fundamental insights into the cellular function of both coding and non-coding RNAs. Several high-throughput technologies have been developed to profile transcriptome-wide RNA structures, i.e., the structurome. However, it is challenging to interpret the profiling data because the observed data represent an average over different RNA conformations and isoforms with different abundance. To address this challenge, we developed an RNA structurome quantification method (RSQ) to statistically model the distribution of reads over both isoforms and RNA conformations, and thus provide accurate quantification of the isoform-specific structurome. The quantified RNA structurome enables the comparison of isoform-specific conformations between different conditions, the exploration of RNA conformation variation affected by single nucleotide polymorphism (SNP) , and the measurement of RNA accessibility for binding of either small RNAs in RNAi-based assays or RNA binding protein in transcriptional regulation. The model used in our method sheds new light on the potential impact of the RNA structurome on gene regulation."}, {"title": "Comparing alternative pipelines for cross-platform microarray gene expression data integration with RNA-seq data in breast cancer", "url": "https://www.biorxiv.org/content/early/2016/06/18/059600", "tag": "Bioinformatics", "abstract": "Background: According to major public repositories statistics an overwhelming majority of the existing and newly uploaded data originates from microarray experiments. Unfortunately, the potential of this data to bring new insights is limited by the effects of individual study-specific biases due to small number of biological samples. Increasing sample size by direct microarray data integration increases the statistical power to obtain a more precise estimate of gene expression in a population of individuals resulting in lower false discovery rates. However, despite numerous recommendations for gene expression data integration, there is a lack of a systematic comparison of different processing approaches aimed to asses microarray platforms diversity and ambiguous probesets to genes correspondence, leading to low number of studies applying integration. Results: Here, we investigated five different approaches of the microarrays data processing in comparison with RNA-seq data on breast cancer samples. We aimed to evaluate different probesets annotations as well as different procedures of choosing between probesets mapped to the same gene. We show that pipelines rankings are mostly preserved across Affymetrix and Illumina platforms. BrainArray approach based on updated annotation and redesigned probesets definition and choosing probeset with the maximum average signal across the samples have best correlation with RNA-seq, while averaging probesets signals as well as scoring the quality of probes sequences mapping to the transcripts of the targeted gene have worse correlation. Finally, randomly selecting probeset among probesets mapped to the same gene significantly decreases the correlation with RNA-seq. Conclusion: We show that methods, which rely on actual probesets signal intensities, are advantageous to methods considering biological characteristics of the probes sequences only and that cross-platform integration of datasets improves correlation with the RNA-seq data. We consider the results obtained in this paper contributive to the integrative analysis as a worthwhile alternative to the classical meta-analysis of the multiple gene expression datasets. Keywords: microarray; cross-platform data integration; probesets to genes correspondence; probesets re-annotation; RNA-seq"}, {"title": "MMinte: An application for predicting metabolic interactions among the microbial species in a community", "url": "https://www.biorxiv.org/content/early/2016/06/17/059550", "tag": "Bioinformatics", "abstract": "Background. The explosive growth of microbiome research has yielded great quantities of data. These data provide us with many answers, but raise just as many questions. 16S rDNA - the backbone of microbiome analyses - allows us to assess alpha-diversity, beta-diversity, and microbe-microbe associations, which characterize the overall properties of an ecosystem. However, we are still unable to use 16S rDNA data to directly assess the microbe-microbe and microbe-environment interactions that determine that system's broader ecology. Thus, properties such as competition, cooperation, and nutrient conditions remain insufficiently analyzed. Here, we apply predictive community metabolic models of microbes identified with 16S rDNA data to probe the ecology of microbial communities. Results. We developed a methodology for the large-scale assessment of microbial metabolic interactions (MMinte) from 16S rDNA data. MMinte assesses the relative growth rates of interacting pairs of organisms within a community metabolic network and whether that interaction has a positive or negative effect. Moreover, MMinte's simulations take into account the nutritional environment, which play a strong role in determining the metabolism of individual microbes. We present two case studies that demonstrate this software's utility. In the first, we show how diet influences the nature of the microbe-microbe interactions. In the second, we use MMinte's modular feature set to better understand how the growth of Desulfovibrio piger is affected by, and affects the growth of, other members in a simplified gut community under metabolic conditions suggested to be determinant for their dynamics. Conclusion. By applying metabolic models to commonly available sequence data, MMinte grants the user insight into the metabolic relationships between microbes, highlighting important features that may relate to ecological stability, susceptibility, and cross-feeding. These relationships are at the foundation of a wide range of ecological questions that impact our ability to understand problems such as microbially-derived toxicity in colon cancer."}, {"title": "In situ replication rates for uncultivated bacteria in microbial communities", "url": "https://www.biorxiv.org/content/early/2016/06/16/057992", "tag": "Bioinformatics", "abstract": "Culture-independent microbiome studies have revolutionized our understanding of the complexity and metabolic potential of microbial communities, but information about in situ growth rates has been lacking. Here, we show that bacterial replication rates can be determined using genome-resolved metagenomics without requirement for complete genome sequences. In human infants, we detected elevated microbial replication rates following administration of antibiotics, and bacterial growth rate anomalies prior to the onset of necrotizing enterocolitis. We studied microorganisms in subsurface communities and determined that a diverse group of groundwater-associated bacteria typically exhibit slow growth rates, despite significant changes in geochemical conditions. All microbiome studies will be advanced by measurements of replication rates that can identify actively growing populations, track organism responses to changing conditions, and provide growth rate information needed for modeling."}, {"title": "RiVIERA-beta: Joint Bayesian inference of risk variants and tissue-specific epigenomic enrichments across multiple complex human diseases", "url": "https://www.biorxiv.org/content/early/2016/06/16/059329", "tag": "Bioinformatics", "abstract": "Genome wide association studies (GWAS) provide a powerful approach for uncovering disease-associated variants in human, but fine-mapping the causal variants remains a challenge. This is partly remedied by prioritization of disease-associated variants that overlap GWAS-enriched epigenomic annotations. Here, we introduce a new Bayesian model RiVIERA-beta (Risk Variant Inference using Epigenomic Reference Annotations) for inference of driver variants by modelling summary statistics p-values in Beta density function across multiple traits using hundreds of epigenomic annotations. In simulation, RiVIERA-beta promising power in detecting causal variants and causal annotations, the multi-trait joint inference further improved the detection power.} We applied RiVIERA-beta to model the existing GWAS summary statistics of 9 autoimmune diseases and Schizophrenia by jointly harnessing the potential causal enrichments among 848 tissue-specific epigenomics annotations from ENCODE/Roadmap consortium covering 127 cell/tissue types and 8 major epigenomic marks. RiVIERA-beta identified meaningful tissue-specific enrichments for enhancer regions defined by H3K4me1 and H3K27ac for Blood T-Cell specifically in the 9 autoimmune diseases and Brain-specific enhancer activities exclusively in Schizophrenia. Moreover, the variants from the 95% credible sets exhibited high conservation and enrichments for GTEx whole-blood eQTLs located within transcription-factor-binding-sites and DNA-hypersensitive-sites. Furthermore, joint modeling the nine immune traits by simultaneously inferring and exploiting the underlying epigenomic correlation between traits further improved the functional enrichments compared to single-trait models."}, {"title": "Navigating the phenotype frontier: The Monarch Initiative", "url": "https://www.biorxiv.org/content/early/2016/06/15/059204", "tag": "Bioinformatics", "abstract": "The principles of genetics apply across the entire tree of life. At the cellular level we share biological mechanisms with species from which we diverged millions, even billions of years ago. We can exploit this common ancestry to learn about health and disease, by analyzing DNA and protein sequences, but also through the observable outcomes of genetic differences, i.e. phenotypes. To solve challenging disease problems we need to unify the heterogeneous data that relates genomics to disease traits. Without a big-picture view of phenotypic data, many questions in genetics are difficult or impossible to answer. The Monarch Initiative (https://monarchinitiative.org) provides tools for genotype-phenotype analysis, genomic diagnostics, and precision medicine across broad areas of disease. These tools depend on the data integrated through computable phenotypes for cross species comparisons."}, {"title": "Heat*seq: an interactive web tool for high-throughput sequencing experiment comparison with public data", "url": "https://www.biorxiv.org/content/early/2016/06/15/049254", "tag": "Bioinformatics", "abstract": "Better protocols and decreasing costs have made high-throughput sequencing experiments now accessible even to small experimental laboratories. However, comparing one or few experiments generated by an individual lab to the vast amount of relevant data freely available in the public domain might be limited due to lack of bioinformatics expertise. Though several tools, including genome browsers, allow such comparison at a single gene level, they do not provide a genome-wide view. We developed Heat*seq, a web-tool that allows genome scale comparison of high throughput experiments (ChIP-seq, RNA-seq and CAGE) provided by a user, to the data in the public domain. Heat*seq currently contains over 12,000 experiments across diverse tissue and cell types in human, mouse and drosophila. Heat*seq displays interactive correlation heatmaps, with an ability to dynamically subset datasets to contextualise user experiments. High quality figures and tables are produced and can be downloaded in multiple formats. Web application: http://www.heatstarseq.roslin.ed.ac.uk/. Source code: https://github.com/gdevailly."}, {"title": "mmgenome: a toolbox for reproducible genome extraction from metagenomes", "url": "https://www.biorxiv.org/content/early/2016/06/15/059121", "tag": "Bioinformatics", "abstract": "Recovery of population genomes is becoming a standard analysis in metagenomics and a multitude of different approaches exists. However, the workflows are complex, requiring data generation, binning, validation and finishing to generate high quality population genome bins. In addition, several different approaches are often used on the same dataset as the optimal strategy to extract a specific population genome varies. Here we introduce mmgenome: a toolbox for reproducible genome extraction from metagenomes. At the core of mmgenome is an R package that facilitates effortless integration of different binning strategies by collecting information on scaffolds. Genome binning is facilitated through integrated tools that support effortless visualizations, validation and calculation of key statistics. Full reproducibility and transparency is ob-tained through Rmarkdown, whereby every step can be recreated."}, {"title": "ANFIS-based Fuzzy Systems for Searching DNA-Protein Binding Sites", "url": "https://www.biorxiv.org/content/early/2016/06/15/058800", "tag": "Bioinformatics", "abstract": "Transcriptional regulation mainly controls how genes are expressed and how cells behave based on the transcription factor (TF) proteins that bind upstream of the transcription start sites (TSSs) of genes. These TF DNA binding sites (TFBSs) are usually short (5-15 base pairs) and degenerate (some positions can have multiple possible alternatives). Traditionally, computational methods scan DNA sequences using the position weight matrix (PWM) of a given TF, calculate binding scores for each K-mer against the PWM, and finally classify a K-mer as to whether it is a putative TFBS or a background sequence based on a cut-off threshold. The FSCAN system, which is proposed in this paper, employs machine learning techniques to build a learner model that is able to identify TFBSs in a set of bound sequences without the need for a cut-off threshold. Our proposed method utilizes fuzzy inference techniques along with a distribution-based filtering algorithm to predict the binding sites of a TF given its PWM model and phastCons scores for the input DNA sequences. Data imbalance reduction techniques are also used to ease the learning of the adaptive-neuro fuzzy inference system (ANFIS) algorithm. The proposed system is tested on 22 ChIP-chip sequence-sets from the Saccharomyces Cerevisiae genome. Our results show that FSCAN outperforms other approaches like MatInspector and MATCH and is quite robust. As more transcriptional data becomes available, our proposed framework encourages the use of fuzzy logic techniques in the prediction of TFBSs."}, {"title": "Visualizing tumor evolution with the fishplot package for R", "url": "https://www.biorxiv.org/content/early/2016/06/15/059055", "tag": "Bioinformatics", "abstract": "Background: Massively-parallel sequencing at depth has enabled tumor heterogeneity and evolution to be characterized in unprecedented detail. Tracking these changes in clonal architecture can inform therapeutic response and resistance. Robust and intuitive data visualizations can greatly aid these interpretations, especially in cases with multiple timepoints . Current data visualization methods are typically manual and laborious, and often only approximate subclonal fractions. Results: We have developed an R package that accurately and intuitively displays changes in clonal structure over time. It requires simple input data and produces illustrative and easy-to-interpret graphs suitable for diagnosis, presentation, and publication. Conclusions: The simplicity, power, and flexibility of this tool make it valuable for visualizing tumor evolution, and it has potential utility in both research and clinical settings. Fishplot is available at https://github.com/chrisamiller/fishplot"}, {"title": "FPD: A comprehensive phosphorylation database in fungi", "url": "https://www.biorxiv.org/content/early/2016/06/14/058867", "tag": "Bioinformatics", "abstract": "Protein phosphorylation, one of the most classic post-translational modification, plays a critical role in the diverse cellular processes including cell cycle, growth and signal transduction pathways. However, the available information of phosphorylation in fungi is limited. Here we provided a Fungi Phosphorylation Database (FPD) that comprises high-confidence in vivo phosphosites identified by MS-based proteomics in various fungal species. This comprehensive phosphorylation database contains 62,272 non-redundant phosphorylation sites in 11,222 proteins across eight organisms, including Aspergillus flavus, Aspergillus nidulans, Fusarium graminearum, Magnaporthe oryzae, Neurospora crassa, Saccharomyces cerevisiae, Schizosaccharomyces pombe and Cryptococcus neoformans. A fungi-specific phosphothreonine motif and several conserved phosphorylation motif were discovered by comparatively analyzing the pattern of phosphorylation sites in fungi, plants and animals."}, {"title": "A cell type-specific expression signature predicts haploinsufficient autism-susceptibility genes", "url": "https://www.biorxiv.org/content/early/2016/06/14/058826", "tag": "Bioinformatics", "abstract": "Recent studies have identified many genes with rare de novo mutations in autism, but a limited number of these have been conclusively established as disease-susceptibility genes due to lack of recurrence and confounding background mutations. Such extreme genetic heterogeneity severely limits recurrence-based statistical power even in studies with a large sample size. In addition, the cellular contexts in which these genomic lesions confer disease risks remain poorly understood. Here we investigate the use of cell-type specific expression profiles to differentiate mutations in autism patients or unaffected siblings. Using 24 distinct cell types isolated from the mouse central nervous system, we identified an expression signature shared by genes with likely gene disrupting (LGD) mutations detected by exome-sequencing in autism cases. The signature reflects haploinsufficiency of risk genes enriched in transcriptional and post-transcriptional regulators, with the strongest positive associations with specific types of neurons in different brain regions, including cortical neurons, cerebellar granule cells, and striatal medium spiny neurons. Based on this signature, we assigned a D score to all human genes to prioritize candidate autism-susceptibility genes. When applied to genes with only a single LGD mutation in cases, the D score achieved a precision of 40% as compared to the 15% baseline with a minimal loss in sensitivity. Further improvement was made by combining D score and mutation intolerance metrics from ExAC which were derived from orthogonal data sources. The ensemble model achieved precision of 60% and predicted 117 high-priority candidates. These prioritized lists can facilitate identification of additional autism-susceptibility genes."}, {"title": "ARGON: fast, whole-genome simulation of the discrete time Wright-Fisher process", "url": "https://www.biorxiv.org/content/early/2016/06/14/036376", "tag": "Bioinformatics", "abstract": "Simulation under the coalescent model is ubiquitous in the analysis of genetic data. The rapid growth of real data sets from multiple human populations led to increasing interest in simulating very large sample sizes at whole-chromosome scales. When the sample size is large, the coalescent model becomes an increasingly inaccurate approximation of the discrete time Wright-Fisher model (DTWF). Analytical and computational treatment of the DTWF, however, is generally harder. We present a simulator (ARGON) for the DTWF process that scales up to hundreds of thousands of samples and whole-chromosome lengths, with a time/memory performance comparable or superior to currently available methods for coalescent simulation. The simulator supports arbitrary demographic history, migration, Newick tree output, variable mutation/recombination rates and gene conversion, and efficiently outputs pairwise identical-by-descent (IBD) sharing data. ARGON (version 0.1) is written in Java, open source, and freely available at https://github.com/pierpal/ARGON."}, {"title": "Leveraging Functional Annotations in Genetic Risk Prediction for Human Complex Diseases", "url": "https://www.biorxiv.org/content/early/2016/06/13/058768", "tag": "Bioinformatics", "abstract": "Genome wide association studies have identified numerous regions in the genome associated with hundreds of human diseases. Building accurate genetic risk prediction models from these data will have great impacts on disease prevention and treatment strategies. However, prediction accuracy remains moderate for most diseases, which is largely due to the challenges in identifying all the disease-associated variants and accurately estimating their effect sizes. We introduce AnnoPred, a principled framework that incorporates diverse functional annotation data to improve risk prediction accuracy, and demonstrate its performance on multiple human complex diseases."}, {"title": "Citizen Science for Mining the Biomedical Literature", "url": "https://www.biorxiv.org/content/early/2016/06/13/038083", "tag": "Bioinformatics", "abstract": "Biomedical literature represents one of the largest and fastest growing collections of unstructured biomedical knowledge. Finding critical information buried in the literature can be challenging. In order to extract information from freeflowing text, researchers need to: 1. identify the entities in the text (named entity recognition), 2. apply a standardized vocabulary to these entities (normalization), and 3. identify how entities in the text are related to one another (relationship extraction.) Researchers have primarily approached these information extraction tasks through manual expert curation, and computational methods. We have previously demonstrated that named entity recognition (NER) tasks can be crowdsourced to a group of nonexperts via the paid microtask platform, Amazon Mechanical Turk (AMT); and can dramatically reduce the cost and increase the throughput of biocuration efforts. However, given the size of the biomedical literature even information extraction via paid microtask platforms is not scalable. With our web-based application Mark2Cure ( http://mark2cure.org ), we demonstrate that NER tasks can also be performed by volunteer citizen scientists with high accuracy. We apply metrics from the Zooniverse Matrices of Citizen Science Success and provide the results here to serve as a basis of comparison for other citizen science projects. Further, we discuss design considerations, issues, and the application of analytics for successfully moving a crowdsourcing workflow from a paid microtask platform to a citizen science platform. To our knowledge, this study is the first application of citizen science to a natural language processing task."}, {"title": "Differential analysis of RNA-Seq incorporating quantification uncertainty", "url": "https://www.biorxiv.org/content/early/2016/06/10/058164", "tag": "Bioinformatics", "abstract": "We describe a novel method for the differential analysis of RNA-Seq data that utilizes bootstrapping in conjunction with response error linear modeling to decouple biological variance from inferential variance. The method is implemented in an interactive shiny app called sleuth that utilizes kallisto quantifications and bootstraps for fast and accurate analysis of RNA-Seq experiments."}, {"title": "IPC - Isoelectric Point Calculator", "url": "https://www.biorxiv.org/content/early/2016/06/10/049841", "tag": "Bioinformatics", "abstract": "Accurate estimation of the isoelectric point (pI) based on the amino acid sequence can be useful for many biochemistry and proteomics techniques such as 2-D polyacrylamide gel electrophoresis, or capillary isoelectric focusing used in combination with high-throughput mass spectrometry. Here, I present the Isoelectric Point Calculator, a web service for the estimation of pI using different sets of dissociation constant (pKa) values, including two new, computationally optimized pKa sets. According to the presented benchmarks, IPC outperform previous algorithms by at least 14.9% for proteins and 0.9% for peptides (on average, 22.1% and 59.6%, respectively), which corresponds to an average error of the pI estimation equal to 0.87 and 0.25 pH units for proteins and peptides, respectively. Peptide and protein datasets used in the study and the precalculated pI for PDB, SwissProt databases are available for large-scale analysis and future development. The IPC can be accessed at http://isoelectric.ovh.org/."}, {"title": "CellProfiler Analyst: interactive data exploration, analysis, and classification of large biological image sets", "url": "https://www.biorxiv.org/content/early/2016/06/09/057976", "tag": "Bioinformatics", "abstract": "Summary: CellProfiler Analyst allows the exploration and visualization of image-based data, together with the classification of complex biological phenotypes, via an interactive user interface designed for biologists and data scientists. CellProfiler Analyst 2.0, completely rewritten in Python, builds on these features and adds enhanced supervised machine learning capabilities (in Classifier), as well as visualization tools to overview an experiment (Plate Viewer and Image Gallery). Availability and implementation: CellProfiler Analyst 2.0 is free and open source, available at http://www.cellprofiler.org and from GitHub (https://github.com/CellProfiler/CellProfiler-Analyst) under the BSD license. It is available as a packaged application for Mac OS X and Microsoft Windows and can be compiled for Linux. We implemented an automatic build process which supports nightly updates and regular release cycles for the software."}, {"title": "False Discovery Rates: A New Deal.", "url": "https://www.biorxiv.org/content/early/2016/06/08/038216", "tag": "Bioinformatics", "abstract": "We introduce a new Empirical Bayes approach for large-scale hypothesis testing, including estimating False Discovery Rates (FDRs), and effect sizes. This approach has two key differences from existing approaches to FDR analysis. First, it assumes that the distribution of the actual (unobserved) effects is unimodal, with a mode at 0. This \"unimodal assumption\" (UA), although natural in many contexts, is not usually incorporated into standard FDR analysis, and we demonstrate how incorporating it brings many benefits. Specifically, the UA facilitates efficient and robust computation -- estimating the unimodal distribution involves solving a simple convex optimization problem -- and enables more accurate inferences provided that it holds. Second, the method takes as its input two numbers for each test (an effect size estimate, and corresponding standard error), rather than the one number usually used (p value, or z score). When available, using two numbers instead of one helps account for variation in measurement precision across tests. It also facilitates estimation of effects, and unlike standard FDR methods our approach provides interval estimates (credible regions) for each effect in addition to measures of significance. To provide a bridge between interval estimates and significance measures we introduce the term \"local false sign rate\" to refer to the probability of getting the sign of an effect wrong, and argue that it is a superior measure of significance than the local FDR because it is both more generally applicable, and can be more robustly estimated. Our methods are implemented in an R package ashr available from http://github.com/stephens999/ashr."}, {"title": "Efficient cardinality estimation for k-mers in large DNA sequencing data sets", "url": "https://www.biorxiv.org/content/early/2016/06/07/056846", "tag": "Bioinformatics", "abstract": "We present an open implementation of the HyperLogLog cardinality estimation sketch for counting fixed-length substrings of DNA strings (k-mers). The HyperLogLog sketch implementation is in C++ with a Python interface, and is distributed as part of the khmer software package. khmer is freely available from \\url{https://github.com/dib-lab/khmer} under a BSD License. The features presented here are included in version 1.4 and later."}, {"title": "Estimating the functional impact of INDELs in transcription factor binding sites: a genome-wide landscape", "url": "https://www.biorxiv.org/content/early/2016/06/07/057604", "tag": "Bioinformatics", "abstract": "Background: Variants in transcription factor binding sites (TFBSs) may have important regulatory effects, as they have the potential to alter transcription factor (TF) binding affinities and thereby affecting gene expression. With recent advances in sequencing technologies the number of variants identified in TFBSs has increased, hence understanding their role is of significant interest when interpreting next generation sequencing data. Current methods have two major limitations: they are limited to predicting the functional impact of single nucleotide variants (SNVs) and often rely on additional experimental data, laborious and expensive to acquire. We propose a purely bioinformatic method that addresses these two limitations while providing comparable results. Results: Our method uses position weight matrices and a sliding window approach, in order to account for the sequence context of variants, and scores the consequences of both SNVs and INDELs in TFBSs. We tested the accuracy of our method in two different ways. Firstly, we compared it to a recent method based on DNase I hypersensitive sites sequencing (DHS-seq) data designed to predict the effects of SNVs: we found a significant correlation of our score both with their DHS-seq data and their prediction model. Secondly, we called INDELs on publicly available DHS-seq data from ENCODE, and found our score to represent well the experimental data. We concluded that our method is reliable and we used it to describe the landscape of variation in TFBSs in the human genome, by scoring all variants in the 1000 Genomes Project Phase 3. Surprisingly, we found that most insertions have neutral effects on binding sites, while deletions, as expected, were found to have the most severe TFBS-scores. We identified four categories of variants based on their TFBS-scores and tested them for enrichment of variants classified as pathogenic, benign and protective in ClinVar: we found that the variants with the most negative TFBS-scores have the most significant enrichment for pathogenic variants. Conclusions: Our method addresses key shortcomings of currently available bioinformatic tools in predicting the effects of INDELs in TFBSs, and provides an unprecedented window into the genome-wide landscape of INDELs, their predicted influences on TF binding, and potential relevance for human diseases. We thus offer an additional tool to help prioritising non-coding variants in sequencing studies."}, {"title": "Phylogenetic and structural analyses reveal the determinants of DNA binding specificities of nucleoid-associated proteins HU and IHF", "url": "https://www.biorxiv.org/content/early/2016/06/07/057489", "tag": "Bioinformatics", "abstract": "Nucleoid-associated proteins (NAPs) are chromosome-organizing factors, which affect the transcriptional landscape of a bacterial cell. HU is an NAP, which binds to DNA with a broad specificity while homologous IHF (Integration Host Factor), binds DNA with moderately higher specificity. Specificity and differential binding affinity of HU/IHF proteins towards their target binding sites play a crucial role in their regulatory dynamics. Decades of biochemical and genomic studies have been carried out for HU and IHF like proteins. Yet, questions related to their DNA binding specificity, and differential ability to bend DNA thus affecting the binding site length remained unanswered. In addition, the problem has not been investigated from an evolutionary perspective. Our phylogenetic analysis revealed three major clades belonging to HU, IHF\u03b1 and IHF\u03b2 like proteins with reference to E. coli. We carried out a comparative analysis of three-dimensional structures of HU/IHF proteins to gain insight into the structural basis of clade division. The present study revealed three major features which contribute to differential DNA binding specificity of HU/IHF proteins, I) conformational restriction of DNA binding residues due to salt-bridge formation II) the enrichment of alanine in the DNA binding site increasing conformational space of flexible side chains in its vicinity and III) nature of DNA binding residue (Arg to Lys bias in different clades) which interacts differentially to DNA bases. Differences in the dimer stabilization strategies between HU and IHF were also observed. Our analysis reveals a comprehensive evolutionary picture, which rationalizes the origin of multi-specificity of HU/IHF proteins using sequence and structure-based determinants, which could also be applied to understand differences in binding specificities of other nucleic acid binding proteins."}, {"title": "cgCorrect: A method to correct for confounding cell-cell variation due to cell growth in single-cell transcriptomics", "url": "https://www.biorxiv.org/content/early/2016/06/06/057463", "tag": "Bioinformatics", "abstract": "Motivation: Accessing gene expression at the single cell level has unraveled often large heterogeneity among seemingly homogeneous cells, which remained obscured in traditional population based approaches. The computational analysis of single-cell transcriptomics data, however, still imposes unresolved challenges with respect to normalization, visualization and modeling the data. One such issue are differences in cell size, which introduce additional variability into the data, for which appropriate normalization techniques are needed. Otherwise, these differences in cell size may obscure genuine heterogeneities among cell populations and lead to overdispersed steady-state distributions of mRNA transcript numbers. Results: We present cgCorrect, a statistical framework to correct for differences in cell size that are due to cell growth in single-cell transcriptomics data. We derive the probability for the cell growth corrected mRNA transcript number given the measured, cell size dependent mRNA transcript number, based on the assumption that the average number of transcripts in a cell increases proportional to the cell's volume during cell cycle. cgCorrect can be used for both data normalization, and to analyze steady-state distributions used to infer the gene expression mechanism. We demonstrate its applicability on both simulated data and single-cell quantitative real-time PCR data from mouse blood stem and progenitor cells. We show that correcting for differences in cell size affects the interpretation of the data obtained by typically performed computational analysis."}, {"title": "Lepbase: the Lepidopteran genome database", "url": "https://www.biorxiv.org/content/early/2016/06/06/056994", "tag": "Bioinformatics", "abstract": "As the generation and use of genomic datasets is becoming increasingly common in all areas of biology, the need for resources to collate, analyse and present data from independent (Tier 1) species-level genome projects into well supported clade-oriented (Tier 2) databases and provide a mechanism for these data to be propagated to pan-taxonomic (Tier 3) databases is becoming more pressing. Lepbase is a Tier 2 genomic resource for the Lepidoptera, supporting a research community using genomic approaches to understand evolution, speciation, olfaction, behaviour and pesticide resistance in a wide range of target species. Lepbase offers a core set of tools to make genomic data widely accessible including an Ensembl genome browser, text and sequence homology searches and bulk downloads of consistently presented and formatted datasets. As a part of the taxonomic community that we serve, we are working directly with Lepidoptera researchers to prioritise analyses and add tools that will be of most value to current research questions."}, {"title": "fluff: exploratory analysis and visualization of high-throughput sequencing data", "url": "https://www.biorxiv.org/content/early/2016/06/06/045526", "tag": "Bioinformatics", "abstract": "In this application note we describe fluff, a software package that allows for simple exploration, clustering and visualization of high-throughput sequencing data mapped to a reference genome. The package contains three command-line tools to generate publication-quality figures in an uncomplicated manner using sensible defaults. Genome-wide data can be aggregated, clustered and visualized in a heatmap, according to different clustering methods. This includes a predefined setting to identify dynamic clusters between different conditions or developmental stages. Alternatively, clustered data can be visualized in a bandplot. Finally, fluff includes a tool to generate genomic profiles. As command-line tools, the fluff programs can easily be integrated into standard analysis pipelines. The installation is straightforward and documentation is available at http://fluff.readthedocs.org."}, {"title": "Active modules for multilayer weighted gene co-expression networks: a continuous optimization approach", "url": "https://www.biorxiv.org/content/early/2016/06/03/056952", "tag": "Bioinformatics", "abstract": "Searching for active connected subgraphs in biological networks has shown important to identifying functional modules. Most existing active modules identification methods need both network structural information and gene activity measures, typically requiring prior knowledge database and high-throughput data. As a pure data-driven gene network, weighted gene co-expression network (WGCN) could be constructed only from expression profile. Searching for modules on WGCN thus has potential values. While traditional clustering based modules detection on WGCN method covers all genes, unavoidable introducing many uninformative ones when annotating modules. We need to find more accurate part of them."}, {"title": "Phased Diploid Genome Assembly with Single Molecule Real-Time Sequencing", "url": "https://www.biorxiv.org/content/early/2016/06/03/056887", "tag": "Bioinformatics", "abstract": "While genome assembly projects have been successful in a number of haploid or inbred species, one of the current main challenges is assembling non-inbred or rearranged heterozygous genomes. To address this critical need, we introduce the open-source FALCON and FALCON-Unzip algorithms (https://github.com/PacificBiosciences/FALCON/) to assemble Single Molecule Real-Time (SMRT(R)) Sequencing data into highly accurate, contiguous, and correctly phased diploid genomes. We demonstrate the quality of this approach by assembling new reference sequences for three heterozygous samples, including an F1 hybrid of the model species Arabidopsis thaliana, the widely cultivated V. vinifera cv. Cabernet Sauvignon, and the coral fungus Clavicorona pyxidata that have challenged short-read assembly approaches. The FALCON-based assemblies were substantially more contiguous and complete than alternate short or long-read approaches. The phased diploid assembly enabled the study of haplotype structures and heterozygosities between the homologous chromosomes, including identifying widespread heterozygous structural variations within the coding sequences."}, {"title": "MODA: MOdule Differential Analysis for weighted gene co-expression network", "url": "https://www.biorxiv.org/content/early/2016/06/03/053496", "tag": "Bioinformatics", "abstract": "Gene co-expression network differential analysis is designed to help biologists understand gene expression patterns under different conditions. We have implemented an R package called MODA (Module Differential Analysis) for gene co-expression network differential analysis. Based on transcriptomic data, MODA can be used to estimate and construct condition-specific gene co-expression networks, and identify differentially expressed subnetworks as conserved or condition specific modules which are potentially associated with relevant biological processes. The usefulness of the method is also demonstrated by synthetic data as well as Daphnia magna gene expression data under different environmental stresses."}, {"title": "Fast and Robust Segmentation of Copy Number Profiles Using Multi-Scale Edge Detection", "url": "https://www.biorxiv.org/content/early/2016/06/02/056705", "tag": "Bioinformatics", "abstract": "Raw copy number data is highly dimensional, noisy and can suffer from so-called genomic wave artifacts. We introduce a novel method based on multi-scale edge detection in derivative space. By using derivatives, the algorithm was very fast and robust against genomic waves. Our method compared very well to existing state-of-the-art segmentation methods and importantly outperformed these if noise and wave artifacts were well present."}, {"title": "Statistical inference of protein structural alignments using information and compression", "url": "https://www.biorxiv.org/content/early/2016/06/02/056598", "tag": "Bioinformatics", "abstract": "Structural molecular biology depends crucially on computational techniques that compare protein three-dimensional structures and generate structural alignments (the assignment of one-to-one correspondences between subsets of amino acids based on atomic coordinates.) Despite its importance, the structural alignment problem has not been formulated, much less solved, in a consistent and reliable way. To overcome these difficulties, we present here a framework for precise inference of structural alignments, built on the Bayesian and information-theoretic principle of Minimum Message Length (MML). The quality of any alignment is measured by its explanatory power -- the amount of lossless compression achieved to explain the protein coordinates using that alignment. We have implemented this approach in the program MMLigner (http://lcb.infotech.monash.edu.au/mmligner) to distinguish statistically significant alignments, not available elsewhere. We also demonstrate the reliability of MMLigner's alignment results compared with the state of the art. Importantly, MMLigner can also discover different structural alignments of comparable quality, a challenging problem for oligomers and protein complexes."}, {"title": "Digital dissection of arsenate reductase enzyme from an arsenic hyperccumulating fern Pteris vittata", "url": "https://www.biorxiv.org/content/early/2016/06/01/056036", "tag": "Bioinformatics", "abstract": "Action of arsenate reductase is crucial for the survival of an organism in arsenic polluted area. Pteris vittata, also known as Chinese ladder brake, was the first identified arsenic hyperaccumulating fern with the capability to convert [As(V)] to arsenite [As(III)]. This study aims at sequence analysis of the most important protein of the arsenic reduction mechanism in this specie. Phosphorylation potential of the protein along with possible interplay of phosphorylation with O-beta-GlcNAcylation was predicted using neural network based webservers. Secondary and tertiary structure of arsenate reductase was then analysed. Active site region of the protein comprised a rhodanese-like domain. Cursory dynamics simulation revealed that folds remained conserved in the rhodanese main but variations were observed in the structure in other regions. This information sheds light on the various characteristics of the protein and may be useful to enzymologists working on the improvement of its traits for arsenic reduction."}, {"title": "The Wnt segment polarity pathway and p24 protein TMED2 interact via a lectin- and decoy-type mechanism", "url": "https://www.biorxiv.org/content/early/2016/06/01/056531", "tag": "Bioinformatics", "abstract": "The Sec14-like protein (280-385 GOLD-domain) in this study scores highly with the carbohydrate binding module (CBM) of the RetS domain from Pseudomonas aeruginosa. The thereupon modeled Cricetulus griseus p24-GOLD domain of p24, a member of intra-Golgi cargo receptors, is shown to interact with Wnt8 (wingless 8) of Xenopus laevis with a \u0394G=-18.3 kcal/mol. Lower ranked models listed a smaller \u0394G (PDBePISA) and energy of DelPhi interaction. Complex/hybrid N-glycans provide increasing energy of binding up to -7.1 kcal/mol to simulated p24-GOLD-ligand interaction. It is likely, that Wnt proteins and p24 cargo-receptors interact analogously to Wnt-Frizzled and that Wnt transport may involve early lectin binding. The possibly promiscuous interaction of p24-GOLD with ligands, including collagen, may shed light on cargo-receptor mediated traffic."}, {"title": "Breaking bud: probing the scalability limits of phylogenetic network inference methods", "url": "https://www.biorxiv.org/content/early/2016/06/01/056572", "tag": "Bioinformatics", "abstract": "Background: Branching events in phylogenetic trees reflect strictly bifurcating and/or multifurcating speciation and splitting events. In the presence of gene flow, a phylogeny cannot be described by a tree but is instead a directed acyclic graph known as a phylogenetic network. Both phylogenetic trees and networks are typically reconstructed using computational analysis of multi-locus sequence data. The advent of high-throughput sequencing technologies has brought about two main scalability challenges: (1) dataset size in terms of the number of taxa and (2) the evolutionary divergence of the taxa in a study. The impact of both dimensions of scale on phylogenetic tree inference has been well characterized by recent studies; in contrast, the scalability limits of phylogenetic network inference methods are largely unknown. In this study, we quantify the performance of state-of-the-art phylogenetic network inference methods on large-scale datasets using empirical data sampled from natural mouse populations and synthetic data capturing a wide range of evolutionary scenarios. Results: We find that, as in the case of phylogenetic tree inference, the performance of leading network inference methods is negatively impacted by both dimensions of dataset scale. In general, we found that topological accuracy degrades as the number of taxa increases; a similar effect was observed with increased sequence mutation rate. The most accurate methods were probabilistic inference methods which maximize either likelihood under coalescent-based models or pseudo-likelihood approximations to the model likelihood. Furthermore, probabilistic inference methods with optimization criteria which did not make use of gene tree root and/or branch length information performed best - a result that runs contrary to widely held assumptions in the literature. The improved accuracy obtained with probabilistic inference methods comes at a computational cost in terms of runtime and main memory usage, which quickly become prohibitive as dataset size grows past thirty taxa. Conclusions: We conclude that the state of the art of phylogenetic network inference lags well behind the scope of current phylogenomic studies. New algorithmic development is critically needed to address this methodological gap."}, {"title": "Aequatus: An open-source homology browser", "url": "https://www.biorxiv.org/content/early/2016/06/01/055632", "tag": "Bioinformatics", "abstract": "Abstract Background: The phylogenetic information inferred from the study of homologous genes helps us to understand the evolution of gene families. The study of homology plays a vital role in finding ancestral gene duplication events as well as identifying regions that are under positive selection within species. Conservation of homologous loci results in syntenic blocks, and there are various tools available to visualise syntenic information between species. These tools provide an overview of syntenic regions as a whole, reaching down to the gene level, but none provide any information about structural changes within genes such as the conservation of ancestral exon boundaries amongst multiple genomes. Findings: We present Aequatus, a standalone web-based tool that provides an in-depth view of gene structure across gene families, with various options to render and filter visualisations. It relies on pre-calculated alignment and gene feature information held in an Ensembl database, typically generated through the Ensembl Compara workflow. We also offer Aequatus.js, a reusable JavaScript module that fulfils the visualisation aspects of Aequatus. Availability: Aequatus is an open-source tool freely available to download under GPLv3 license at https://github.com/TGAC/Aequatus and a demo is available at http://aequatus.tgac.ac.uk"}, {"title": "Genotyping of Inversions and Tandem Duplications", "url": "https://www.biorxiv.org/content/early/2016/06/01/056432", "tag": "Bioinformatics", "abstract": "Motivation: Next Generation Sequencing (NGS) has enabled studying structural genomic variants (SVs) such as duplications and inversions in large cohorts. SVs have been shown to play important roles in multiple diseases, including cancer. As costs for NGS continue to decline and variant databases become ever more complete, the relevance of genotyping also SVs from NGS data increases steadily, which is in stark contrast to the lack of tools to do so. Results: We introduce a novel statistical approach, called DIGTYPER (Duplication and Inversion GenoTYPER), which computes genotype likelihoods for a given inversion or duplication and reports the maximum likelihood genotype. In contrast to purely coverage-based approaches, DIGTYPER uses breakpoint-spanning read pairs as well as split alignments for genotyping, enabling typing also of small events. We tested our approach on simulated and on real data and compared the genotype predictions to those made by DELLY, which discovers SVs and computes genotypes. DIGTYPER compares favorable especially for duplications (of all lengths) and for shorter inversions (up to 300 bp). In contrast to DELLY, our approach can genotype SVs from data bases without having to rediscover them."}, {"title": "RAFTS3: Rapid Alignment-Free Tool for Sequence Similarity Search", "url": "https://www.biorxiv.org/content/early/2016/05/31/055269", "tag": "Bioinformatics", "abstract": "Similarity search of a given protein sequence against a database is an essential task in genome analysis. Sequence alignment is the most used method to perform such analysis. Although this approach is efficient, the time required to perform searches against large databases is always a challenge. Alignment-free techniques offer alternatives to comparing sequences without the need of alignment. We developed RAFTS3, a fast protein similarity search tool that utilizes a filter step for candidate selection based on shared k-mers and a comparison measure using a binary matrix of co-occurrence of amino acid residues. RAFTS3 performed searches many times faster than those with BLASTp against large protein databases, such as NR, Pfam or UniRef, with a small loss of sensitivity depending on the similarity degree of the sequences. RAFTS3 is a new alternative for fast comparison of protein sequences, genome annotation and biological data mining. The source code and the standalone files for Windows and Linux platform are available at: https://sourceforge.net/projects/rafts3/"}, {"title": "The Lair: A resource for exploratory analysis of published RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2016/05/31/056200", "tag": "Bioinformatics", "abstract": "Increased emphasis on reproducibility of published research in the last few years has led to the large-scale archiving of sequencing data. While this data can, in theory, be used to reproduce results in papers, it is typically not easily usable in practice. We introduce a series of tools for processing and analyzing RNA-Seq data in the Short Read Archive, that together have allowed us to build an easily extendable resource for analysis of data underlying published papers. Our system makes the exploration of data easily accessible and usable without technical expertise. Our database and associated tools can be accessed at The Lair: http://pachterlab.github.io/lair"}, {"title": "Signatures of personality on dense 3D facial images", "url": "https://www.biorxiv.org/content/early/2016/05/30/055590", "tag": "Bioinformatics", "abstract": "It has long been speculated that there exist cues on human face that allow observers to make reliable judgments of others' personality traits. However, direct evidences of association between facial shapes and personality are missing. This study assessed the personality attributes for 834 Han Chinese volunteers (405 males and 429 females) utilizing the five-factor personality model (the 'Big Five' model), and collected their neutral 3D facial images. Dense anatomical correspondence was established across the 3D facial images to allow high-dimensional quantitative analyses on the face phenotypes. Two different approaches, Composite Partial Least Square Component (CPLSC) and principle component analysis (PCA) were used to test the associations between the self-tested personality scores and the dense 3D face image data. Among the five personality factors, Agreeableness and Conscientiousness in male, and Extraversion in female were significantly associated to specific facial patterns. The personality-related facial patterns were extracted and their effects were extrapolated on simulated 3D facial models."}, {"title": "Choosing panels of genomics assays using submodular optimization", "url": "https://www.biorxiv.org/content/early/2016/05/27/036137", "tag": "Bioinformatics", "abstract": "Although the cost of high-throughput DNA sequencing continues to drop, extensively characterizing a given cell type using assays such as ChIP-seq and DNase-seq is still expensive. As a result, epigenomic characterization of a cell type is typically carried out using a small panel of assay types. Deciding a priori which assays to perform---e.g., a few complementary histone modification ChIP-seq experiments, perhaps an open chromatin assay, plus a few diverse transcription factor assays---is thus a critical step in many studies. Unfortunately, the field currently lacks a principled method for making these choices. We present submodular selection of assays (SSA), a method for choosing a diverse panel of genomic assays that leverages methods from the field of submodular optimization. We also describe a series of evaluation methods that allow us to measure the quality of a selected assay panel in the context of inference tasks such as data imputation, functional element prediction, and semi-automated genome annotation. Applying this evaluation framework to data from the ENCODE and Roadmap Epigenomics Consortia, we provide empirical evidence that SSA provides high quality panels of assays. The method is computationally efficient and is theoretically optimal under certain assumptions. SSA is extremely flexible, and can be employed to select assays for a new cell type or to select additional assays to be performed in a partially characterized cell type. More generally, this application serves as a model for how submodular optimization can be applied to other discrete problems in biology. SSA is available at http://melodi.ee.washington.edu/assay_panel_selection.html."}, {"title": "Boiler: Lossy compression of RNA-seq alignments using coverage vectors", "url": "https://www.biorxiv.org/content/early/2016/05/27/040634", "tag": "Bioinformatics", "abstract": "We describe Boiler, a new software tool for compressing and querying large collections of RNA-seq alignments. Boiler discards most per-read data, keeping only a genomic coverage vector plus a few empirical distributions summarizing the alignments. Since most per-read data is discarded, storage footprint is often much smaller than that achieved by other compression tools. Despite this, the most relevant per-read data can be recovered; we show that Boiler compression has only a slight negative impact on results given by downstream tools for isoform assembly and quantification. Boiler also allows the user to pose fast and useful queries without decompressing the entire file. Boiler is free open source software available from https://github.com/jpritt/boiler."}, {"title": "Improved insights into protein thermal stability: from the molecular to the structurome scale", "url": "https://www.biorxiv.org/content/early/2016/05/27/055897", "tag": "Bioinformatics", "abstract": "Despite the intense efforts of the last decades to understand the thermal stability of proteins, the mechanisms responsible for its modulation still remain debated. In this investigation, we tackle this issue by showing how a multi-scale perspective can yield new insights. With the help of temperature-dependent statistical potentials, we analyzed some amino acid interactions at the molecular level, which are suggested to be relevant for the enhancement of thermal resistance. We then investigated the thermal stability at the protein level by quantifying its modification upon amino acid substitutions. Finally, a large scale analysis of protein stability - at the structurome level - contributed to the clarification of the relation between stability and natural evolution, thereby showing that the mutational profile of thermostable and mesostable proteins differ. Some final considerations on how the multi-scale approach could help unraveling the protein stability mechanisms are briefly discussed."}, {"title": "PIPI: PTM-Invariant Peptide Identification Using Coding Method", "url": "https://www.biorxiv.org/content/early/2016/05/27/055806", "tag": "Bioinformatics", "abstract": "In computational proteomics, identification of peptides with an unlimited number of post-translational modification (PTM) types is a challenging task. The computational cost increases exponentially with respect to the number of modifiable amino acids and linearly with respect to the number of potential PTM types at each amino acid. The problem becomes intractable very quickly if we want to enumerate all possible modi- fication patterns. Existing tools (e.g., MS-Alignment, ProteinProspector, and MODa) avoid enumerating modification patterns in database search by using an alignment- based approach to localize and characterize modified amino acids. This approach avoids enumerating all possible modification patterns in a database search. However, due to the large search space and PTM localization issue, the sensitivity of these tools is low. This paper proposes a novel method named PIPI to achieve PTM-invariant peptide identification. PIPI first codes peptide sequences into Boolean vectors and converts experimental spectra into real-valued vectors. Then, it finds the top 10 peptide-coded vectors for each spectrum-coded vector. After that, PIPI uses a dynamic program- ming algorithm to localize and characterize modified amino acids. Simulations and real data experiments have shown that PIPI outperforms existing tools by identifying more peptide-spectrum matches (PSMs) and reporting fewer false positives. It also runs much faster than existing tools when the database is large."}, {"title": "Knowledge.Bio: A Web application for exploring, building and sharing webs of biomedical relationships mined from PubMed", "url": "https://www.biorxiv.org/content/early/2016/05/26/055525", "tag": "Bioinformatics", "abstract": "Knowledge.Bio is a web platform that enhances access and interpretation of knowledge networks extracted from biomedical research literature. The interaction is mediated through a collaborative graphical user interface for building and evaluating maps of concepts and their relationships, alongside associated evidence. In the first release of this platform, conceptual relations are drawn from the Semantic Medline Database and the Implicitome, two complementary resources derived from text mining of PubMed abstracts."}, {"title": "Accounting for tumor heterogeneity using a sample-specific error model improves sensitivity and specificity in mutation calling for sequencing data", "url": "https://www.biorxiv.org/content/early/2016/05/25/055467", "tag": "Bioinformatics", "abstract": "Subclonal mutations reveal important features of the genetic architecture of tumors. However, accurate detection of mutations in genetically heterogeneous tumor cell populations using NGS remains challenging. We developed MuSE (http://bioinformatics.mdanderson.org/main/MuSE), mutation calling using a Markov substitution model for evolution, a novel approach modeling the evolution of the allelic composition of the tumor and normal tissue at each reference base. MuSE adopts a sample-specific error model that reflects the underlying tumor heterogeneity to greatly improve overall accuracy. We demonstrate the accuracy of MuSE in calling subclonal mutations in the context of large-scale tumor sequencing projects using whole exome and whole genome sequence."}, {"title": "A cross-package Bioconductor workflow for analysing methylation array data", "url": "https://www.biorxiv.org/content/early/2016/05/25/055087", "tag": "Bioinformatics", "abstract": "Methylation in the human genome is known to be associated with development and disease. The Illumina Infinium methylation arrays are by far the most common way to interrogate methylation across the human genome. This paper provides a Bioconductor workflow using multiple packages for the analysis of methylation array data. Specifically, we demonstrate the steps involved in a typical differential methylation analysis workflow including: quality control, filtering, normalization, data exploration and statistical testing for probe-wise differential methylation. We further outline other analyses such as differential methylation of regions, differential variability analysis, estimating cell type composition and gene ontology testing. Finally, we provide some examples of how to visualise methylation array data."}, {"title": "Centrifuge: rapid and sensitive classification of metagenomic sequences", "url": "https://www.biorxiv.org/content/early/2016/05/25/054965", "tag": "Bioinformatics", "abstract": "Centrifuge is a novel microbial classification engine that enables rapid, accurate and sensitive labeling of reads and quantification of species on desktop computers. The system uses an indexing scheme based on the Burrows-Wheeler transform (BWT) and the Ferragina-Manzini (FM) index, optimized specifically for the metagenomic classification problem. Centrifuge requires a relatively small index (4.2 GB for 4,078 bacterial and 200 archaeal genomes) and classifies sequences at very high speed, allowing it to process the millions of reads from a typical high-throughput DNA sequencing run within a few minutes. Together these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers. Because of its space-optimized indexing schemes, Centrifuge also makes it possible to index the entire NCBI non-redundant nucleotide sequence database (a total of 109 billion bases) with an index size of 69 GB, in contrast to k-mer based indexing schemes, which require far more extensive space. Centrifuge is available as free, open-source software from http://www.ccb.jhu.edu/software/centrifuge."}, {"title": "APRICOT: an integrated computational pipeline for the sequence-based identification and characterization of RNA-binding proteins", "url": "https://www.biorxiv.org/content/early/2016/05/24/055178", "tag": "Bioinformatics", "abstract": "RNA-binding proteins (RBPs) have been established as core components of several post-transcriptional gene regulation mechanisms. Experimental techniques such as cross-linking and co-immunoprecipitation have enabled the identification of RBPs, RNA-binding domains (RBDs), and their regulatory roles in the eukaryotic species such as human and yeast in large-scale. In contrast, our knowledge of the number and potential diversity of RBPs in bacteria is poorer due to the technical challenges associated with the existing global screening approaches. We introduce APRICOT, a computational pipeline for the sequence-based identification and characterization of proteins using RBDs known from experimental studies. The pipeline identifies functional motifs in protein sequences using Position Specific Scoring Matrices and Hidden Markov Models of the functional domains and statistically scores them based on a series of sequence-based features. Subsequently, APRICOT identifies putative RBPs and characterizes them by several biological properties. Here we demonstrate the application and adaptability of the pipeline on large-scale protein sets, including the bacterial proteome of Escherichia coli. APRICOT showed better performance on various datasets compared to other existing tools for the sequence-based prediction of RBPs by achieving an average sensitivity and specificity of 0.90 and 0.91 respectively. The command-line tool and its documentation are available at https://pypi.python.org/pypi/bio-apricot."}, {"title": "Reference-free comparison of microbial communities via de Bruijn graphs", "url": "https://www.biorxiv.org/content/early/2016/05/24/055020", "tag": "Bioinformatics", "abstract": "Microbial communities inhabiting the human body exhibit significant variability across different individuals and tissues, and are suggested to play an important role in health and disease. High-throughput sequencing offers unprecedented possibilities to profile microbial community composition, but limitations of existing taxonomic classification methods (including incompleteness of existing microbial reference databases) limits the ability to accurately compare microbial communities across different samples. In this paper, we present a method able to overcome these limitations by circumventing the classification step and directly using the sequencing data to compare microbial communities. The proposed method provides a powerful reference-free way to assess differences in microbial abundances across samples. This method, called EMDeBruijn, condenses the sequencing data into a de Bruijn graph. The Earth Mover's Distance (EMD) is then used to measure similarities and differences of the microbial communities associated with the individual graphs. We apply this method to RNA-Seq data sets from a coronary artery calcification (CAC) study and shown that EMDeBruijn is able to differentiate between case and control CAC samples while utilizing all the candidate microbial reads. We compare these results to current reference-based methods, which are shown to have a limited capacity to discriminate between case and control samples. We conclude that this reference-free approach is a viable choice in comparative metatranscriptomic studies."}, {"title": "Improved Annotation with de novo Transcriptome Assembly in Four Social Amoeba Species", "url": "https://www.biorxiv.org/content/early/2016/05/24/054536", "tag": "Bioinformatics", "abstract": "Background: Annotation of gene models and transcripts is a fundamental step in genome sequencing projects. Often this is performed with automated prediction pipelines, which can miss complex and atypical genes or transcripts. RNA-seq data can aid the annotation with empirical data. Here we present de novo transcriptome assemblies generated from RNA-seq data in four Dictyostelid species: D. discoideum, P. pallidum, D. fasciculatum and D. lacteum. The assemblies were incorporated with existing gene models to determine corrections and improvement on a whole-genome scale. This is the first time this has been performed in these eukaryotic species. Results: An initial de novo transcriptome assembly was generated by Trinity for each species and then refined with Program to Assemble Spliced Alignments (PASA). The completeness and quality were assessed with the Core Eukaryotic Genes Mapping Approach (CEGMA) and Transrate tools at each stage of the assemblies. The final datasets of 11,315-12,849 transcripts contained 5,610-7,712 updates and corrections to >50% of existing gene models including changes to hundreds or thousands of protein products. Putative novel genes are also identified and alternative splice isoforms were observed for the first time in P. pallidum, D. lacteum and D. fasciculatum. Conclusions: In taking a whole transcriptome approach to genome annotation with empirical data we have been able to enrich the annotations of four existing genome sequencing projects. In doing so we have identified updates to the majority of the gene annotations across all four species under study and found putative novel genes and transcripts which could be worthy for follow-up. The new transcriptome data we present here will be a valuable resource for genome curators in the Dictyostelia and we propose this effective methodology for use in other genome annotation projects."}, {"title": "Beta regression improves the detection of differential DNA methylation for epigenetic epidemiology", "url": "https://www.biorxiv.org/content/early/2016/05/23/054643", "tag": "Bioinformatics", "abstract": "DNA methylation is the most readily assayed epigenetic mark, possessing confirmed relationships with gene expression, imprinting, and chromatin accessibility. Given the increasingly widespread use of DNA methylation microarrays in population-scale epidemiological applications, we sought to determine which methods provided the greatest statistical power to reproducibly detect differences in DNA methylation across various conditions, using publicly available data sets on tissue type and aging. Beta regression, as proposed originally by Ferrari and Cribari-Neto, yielded more validated hits in each of our comparisons than any other method under consideration, both in a regression setting and in comparisons to two-group tests such as the Wilcoxon-Mann-Whitney, Student t, and Welch t tests. In large cohorts of whole blood samples, we corrected for compositional differences and batch effects, and found that marginal likelihood ratio tests from beta regression models uniformly dominate popular alternatives based on linear models. The superior sensitivity and specificity exhibited by beta regression in epidemiologically relevant cohort sizes corresponded to approximately a 2% increase in sensitivity at the same specificity when compared to linear models fitted on raw beta values (proportion of signal intensity due to the methylated allele), M-values, or rank quantile normalized values. Investigators should consider beta regression to maximize statistical power in studies of DNA methylation using microarrays. At epidemiologically relevant sample sizes, with typical quality control procedures (compositional and batch effect correction), cross-cohort agreement uniformly favors beta regression over popular alternatives."}, {"title": "Scaffolding and Completing Genome Assemblies in Real-time with Nanopore Sequencing", "url": "https://www.biorxiv.org/content/early/2016/05/22/054783", "tag": "Bioinformatics", "abstract": "Genome assemblies obtained from short read sequencing technologies are often fragmented into many contigs because of the abundance of repetitive sequences. Long read sequencing technologies allow the generation of reads spanning most repeat sequences, providing the opportunity to complete these genome assemblies. However, substantial amounts of sequence data and computational resources are required to overcome the high per-base error rate inherent to these technologies. Furthermore, most existing methods only assemble the genomes after sequencing has completed which could result in either generation of more sequence data at greater cost than required or a low-quality assembly if insufficient data are generated. Here we present the first computational method which utilises real-time nanopore sequencing to scaffold and complete short-read assemblies while the long read sequence data is being generated. The method reports the progress of completing the assembly in real-time so users can terminate the sequencing once an assembly of sufficient quality and completeness is obtained. We use our method to complete four bacterial genomes and one eukaryotic genome, and show that it is able to construct more complete and more accurate assemblies, and at the same time, requires less sequencing data and computational resources than existing pipelines. We also demonstrate that the method can facilitate real-time analyses of positional information such as identification of bacterial genes encoded in plasmids and pathogenicity islands."}, {"title": "The Use of Informativity in the Development of Robust Metaviromics-based Examinations", "url": "https://www.biorxiv.org/content/early/2016/05/21/054635", "tag": "Bioinformatics", "abstract": "The field of metagenomics has developed insight into many of the complex microbial communities responsible for maintaining life on this planet. Sequencing efforts often uncover novel genetic content; this is most evident for viral metagenomics, in which upwards of 90% of all sequences demonstrate no sequence similarity with present databases. For the small fraction which can be identified, the top BLAST hit is often posited as being representative of the phage taxon. However, as previous research has shown, the top BLAST hit is sometimes misinterpreted. Furthermore, the appearance of a particular gene homolog is frequently not representative of the presence of the particular taxon in question. To circumvent these limitations, we have developed a new method for the analysis of metaviromic datasets. BLAST hits are weighted, integrating the sequence identity and length of alignments as well as a phylogenetic signal. A genic rather than genomic approach is presented in which each gene is evaluated with respect to its information content. Through this quantifiable metric, predictions of viral community structure can be made with greater confidence. As a proof-of-concept, the approach presented here was implemented and applied to seven metaviromes. While providing a more robust means of evaluating metaviromic data, the tool is versatile and can easily be customized to investigations of any environment or biome."}, {"title": "Arkas: Rapid and Reproducible RNAseq Analysis for End Users", "url": "https://www.biorxiv.org/content/early/2016/05/20/031435", "tag": "Bioinformatics", "abstract": "The recently introduced \\href{dx.doi.org/10.1038/nbt.3519}{Kallisto}\\cite{Bray} pseudoaligner has radically simplified the quantification of transcripts in RNA-sequencing experiments. However, as with all computational advances, reproducibility across experiments requires attention to detail. The elegant approach of Kallisto reduces dependencies, but we noted differences in quantification between versions of Kallisto, and both upstream preparation and downstream interpretation benefit from an environment that enforces a requirement for equivalent processing when comparing groups of samples. Therefore, we created the \\href{https://github.com/RamsinghLab/arkas}{Arkas}\\cite{Colombo} and \\href{https://github.com/RamsinghLab/TxDbLite}{TxDbLite}\\cite{TricheJr} R packages to meet these needs and to ease cloud-scale deployment of the above. TxDbLite extracts structured information directly from source FASTA files with per-contig metadata, while Arkas enforces versioning of the derived indices and annotations, to ensure tight coupling of inputs and outputs while minimizing external dependencies. The two packages are combined in Illumina's BaseSpace cloud computing environment to offer a massively parallel and distributed quantification step for power users, loosely coupled to biologically informative downstream analyses via gene set analysis (with special focus on Reactome annotations for ENSEMBL transcriptomes). Previous work (e.g. \\href{https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0862-3}{Soneson et al., 2016}\\cite{Soneson}) has revealed that filtering transcriptomes to exclude lowly-expressed isoforms can improve statistical power, while more-complete transcriptome assemblies improve sensitivity in detecting differential transcript usage. Based on earlier work by \\href{http://www.pnas.org/content/107/21/9546.full}{Bourgon et al., 2010}\\cite{Bourgon}, we included this type of filtering for both gene- and transcript-level analyses within Arkas. For reproducible and versioned downstream analysis of results, we focused our efforts on ENSEMBL and \\href{https://github.com/reactome}{Reactome}\\cite{Reactome} integration within the \\href{http://bioconductor.org/packages/release/bioc/html/qusage.html}{qusage}\\cite{Yaari} framework, adapted to take advantage of the parallel and distributed environment in Illumina's BaseSpace cloud platform. We show that quantification and interpretation of repetitive sequence element transcription is eased in both basic and clinical studies by just-in-time annotation and visualization. The option to retain pseudoBAM output for structural variant detection and annotation, while not insignificant in its demand for computation and storage, nonetheless provides a middle ground between \\emph{de novo} transcriptome assembly and routine quantification, while consuming a fraction of the resources used by popular fusion detection pipelines and providing options to quantify gene fusions with known breakpoints without reassembly. Finally, we describe common use cases where investigators are better served by cloud-based computing platforms such as BaseSpace due to inherent efficiencies of scale and enlightened common self-interest. Our experiences suggest a common reference point for methods development, evaluation, and experimental interpretation."}, {"title": "LMethyR-SVM: Predict human enhancers using low methylated regions based on weighted support vector machines", "url": "https://www.biorxiv.org/content/early/2016/05/19/054221", "tag": "Bioinformatics", "abstract": "Abstract Background: The identification of enhancer is a challenging task. Various types of epigenetic information including histone modification have been utilized in the construction of enhancer prediction models based on a diverse panel of machine learning models. However, DNA methylation profiles generated from the whole genome bisulfate sequencing (WGBS) have not been fully explored for their potential in enhancer prediction despite the fact that low methylated regions (LMRs) have been implied to be distal active regulatory regions. Method: In this work we propose a prediction framework, LMethyR-SVM, using LMRs identified from cell-type-specific WGBS DNA methylation profiles based on an unlabeled-negative learning framework. In LMethyR-SVM, the set of cell-type-specific LMRs is further divided into three sets: reliable positive, like positive, and likely negative, according to their resemblance to a small set of experimentally validated enhancers in the VISTA database based on an estimated non-parametric density distribution. Then, the prediction model is trained by solving a weighted support vector machine. Results: We demonstrate the performance of LMethyR-SVM by using the WGBS DNA methylation profiles derived from the H1 human embryonic stem cell type (H1) and the fetal lung fibroblast cell type (IMR90). The predicted enhancers are highly conserved with a reasonable validation rate based on a set of commonly used positive markers including transcription factors, p300 binding and DNase-I hypersensitive sites. In addition, we show evidence that the large fraction of LMethyR-SVM predicted enhancers are not predicted by ChromHMM in H1 cell type and they are more enriched for the FANTOM5 enhancers. Conclusion: Our work suggests that low methylated regions detected from the WGBS data are useful as complementary resources to histone modification marks in developing models for the prediction of cell type-specific enhancers."}, {"title": "Flexible expressed region analysis for RNA-seq with derfinder", "url": "https://www.biorxiv.org/content/early/2016/05/19/015370", "tag": "Bioinformatics", "abstract": "Background: Differential expression analysis of RNA sequencing (RNA-seq) data typically relies on reconstructing transcripts or counting reads that overlap known gene structures. We previously introduced an intermediate statistical approach called differentially expressed region (DER) finder that seeks to identify contiguous regions of the genome showing differential expression signal at single base resolution without relying on existing annotation or potentially inaccurate transcript assembly. Results We present the derfinder software that improves our annotation-agnostic approach to RNA-seq analysis by: (1) implementing a computationally efficient bump-hunting approach to identify DERs which permits genome-scale analyses in a large number of samples, (2) introducing a flexible statistical modeling framework, including multi-group and time-course analyses and (3) introducing a new set of data visualizations for expressed region analysis. We apply this approach to public RNA-seq data from the Genotype-Tissue Expression (GTEx) project and BrainSpan project to show that derfinder permits the analysis of hundreds of samples at base resolution in R, identifies expression outside of known gene boundaries and can be used to visualize expressed regions at base-resolution. In simulations our base resolution approaches enable discovery in the presence of incomplete annotation and is nearly as powerful as feature-level methods when the annotation is complete. Conclusions derfinder analysis using expressed region-level and single base-level approaches provides a compromise between full transcript reconstruction and feature-level analysis. The package is available from Bioconductor at www.bioconductor.org/packages/derfinder"}, {"title": "A comprehensive and scalable database search system for metaproteomics", "url": "https://www.biorxiv.org/content/early/2016/05/18/053975", "tag": "Bioinformatics", "abstract": "Background Mass spectrometry-based shotgun proteomics experiments rely on accurate matching of experimental spectra against a database of protein sequences. Existing computational analysis methods are limited in the size of their sequence databases, which severely restricts the proteomic sequencing depth and functional analysis of highly complex samples. The growing amount of public high-throughput sequencing data will only exacerbate this problem. We designed a broadly applicable metaproteomic analysis method (ComPIL) that addresses protein database size limitations. Results Our approach to overcome this significant limitation in metaproteomics was to design a scalable set of sequence databases assembled for optimal library querying speeds. ComPIL was integrated with a modified version of the search engine ProLuCID (termed \"Blazmass\") to permit rapid matching of experimental spectra. Proof-of-principle analysis of human HEK293 lysate with a ComPIL database derived from high-quality genomic libraries was able to detect nearly all of the same peptides as a search with a human database (~500x fewer peptides in the database), with a small reduction in sensitivity. We were also able to detect proteins from the adenovirus used to immortalize these cells. We applied our method to a set of healthy human gut microbiome proteomic samples and showed a substantial increase in the number of identified peptides and proteins compared to previous metaproteomic analyses, while retaining a high degree of protein identification accuracy, and allowing for a more in-depth characterization of the functional landscape of the samples. Conclusions The combination of ComPIL with Blazmass allows proteomic searches to be performed with database sizes much larger than previously possible. These large database searches can be applied to complex meta-samples with unknown composition or proteomic samples where unexpected proteins may be identified. The protein database, proteomics search engine, and the proteomic data files for the 5 microbiome samples characterized and discussed herein are open source and available for use and additional analysis."}, {"title": "PEDLA: predicting enhancers with a deep learning-based algorithmic framework", "url": "https://www.biorxiv.org/content/early/2016/05/18/036129", "tag": "Bioinformatics", "abstract": "Transcriptional enhancers are non-coding segments of DNA that play a central role in the spatiotemporal regulation of gene expression programs. However, systematically and precisely predicting enhancers on a genome-wide scale remain a major challenge. Although existing methods have achieved some success in enhancer prediction, they still suffer from a limited number of training samples, a simplicity of features, class-imbalanced data, and inconsistent performance across diverse cell types/tissues. Here, we developed a deep learning-based algorithmic framework named PEDLA (https://github.com/wenjiegroup/PEDLA), which can directly learn an enhancer predictor from massively heterogeneous data and generalize in ways that are mostly consistent across various cell types/tissues. We first trained PEDLA with 1,114-dimensional heterogeneous features in H1 cells, and we demonstrated that our PEDLA framework integrates diverse heterogeneous features and gives state-of-the-art performance relative to five existing methods for enhancer prediction. We further extended PEDLA to continuously learn from 22 training cell types/tissues, and the results showed that PEDLA manifested superior performance consistency in both training and independent test sets. On average, PEDLA achieved 95.0% accuracy and a 96.8% geometric mean (GM) across 22 training cell types/tissues, as well as 95.7% accuracy and a 96.8% GM across 20 independent test cell types/tissues. Together, our work illustrates the power of harnessing state-of-the-art deep learning techniques to consistently identify regulatory elements at a genome-wide scale from massively heterogeneous data across diverse cell types/tissues."}, {"title": "k-BOOM: A Bayesian approach to ontology structure inference, with applications in disease ontology construction", "url": "https://www.biorxiv.org/content/early/2016/05/18/048843", "tag": "Bioinformatics", "abstract": "One strategy for building ontologies covering domains such as disease or anatomy is to weave together existing knowledge sources (databases, vocabularies and ontologies) into single cohesive whole. A first step in this process is to generate mappings between the elements of these different sources. There are a number of well-known techniques for generating mappings, both manual and automatic. Sometimes mappings are seen as an end in themselves, with the sources remaining in a loosely connected state. However, if we want to take the next step and use the mappings to weave together the different sources into a cohesive reference ontology, then we need to translate the mappings into precise logical relationships. This will allow us to safely merge equivalent concepts, creating a unified ontology. This translation is a non-trivial step, as each mapping can be interpreted as multiple different logical relationships, with each interpretation affecting the likelihood of the others. There is a lack of automated methods to assist with this last step; this resolution is typically performed by expert ontologists. Here we describe an ontology construction technique that takes two or more ontologies linked by hypothetical axioms, and estimates the most likely unified logical ontology. Hypothetical axioms can themselves be derived from semantically loose mappings. The method combines deductive reasoning and probabilistic inference and is called Bayesian OWL Ontology Merging (BOOM). We describe a special form k-BOOM that works by factorizing the probabilistic ontology into k submodules. We also briefly describe a supplemental lexical and knowledge-based technique for generating a set of hypothetical axioms from loose mappings. We are currently using this technique to build a merged disease ontology (Monarch Disease Ontology; MonDO) that unifies a broad range of vocabularies into a consistent and coherent whole."}, {"title": "Protein complex prediction for large protein protein interaction networks with the Core&Peel Method", "url": "https://www.biorxiv.org/content/early/2016/05/17/053876", "tag": "Bioinformatics", "abstract": "Motivations. Biological networks play an increasingly important role in the exploration of functional modularity and cellular organization at a systemic level. Quite often the first tools used to analyze these networks are clustering algorithms. We concentrate here on the specific task of predicting protein complexes (PC) in large protein-protein interaction networks (PPIN). Currently, many state-of-the-art algorithms work well for networks of small or moderate size. However, their performance on much larger networks, which are becoming increasingly common in modern proteome-wise studies, needs to be re-assessed. Our aim is to push forward the state-of the-art in PPIN clustering providing an algorithmic solution with polynomial running time that attains experimentally demonstrable good output quality and speed on challenging large real networks. Results. We present a new fast algorithm for clustering large sparse networks: Core&Peel, which runs essentially in time and storage O(a(G)m+n) for a network G of n nodes and m arcs, where a(G) is the arboricity of G (which is roughly proportional to the maximum average degree of any induced subgraph in G). We evaluated Core&Peel on five PPI networks of large size and one of medium size from both yeast and homo sapiens, comparing its performance against those of ten state-of-the-art methods. We demonstrate that Core&Peel consistently outperforms the ten competitors in its ability to identify known protein complexes and in the functional coherence of its predictions. Our method is remarkably robust, being quite insensible to the injection of random interactions. Core&Peel is also empirically efficient attaining the second best running time over large networks among the tested algorithms. Availability: http://bioalgo.iit.cnr.it (via web interface) Contact: marco.pellegrini@iit.cnr.it"}, {"title": "EGAD: Ultra-fast functional analysis of gene networks", "url": "https://www.biorxiv.org/content/early/2016/05/17/053868", "tag": "Bioinformatics", "abstract": "Summary: Evaluating gene networks with respect to known biology is a common task but often a computationally costly one. Many computational experiments are difficult to apply exhaustively in network analysis due to run-times. To permit high-throughput analysis of gene networks, we have implemented a set of very efficient tools to calculate functional properties in networks based on guilt-by-association methods. EGAD (Extending 'Guilt-by-Association' by Degree) allows gene networks to be evaluated with respect to hundreds or thousands of gene sets. The methods predict novel members of gene groups, assess how well a gene network groups known sets of genes, and determines the degree to which generic predictions drive performance. By allowing fast evaluations, whether of random sets or real functional ones, EGAD provides the user with an assessment of performance which can easily be used in controlled evaluations across many parameters. Availability and Implementation: The software package is freely available at https://github.com/sarbal/EGAD and implemented for use in R and Matlab. The package is also freely available under the LGPL license from the Bioconductor web site (http://bioconductor.org) https://bioconductor.org/packages/release/bioc/html/EGAD.html."}, {"title": "Algorithmic Methods to Infer the Evolutionary Trajectories in Cancer Progression", "url": "https://www.biorxiv.org/content/early/2016/05/16/027359", "tag": "Bioinformatics", "abstract": "The genomic evolution inherent to cancer relates directly to a renewed focus on the voluminous next generation sequencing (NGS) data, and machine learning for the inference of explanatory models of how the (epi)genomic events are choreographed in cancer initiation and development. However, despite the increasing availability of multiple additional -omics data, this quest has been frustrated by various theoretical and technical hurdles, mostly stemming from the dramatic heterogeneity of the disease. In this paper, we build on our recent works on \"selective advantage\" relation among driver mutations in cancer progression and investigate its applicability to the modeling problem at the population level. Here, we introduce PiCnIc (Pipeline for Cancer Inference), a versatile, modular and customizable pipeline to extract ensemble-level progression models from cross-sectional sequenced cancer genomes. The pipeline has many translational implications as it combines state-of-the-art techniques for sample stratification, driver selection, identification of fitness-equivalent exclusive alterations and progression model inference. We demonstrate PiCnIc's ability to reproduce much of the current knowledge on colorectal cancer progression, as well as to suggest novel experimentally verifiable hypotheses."}, {"title": "AlmostSignificant: Simplifying quality control of high-throughput sequencing data.", "url": "https://www.biorxiv.org/content/early/2016/05/16/053702", "tag": "Bioinformatics", "abstract": "Motivation: The current generation of DNA sequencing technologies produce a large amount of data quickly. All of these data need to pass some form of quality control processing and checking before they can be used for any analysis. The large number of samples that are run through Illumina sequencing machines makes the process of quality control an onerous and time-consuming task that requires multiple pieces of information from several sources. Results: AlmostSignificant is an open-source platform for aggregating multiple sources of quality metrics as well as meta-data associated with DNA sequencing runs from Illumina sequencing machines. AlmostSignificant is a graphical platform to streamline the quality control of DNA sequencing data, to collect and store these data for future reference and to collect extra meta-data associated with the sequencing runs to check for errors and monitor the volume of data produced by the associated machines. AlmostSignificant has been used to track the quality of over 80 sequencing runs covering over 2500 samples produced over the last three years. Availability: The code and documentation for AlmostSignificant is freely available at https://github.com/bartongroup/AlmostSignificant."}, {"title": "gmos: Rapid detection of genome mosaicism over short evolutionary distances", "url": "https://www.biorxiv.org/content/early/2016/05/16/053694", "tag": "Bioinformatics", "abstract": "Prokaryotic and viral genomes are often altered by recombination and horizontal gene transfer. The existing methods for detecting recombination are primarily aimed at viral genomes or sets of loci, since the expensive computation of underlying statistical models often hinders the comparison of complete prokaryotic genomes. As an alternative, alignment-free solutions are more efficient, but cannot map (align) a query to subject genomes. To address this problem, we have developed gmos (Genome MOsaic Structure), a new program that determines the mosaic structure of query genomes when compared to a set of closely related subject genomes. The program first computes local alignments between query and subject genomes and then reconstructs the query mosaic structure by choosing the best local alignment for each query region. To accomplish the analysis quickly, the program mostly relies on pairwise alignments and constructs multiple sequence alignments over short overlapping subject regions only when necessary. This fine-tuned implementation achieves an efficiency comparable to an alignment-free tool. The program performs well for simulated and real data sets of closely related genomes and can be used for fast recombination detection; for instance, when a new prokaryotic pathogen is discovered. As an example, gmos was used to detect genome mosaicism in a pathogenic Enterococcus faecium strain compared to seven closely related genomes. The analysis took less than two minutes on a single 2.1 GHz processor. The output is available in fasta format and can be visualized using an accessory program, gmosDraw (freely available with gmos)."}, {"title": "Accuracy, speed and error tolerance of short DNA sequence aligners", "url": "https://www.biorxiv.org/content/early/2016/05/16/053686", "tag": "Bioinformatics", "abstract": "Aligning short DNA sequence reads to the genome is an early step in the processing of many types of genomics data, and impacts on the fidelity of downstream results. In this work, the accuracy, speed and tolerance to errors are evaluated in read of varied length for six commonly used mapping tools; BWA aln, BWA mem, Bowtie2, Soap2, Subread and STAR. The accuracy evaluation using Illumina-like simulated reads showed that accuracy varies by read length, but overall BWA aln was most accurate, followed by BWA mem and Bowtie2. BWA mem was most accurate with Ion Torrent-like read sets. STAR was at least 5 fold faster than Bowtie2 or BWA mem. BWA mem tolerated the highest density of mismatches and indels compared to other mappers. These data provide important accuracy and speed benchmarks for commonly used mapping software."}, {"title": "Synthesizer: Expediting synthesis studies from context-free data with natural language processing", "url": "https://www.biorxiv.org/content/early/2016/05/16/053629", "tag": "Bioinformatics", "abstract": "Today's low cost digital data provides unprecedented opportunities for scientific discovery from synthesis studies. For example, the medical field is revolutionizing patient care by creating personalized treatment plans based upon mining electronic medical records, imaging, and genomics data. Standardized annotations are essential to subsequent analyses for synthesis studies. However, accurately combining records from diverse studies requires tedious and error-prone human curation, posing a significant barrier to synthesis studies. We propose a novel natural language processing (NLP) algorithm, Synthesize, to merge data annotations automatically. Application to patient characteristics for diverse human cancers and ecological datasets demonstrates the accuracy of Synthesize in diverse scientific disciplines. This NLP approach is implemented in an open-source software package, Synthesizer. Synthesizer is a generalized, user-friendly system for error-free data merging."}, {"title": "VERSE: a versatile and efficient RNA-Seq read counting tool", "url": "https://www.biorxiv.org/content/early/2016/05/14/053306", "tag": "Bioinformatics", "abstract": "Motivation: RNA-Seq is a powerful technology that delivers digital gene expression data. To measure expression strength at the gene level, one popular approach is direct read counting after aligning the reads to a reference genome/transcriptome. HTSeq is one of the most popular ways of counting reads, yet its slow running speed of poses a bottleneck to many RNA-Seq pipelines. Gene level counting programs also lack a robust scheme for quantifying reads that map to non-exonic genomic features, such as intronic and intergenic regions, even though these reads are prevalent in most RNA-Seq data. Results: In this paper we present VERSE, an RNA-Seq read counting tool which builds upon the speed of featureCounts and implements the counting modes of HTSeq. VERSE is more than 30x faster than HTSeq when computing the same gene counts. VERSE also supports a hierarchical assignment scheme, which allows reads to be assigned uniquely and sequentially to different types of features according to user-defined priorities."}, {"title": "Ten Simple Rules for Taking Advantage of git and GitHub", "url": "https://www.biorxiv.org/content/early/2016/05/13/048744", "tag": "Bioinformatics", "abstract": "A 'Ten Simple Rules' guide to git and GitHub. We describe and provide examples on how to use these software to track projects, as users, teams and organizations. We document collaborative development using branching and forking, interaction between collaborators using issues and continuous integration and automation using, for example, Travis CI and codecov. We also describe dissemination and social aspects of GitHub such as GitHub pages, following and watching repositories, and give advice on how to make code citable."}, {"title": "MetaGxData: Breast and Ovarian Clinically Annotated Transcriptomics Datasets", "url": "https://www.biorxiv.org/content/early/2016/05/12/052910", "tag": "Bioinformatics", "abstract": "A wealth of transcriptomic and clinical data on breast and ovarian cancers are under-utilized due to unharmonized data storage and format. We have developed the MetaGxData package compendium, which includes manually-curated and standardized clinical, pathological, survival, and treatment metadata across both breast and ovarian cancer microarray data. MetaGxData is the largest compendium of breast and ovarian microarray data to date, spanning 65 datasets and encompassing 13,756 samples. Standardization of metadata across the two cancer types promotes the use of their expression datasets in a variety of cross-tumour analyses, including identification of common biomarkers, establishing common patterns of co-expression networks, assessing the validity of prognostic signatures, and the identification of new consensus signatures that reflects upon common biological mechanisms. Here, we present our flexible framework, unified nomenclature, as well as applications that demonstrate the analytical power that is harnessed by combining breast and ovarian cancer datasets."}, {"title": "Deep Machine Learning provides state-of-the-art performance in image-based plant phenotyping", "url": "https://www.biorxiv.org/content/early/2016/05/12/053033", "tag": "Bioinformatics", "abstract": "Deep learning is an emerging field that promises unparalleled results on many data analysis problems. We show the success offered by such techniques when applied to the challenging problem of image-based plant phenotyping, and demonstrate state-of-the-art results for root and shoot feature identification and localisation. We predict a paradigm shift in image-based phenotyping thanks to deep learning approaches."}, {"title": "3D perception of maximum density zone on Ramachandran plots for Zika virus protein structures", "url": "https://www.biorxiv.org/content/early/2016/05/12/053074", "tag": "Bioinformatics", "abstract": "The Ramachandran plot is among the most central concepts in structural biology which uses torsion angles to describe polypeptide and protein conformation. To help visualize the features of high-fidelity Ramachandran plots, it is helpful to look beyond the common two-dimensional psi-phi-plot, which for a large dataset does not serve very well to convey the true nature of the distribution. In particular, when a large subset of the observations is found very narrowly distributed within one small region, this is not well seen in the simple plot because the data points congest one another. Zika Virus (ZIKV) protein databank has been chosen as specimen for analysis. This is because the structure, tropism, and pathogenesis of ZIKV are largely unknown and are the focus of current investigations in an effort to address the need for rapid development of vaccines and therapeutics. After a brief survey on Zika Virus, it is shown that when a dense dataset of ZIKV protein databank is passed through a colour-coded scaled algorithm, a three dimensional plot gets generated which gives a much more compelling impression of the proportions of residues in the different parts of the protein rather than representing it in a normal two dimensional psi-phi plot."}, {"title": "Maftools: Efficient analysis, visualization and summarization of MAF files from large-scale cohort based cancer studies.", "url": "https://www.biorxiv.org/content/early/2016/05/11/052662", "tag": "Bioinformatics", "abstract": "Mutation Annotation Format (MAF) has become a standard file format for storing somatic/germline variants derived from sequencing of large cohort of cancer samples. MAF files contain a list of all variants detected in a sample along with various annotations associated with the putative variant. MAF file forms the basis for many downstream analyses and provides complete landscape of the cohort. Here we introduce maftools - an R package that provides rich source of functions for performing various analyses, visualizations and summarization of MAF files. Maftools uses data.table library for faster processing/summarization and ggplot2 for generating rich and publication quality visualizations. Maftools also takes advantages of S4 class system for better data representation, with easy to use and flexible functions. Availability and Implementation: maftools is implemented as an R package available at https://github.com/PoisonAlien/maftools Contact: csiamt@nus.edu.sg"}, {"title": "Robust high throughput prokaryote de novo assembly and improvement pipeline for Illumina data", "url": "https://www.biorxiv.org/content/early/2016/05/11/052688", "tag": "Bioinformatics", "abstract": "The rapidly reducing cost of bacterial genome sequencing has lead to its routine use in large scale microbial analysis. Though mapping approaches can be used to find differences relative to the reference, many bacteria are subject to constant evolutionary pressures resulting in events such as the loss and gain of mobile genetic elements, horizontal gene transfer through recombination and genomic rearrangements. De novo assembly is the reconstruction of the underlying genome sequence, an essential step to understanding bacterial genome diversity. Here we present a high throughput bacterial assembly and improvement pipeline that has been used to generate nearly 20,000 draft genome assemblies in public databases. We demonstrate its performance on a public data set of 9,404 genomes. We find all the genes used in MLST schema present in 99.6% of assembled genomes. When tested on low, neutral and high GC organisms, more than 94% of genes were present and completely intact. The pipeline has proven to be scalable and robust with a wide variety of datasets without requiring human intervention. All of the software is available on GitHub under the GNU GPL open source license."}, {"title": "iVirus: facilitating new insights in viral ecology with software and community datasets imbedded in a cyberinfrastructure", "url": "https://www.biorxiv.org/content/early/2016/05/10/052597", "tag": "Bioinformatics", "abstract": "Microbes impact nutrient and energy transformations throughout the world's ecosystems, yet they do so under viral constraints. In complex communities, viral metagenome (virome) sequencing is transforming our ability to quantify viral diversity and impacts. While some bottlenecks, e.g., few reference genomes and non-quantitative viromics, have been overcome, the void of centralized datasets and specialized tools now prevents viromics from being broadly applied to answer fundamental ecological questions. Here we present iVirus, a community resource that leverages the CyVerse cyberinfrastructure to provide access to viromic tools and datasets. The iVirus Data Commons contains both raw and processed data from 1866 samples and 73 projects derived from global ocean expeditions, as well as existing and legacy public repositories. Through the CyVerse Discovery Environment, users can interrogate these datasets using existing analytical tools (software applications known as \"Apps\") for assembly, ORF prediction, and annotation, as well as several new Apps specifically developed for analyzing viromes. Because Apps are web-based and powered by CyVerse super-computing resources, they enable scalable analyses for a broad user base. Finally, a use-case scenario documents how to apply these advances towards new data. This growing iVirus resource should help researchers utilize viromics as yet another tool to elucidate viral roles in nature."}, {"title": "myCircos: Facilitating the Creation and Use of Circos Plots Online", "url": "https://www.biorxiv.org/content/early/2016/05/10/052605", "tag": "Bioinformatics", "abstract": "Circos plots were designed to display large amounts of processed genomic information on a single graphical representation. The creation of such plots remains challenging for less technical users as the leading tool requires command-line proficiency. Here, we introduce myCircos, a web application that facilitates the generation of Circos plots by providing an intuitive user interface, adding interactive functionalities to the representation and providing persistence of previous requests. myCircos is available at: http://mycircos.iric.ca. Non registered users can explore the application through the Guest user. Source code (for local server installation) is available upon request."}, {"title": "UMI-tools: Modelling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy", "url": "https://www.biorxiv.org/content/early/2016/05/10/051755", "tag": "Bioinformatics", "abstract": "Unique Molecular Identifiers (UMIs) are random oligonucleotide barcodes that are increasingly used in high-throughout sequencing experiments. Through a UMI, identical copies arising from distinct molecules can be distinguished from those arising through PCR amplification of the same molecule. However, bioinformatic methods to leverage the information from UMIs have yet to be formalised. In particular, sequencing errors in the UMI sequence are often ignored, or else resolved in an ad-hoc manner. We show that errors in the UMI sequence are common and introduce network based methods to account for these errors when identifying PCR duplicates. Using these methods, we demonstrate improved quantification accuracy both under simulated conditions and real iCLIP and single cell RNA-Seq datasets. Reproducibility between iCLIP replicates and single cell RNA Seq clustering are both improved using our proposed network-based method, demonstrating the value of properly accounting for errors in UMIs. These methods are implemented in the open source UMI-tools software package (https://github.com/CGATOxford/UMI-tools)."}, {"title": "Improved Placement of Multi-Mapping Small RNAs", "url": "https://www.biorxiv.org/content/early/2016/05/09/044099", "tag": "Bioinformatics", "abstract": "High-throughput sequencing of small RNAs (sRNA-seq) is a popular method used to discover and annotate microRNAs (miRNAs), endogenous short interfering RNAs (siRNAs) and Piwi-associated RNAs (piRNAs). One of the key steps in sRNA-seq data analysis is alignment to a reference genome. sRNA-seq libraries often have a high proportion of reads which align to multiple genomic locations, which makes determining their true origins difficult. Commonly used sRNA-seq alignment methods result in either very low precision (choosing an alignment at random) or sensitivity (ignoring multi-mapping reads). Here, we describe and test an sRNA-seq alignment strategy that uses local genomic context to guide decisions on proper placements of multi-mapped sRNA-seq reads. Tests using simulated sRNA-seq data demonstrated that this local-weighting method outperforms other alignment strategies using three different plant genomes. Experimental analyses with real sRNA-seq data also indicate superior performance of local-weighting methods for both plant miRNAs and heterochromatic siRNAs. The local-weighting methods we have developed are implemented as part of the sRNA-seq analysis program ShortStack, which is freely available under a general public license. Improved genome alignments of sRNA-seq data should increase the quality of downstream analyses and genome annotation efforts."}, {"title": "GeMSTONE: Orchestrated Prioritization of Human Germline Mutations in the Cloud", "url": "https://www.biorxiv.org/content/early/2016/05/09/052001", "tag": "Bioinformatics", "abstract": "Integrative analysis of whole-genome/exome-sequencing data has been challenging, especially for the non-programming research community, as it requires leveraging an inordinate number of computational tools. Even computational biologists find it unexpectedly difficult to reproduce results from others or optimize their own strategies in an end-to-end workflow. We introduce Germline Mutation Scoring Tool fOr Next-generation sEquencing data (GeMSTONE), a cloud- based variant prioritization tool with high-level customization and a comprehensive collection of bioinformatics tools and data libraries (http://gemstone.yulab.org/). GeMSTONE generates and readily accepts a sharable 'recipe' file for each run to either replicate existing results or analyze new data with identical parameters."}, {"title": "HapIso : An Accurate Method for the Haplotype-Specific Isoforms Reconstruction from Long Single-Molecule Reads", "url": "https://www.biorxiv.org/content/early/2016/05/09/050906", "tag": "Bioinformatics", "abstract": "Sequencing of RNA provides the possibility to study an individual's transcriptome landscape and determine allelic expression ratios. Single-molecule protocols generate multi-kilobase reads longer than most transcripts allowing sequencing of complete haplotype isoforms. This allows partitioning the reads into two parental haplotypes. While the read length of the single-molecule protocols is long, the relatively high error rate limits the ability to accurately detect the genetic variants and assemble them into the haplotype-specific isoforms. In this paper, we present HapIso (Haplotype-specific Isoform Reconstruction), a method able to tolerate the relatively high error-rate of the single-molecule platform and partition the isoform reads into the parental alleles. Phasing the reads according to the allele of origin allows our method to efficiently distinguish between the read errors and the true biological mutations. HapIso uses a k-means clustering algorithm aiming to group the reads into two meaningful clusters maximizing the similarity of the reads within cluster and minimizing the similarity of the reads from different clusters. Each cluster corresponds to a parental haplotype. We use family pedigree information to evaluate our approach. Experimental validation suggests that HapIso is able to tolerate the relatively high error-rate and accurately partition the reads into the parental alleles of the isoform transcripts. Furthermore, our method is the first method able to reconstruct the haplotype-specific isoforms from long single-molecule reads."}, {"title": "Localized structural frustration for evaluating the impact of sequence variants", "url": "https://www.biorxiv.org/content/early/2016/05/08/052027", "tag": "Bioinformatics", "abstract": "The rapidly declining costs of sequencing human genomes and exomes are providing deeper insights into genomic variation than previously possible. Growing sequence datasets are uncovering large numbers of rare single-nucleotide variants (SNVs) in coding regions, many of which may even be unique to single individuals. The rarity of such variants makes it difficult to use conventional variant-phenotype associations as a means of predicting their potential impacts. As such, protein structures may help to provide the needed means for inferring otherwise difficult-to-discern rare SNV-phenotype associations. Previous efforts have sought to quantify the effects of SNVs on structures by evaluating their impacts on global stability. However, local perturbations can severely impact functionality (such as catalysis,allosteric regulation, interactions and specificity) without strongly disrupting global stability.Here, we describe a workflow in which localized frustration (which quantifies unfavorable residue-residue interactions) is employed as a metric to investigate such effects. We apply frustration to study the impacts of a large number of SNVs available throughout a number of next-generation sequencing datasets. Most of our observations are intuitively consistent: we observe that disease-associated SNVs have a strong proclivity to induce strong changes in localized frustration, and rare variants tend to disrupt local interactions to a larger extent than do common variants. Furthermore, we observe that somatic SNVs associated with oncogenes induce stronger perturbations at the surface, whereas those associated with tumor suppressor genes (TSGs) induce stronger perturbations in the interior. These findings are consistent with the notion that gain-of-function (for oncogenes) and loss-of-function events (for TSGs) may act through changes in regulatory interactions and basic functionality, respectively."}, {"title": "Gene- and pathway-based association tests for multiple traits with GWAS summary statistics", "url": "https://www.biorxiv.org/content/early/2016/05/07/052068", "tag": "Bioinformatics", "abstract": "To identify novel genetic variants associated with complex traits and to shed new insights on underlying biology in addition to the most popular single SNP-single trait association analysis, it would be useful to explore multiple correlated (intermediate) traits at the gene- or pathway-level by mining existing single GWAS or meta-analyzed GWAS data. For this purpose, we present an adaptive gene-based test and a pathway-based test for association analysis of multiple traits with GWAS summary statistics. The proposed tests are adaptive at both the SNP- and trait-levels; that is, they account for possibly varying association patterns (e.g. signal sparsity levels) across SNPs and traits, thus maintaining high power across a wide range of situations. Furthermore, the proposed methods are general: they can be applied to mixed types of traits, and to Z-statistics or p-values as summary statistics obtained from either a single GWAS or a meta-analysis of multiple GWAS. Our numerical studies with simulated and real data demonstrated the promising performance of the proposed methods. The methods are implemented in R package aSPU, freely and publicly available on CRAN."}, {"title": "CAGEd-oPOSSUM: motif enrichment analysis from CAGE-derived TSSs", "url": "https://www.biorxiv.org/content/early/2016/05/06/040667", "tag": "Bioinformatics", "abstract": "Summary: With the emergence of large-scale Cap Analysis of Gene Expression (CAGE) data sets from individual labs and the FANTOM consortium, one can now analyze the cis-regulatory regions associated with gene transcription at an unprecedented level of refinement. By coupling transcription factor binding site (TFBS) enrichment analysis with CAGE-derived genomic regions, CAGEd-oPOSSUM can identify TFs that act as key regulators of genes involved in specific mammalian cell and tissue types. The webtool allows for the analysis of CAGE-derived transcription start sites (TSSs) either provided by the user or selected from \u223c1,300 mammalian samples from the FANTOM5 project with pre-computed TFBS predicted with JASPAR TF binding profiles. The tool helps power insights into the regulation of genes through the study of the specific usage of TSSs within specific cell types and/or under specific conditions. Availability and implementation: The CAGEd-oPOSUM web tool is implemented in Perl, MySQL, and Apache and is available at http://cagedop.cmmt.ubc.ca/CAGEd_oPOSSUM. Supporting Information: Supplementary Text, Figures, and Data are available online at bioRxiv."}, {"title": "Understanding properties of the master effector of phage shock operon in Mycobacterium tuberculosis via bioinformatics approach", "url": "https://www.biorxiv.org/content/early/2016/05/05/050047", "tag": "Bioinformatics", "abstract": "The phage shock protein (Psp) is a part of the Psp operon, which assists in safeguarding the survival of bacterium in stress and shields the cell against proton motif force challenge. It is strongly induced by bacterium allied phages, improperly localized mutant porins and various other stresses. Master effector of the operon, PspA has been modeled and simulated, illustrating how it undergoes significant conformational transition at the far end in Mycobacterium tuberculosis. Association of this key protein of the operon influences action of Psp system on the whole. We are further working on the impact of phosphorylation perturbation and changes in the structure of PspA during complex formation with other moieties of interest."}, {"title": "Bracken: Estimating species abundance in metagenomics data", "url": "https://www.biorxiv.org/content/early/2016/05/05/051813", "tag": "Bioinformatics", "abstract": "We describe a new, highly accurate statistical method that computes the abundance of species in DNA sequences from a metagenomics sample. Bracken (Bayesian Reestimation of Abundance after Classification with KrakEN) uses the taxonomy labels assigned by Kraken, a highly accurate metagenomics classification algorithm, to estimate the number of reads originating from each species present in a sample. Kraken classifies reads to the best matching location in the taxonomic tree, but does not estimate abundances of species. We use the Kraken database itself to derive probabilities that describe how much sequence from each genome is shared with other genomes in the database, and combine this information with the assignments for a particular sample to estimate abundance at the species level, the genus level, or above. Combined with the Kraken classifier, Bracken produces accurate species- and genus-level abundance estimates even when a sample contains multiple near-identical species."}, {"title": "Biophysically motivated regulatory network inference: progress and prospects", "url": "https://www.biorxiv.org/content/early/2016/05/04/051847", "tag": "Bioinformatics", "abstract": "Via a confluence of genomic technology and computational developments the possibility of network inference methods that automatically learn large comprehensive models of cellular regulation is closer than ever. This perspective will focus on enumerating the elements of computational strategies that, when coupled to appropriate experimental designs, can lead to accurate large-scale models of chromatin-state and transcriptional regulatory structure and dynamics. We highlight four research questions that require further investigation in order to make progress in network inference: using overall constraints on network structure like sparsity, use of informative priors and data integration to constrain individual model parameters, estimation of latent regulatory factor activity under varying cell conditions, and new methods for learning and modeling regulatory factor interactions. We conclude that methods combining advances in these four categories of required effort with new genomic technologies will result in biophysically motivated dynamic genome-wide regulatory network models for several of the best studied organisms and cell types."}, {"title": "Pan- and core- network analysis of co-expression genes in a model plant", "url": "https://www.biorxiv.org/content/early/2016/05/03/051656", "tag": "Bioinformatics", "abstract": "Genome-wide gene expression experiments have been performed using the model plant Arabidopsis during the last decade. Some studies involved construction of coexpression networks, a popular technique used to identify groups of co-regulated genes, to infer unknown gene functions. One approach is to construct a single coexpression network by combining multiple expression datasets generated in different labs. We advocate a complementary approach in which we construct a large collection of 134 coexpression networks based on expression datasets reported in individual publications. To this end we reanalyzed public expression data. To describe this collection of networks we introduced concepts of pan-network and core-network representing union and intersection between a sizeable fractions of individual networks, respectively. We showed that these two types of networks are different both in terms of their topology and biological function of interacting genes. For example, the modules of the pan-network are enriched in regulatory and signaling functions, while the modules of the core-network tend to include components of large macromolecular complexes such as ribosomes and photosynthetic machinery. Our analysis is aimed to help the plant research community to better explore the information contained within the existing vast collection of gene expression data in Arabidopsis."}, {"title": "LD Hub: a centralized database and web interface to perform LD score regression that maximizes the potential of summary level GWAS data for SNP heritability and genetic correlation analysis", "url": "https://www.biorxiv.org/content/early/2016/05/03/051094", "tag": "Bioinformatics", "abstract": "Motivation: LD score regression is a reliable and efficient method of using genome-wide association study (GWAS) summary-level results data to estimate the SNP heritability of complex traits and diseases, partition this heritability into functional categories, and estimate the genetic correlation between different phenotypes. Because the method relies on summary level results data, LD score regression is computationally tractable even for very large sample sizes. However, publicly available GWAS summary-level data are typically stored in different databases and have different formats, making it difficult to apply LD score regression to estimate genetic correlations across many different traits simultaneously. Results: In this manuscript, we describe LD Hub - a centralized database of summary-level GWAS results for 177 diseases/traits from different publicly available resources/consortia and a web interface that automates the LD score regression analysis pipeline. To demonstrate functionality and validate our software, we replicated previously reported LD score regression analyses of 49 traits/diseases using LD Hub; and estimated SNP heritability and the genetic correlation across the different phenotypes. We also present new results obtained by uploading a recent atopic dermatitis GWAS meta-analysis to examine the genetic correlation between the condition and other potentially related traits. In response to the growing availability of publicly accessible GWAS summary-level results data, our database and the accompanying web interface will ensure maximal uptake of the LD score regression methodology, provide a useful database for the public dissemination of GWAS results, and provide a method for easily screening hundreds of traits for overlapping genetic aetiologies. Availability and implementation: The web interface and instructions for using LD Hub are available at http://ldsc.broadinstitute.org/"}, {"title": "trio-sga: facilitating de novo assembly of highly heterozygous genomes with parent-child trios", "url": "https://www.biorxiv.org/content/early/2016/05/03/051516", "tag": "Bioinformatics", "abstract": "Motivation: Most DNA sequence in diploid organisms is found in two copies, one contributed by the mother and the other by the father. The high density of differences between the maternally and paternally contributed sequences (heterozygous sites) in some organisms makes de novo genome assembly very challenging, even for algorithms specifically designed to deal with these cases. Therefore, various approaches, most commonly inbreeding in the laboratory, are used to reduce heterozygosity in genomic data prior to assembly. However, many species are not amenable to these techniques. Results: We introduce trio-sga, a set of three algorithms designed to take advantage of mother-father-offspring trio sequencing to facilitate better quality genome assembly in organisms with moderate to high levels of heterozygosity. Two of the algorithms use haplotype phase information present in the trio data to eliminate the majority of heterozygous sites before the assembly commences. The third algorithm is designed to reduce sequencing costs by enabling the use of parents' reads in the assembly of the genome of the offspring. We test these algorithms on a 'simulated trio' from four haploid datasets, and further demonstrate their performance by assembling three highly heterozygous Heliconius butterfly genomes. While the implementation of trio-sga is tuned towards Illumina-generated data, we note that the trio approach to reducing heterozygosity is likely to have cross-platform utility for de novo assembly. Availability: trio-sga is an extension of the sga genome assembler. It is available at https://github.com/millanek/trio-sga, written in C++, and runs multithreaded on UNIX- based systems. Contact: millanek@gmail.com, rd@sanger.ac.uk"}, {"title": "A Graph Extension of the Positional Burrows-Wheeler Transform and its Applications", "url": "https://www.biorxiv.org/content/early/2016/05/02/051409", "tag": "Bioinformatics", "abstract": "We present a generalization of the Positional Burrows-Wheeler Transform (PBWT) to genome graphs, which we call the gPBWT. A genome graph is a collapsed representation of a set of genomes described as a graph. In a genome graph, a haplotype corresponds to a restricted form of walk. The gPBWT is a compressible representation of a set of these graph-encoded haplotypes that allows for efficient subhaplotype match queries. We give efficient algorithms for gPBWT construction and query operations. We describe our implementation, showing the compression and search of 1000 Genomes data. As a demonstration, we use the gPBWT to quickly count the number of haplotypes consistent with random walks in a genome graph, and with the paths taken by mapped reads; results suggest that haplotype consistency information can be practically incorporated into graph-based read mappers."}, {"title": "Analyzing Repast Symphony models in R with RRepast package", "url": "https://www.biorxiv.org/content/early/2016/05/02/047985", "tag": "Bioinformatics", "abstract": "In order to produce dependable results, the output of models must be carefully evaluated and compared to the experimental data. One of the main goals of analyzing a model is the understanding the effect of input factors on the model output. This task is carried out using a methodology known as sensitivity analysis. The analysis of Individual-based Models is hindered by the lack of simple tools allowing a complete and throughout evaluation without much effort. This kind of model tends to have a high level of complexity and the manual execution of a large experimental setup is generally not a feasible choice. Thus, it is required that model evaluation should ideally be simple and robust without demanding a high level of knowledge from modelers. In this work we present the RRepast, an open source GNU R package for executing, calibrating and analyzing Repast Symphony models directly from the R environment."}, {"title": "Eliminating redundancy among protein sequences using submodular optimization", "url": "https://www.biorxiv.org/content/early/2016/05/02/051201", "tag": "Bioinformatics", "abstract": "Submodular optimization, a discrete analogue to continuous convex optimization, has been used with great success in many fields but is not yet widely used in biology. We demonstrate how submodular optimization can be applied to the problem of removing redundancy in protein sequence data sets, a common step in many bioinformatics and structural biology workflows. We show that an approach based on submodular optimization results in representative protein sequence subsets with greater functional diversity than sets chosen with existing methods. In particular, we compare to a widely used, heuristic algorithm implemented in software tools such as CD-HIT, using as a gold standard the SCOPe library of protein domain structures. In this setting, submodular optimization consistently yields protein sequence subsets that include more SCOPe domain families than sets of the same size selected by the heuristic approach. This framework is theoretically optimal under some assumptions, and it is flexible and intuitive because it applies generic methods to optimize one of a variety of objective functions. This application serves as a model for how submodular optimization can be applied to other discrete problems in biology."}, {"title": "Graph analysis of structural brain networks in Alzheimer's disease", "url": "https://www.biorxiv.org/content/early/2016/04/28/050708", "tag": "Bioinformatics", "abstract": "ABSTRACT Background Changes in brain connectivity in patients with early Alzheimer's disease (AD) have been investigated using graph analysis. However, these studies were based on small data sets, explored a limited range of network parameters, and did not focus on more restricted sub-networks, where neurodegenerative processes may introduce more prominent alterations. Methods In this study, we constructed structural brain networks out of 87 regions by using data from 135 healthy elders and 100 early AD patients selected from the Open Access Series of Imaging Studies (OASIS) database. We evaluated the graph properties of these networks by investigating metrics of network efficiency, small world properties, segregation, product measures of complexity, and entropy. Because degenerative processes take place at different rates in different brain areas, analysis restricted to sub-networks may reveal changes otherwise undetected. Therefore, we first analyzed the graph properties of a network encompassing all brain areas considered together, and then repeated the analysis after dividing the brain areas into two sub-networks constructed by applying a clustering algorithm. Results At the level of large scale network, the analysis did not reveal differences between AD patients and controls. In contrast, the same analysis performed on the two sub-networks revealed modifications accompanying AD. Changes in small world properties suggested that the ability to engage concomitantly in integration and segregation of information diminished with AD in the sub-network containing the areas of medial temporal lobe known to be heaviest and earliest affected. In contrast, we found that the second network showed an increase in small world propensity, a novel metric that unbiasedly quantifies small world structure. Complexity and entropy measures indicated that the intricacy of connection patterns and structural diversity decreased in both sub-networks. Conclusions These results show that neurodegenerative processes impact volumetric networks in a non-global fashion. Our findings provide new quantitative insights into topological principles of structural brain networks and their modifications during early stages of Alzheimer's disease."}, {"title": "Accurate classification of protein subcellular localization from high throughput microscopy images using deep learning", "url": "https://www.biorxiv.org/content/early/2016/04/28/050757", "tag": "Bioinformatics", "abstract": "High throughput microscopy of many single cells generates high-dimensional data that are far from straightforward to analyze. One important problem is automatically detecting the cellular compartment where a fluorescently tagged protein resides, a task relatively simple for an experienced human, but difficult to automate on a computer. Here, we train an 11-layer neural network on data from mapping thousands of yeast proteins, achieving per cell localization classification accuracy of 91%, and per protein accuracy of 99% on held out images. We confirm that low-level network features correspond to basic image characteristics, while deeper layers separate localization classes. Using this network as a feature calculator, we train standard classifiers that assign proteins to previously unseen compartments after observing only a small number of training examples. Our results are the most accurate subcellular localization classifications to date, and demonstrate the usefulness of deep learning for high throughput microscopy."}, {"title": "Long single-molecule reads can resolve the complexity of the Influenza virus composed of rare, closely related mutant variants", "url": "https://www.biorxiv.org/content/early/2016/04/28/036392", "tag": "Bioinformatics", "abstract": "As a result of a high rate of mutations and recombination events, an RNA-virus exists as a heterogeneous \u201cswarm\u201d. The ability of next-generation sequencing to produce massive quantities of genomic data inexpensively has allowed virologists to study the structure of viral populations from an infected host at an unprecedented resolution. However, high similarity and low frequency of the viral variants impose a huge challenge to assembly of individual full-length genomes. The long read length offered by single-molecule sequencing technologies allows each mutant variant to be sequenced in a single pass. However, high error rate limits the ability to reconstruct heterogeneous viral population composed of rare, related mutant variants. In this paper, we present 2SNV, a method able to tolerate the high error-rate of the single-molecule protocol and reconstruct mutant variants. The proposed protocol is able to eliminate sequencing errors and reconstruct closely related viral mutant variants. 2SNV uses linkage between single nucleotide variations to efficiently distinguish them from read errors. To benchmark the sensitivity of 2SNV, we performed a single-molecule sequencing experiment on a sample containing a titrated level of known viral mutant variants. Our method is able to accurately reconstruct clone with frequency of 0.2% and distinguish clones that differed in only two nucleotides distantly located on the genome. 2SNV outperforms existing methods for full-length viral mutant reconstruction. With high sensitivity and accuracy, 2SNV is anticipated to facilitate not only viral quasispecies reconstruction, but also other biological questions that require detection of rare haplotypes such as genetic diversity in cancer cell population, and monitoring B-cell and T-cell receptor repertoire. The open source implementation of 2SNV is freely available for download at http://alan.cs.gsu.edu/NGS/?q=content/2snv"}, {"title": "MALT: Fast alignment and analysis of metagenomic DNA sequence data applied to the Tyrolean Iceman", "url": "https://www.biorxiv.org/content/early/2016/04/27/050559", "tag": "Bioinformatics", "abstract": "Modern next generation sequencing technologies produce vast amounts of data in the context of large-scale metagenomic studies, in which complex microbial communities can be reconstructed to an unprecedented level of detail. Most prominent examples are human microbiome studies that correlate the bacterial taxonomic profile with specific physiological conditions or diseases. In order to perform these analyses high-throughput computational tools are needed that are able to process these data within a short time while preserving a high level of sensitivity and specificity. Here we present MALT (MEGAN ALignment Tool) a program for the ultrafast alignment and analysis of metagenomic DNA sequencing data. MALT processes hundreds of millions of sequencing reads within only a few hours. In addition to the alignment procedure MALT implements a taxonomic binning algorithm that is able to specifically assign reads to bacterial species. Its tight integration with the interactive metagenomic analysis software MEGAN allows for visualization and further analyses of results. We demonstrate MALT by its application to the metagenomic analysis of two ancient microbiomes from oral cavity and lung samples of the 5,300-year-old Tyrolean Iceman. Despite the strong environmental background, MALT is able to pick up the weak signal of the original microbiomes and identifies multiple species that are typical representatives of the respective host environment."}, {"title": "Charting Improvements in US Registry HLA Typing Ambiguity Using a Typing Resolution Score", "url": "https://www.biorxiv.org/content/early/2016/04/26/050443", "tag": "Bioinformatics", "abstract": "Unrelated stem cell registries have been collecting HLA typing of volunteer bone marrow donors for over 25 years. Donor selection for hematopoietic stem cell transplantation is based primarily on matching the alleles of donors and patients at five polymorphic HLA loci. As HLA typing technologies have continually advanced since the beginnings of stem cell transplantation, registries have accrued typings of varied HLA typing ambiguity. We present a new typing resolution score, based on the likelihood of self-match, that allows the systematic comparison of HLA typings across different methods, data sets and populations. We apply the typing resolution score to chart improvement in HLA typing within the Be The Match Registry of the United States from the initiation of DNA-based HLA typing to the current state of high-resolution typing using next-generation sequencing technologies. In addition, we present a publicly available online tool for evaluation of any given HLA typing. This typing resolution score objectively evaluates HLA typing methods and can help define standards for acceptable recruitment HLA typing."}, {"title": "Assessing the accuracy of Approximate Bayesian Computation approaches to infer epidemiological parameters from phylogenies", "url": "https://www.biorxiv.org/content/early/2016/04/26/050211", "tag": "Bioinformatics", "abstract": "Phylodynamics typically rely on likelihood-based methods to infer epidemiological parameters from dated phylogenies. These methods are essentially based on simple epidemiological models because of the difficulty in expressing the likelihood function analytically. Computing this function numerically raises additional challenges, especially for large phylogenies. Here, we use Approximate Bayesian Computation (ABC) to circumvent these problems. ABC is a likelihood-free method of parameter inference, based on simulation and comparison between target data and simulated data, using summary statistics. We simulated target trees under several epidemiological scenarios in order to assess the accuracy of ABC methods for inferring epidemiological parameter such as the basic reproduction number (R0), the mean duration of infection, and the effective host population size. We designed many summary statistics to capture the information in a phylogeny and its corresponding lineage-through-time plot. We then used the simplest ABC method, called rejection, and its modern derivative complemented with adjustment of the posterior distribution by regression. The availability of machine learning techniques including variable selection, motivated us to compute many summary statistics on the phylogeny. We found that ABC-based inference reaches an accuracy comparable to that of likelihood-based methods for birth-death models and can even outperform existing methods for more refined models and large trees. By re-analysing data from the early stages of the recent Ebola epidemic in Sierra Leone, we also found that ABC provides more realistic estimates than the likelihood-based methods, for some parameters. This work shows that the combination of ABC-based inference using many summary statistics and sophisticated machine learning methods able to perform variable selection is a promising approach to analyse large phylogenies and non-trivial models."}, {"title": "Phylogeny-aware Identification and Correction of Taxonomically Mislabeled Sequences", "url": "https://www.biorxiv.org/content/early/2016/04/26/042200", "tag": "Bioinformatics", "abstract": "Molecular sequences in public databases are mostly annotated by the submitting authors without further validation. This procedure can generate erroneous taxonomic sequence labels. Mislabeled sequences are hard to identify, and they can induce downstream errors because new sequences are typically annotated using existing ones. Furthermore, taxonomic mislabelings in reference sequence databases can bias metagenetic studies which rely on the taxonomy. Despite significant efforts to improve the quality of taxonomic annotations, the curation rate is low because of the labour-intensive manual curation process. Here, we present SATIVA, a phylogeny-aware method to automatically identify taxonomically mislabeled sequences ('mislabels') using statistical models of evolution. We use the Evolutionary Placement Algorithm (EPA) to detect and score sequences whose taxonomic annotation is not supported by the underlying phylogenetic signal, and automatically propose a corrected taxonomic classification for those. Using simulated data, we show that our method attains high accuracy for identification (96.9% sensitivity / 91.7% precision) as well as correction (94.9% sensitivity / 89.9% precision) of mislabels. Furthermore, an analysis of four widely used microbial 16S reference databases (Greengenes, LTP, RDP and SILVA) indicates that they currently contain between 0.2% and 2.5% mislabels. Finally, we use SATIVA to perform an in-depth evaluation of alternative taxonomies for Cyanobacteria. SATIVA is freely available at https://github.com/amkozlov/sativa."}, {"title": "Identification and analysis of integrons and cassette arrays in bacterial genomes", "url": "https://www.biorxiv.org/content/early/2016/04/26/030866", "tag": "Bioinformatics", "abstract": "Integrons recombine gene arrays and favor the spread of antibiotic resistance. Their broader roles in bacterial adaptation remain mysterious, partly due to lack of computational tools. We made a program - IntegronFinder - to identify integrons with high accuracy and sensitivity. IntegronFinder is available as a standalone program and as a web application. It searches for attC sites using covariance models, for integron-integrases using HMM profiles, and for other features (promoters, attI site) using pattern matching. We searched for integrons, integron-integrases lacking attC sites, and clusters of attC sites lacking a neighboring integron-integrase in bacterial genomes. All these elements are especially frequent in genomes of intermediate size. They are missing in some key phyla, such as \u03b1-Proteobacteria, which might reflect selection against cell lineages that acquire integrons. The similarity between attC sites is proportional to the number of cassettes in the integron, and is particularly low in clusters of attC sites lacking integron-integrases. The latter are unexpectedly abundant in genomes lacking integron-integrases or their remains, and have a large novel pool of cassettes lacking homologs in the databases. They might represent an evolutionary step between the acquisition of genes within integrons and their stabilization in the new genome."}, {"title": "Integrated Biomedical System", "url": "https://www.biorxiv.org/content/early/2016/04/25/050138", "tag": "Bioinformatics", "abstract": "Capabilities for generating and storing large amounts of data relevant to individual health and performance are rapidly evolving and have the potential to accelerate progress toward quantitative and individualized understanding of many important issues in health and medicine. Recent advances in clinical and laboratory technologies provide increasingly complete and dynamic characterization of individual genomes, gene expression levels for genes, relative abundance of thousands of proteins, population levels for thousands of microbial species, quantitative imaging data, and more, all on the same individual. Personal and wearable electronic devices are increasingly enabling these same individuals to routinely and continuously capture vast amounts of quantitative data including activity, sleep, nutrition, environmental exposures, physiological signals, speech, and neurocognitive performance metrics at unprecedented temporal resolution and scales. While some of the companies offering these measurement technologies have begun to offer systems for integrating and displaying correlated individual data, these are either closed/proprietary platforms that provide limited access to sensor data or have limited scope that focus primarily on one data domain (e.g. steps/calories/activity, genetic data, etc.). The Integrated Biomedical System is being developed to demonstrate an adaptable open-source tool for reducing the burden associated with integrating heterogeneous genome, interactome, and exposome data from a constantly evolving landscape of biomedical data generating technologies. The Integrated Biomedical System provides a scalable and modular framework that can be extended to include support for numerous types of analyses and applications at scales ranging from personal users, communities and groups, to large populations."}, {"title": "LoLoPicker: Detecting Low Allelic-Fraction Variants in Low-Quality Cancer Samples from Whole-exome Sequencing Data", "url": "https://www.biorxiv.org/content/early/2016/04/24/043612", "tag": "Bioinformatics", "abstract": "Summary: We developed an efficient tool dedicated to call somatic variants from next generation sequencing (NGS) data with the help of a user-defined control panel of non-cancer samples. Compared with other methods, we showed superior performance of LoLoPicker with significantly improved specificity. The algorithm of LoLoPicker is particularly useful for calling low allelic-fraction variants from low-quality cancer samples such as formalin-fixed and paraffin-embedded (FFPE) samples. Implementation and Availability: The main scripts are implemented in Python 2.7.8 and the package is released at https://github.com/jcarrotzhang/LoLoPicker."}, {"title": "Fused regression for multi-source gene regulatory network inference", "url": "https://www.biorxiv.org/content/early/2016/04/22/049775", "tag": "Bioinformatics", "abstract": "Understanding gene regulatory networks is critical to understanding cellular differentiation and response to external stimuli. Methods for global network inference have been developed and applied to a variety of species. Most approaches consider the problem of network inference independently in each species, despite evidence that gene regulation can be conserved even in distantly related species. Further, network inference is often confined to single data-types (single platforms) and single cell types. We introduce a method for multi-source network inference that allows simultaneous estimation of gene regulatory networks in multiple species or biological processes through the introduction of priors based on known gene relationships such as orthology incorporated using fused regression. This approach improves network inference performance even when orthology mapping and conservation are incomplete. We refine this method by presenting an algorithm that extracts the true conserved subnetwork from a larger set of potentially conserved interactions and demonstrate the utility of our method in cross species network inference. Last, we demonstrate our method's utility in learning from data collected on different experimental platforms."}, {"title": "Inferring intrinsic and extrinsic noise from a dual fluorescent reporter", "url": "https://www.biorxiv.org/content/early/2016/04/21/049486", "tag": "Bioinformatics", "abstract": "Dual fluorescent reporter constructs, which measure gene expression from two identical promoters within the same cell, allow total gene expression noise to be decomposed into an extrinsic component, roughly associated with cell-to-cell fluctuations in cellular component concentrations, and intrinsic noise, roughly associated with inherent stochasticity of the biochemical reactions involved in gene expression [1]. A recent paper by Fu and Pachter presented frequentist statistical estimators for intrinsic and extrinsic noise using data from dual reporters [2]. For comparison, I here present results of a Bayesian analysis of this problem. I show that the orthodox estimators suffer from pathologies such as predicting negative values for a manifestly non-negative quantity, i.e. variance, and show that the Bayesian estimators do not suffer from such pathologies. In addition, I show that the Bayesian analysis automatically identifies that optimal estimates of intrinsic and extrinsic noise depend on a subtle combination of two statistics of the data, allowing for accuracies that are up to twice the accuracy of the orthodox estimators in some parameter regimes. I hope up this little worked out example contrasting orthodox statistical analysis based on ad hoc estimators with estimators resulting from a Bayesian analysis, will be educational for others in the field. I distribute a Mathematica Notebook with this paper that allows users to easily reproduce all results and figures of the paper."}, {"title": "Prediction of kinase-specific phosphorylation sites through an integrative model of protein context and sequence", "url": "https://www.biorxiv.org/content/early/2016/04/21/043679", "tag": "Bioinformatics", "abstract": "The identification of kinase substrates and the specific phosphorylation sites they regulate is an important factor in understanding protein function regulation and signalling pathways. Computational prediction of kinase targets -- assigning kinases to putative substrates, and selecting from protein sequence the sites that kinases can phosphorylate -- requires the consideration of both the cellular context that kinases operate in, as well as their binding affinity. This consideration enables investigation of how phosphorylation influences a range of biological processes. We report here a novel probabilistic model for the classification of kinase-specific phosphorylation sites from sequence across three model organisms: human, mouse and yeast. The model incorporates position-specific amino acid frequencies, and counts of co-occurring amino acids from kinase binding sites in a kinase- and family-specific manner. We show how this model can be seamlessly integrated with protein interactions and cell-cycle abundance profiles. When evaluating the prediction accuracy of our method, PhosphoPICK, on an independent hold-out set of kinase-specific phosphorylation sites, we found it achieved an average specificity of 97% while correctly predicting 32% of true positives. We also compared PhosphoPICK's ability, through cross-validation, to predict kinase-specific phosphorylation sites with alternative methods, and found that at high levels of specificity PhosphoPICK outperforms alternative methods for most comparisons made. We investigated the relationship between experimentally confirmed phosphorylation sites and predicted nuclear localisation signals by predicting the most likely kinases to be regulating the phosphorylated residues immediately upstream or downstream from the localisation signal. We show that kinases PKA, Akt1 and AurB have an over-representation of predicted binding sites at particular positions downstream from predicted nuclear localisation signals, demonstrating an important role for these kinases in regulating the nuclear import of proteins."}, {"title": "From Visual Exploration to Storytelling and Back Again", "url": "https://www.biorxiv.org/content/early/2016/04/20/049585", "tag": "Bioinformatics", "abstract": "The primary goal of visual data exploration tools is to enable the discovery of new insights. To justify and reproduce insights, the discovery process needs to be documented and communicated. A common approach to documenting and presenting findings is to capture visualizations as images or videos. Images, however, are insufficient for telling the story of a visual discovery, as they lack full provenance information and context. Videos are difficult to produce and edit, particularly due to the non-linear nature of the exploratory process. Most importantly, however, neither approach provides the opportunity to return to any point in the exploration in order to review the state of the visualization in detail or to conduct additional analyses. In this paper we present CLUE (Capture, Label, Understand, Explain), a model that tightly integrates data exploration and presentation of discoveries. Based on provenance data captured during the exploration process, users can extract key steps, add annotations, and author 'Vistories', visual stories based on the history of the exploration. These Vistories can be shared for others to view, but also to retrace and extend the original analysis. We discuss how the CLUE approach can be integrated into visualization tools and provide a prototype implementation. Finally, we demonstrate the general applicability of the model in two usage scenarios: a Gapminder-inspired visualization to explore public health data and an example from molecular biology that illustrates how Vistories could be used in scientific journals."}, {"title": "CNView: a visualization and annotation tool for copy number variation from whole-genome sequencing", "url": "https://www.biorxiv.org/content/early/2016/04/20/049536", "tag": "Bioinformatics", "abstract": "Summary: Copy number variation (CNV) is a major component of structural differences between individual genomes. The recent emergence of population-scale whole-genome sequencing (WGS) datasets has enabled genome-wide CNV delineation. However, molecular validation at this scale is impractical, so visualization is an invaluable preliminary screening approach when evaluating CNVs. Standardized tools for visualization of CNVs in large WGS datasets are therefore in wide demand. Methods & Results: To address this demand, we developed a software tool, CNView, for normalized visualization, statistical scoring, and annotation of CNVs from population-scale WGS datasets. CNView surmounts challenges of sequencing depth variability between individual libraries by locally adapting to cohort-wide variance in sequencing uniformity at any locus. Importantly, CNView is broadly extensible to any reference genome assembly and most current WGS data types. Availability and Implementation: CNView is written in R, is supported on OS X, MS Windows, and Linux, and is freely distributed under the MIT license. Source code and documentation are available from https://github.com/RCollins13/CNView Contact: talkowski@chgr.mgh.harvard.edu Supplementary Information: Supplementary data are available at Bioinformatics online."}, {"title": "Temporal microbiome road-maps guided by perturbations", "url": "https://www.biorxiv.org/content/early/2016/04/20/049510", "tag": "Bioinformatics", "abstract": "Motivation: There are few tools that allow longitudinal analysis of metagenomic data subjected to distinct perturbations. Methods: This study examines longitudinal metagenomics data modelled as a Markov Decision Process (MDP). Given an external perturbation, the MDP predicts the next microbiome state in a temporal sequence, selected from a finite set of possible microbiome states. Results: We examined three distinct datasets to demonstrate this approach. An MDP created for a vaginal microbiome time series generates a variety of behaviour policies. For example, that moving from a state associated with bacterial vaginosis to a healthier one, requires avoiding perturbations such as lubricant, sex toys, tampons and anal sex. The flexibility of our proposal is verified after applying MDPs to human gut and chick gut microbiomes, taking nutritional intakes, or salmonella and probiotic treatments, respectively, as perturbations. In the latter case, MDPs provided a quantitative explanation for why salmonella vaccine accelerates microbiome maturation in chicks. This novel analytical approach has applications in, for example, medicine where the MDP could suggest the sequence of perturbations (e.g. clinical interventions) to apply to follow the best path from any given starting state, to a desired (healthy) state, avoiding strongly negative states."}, {"title": "plasmidSPAdes: Assembling Plasmids from Whole Genome Sequencing Data", "url": "https://www.biorxiv.org/content/early/2016/04/20/048942", "tag": "Bioinformatics", "abstract": "Motivation: Plasmids are stably maintained extra-chromosomal genetic elements that replicate independently from the host cell's chromosomes. Although plasmids harbor biomedically important genes, (such as genes involved in virulence and antibiotics resistance), there is a shortage of specialized software tools for extracting and assembling plasmid data from whole genome sequencing projects. Results: We present the plasmidSPAdes algorithm and software tool for assembling plasmids from whole genome sequencing data and benchmark its performance on a diverse set of bacterial genomes. Availability and implementation: PLASMID SPADES is publicly available at http://spades.bioinf.spbau.ru/plasmidSPAdes/"}, {"title": "Ten simple rules for managing high-throughput nucleotide sequencing data", "url": "https://www.biorxiv.org/content/early/2016/04/19/049338", "tag": "Bioinformatics", "abstract": "The challenges posed by large data volumes produced by high-throughput nucleotide sequencing technologies are well known. This document establishes ten simple rules for coping with these challenges. At the level of master data management, (1) data triage reduces data volumes; (2) some lossless data representations are much more compact than others; (3) careful management of data replication reduces wasted storage space. At the level of data analysis, (4) automated analysis pipelines obviate the need for storing work files; (5) virtualization reduces the need for data movement and bandwidth consumption; (6) tracking of data and analysis provenance will generate a paper trail to better understand how results were produced. At the level of data access and sharing, (7) careful modeling of data movement patterns reduces bandwidth consumption and haphazard copying; (8) persistent, resolvable identifiers for data reduce ambiguity caused by data movement; (9) sufficient metadata enables more effective collaboration. Finally, because of rapid developments in HTS technologies, (10) agile practices that combine loosely coupled modules operating on standards-compliant data are the best approach for avoiding lock-in. A generalized scenario is presented for data management from initial raw data generation to publication of result data."}, {"title": "Impact of knowledge accumulation on pathway enrichment analysis", "url": "https://www.biorxiv.org/content/early/2016/04/19/049288", "tag": "Bioinformatics", "abstract": "Pathway-based interpretation of gene lists is a staple of genome analysis. It depends on frequently updated gene annotation databases. We analyzed the evolution of gene annotations over the past seven years and found that the vocabulary of pathways and processes has doubled. This strongly impacts practical analysis of genes: 80% of publications we surveyed in 2015 used outdated software that only captured 20% of pathway enrichments apparent in current annotations."}, {"title": "Mash: fast genome and metagenome distance estimation using MinHash", "url": "https://www.biorxiv.org/content/early/2016/04/19/029827", "tag": "Bioinformatics", "abstract": "Mash extends the MinHash dimensionality-reduction technique to include a pairwise mutation distance and P-value significance test, enabling the efficient clustering and search of massive sequence collections. Mash reduces large sequences and sequence sets to small, representative sketches, from which global mutation distances can be rapidly estimated. We demonstrate several use cases, including the clustering of all 54,118 NCBI RefSeq genomes in 33 CPU hours; real-time database search using assembled or unassembled Illumina, Pacific Biosciences, and Oxford Nanopore data; and the scalable clustering of hundreds of metagenomic samples by composition. Mash is freely released under a BSD license (https://github.com/marbl/mash)."}, {"title": "AC-PCA: simultaneous dimension reduction and adjustment for confounding variation", "url": "https://www.biorxiv.org/content/early/2016/04/19/040485", "tag": "Bioinformatics", "abstract": "Dimension reduction methods are commonly applied to high-throughput biological datasets. However, the results can be hindered by confounding factors, either biologically or technically originated. In this study, we extend Principal Component Analysis to propose AC-PCA for simultaneous dimension reduction and adjustment for confounding variation. We show that AC-PCA can adjust for a) variations across individual donors present in a human brain exon array dataset, and b) variations of different species in a model organism ENCODE RNA-Seq dataset. Our approach is able to recover the anatomical structure of neocortical regions, and to capture the shared variation among species during embryonic development. For gene selection purposes, we extend AC-PCA with sparsity constraints, and propose and implement an efficient algorithm. The methods developed in this paper can also be applied to more general settings."}, {"title": "Omics Discovery Index - Discovering and Linking Public Omics Datasets", "url": "https://www.biorxiv.org/content/early/2016/04/18/049205", "tag": "Bioinformatics", "abstract": "Biomedical data, in particular omics datasets are being generated at an unprecedented rate. This is due to the falling costs of generating experimental data, improved accuracy and better accessibility to different omics platforms such as genomics, proteomics and metabolomics. As a result, the number of deposited datasets in public repositories originating from various omics approaches has increased dramatically in recent years. This increase in public data deposition of omics results is a good starting point, but opens up a series of new challenges. For example the research community must now find more efficient ways for storing, organizing and providing access to biomedical data across platforms. These challenges range from achieving a common representation framework for the datasets and the associated metadata from different omics fields, to the availability of efficient methods, protocols and file formats for data exchange between multiple repositories. Therefore, there is a great need for development of new platforms and applications to make possible to search datasets across different omics fields, making such information accessible to the end-user. In this context, we introduce the Omics Discovery Index (OmicsDI - http://www.ebi.ac.uk/Tools/omicsdi), an integrated and open source platform facilitating the access and dissemination of omics datasets. OmicsDI provides a unique infrastructure to integrate datasets coming from multiple omics studies, including at present proteomics, genomics and metabolomics, as a distributed resource."}, {"title": "Network-based Computational Drug Combination Prediction", "url": "https://www.biorxiv.org/content/early/2016/04/16/049015", "tag": "Bioinformatics", "abstract": "Cancers are complex diseases that are regulated by multiple signaling pathways. Patients often acquire resistance to single drug treatment. Use of drug combinations that target multiple parallel pathways is a promising strategy to reduce the drug resistance. Pharmacogenomics big data are being generated to uncover complex signaling mechanisms of cancers and correlate cancer-specific signaling with diverse drug responses. Thus, converting pharmacogenomics big data into knowledge can help the discovery of synergistic drug combination. However, it is challenging and remains an open problem due to the enormous number of combination possibilities and noise of genomics data."}, {"title": "Convert Your Favorite Protein Modeling Program Into A Mutation Predictor: \"MODICT\"", "url": "https://www.biorxiv.org/content/early/2016/04/15/038992", "tag": "Bioinformatics", "abstract": "Abstract Motivation: Predict whether a mutation is deleterious based on the custom 3D model of a protein. Methods: We have developed modict, a mutation prediction tool which is based on per residue rmsd (root mean square deviation) values of superimposed 3D protein models. Our mathematical algorithm was tested for 42 described mutations in multiple genes including renin, beta-tubulin, biotinidase, sphingomyelin phosphodiesterase-1, phenylalanine hydroxylase and medium chain Acyl-Coa dehydrogenase. Moreover, modict scores corresponded to experimentally verified residual enzyme activities in mutated biotinidase, phenylalanine hydroxylase and medium chain Acyl-CoA dehydrogenase. Several commercially available prediction algorithms were tested and results were compared. The modict perl package and the manual can be downloaded from https://github.com/MODICT/MODICT. Conclusion: We show here that modict is capable tool for mutation effect prediction at the protein level, using superimposed 3D protein models instead of sequence based algorithms used by polyphen and sift. Keywords: prediction; 3D protein model; bioinformatics"}, {"title": "Variance Adaptive Shrinkage (vash): Flexible Empirical Bayes estimation of variances", "url": "https://www.biorxiv.org/content/early/2016/04/13/048660", "tag": "Bioinformatics", "abstract": "We consider the problem of estimating variances on a large number of \"similar\" units, when there are relatively few observations on each unit. This problem is important in genomics, for example, where it is often desired to estimate variances for thousands of genes (or some other genomic unit) from just a few measurements on each. A common approach to this problem is to use an Empirical Bayes (EB) method that assumes the variances among genes follow an inverse-gamma distribution. Here we describe a more flexible EB method, whose main assumption is that the distribution of the variances (or, as an alternative, the precisions) is unimodal. We show that this more flexible assumption provides competitive performance with existing methods when the variances truly come from an inverse-gamma distribution, and can outperform them when the distribution of the variances is more complex. In analyses of several human gene expression datasets from the Genotype Tissues Expression (GTEx) consortium, we find that our more flexible model often fits the data appreciably better than the single inverse gamma distribution. At the same time we find that, for variance estimation, the differences between methods is often small, suggesting that the simpler methods will often suffice in practice. Our methods are implemented in an R package vashr available from http://github.com/mengyin/vashr."}, {"title": "Assembly of Long Error-Prone Reads Using de Bruijn Graphs", "url": "https://www.biorxiv.org/content/early/2016/04/13/048413", "tag": "Bioinformatics", "abstract": "The recent breakthroughs in assembling long error-prone reads (such as reads generated by Single Molecule Real Time technology) were based on the overlap-layout-consensus approach and did not utilize the strengths of the alternative de Bruijn graph approach to genome assembly. Moreover, these studies often assume that applications of the de Bruijn graph approach are limited to short and accurate reads and that the overlap-layout-consensus approach is the only practical paradigm for assembling long error-prone reads. Below we show how to generalize de Bruijn graphs to assemble long error-prone reads and describe the ABruijn assembler, which results in more accurate genome reconstructions than the existing state-of-the-art algorithms."}, {"title": "Third-generation sequencing and the future of genomics", "url": "https://www.biorxiv.org/content/early/2016/04/13/048603", "tag": "Bioinformatics", "abstract": "Third-generation long-range DNA sequencing and mapping technologies are creating a renaissance in high-quality genome sequencing. Unlike second-generation sequencing, which produces short reads a few hundred base-pairs long, third-generation single-molecule technologies generate over 10,000 bp reads or map over 100,000 bp molecules. We analyze how increased read lengths can be used to address long-standing problems in de novo genome assembly, structural variation analysis and haplotype phasing."}, {"title": "Assessment of pharmacogenomic agreement", "url": "https://www.biorxiv.org/content/early/2016/04/13/048470", "tag": "Bioinformatics", "abstract": "In 2013 we published an analysis demonstrating that drug response data and gene-drug associations reported in two independent large-scale pharmacogenomic screens, Genomics of Drug Sensitivity in Cancer (GDSC) and Cancer Cell Line Encyclopedia (CCLE), were inconsistent. The GDSC and CCLE investigators recently reported that their respective studies exhibit reasonable agreement and yield similar molecular predictors of drug response, seemingly contradicting our previous findings. Reanalyzing the authors' published methods and results, we found that their analysis failed to account for variability in the genomic data and more importantly compared different drug sensitivity measures from each study, which substantially deviate from our more stringent consistency assessment. Our comparison of the most updated genomic and pharmacological data from the GDSC and CCLE confirms our published findings that the measures of drug response reported by these two groups are not consistent. We believe that a principled approach to assess the reproducibility of drug sensitivity predictors is necessary before envisioning their translation into clinical settings."}, {"title": "Fast Bayesian Inference of Copy Number Variants using Hidden Markov Models with Wavelet Compression", "url": "https://www.biorxiv.org/content/early/2016/04/13/023705", "tag": "Bioinformatics", "abstract": "By combining Haar wavelets with Bayesian Hidden Markov Models, we improve detection of genomic copy number variants (CNV) in array CGH experiments compared to the state-of-the-art, including standard Gibbs sampling. At the same time, we achieve drastically reduced running times, as the method concentrates computational effort on chromosomal segments which are difficult to call, by dynamically and adaptively recomputing consecutive blocks of observations likely to share a copy number. This makes routine diagnostic use and re-analysis of legacy data collections feasible; to this end, we also propose an effective automatic prior. An open source software implementation of our method is available at http://bioinformatics.rutgers.edu/Software/HaMMLET/. The web supplement is at http://bioinformatics.rutgers.edu/Supplements/HaMMLET/."}, {"title": "Allele-Specific Quantification of Structural Variations in Cancer Genomes", "url": "https://www.biorxiv.org/content/early/2016/04/12/048207", "tag": "Bioinformatics", "abstract": "One of the hallmarks of cancer genome is aneuploidy, resulting in abnormal copy numbers of alleles. Structural variations (SVs) can further modify the aneuploid cancer genomes into a mixture of rearranged genomic segments with extensive range of somatic copy number alterations (CNAs). Indeed, aneuploid cancer genomes have significantly higher rate of CNAs and SVs. However, although methods have been developed to identify SVs and allele-specific copy number of genome (ASCNG) separately, no existing algorithm can simultaneously analyze SVs and ASCNG. Such integrated approach is particularly important to fully understand the complexity of cancer genomes. Here we introduce a new algorithm called Weaver to provide allele-specific quantification of SVs and CNAs in aneuploid cancer genomes. Weaver uses a probabilistic graphical model by utilizing cancer whole genome sequencing data to simultaneously estimate the digital copy number and inter-connectivity of SVs. Our simulation evaluation, comparison with single-molecule Optical Mapping analysis, and real data applications (including MCF-7, HeLa, and TCGA whole genome sequencing samples) demonstrated that Weaver is highly accurate and can greatly refine the analysis of complex cancer genome structure."}, {"title": "GenRank: an R/Bioconductor package for prioritization of candidate genes", "url": "https://www.biorxiv.org/content/early/2016/04/12/048264", "tag": "Bioinformatics", "abstract": "Summary: Modern high-throughput studies often yield long lists of genes, of which a fraction are of high relevance to the phenotype of interest. To prioritize the candidate genes of complex genetic traits, our R/Bioconductor package GenRank provides methods that are based on convergent evidence obtained from multiple independent evidence layers. The package facilitates an extensible framework that allows a further addition of novel methods for candidate gene prioritization. Availability and Implementation: The methods are implemented in R and available as a package in Bioconductor repository (http://bioconductor.org/packages/GenRank/). Contact: chakra.kanduri@gmail.com"}, {"title": "Fast haplotype matching in very large cohorts using the Li and Stephens model", "url": "https://www.biorxiv.org/content/early/2016/04/12/048280", "tag": "Bioinformatics", "abstract": "The Li and Stephens model, which approximates the coalescent describing the pattern of variation in a population, underpins a range of key tools and results in genetics. Although highly efficient compared to the coalescent, standard implemen- tations of this model still cannot deal with the very large reference cohorts that are starting to become available, and practical implementations use heuristics to achieve reasonable runtimes. Here I describe a new, exact algorithm (fastLS) that implements the Li and Stephens model and achieves runtimes independent of the size of the reference cohort. Key to achieving this runtime is the use of the Burrows-Wheeler transform, allowing the algorithm to efficiently identify partial haplotype matches across a cohort. I show that the proposed data structure is very similar to, and generalizes, Durbin's positional Burrows-Wheeler transform."}, {"title": "Implementation of an Open Source Software solution for Laboratory Information Management and automated RNAseq data analysis in a large-scale Cancer Genomics initiative using BASE with extension package Reggie.", "url": "https://www.biorxiv.org/content/early/2016/04/12/038976", "tag": "Bioinformatics", "abstract": "Background Large-scale cancer genomics initiatives and next-generation sequencing for transcriptome profiling allow for detailed molecular characterization of tumors, and provide opportunities for clinical tools to improve diagnosis, prognosis, and treatment decisions. Laboratory information, data management, and data sharing in large-scale genomics projects is a challenge. Aiming to introduce such technologies in a clinical setting offer additional challenges associated with requirements of short lead-times and specialized tracking of biomaterials, data, and analysis results. Results Using the free open-source BioArray Software Environment (BASE) and extension package Reggie we have implemented a laboratory information management system and an automated RNAseq data analysis pipeline that successfully manage a large regional cancer genomics initiative. The system manages enrolled cancer patients, tumor biopsies, extraction of nucleic acid, and whole transcriptome RNA-sequencing through to data analysis and quality control. The implementation offers integration of laboratory equipment and operating procedures, and information tracking in a module based fashion enabling efficient and flexible use of personnel resources. The system provides two-factor authentication and transaction control and seamless integration of freely available software for RNAseq analysis such as Tophat, Cufflinks, and Picard. As of February 2016 more than 8000 patients and over 6000 tumor biopsies have been successfully processed. Lead-time from biopsy arrival to summarized reports based on RNAseq data is less than 5 days, in line with regional clinical requirements. BASE and Reggie are freely available and released as open-source under the GNU General Public License and GNU Affero General Public License, respectively. Conclusion Using free open-source software together with BASE and a customized extension package, Reggie, we have implemented a system capable of managing large collections of quality controlled and curated material for use in research and development and tailored to meet requirements for clinical use. Featuring high degree of automation and interactivity the system allows for resource efficient laboratory procedures and short lead-times with demonstrated use of RNAseq data analyses in a clinical setting."}, {"title": "A method for downstream analysis of gene set enrichment results facilitates the biological interpretation of vaccine efficacy studies", "url": "https://www.biorxiv.org/content/early/2016/04/11/043158", "tag": "Bioinformatics", "abstract": "Gene set enrichment analysis (GSEA) is a widely employed method for analyzing gene expression profiles. The approach uses annotated sets of genes, identifies those that are coordinately up- or down-regulated in a biological comparison of interest, and thereby elucidates underlying biological processes relevant to the comparison. As the number of gene sets available in various collections for enrichment analysis has grown, the resulting lists of significant differentially regulated gene sets may also become larger, leading to the need for additional downstream analysis of GSEA results. Here we present a method that allows the rapid identification of a small number of co-regulated groups of genes - \"leading edge metagenes\" (LEMs) - from high scoring sets in GSEA results. LEM are sub-signatures which are common to multiple gene sets and that \"explain\" their enrichment specific to the experimental dataset of interest. We show that LEMs contain more refined lists of context-dependent and biologically meaningful genes than the parental gene sets. LEM analysis of the human vaccine response using a large database of immune signatures identified core biological processes induced by five different vaccines in datasets from human peripheral blood mononuclear cells (PBMC). Further study of these biological processes over time following vaccination showed that at day 3 post-vaccination, vaccines derived from viruses or viral subunits exhibit patterns of biological processes that are distinct from protein conjugate vaccines; however, by day 7 these differences were less pronounced. This suggests that the immune response to diverse vaccines eventually converge to a common transcriptional response. LEM analysis can significantly reduce the dimensionality of enriched gene sets, improve the identification of core biological processes active in a comparison of interest, and simplify the biological interpretation of GSEA results."}, {"title": "In-silico analysis of Salmonella typhimurium and E.coli Methionyl tRNA synthetase at primary, secondary, tertiary level with protein disorder and functional association of differences", "url": "https://www.biorxiv.org/content/early/2016/04/10/048009", "tag": "Bioinformatics", "abstract": "The identification changes in amino acid for same protein in closely related species are necessary in order to identify its effect at various structural and functional levels. Salmonella typhimurium and E.coli Methionyl tRNA synthetase taken in current study as these bacteria are closely related to each other and have fewer differences in amino acid sequences for MetG. This study helps to identify various structural and functional differences at primary, secondary and tertiary levels, with functional differences by Docking study with Methionine. Study involves analysis of differences based on observation of differences in modeled 3D protein for sequences available at NCBI and its comparison with Known 3D structure. As sequences difference are in functional protein from non-mutant species, the differences are analysed in context of Primary, secondary, tertiary structure differences, Disorder differences, and docking differences."}, {"title": "SLICER: Inferring Branched, Nonlinear Cellular Trajectories from Single Cell RNA-seq Data", "url": "https://www.biorxiv.org/content/early/2016/04/09/047845", "tag": "Bioinformatics", "abstract": "Single cell experiments provide an unprecedented opportunity to reconstruct a sequence of changes in a biological process from individual \"snapshots\" of cells. However, nonlinear gene expression changes, genes unrelated to the process, and the possibility of branching trajectories make this a challenging problem. We developed SLICER (Selective Locally Linear Inference of Cellular Expression Relationships) to address these challenges. SLICER can infer highly nonlinear trajectories, select genes without prior knowledge of the process, and automatically determine the location and number of branches and loops. SLICER more accurately recovers the ordering of points along simulated trajectories than existing methods. We demonstrate the effectiveness of SLICER on previously published data from mouse lung cells and neural stem cells."}, {"title": "Bayesian Markov models consistently outperform PWMs at predicting motifs in nucleotide sequences", "url": "https://www.biorxiv.org/content/early/2016/04/07/047647", "tag": "Bioinformatics", "abstract": "Position weight matrices (PWMs) are the standard model for DNA and RNA regulatory motifs. In PWMs nucleotide probabilities are independent of nucleotides at other positions. Models that account for dependencies need many parameters and are prone to overfitting. We have developed a Bayesian approach for motif discovery using Markov models in which conditional probabilities of order k\u22121 act as priors for those of order k. This Bayesian Markov model (BMM) training automatically adapts model complexity to the amount of available data. We also derive an EM algorithm for de-novo discovery of enriched motifs. For transcription factor binding, BMMs achieve significantly (p < 0.063) higher cross-validated partial AUC than PWMs in 97% of 446 ChIP-seq ENCODE datasets and improve performance by 36% on average. BMMs also learn complex multipartite motifs, improving predictions of transcription start sites, polyadenylation sites, bacterial pause sites, and RNA binding sites by 26% \u2212 101%. BMMs never performed worse than PWMs. These robust improvements argue in favour of generally replacing PWMs by BMMs. The Bayesian Markov Model motif discovery software BaMM!motif is available under GPL at http://github.com/soedinglab/BaMMmotif."}, {"title": "PALADIN:Protein Alignment for Functional Profiling Whole Metagenome Shotgun Data", "url": "https://www.biorxiv.org/content/early/2016/04/07/047712", "tag": "Bioinformatics", "abstract": "Whole metagenome shotgun sequencing is a powerful approach for assaying the functional potential of microbial communities. Presently, we lack tools that efficiently and accurately align DNA reads against protein references, the technique necessary for constructing a functional profile. Here, we present PALADIN - a novel modification of Burrows-Wheeler Aligner that provides more accurate alignment and orders-of-magnitude improved efficiency by directly mapping in protein space."}, {"title": "Streaming algorithms for identification of pathogens and antibiotic resistance potential from real-time MinION sequencing", "url": "https://www.biorxiv.org/content/early/2016/04/06/019356", "tag": "Bioinformatics", "abstract": "The recently introduced Oxford Nanopore MinION platform generates DNA sequence data in real-time. This opens immense potential to shorten the sample-to-results time and is likely to lead to enormous benefits in rapid diagnosis of bacterial infection and identification of drug resistance. However, there are very few tools available for streaming analysis of real-time sequencing data. Here, we present a framework for streaming analysis of MinION real-time sequence data, together with probabilistic streaming algorithms for species typing, multi-locus strain typing, gene presence strain-typing and antibiotic resistance profile identification. Using three culture isolate samples as well as a mixed-species sample, we demonstrate that bacterial species and strain information can be obtained within 30 minutes of sequencing and using about 500 reads, initial drug-resistance profiles within two hours, and complete resistance profiles within 10 hours. Multi-locus strain typing required more than 15x coverage to generate confident assignments, whereas gene-presence typing could detect the presence of a known strain with 0.5x coverage. We also show that our pipeline can process over 100 times more data than the current throughput of the MinION on a desktop computer."}, {"title": "Human CODIS STR Loci Profiling from HTS Data", "url": "https://www.biorxiv.org/content/early/2016/04/06/047225", "tag": "Bioinformatics", "abstract": "Human DNA identification is currently performed by amplifying a small, defined set of short tandem repeat (STR) loci (e.g. CODIS) and analyzing the size of the alleles present at those loci by capillary electrophoresis. High-throughput DNA sequencing (HTS) could enable the simultaneous analysis of many additional STR and single nucleotide polymorphism (SNP) loci, improving accuracy and discrimination. However, it is necessary to demonstrate that HTS can generate accurate data on the CODIS loci to enable backwards compatibility with the FBI NDIS database. Sequencing can also detect novel polymorphisms within alleles that migrate with identical sizes by capillary electrophoresis, improving allele discrimination, and enhancing human identification analysis. All CODIS alleles from an individual can be amplified in a single, multiplex PCR reaction, and combined with additional barcoded samples prior to sequencing. A computational tool for allele identification from multiplexed sequence data has been developed. With longer-readlength platforms, 99.6% allele calling accuracy can be achieved. In the course of STR sequencing protocol development, 12 novel allele sequences have been identified for multiple loci. Sequencing STR loci combined with SNPs will enable new forensic applications."}, {"title": "Selecting Reads for Haplotype Assembly", "url": "https://www.biorxiv.org/content/early/2016/04/06/046771", "tag": "Bioinformatics", "abstract": "Haplotype assembly or read-based phasing is the problem of reconstructing both haplotypes of a diploid genome from next-generation sequencing data. This problem is formalized as the Minimum Error Correction (MEC) problem and can be solved using algorithms such as WhatsHap. The runtime of WhatsHap is exponential in the maximum coverage, which is hence controlled in a pre-processing step that selects reads to be used for phasing. Here, we report on a heuristic algorithm designed to choose beneficial reads for phasing, in particular to increase the connectivity of the phased blocks and the number of correctly phased variants compared to the random selection previously employed in by WhatsHap. The algorithm we describe has been integrated into the WhatsHap software, which is available under MIT licence from https://bitbucket.org/whatshap/whatshap."}, {"title": "KinLinks: Software Toolkit for Kinship Analysis and Pedigree Generation from HTS Datasets", "url": "https://www.biorxiv.org/content/early/2016/04/06/046938", "tag": "Bioinformatics", "abstract": "The ability to predict familial relationships from source DNA in multiple samples has a number of forensic and medical applications. Kinship testing of suspect DNA profiles against relatives in a law enforcement database can provide valuable investigative leads, determination of familial relationships can inform immigration decisions, and remains identification can provide closure to families of missing individuals. The proliferation of High-Throughput Sequencing technologies allows for enhanced capabilities to accurately predict familial relationships to the third degree and beyond. KinLinks, developed by MIT Lincoln Laboratory, is a software tool that predicts pairwise relationships and reconstructs kinship pedigrees for multiple input samples using single-nucleotide polymorphism (SNP) profiles. The software has been trained and evaluated on a set of 175 subjects (30,450 pairwise relationships), consisting of three multi-generational families and 52 geographically diverse subjects. Though a panel of 5396 SNPs was selected for kinship prediction, KinLinks is highly modular, allowing for the substitution of expanded SNP panels and additional training models as sequencing capabilities continue to progress. KinLinks builds on the SNP-calling capabilities of Sherlocks Toolkit, and is fully integrated with the Sherlocks Toolkit pipeline."}, {"title": "Order under uncertainty: robust differential expression analysis using probabilistic models for pseudotime inference", "url": "https://www.biorxiv.org/content/early/2016/04/05/047365", "tag": "Bioinformatics", "abstract": "Single cell gene expression profiling can be used to quantify transcriptional dynamics in temporal processes, such as cell differentiation, using computational methods to label each cell with a `pseudotime' where true time series experimentation is too difficult to perform. However, owing to the high variability in gene expression between individual cells, there is an inherent uncertainty in the precise temporal ordering of the cells. Preexisting methods for pseudotime ordering have predominantly given point estimates precluding a rigorous analysis of the implications of uncertainty. We use probabilistic modelling techniques to quantify pseudotime uncertainty and propagate this into downstream differential expression analysis. We demonstrate that reliance on a point estimate of pseudotime can lead to inflated false discovery rates compared and that probabilistic approaches provide greater robustness and measures of the temporal resolution that can be obtained from pseudotime inference."}, {"title": "AptaTRACE: Elucidating Sequence-Structure Binding Motifs by Uncovering Selection Trends in HT-SELEX Experiments", "url": "https://www.biorxiv.org/content/early/2016/04/05/047357", "tag": "Bioinformatics", "abstract": "Aptamers, short synthetic RNA/DNA molecules binding specific targets with high affinity and specificity, are utilized in an increasing spectrum of bio-medical applications. Aptamers are identified in vitro via the Systematic Evolution of Ligands by Exponential Enrichment (SELEX) protocol. SELEX selects binders through an iterative process that, starting from a pool of random ssDNA/RNA sequences, amplifies target-affine species through a series of selection cycles. HT-SELEX, which combines SELEX with high throughput sequencing, has recently transformed aptamer development and has opened the field to even more applications. HT-SELEX is capable of generating over half a billion data points, challenging computational scientists with the task of identifying aptamer properties such as sequence structure motifs that determine binding. While currently available motif finding approaches suggest partial solutions to this question, none possess the generality or scalability required for HT-SELEX data, and they do not take advantage of important properties of the experimental procedure. We present AptaTRACE, a novel approach for the identification of sequence-structure binding motifs in HT-SELEX derived aptamers. Our approach leverages the experimental design of the SELEX protocol and identifies sequence-structure motifs that show a signature of selection. Because of its unique approach, AptaTRACE can uncover motifs even when these are present in only a minuscule fraction of the pool. Due to these features, our method can help to reduce the number of selection cycles required to produce aptamers with the desired properties, thus reducing cost and time of this rather expensive procedure. The performance of the method on simulated and real data indicates that AptaTRACE can detect sequence-structure motifs even in highly challenging data."}, {"title": "qpMerge: Merging different peptide isoforms using a motif centric strategy", "url": "https://www.biorxiv.org/content/early/2016/04/05/047100", "tag": "Bioinformatics", "abstract": "Accurate quantification and enumeration of peptide motifs is hampered by redundancy in peptide identification. A single phosphorylation motif may be split across charge states, alternative modifications (e.g. acetylation and oxidation), and multiple miss-cleavage sites which render the biological interpretation of MS data a challenge. In addition motif redundancy can affect quantitative and statistical analysis and prevent a realistic comparison of peptide numbers between datasets. In this study, we present a merging tool set developed for the Galaxy workflow environment to achieve a non-redundant set of quantifications for phospho-motifs. We present a Galaxy workflow to merge three exemplar dataset, and observe reduced phospho-motif redundancy and decreased replicate variation. The qpMerge tools provide a straightforward and reusable approach to facilitating phospho-motif analysis. The source-code and wiki documentation is publically available at http://sourceforge.net/projects/ppmerge. The galaxy pipeline used in the exemplar analysis can be found at http://www.myexperiment.org/workflows/4186."}, {"title": "The healthy ageing gene expression signature for Alzheimer's disease diagnosis: a random sampling perspective", "url": "https://www.biorxiv.org/content/early/2016/04/05/047050", "tag": "Bioinformatics", "abstract": "In a recent publication, Sood et al. presented a set of 150 probe-sets which could be used in a diagnosis of Alzheimer disease (AD) based on gene expression. We reproduce some of their experiments, and show that the performance of their particular set of 150 probe-sets does not stand out compared to that of randomly sampled sets of 150 probe-sets from the same array."}, {"title": "Livestock market data for modeling disease spread among US cattle", "url": "https://www.biorxiv.org/content/early/2016/04/04/021980", "tag": "Bioinformatics", "abstract": "Transportation of livestock carries the risk of spreading foreign animal diseases, leading to costly public and private sector expenditures on disease containment and eradication. Livestock movement tracing systems in Europe, Australia and Japan have allowed epidemiologists to model the risks engendered by transportation of live animals and prepare responses designed to protect the livestock industry. Within the US, data on livestock movement is not sufficient for direct parameterization of models for disease spread, but network models that assimilate limited data provide a path forward in model development to inform preparedness for disease outbreaks in the US. Here, we develop a novel data stream, the information publicly reported by US livestock markets on the origin of cattle consigned at live auctions, and demonstrate the potential for estimating a national-scale network model of cattle movement. By aggregating auction reports generated weekly at markets in several states, including some archived reports spanning several years, we obtain a market-oriented sample of edges from the dynamic cattle transportation network in the US. We first propose a sampling framework that allows inference about shipments originating from operations not explicitly sampled and consigned at non-reporting livestock markets in the US, and we report key predictors that are influential in extrapolating beyond our opportunistic sample. As a demonstration of the utility gained from the data and fitted parameters, we model the critical role of market biosecurity procedures in the context of a spatially homogeneous but temporally dynamic representation of cattle movements following an introduction of a foreign animal disease. We conclude that auction market data fills critical gaps in our ability to model intrastate cattle movement for infectious disease dynamics, particularly with an ability to addresses the capacity of markets to amplify or control a livestock disease outbreak."}, {"title": "moGSA: integrative single sample gene-set analysis of multiple omics data", "url": "https://www.biorxiv.org/content/early/2016/04/03/046904", "tag": "Bioinformatics", "abstract": "Background: The increasing availability of multi-omics datasets has created an opportunity to understand how different biological pathways and molecules interact to cause disease. However, there is a lack of analysis methods that can integrate and interpret multiple experimental and molecular data types measured over the same set of samples. Result: To address this challenge, we introduce moGSA, a multivariate single sample gene-set analysis method. It uses multivariate latent variable decomposition to discover correlated global variance structure across datasets and calculates an integrated gene set enrichment score using the most informative features in each data type. Integrating multiple diverse sources of data reduces the impact of missing or unreliable information in any single data type, and may increase the power to discover subtle changes in gene-sets. We show that integrative analysis with moGSA outperforms existing single sample GSA methods on simulated data. We apply moGSA to two studies with real data. First, we discover similarities and differences in mRNA, protein and phosphorylation profiles of induced pluripotent and embryonic stem cell lines. Secondly, we report that three molecular subtypes are robustly discovered when copy number variation and mRNA profiling data of 308 bladder cancers from The Cancer Genome Atlas are integrated using moGSA. Our method provides positive or negative gene-set scores (with p-values) of each gene set in each sample. We demonstrate how to assess the influence of each data type or gene to a moGSA gene set score. With moGSA, there is no requirement to filter data to the intersect of features, therefore, all molecular features on all platforms may be included in the analysis. Conclusion: moGSA provides a powerful yet simple tool to perform integrated simple sample gene-set analysis. Its latent variable approach is fundamentally different to existing single sample GSA approaches. It is an attractive approach for data integration and is particularly suited to integrated cluster or molecular subtype discovery. It is available in the Bioconductor R package \"mogsa\"."}, {"title": "Privacy-Preserving Read Mapping Using Locality Sensitive Hashing and Secure Kmer Voting", "url": "https://www.biorxiv.org/content/early/2016/04/03/046920", "tag": "Bioinformatics", "abstract": "The recent explosion in the amount of available genome sequencing data imposes high computational demands on the tools designed to analyze it. Low-cost cloud computing has the potential to alleviate this burden. However, moving personal genome data analysis to the cloud raises serious privacy concerns. Read alignment is a critical and computationally intensive first step of most genomic data analysis pipelines. While significant effort has been dedicated to optimize the sensitivity and runtime efficiency of this step, few approaches have addressed outsourcing this computation securely to an untrusted party. The few secure solutions that have been proposed either do not scale to whole genome sequencing datasets or are not competitive with the state of the art in read mapping. In this paper, we present BALAUR, a privacy-preserving read mapping algorithm based on locality sensitive hashing and secure kmer voting. BALAUR securely outsources a significant portion of the computation to the public cloud by formulating the alignment task as a voting scheme between encrypted read and reference kmers. Our approach can easily handle typical genome-scale datasets and is highly competitive with non-cryptographic state-of-the-art read aligners in both accuracy and runtime performance on simulated and real read data. Moreover, our approach is significantly faster than state-of-the-art read aligners in long read mapping."}, {"title": "AVOCADO: Visualization of Workflow-Derived Data Provenance for Reproducible Biomedical Research", "url": "https://www.biorxiv.org/content/early/2016/04/01/044164", "tag": "Bioinformatics", "abstract": "A major challenge of data-driven biomedical research lies in the collection and representation of data provenance information to ensure reproducibility of findings. In order to communicate and reproduce multi-step analysis workflows executed on datasets that contain data for dozens or hundreds of samples, it is crucial to be able to visualize the provenance graph at different levels of aggregation. Most existing approaches are based on node-link diagrams, which do not scale to the complexity of typical data provenance graphs. In our proposed approach we reduce the complexity of the graph using hierarchical and motif-based aggregation. Based on user action and graph attributes a modular degree-of-interest (DoI) function is applied to expand parts of the graph that are relevant to the user. This interest-driven adaptive provenance visualization approach allows users to review and communicate complex multi-step analyses, which can be based on hundreds of files that are processed by numerous workflows. We integrate our approach into an analysis platform that captures extensive data provenance information and demonstrate its effectiveness by means of a biomedical usage scenario."}, {"title": "msVolcano: a flexible web application for visualizing quantitative proteomics data.", "url": "https://www.biorxiv.org/content/early/2016/04/01/038356", "tag": "Bioinformatics", "abstract": "We introduce msVolcano, a web application, for the visualization of label-free mass spectrometric data. It is optimized for the output of the MaxQuant data analysis pipeline of interactomics experiments and generates volcano plots with lists of interacting proteins. The user can optimize the cutoff values to find meaningful significant interactors for the tagged protein of interest. Optionally, stoichiometries of interacting proteins can be calculated. Several customization options are provided to the user for flexibility and publication-quality outputs can also be downloaded (tabular and graphical)."}, {"title": "Automated population identification and sorting algorithms for high-dimensional single-cell data", "url": "https://www.biorxiv.org/content/early/2016/03/31/046664", "tag": "Bioinformatics", "abstract": "Cell sorting or gating homogenous subpopulations from single-cell data enables cell-type specific characterization, such as cell-type genomic profiling as well as the study of tumor progression. This highlight summarizes recently developed automated gating algorithms that are optimized for both population identification and sorting homogeneous single cells in heterogeneous single-cell data. Data-driven gating strategies identify and/or sort homogeneous subpopulations from a heterogeneous population without relying on expert knowledge thereby removing human bias and variability. We further describe an optimized cell sorting strategy called CCAST based on Clustering, Classification and Sorting Trees which identifies the relevant gating markers, gating hierarchy and partitions that define underlying cell subpopulations. CCAST identifies more homogeneous subpopulations in several applications compared to prior sorting strategies and reveals simultaneous intracellular signaling across different lineage subtypes under different experimental conditions."}, {"title": "Evaluating performance of metagenomic characterization algorithms using in silico datasets generated with FASTQSim", "url": "https://www.biorxiv.org/content/early/2016/03/31/046532", "tag": "Bioinformatics", "abstract": "Background In silico bacterial, viral, and human truth datasets were generated to evaluate available metagenomics algorithms. Sequenced datasets include background organisms, creating ambiguity in the true source organism for each read. Bacterial and viral datasets were created with even and staggered coverage to evaluate organism identification, read mapping, and gene identification capabilities of available algorithms. These truth datasets are provided as a resource for the development and refinement of metagenomic algorithms. Algorithm performance on these truth datasets can inform decision makers on strengths and weaknesses of available algorithms and how the results may be best leveraged for bacterial and viral organism identification and characterization. Source organisms were selected to mirror communities described in the Human Microbiome Project as well as the emerging pathogens listed by the National Institute of Allergy and Infectious Diseases. The six in silico datasets were used to evaluate the performance of six leading metagenomics algorithms: MetaScope, Kraken, LMAT, MetaPhlAn, MetaCV, and MetaPhyler. Results Algorithms were evaluated on runtime, true positive organisms identified to the genus and species levels, false positive organisms identified to genus and species level, read mapping, relative abundance estimation, and gene calling. No algorithm out performed the others in all categories, and the algorithm or algorithms of choice strongly depends on analysis goals. MetaPhlAn excels for bacteria and LMAT for viruses. The algorithms were ranked by overall performance using a normalized weighted sum of the above metrics, and MetaScope emerged as the overall winner, followed by Kraken and LMAT. Conclusions Simulated FASTQ datasets with well-characterized truth data about microbial community composition reveal numerous insights about the relative strengths and weaknesses of the metagenomics algorithms evaluated. The simulated datasets are available to download from the Sequence Read Archive (SRP062063)."}, {"title": "OncoPhase: Quantification of somatic mutation cellular prevalence using phase information", "url": "https://www.biorxiv.org/content/early/2016/03/31/046631", "tag": "Bioinformatics", "abstract": "The impact of evolutionary processes in cancer and its implications for drug response, biomarker validation and clinical outcome requires careful consideration of the evolving mutational landscape of the cancer. Genome sequencing allows us to identify mutations but the prevalence of those mutations in heterogeneous tumours must be inferred. We describe a method that we call OncoPhase to compute the prevalence of somatic point mutations from genome sequencing analysis of heterogeneous tumours that combines information from nearby phased germline variants. We show using simulations that the use of phased germline information can give improved prevalence estimates over the use of somatic variants only."}, {"title": "Sensitive detection of rare disease-associated cell subsets via representation learning.", "url": "https://www.biorxiv.org/content/early/2016/03/31/046508", "tag": "Bioinformatics", "abstract": "Rare cell populations play a pivotal role in the initiation and progression of diseases like cancer. However, the identification of such subpopulations remains a difficult task. This work describes CellCnn, a representation learning approach to detect rare cell subsets associated with disease using high dimensional single cell measurements. Using CellCnn, we identify paracrine signaling and AIDS onset associated cell subsets in peripheral blood, and minimal residual disease associated populations in leukemia with frequencies as low as 0.005%."}, {"title": "FOCUS2: agile and sensitive classification of metagenomics data using a reduced database", "url": "https://www.biorxiv.org/content/early/2016/03/31/046425", "tag": "Bioinformatics", "abstract": "Summary: Metagenomics approaches rely on identifying the presence of organisms in the microbial community from a set of unknown DNA sequences. Sequence classification has valuable applications in multiple important areas of medical and environmental research. Here we introduce FOCUS2, an update of the previously published computational method FOCUS. FOCUS2 was tested with 10 simulated and 543 real metagenomes demonstrating that the program is more sensitive, faster, and more computationally efficient than existing methods. Availability: The Python implementation is freely available at https://edwards.sdsu.edu/FOCUS2."}, {"title": "Zika virus outbreak in the Americas: Is Aedes albopictus an overlooked culprit?", "url": "https://www.biorxiv.org/content/early/2016/03/30/044594", "tag": "Bioinformatics", "abstract": "Codon usage patterns of viruses reflect a series of evolutionary changes that enable viruses to shape their survival rates and fitness toward the external environment and, most importantly, their hosts. In the present study, we employed multiple codon usage analysis indices to determine genotype specific codon usage patterns of Zika virus (ZIKV) strains from the current outbreak and those reported previously. Several genotype specific and common codon usage traits were noted in ZIKV coding sequences, indicative of independent evolutionary origins from a common ancestor. The overall influence of natural selection was found to be more profound than that of mutation pressure and acting on specific set of viral genes belonging to ZIKV strains of Asian genotype from the recent outbreak. Furthermore, an interplay of codon adaptation and deoptimization have been observed in ZIKV genomes. The collective findings of codon analysis in association with the geographical data of Aedes populations in the Americas suggests that ZIKV have evolved a dynamic set of codon usage patterns in order to maintain a successful replication and transmission chain within multiple hosts and vectors."}, {"title": "Data-driven hypothesis weighting increases detection power in multiple testing", "url": "https://www.biorxiv.org/content/early/2016/03/30/034330", "tag": "Bioinformatics", "abstract": "Hypothesis weighting is a powerful approach for improving the power of data analyses that employ multiple testing. However, in general it is not evident how to choose the weights in a data-dependent manner. We describe independent hypothesis weighting (IHW), a method for making use of informative covariates that are independent of the test statistic under the null, but informative of each test's power or prior probability of the null hypothesis. Covariates can be continuous or categorical and need not fulfill any particular assumptions. The method increases statistical power in applications while controlling the false discovery rate (FDR) and produces additional insight by revealing the covariate-weight relationship. Independent hypothesis weighting is a practical approach to discovery of associations in large datasets."}, {"title": "BIGMAC : Breaking Inaccurate Genomes and Merging Assembled Contigs for long read metagenomic assembly", "url": "https://www.biorxiv.org/content/early/2016/03/29/045690", "tag": "Bioinformatics", "abstract": "The problem of de-novo assembly for metagenomes using only long reads is gaining attention. We study whether post-processing metagenomic assemblies with the original input long reads can result in quality improvement. Previous approaches have focused on pre-processing reads and optimizing assemblers. BIGMAC takes an alternative perspective to focus on the post-processing step. Using both the assembled contigs and original long reads as input, BIGMAC first breaks the contigs at potentially mis-assembled locations and subsequently scaffolds contigs. Our experiments on metagenomes assembled from long reads show that BIGMAC can improve assembly quality by reducing the number of mis-assemblies while maintaining/increasing N50 and N75. The software is available at https://github.com/kakitone/BIGMAC"}, {"title": "SAMSA: A comprehensive metatranscriptome analysis pipeline", "url": "https://www.biorxiv.org/content/early/2016/03/29/046201", "tag": "Bioinformatics", "abstract": "Background Although metatranscriptomics - the study of diverse microbial population activity based on RNA-seq data - is rapidly growing in popularity, there are limited options for biologists to analyze this type of data. Current approaches for processing metatranscriptomes rely on restricted databases and a dedicated computing cluster, or metagenome-based approaches that have not been fully evaluated for processing metatranscriptomic datasets. We created a new bioinformatics pipeline, SAMSA, designed specifically for metatranscriptome dataset analysis, which runs either in-house or in conjunction with Metagenome-RAST (MG-RAST) servers. Designed for use by researchers with relatively little bioinformatics experience, SAMSA offers a breakdown of metatranscriptome activity by organism or transcript function, and is fully open source. We next used this new tool to evaluate best practices for sequencing stool metatranscriptomes. Results Working with the MG-RAST annotation server, we constructed the Simple Annotation of Metatranscriptomes by Sequence Analysis (SAMSA) software package, a complete pipeline for the analysis of gut microbiome data. In creating this package, we determined optimal parameters in data collection and processing. SAMSA can summarize and evaluate raw annotation results, identifying abundant species and significant functional differences between metatranscriptomes. Using pilot data and simulated subsets, we determined experimental requirements for fecal gut metatranscriptomes. Sequences need to be either long reads (longer than 100bp) or paired-end reads that can be joined. Each sample nees 40-50 million raw sequences which can be expected to yield the 5-10 million annotated reads necessary for accurate abundance measures. We also demonstrated that ribosomal RNA depletion does not equally deplete ribosomes from all species within a sample, and remaining rRNA sequences should be discarded. Using publicly available metatranscriptome data in which rRNA was not depleted, we were able to demonstrate that organism activity can be measured using mRNA counts. We were also able to detect significant differences between control and experimental groups in both organism activity and functional activity. Conclusions By making this new pipeline publicly available, we have created a powerful new tool for metatranscriptomics research, offering a new method for greater insight into the activity of diverse microbial communities. We further recommend that stool metatranscriptomes be ribodepleted and sequenced in a 100bp paired end format with a minimum of 40 million reads per sample."}, {"title": "dupRadar: a Bioconductor package for the assessment of PCR artifacts in RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2016/03/29/046243", "tag": "Bioinformatics", "abstract": "Background: PCR clonal artefacts originating from NGS library preparation can affect both genomic as well as RNA-Seq applications when protocols are pushed to their limits. In RNA-Seq however the artifactual reads are not easy to tell apart from normal read duplication due to natural over-sequencing of highly expressed genes. Especially when working with little input material or single cells assessing the fraction of duplicate reads is an important quality control step for NGS data sets. Up to now there are only tools to calculate the global duplication rates that do not take into account the effect of gene expression levels which leaves them of limited use for RNA-Seq data. Results: Here we present the tool dupRadar, which provides an easy means to distinguish artefactual from natural duplicate reads in RNA-Seq data. dupRadar assesses the fraction of duplicate reads per gene dependent on the expression level. Apart from the Bioconductor package dupRadar we provide shell scripts for easy integration into processing pipelines."}, {"title": "SCPattern: A statistical approach to identify and classify expression changes in single cell RNA-seq experiments with ordered conditions", "url": "https://www.biorxiv.org/content/early/2016/03/29/046110", "tag": "Bioinformatics", "abstract": "Motivation: With the development of single cell RNA-seq (scRNA-seq) technology, scRNA-seq experiments with ordered conditions (e.g. time-course) are becoming common. Methods developed for analyzing ordered bulk RNA-seq experiments are not applicable to scRNA-seq, since their distributional assumptions are often violated by additional heterogeneities prevalent in scRNA-seq. Here we present SCPattern - an empirical Bayes model to characterize genes with expression changes in ordered scRNA-seq experiments. SCPattern utilizes the non-parametrical Kolmogorov-Smirnov statistic, thus it has the flexibility to identify genes with a wide variety of types of changes. Additionally, the Bayes framework allows SCPattern to classify genes into expression patterns with probability estimates. Results: Simulation results show that SCPattern is well powered for identifying genes with expression changes while the false discovery rate is well controlled. SCPattern is also able to accurately classify these dynamic genes into directional expression patterns. Applied to a scRNA-seq time course dataset studying human embryonic cell differentiation, SCPattern detected a group of important genes that are involved in mesendoderm and definitive endoderm cell fate decisions, positional patterning, and cell cycle. Availability and Implementation: The SCPattern is implemented as an R package along with a user-friendly graphical interface, which are available at: https://github.com/lengning/SCPattern Contact: rstewart@morgridge.org"}, {"title": "NextflowWorkbench: Reproducible and Reusable Workflows for Beginners and Experts", "url": "https://www.biorxiv.org/content/early/2016/03/28/041236", "tag": "Bioinformatics", "abstract": "Computational workflows and pipelines are often created to automate series of processing steps. For instance, workflows enable one to standardize analysis for large projects or core facilities, but are also useful for individual biologists who need to perform repetitive data processing. Some workflow systems, designed for beginners, offer a graphical user interface and have been very popular with biologists. In practice, these tools are infrequently used by more experienced bioinformaticians, who may require more flexibility or performance than afforded by the user interfaces, and seem to prefer developing workflows with scripting or command line tools. Here, we present a workflow system, the NextflowWorkbench (NW), which was designed for both beginners and experts, and blends the distinction between user interface and scripting language. This system extends and reuses the popular Nextflow workflow description language and shares its advantages. In contrast to Nextflow, NextflowWorkbench offers an integrated development environment that helps complete beginners get started with workflow development. Auto-completion helps beginners who do not know the syntax of the Nextflow language. Reusable processes provide modular workflows. Programmers will benefit from unique interactive features that help users work more productively with docker containers. We illustrate this tool with a workflow to estimate RNA-Seq counts using Kallisto. We found that beginners can be taught how to assemble this workflow in a two hours training session. NW workflows are portable and can execute on laptop/desktop computers with docker, on a lab cluster, or in the cloud to facilitate training. NextflowWorkbench is open-source and available at http://workflow.campagnelab.org."}, {"title": "Nanocall: An Open Source Basecaller for Oxford Nanopore Sequencing Data", "url": "https://www.biorxiv.org/content/early/2016/03/28/046086", "tag": "Bioinformatics", "abstract": "Motivation: The highly portable Oxford Nanopore MinION sequencer has enabled new applications of genome sequencing directly in the field. However, the MinION currently relies on a cloud computing platform, Metrichor (metrichor.com), for translating locally generated sequencing data into basecalls. Results: To allow offline and private analysis of MinION data, we created Nanocall. Nanocall is the first freely-available, open-source basecaller for Oxford Nanopore sequencing data and does not require an internet connection. On two ecoli and two human samples, with natural as well as PCR-amplified DNA, Nanocall reads have ~68% identity, directly comparable to Metrichor \"1D\" data. Further, Nanocall is efficient, processing ~500Kbp of sequence per core hour, and fully parallelized. Using 8 cores, Nanocall could basecall a MinION sequencing run in real time. Metrichor provides the ability to integrate the \"1D\" sequencing of template and complement strands of a single DNA molecule, and create a \"2D\" read. Nanocall does not currently integrate this technology, and addition of this capability will be an important future development. In summary, Nanocall is the first open-source, freely available, off-line basecaller for Oxford Nanopore sequencing data. Availability: Nanocall is available at github.com/mateidavid/nanocall, released under the MIT license. Contact: matei.david at oicr.on.ca"}, {"title": "MBPpred: Proteome-wide detection of membrane lipid-binding proteins using profile Hidden Markov Models", "url": "https://www.biorxiv.org/content/early/2016/03/26/034942", "tag": "Bioinformatics", "abstract": "A large number of modular domains that exhibit specific lipid binding properties are present in many membrane proteins involved in trafficking and signal transduction. These domains are present in either eukaryotic peripheral membrane or transmembrane proteins and are responsible for the non-covalent interactions of these proteins with membrane lipids. Here we report a profile Hidden Markov Model based method capable of detecting Membrane Binding Proteins (MBPs) from information encoded in their amino acid sequence, called MBPpred. The method identifies MBPs that contain one or more of the Membrane Binding Domains (MBDs) that have been described to date, and further classifies these proteins based on their position in respect to the membrane, either as peripheral or transmembrane. MBPpred is available online at http://bioinformatics.biol.uoa.gr/MBPpred. This method was applied in selected eukaryotic proteomes, in order to examine the characteristics they exhibit in various eukaryotic kingdoms and phylums."}, {"title": "GenVisR: Genomic Visualizations in R", "url": "https://www.biorxiv.org/content/early/2016/03/25/043604", "tag": "Bioinformatics", "abstract": "Summary: Visualizing and summarizing data from genomic studies continues to be a challenge. Here we introduce the GenVisR package to addresses this challenge by providing highly customizable, publication-quality graphics focused on cohort level genome analyses. GenVisR provides a rapid and easy-to-use suite of genomic visualization tools, while maintaining a high degree of flexibility by leveraging the abilities of ggplot2 and bioconductor. Availability and Implementation: GenVisR is an R package available via bioconductor (https://bioconductor.org/packages/GenVisR) under GPLv3. Support is available via GitHub (https://github.com/griffithlab/GenVisR/issues) and the Bioconductor support website."}, {"title": "Clusterflock: A Flocking Algorithm for Isolating Congruent Phylogenomic Datasets", "url": "https://www.biorxiv.org/content/early/2016/03/25/045773", "tag": "Bioinformatics", "abstract": "Background Collective animal behavior such as the flocking of birds or the shoaling of fish has inspired a class of algorithms designed to optimize distance-based clusters in various applications including document analysis and DNA microarrays. In the flocking model, individual agents respond only to their immediate environment and move according to a few simple rules. After several iterations the agents self-organize and clusters emerge without the need for partitional seeds. In addition to their unsupervised nature, flocking offers several computational advantages including the potential to decrease the number of required comparisons. Findings In Clusterflock, we implement a flocking algorithm designed to find groups (flocks) of orthologous gene families (OGFs) that share a common evolutionary history. Pairwise distances that measure the phylogenetic incongruence between OGFs guide flock formation. We test this approach on several simulated datasets varying the number of underlying topologies, the proportion of missing data, and evolutionary rates, and show that in datasets containing high levels of missing data and rate heterogeneity, clusterflock outperforms other well-established clustering techniques. We also demonstrate its utility on a known, large-scale recombination event in Staphylococcus aureus. By isolating sets of OGFs with divergent phylogenetic signal, we can pinpoint the recombined region without forcing a pre-determined number of groupings or defining a pre-determined incongruence threshold. Conclusions Clusterflock is an open source tool that can be used to discover horizontally transferred genes, recombining areas of chromosomes, and the phylogenetic 'core' of a genome. Though we use it in an evolutionary context, it is generalizable to any clustering problem. Users can write extensions to calculate any distance metric on the unit interval and use these distances to flock any type of data."}, {"title": "Tools and techniques for computational reproducibility", "url": "https://www.biorxiv.org/content/early/2016/03/24/022707", "tag": "Bioinformatics", "abstract": "When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. Computers can be programmed to execute analysis tasks, and those programs can be repeated and shared with others. Due to the deterministic nature of most computer programs, the same analysis tasks, applied to the same data, will often produce the same outputs. However, in practice, computational findings often cannot be reproduced due to complexities in how software is packaged, installed, and executed--and due to limitations in how scientists document analysis steps. Many tools and techniques are available to help overcome these challenges. Here we describe seven such strategies. With a broad scientific audience in mind, we describe strengths and limitations of each approach, as well as circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches."}, {"title": "OPERA-LG: Efficient and exact scaffolding of large, repeat-rich eukaryotic genomes with performance guarantees", "url": "https://www.biorxiv.org/content/early/2016/03/22/020230", "tag": "Bioinformatics", "abstract": "The assembly of large, repeat-rich eukaryotic genomes continues to represent a significant challenge in genomics. While long-read technologies have made the high-quality assembly of small, microbial genomes increasingly feasible, data generation can be prohibitively expensive for larger genomes. Fundamental advances in assembly algorithms are thus essential to exploit the characteristics of short and long-read sequencing technologies to consistently and reliably provide high-qualities assemblies in a cost-efficient manner. Here we present a scalable, exact algorithm (OPERA-LG) for the scaffold assembly of large, repeat-rich genomes that exhibits almost an order of magnitude improvement over the state-of-the-art programs in both correctness (>5X on average) and contiguity (>10X). This provides a systematic approach for combining data from different sequencing technologies, as well as a rigorous framework for scaffolding of repetitive sequences. OPERA-LG represents the first in a new class of algorithms that can efficiently assemble large genomes while providing formal guarantees about assembly quality, providing an avenue for systematic augmentation and improvement of 1000s of existing draft eukaryotic genome assemblies."}, {"title": "Mining Large Heterogeneous Cancer Data Sets Using Boolean Implications", "url": "https://www.biorxiv.org/content/early/2016/03/21/045021", "tag": "Bioinformatics", "abstract": "Boolean implications (if-then rules) provide a conceptually simple, uniform and highly scalable way to find associations between pairs of random variables. In this paper, we describe their usage in mining associations from large, heterogeneous cancer data sets. Next, we illustrate how Boolean implications were used to discover a new causal association between a mutation and aberrant DNA hypermethylation in acute myeloid leukemia as well as the therapeutic implications of this discovery. We conclude with a brief description of how Boolean implications can be extracted from a given data set."}, {"title": "AuPairWise: a method to estimate RNA-seq replicability through co-expression", "url": "https://www.biorxiv.org/content/early/2016/03/21/044669", "tag": "Bioinformatics", "abstract": "In addition to detecting novel transcripts and higher dynamic range, a principal claim for RNA-sequencing has been greater replicability, typically measured in sample-sample correlations of gene expression levels. Through a re-analysis of ENCODE data, we show that replicability of transcript abundances will provide misleading estimates of the replicability of conditional variation in transcript abundances (i.e., most expression experiments). Heuristics which implicitly address this problem have emerged in quality control measures to obtain 'good' differential expression results. However, these methods involve strict filters such as discarding low expressing genes or using technical replicates to remove discordant transcripts, and are costly or simply ad hoc. As an alternative, we model gene-level replicability of differential activity using co-expressing genes. We find that sets of housekeeping interactions provide a sensitive means of estimating the replicability of expression changes, where the co-expressing pair can be regarded as pseudo-replicates of one another. We model the effects of noise that perturbs a gene's expression within its usual distribution of values and show that perturbing expression by only 5% within that range is readily detectable (AUROC~0.73). We have made our method available as a set of easily implemented R scripts."}, {"title": "Construction and Experimental Validation of a Petri net Model of Wnt/\u03b2-catenin Signaling", "url": "https://www.biorxiv.org/content/early/2016/03/21/044966", "tag": "Bioinformatics", "abstract": "The Wnt/\u03b2-catenin signaling pathway is important for multiple developmental processes and tissue maintenance in adults. Consequently, deregulated signaling is involved in a range of human diseases including cancer and developmental defects. A better understanding of the intricate regulatory mechanism and effect of physiological (active) and pathophysiological (hyperactive) WNT signaling is important for predicting treatment response and developing novel therapies. The constitutively expressed CTNNB1 (commonly and hereafter referred to as \u03b2-catenin) is degraded by a destruction complex, composed of amongst other AXIN1 and GSK3. The destruction complex is inhibited during active signaling leading to \u03b2-catenin stabilization and induction of \u03b2-catenin/TCF target genes. In this study we investigated the mechanism and effect of \u03b2-catenin stabilization during active and hyperactive WNT signaling in a combined in silico and in vitro approach. We constructed a Petri net model of Wnt/\u03b2-catenin signaling including main players from the plasma membrane (WNT ligands and receptors), cytoplasmic effectors and the downstream negative feedback target gene AXIN2. We simulated the model with active (i.e. WNT stimulation) and hyperactive (i.e. GSK3 inhibition) signaling, which led to the following observations: 1) A dose- and time-dependent response was observed for both WNT stimulation and GSK3 inhibition. 2) The Wnt-pathway activity was 2-fold higher for GSK3 inhibition compared to WNT stimulation. Both of these observations were corroborated by TCF/LEF luciferase reporter assays. Using this experimentally validated model we simulated the effect of the negative feedback regulator AXIN2 upon WNT stimulation and observed an attenuated \u03b2-catenin stabilization. We furthermore simulated the effect of APC inactivating mutations, yielding a stabilization of \u03b2-catenin levels comparable to the Wnt-pathway activities observed in colorectal and breast cancer. Our model can be used for further investigation and viable predictions of the role of Wnt/\u03b2-catenin signaling in oncogenesis and development."}, {"title": "Identification of combinatorial and singular genomic signatures of host adaptation in influenza A H1N1 and H3N2 subtypes", "url": "https://www.biorxiv.org/content/early/2016/03/20/044909", "tag": "Bioinformatics", "abstract": "Background The underlying strategies used by influenza A viruses (IAVs) to adapt to new hosts while crossing the species barrier are complex and yet to be understood completely. Several studies have been published identifying singular genomic signatures that indicate such a host switch. The complexity of the problem suggested that in addition to the singular signatures, there might be a combinatorial use of such genomic features, in nature, defining adaptation to hosts.. Results We used computational rule-based modeling to identify combinatorial sets of interacting amino acid (aa) residues in 12 proteins of IAVs of H1N1 and H3N2 subtypes. We built highly accurate rule-based models for each protein that could differentiate between viral aa sequences coming from avian and human hosts, . We found 68 combinations of aa residues associated to host adaptation (HAd) on HA, M1, M2, NP, NS1, NEP, PA, PA-X, PB1 and PB2 proteins of the H1N1 subtype and 24 on M1, M2, NEP, PB1 and PB2 proteins of the H3N2 subtypes. In addition to these combinations, we found 132 novel singular aa signatures distributed among all proteins, including the newly discovered PA-X protein, of both subtypes. We showed that HA, NA, NP, NS1, NEP, PA-X and PA proteins of the H1N1 subtype carry H1N1-specific and HA, NA, PA-X, PA, PB1-F2 and PB1 of the H3N2 subtype carry H3N2-specific HAd signatures. M1, M2, PB1-F2, PB1 and PB2 of H1N1 subtype, in addition to H1N1 signatures, also carry H3N2 signatures. Similarly M1, M2, NP, NS1, NEP and PB2 of H3N2 subtype were shown to carry both H3N2 and H1N1 HAd signatures. Conclusions To sum it up, we computationally constructed simple IF-THEN rule-based models that could distinguish between aa sequences of virus particles originating from avian and human hosts. From the rules we identified combinations of aa residues as signatures facilitating the adaptation to specific hosts. The identification of combinatorial aa signatures suggests that the process of adaptation of IAVs to a new host is more complex than previously suggested. The present study provides a basis for further detailed studies with the aim to elucidate the molecular mechanisms providing the foundation for the adaptation process."}, {"title": "Automated SWATH Data Analysis Using Targeted Extraction of Ion Chromatograms", "url": "https://www.biorxiv.org/content/early/2016/03/19/044552", "tag": "Bioinformatics", "abstract": "Targeted mass spectrometry comprises a set of methods able to quantify protein analytes in complex mixtures with high accuracy and sensitivity. These methods, e.g., Selected Reaction Monitoring (SRM) and SWATH MS, use specific mass spectrometric coordinates (assays) for reproducible detection and quantification of proteins. In this protocol, we describe how to analyze in a targeted manner data from a SWATH MS experiment aimed at monitoring thousands of proteins reproducibly over many samples. We present a standard SWATH MS analysis workflow, including manual data analysis for quality control (based on Skyline) as well as automated data analysis with appropriate control of error rates (based on the OpenSWATH workflow). We also discuss considerations to ensure maximal coverage, reproducibility and quantitative accuracy."}, {"title": "NanoSim: nanopore sequence read simulator based on statistical characterization", "url": "https://www.biorxiv.org/content/early/2016/03/18/044545.1", "tag": "Bioinformatics", "abstract": "Motivation: In 2014, Oxford Nanopore Technologies (ONT) announced a new sequencing platform called MinION. The particular features of MinION reads, longer read lengths and single-molecule sequencing in particular, show potential for genome characterization. As of yet, the pre-commercial technology is exclusively available through early-access, and only a few datasets are publically available for testing. Further, no software exists that simulates MinION platform reads with genuine ONT characteristics. Results: In this article, we introduce NanoSim, a fast and scalable read simulator that captures the technology-specific features of ONT data, and allows for adjustments upon improvement of nanopore sequencing technology. Availability: NanoSim is written in Python and R. The source files and manual are available at the Genome Sciences Centre website: http://www.bcgsc.ca/platform/bioinfo/software/nanosim"}, {"title": "ACE: adaptive cluster expansion for maximum entropy graphical model inference", "url": "https://www.biorxiv.org/content/early/2016/03/18/044677", "tag": "Bioinformatics", "abstract": "Graphical models are often employed to interpret patterns of correlations observed in data through a network of interactions between the variables. Recently, Ising/Potts models, also known as Markov random fields, have been productively applied to diverse problems in biology, including the prediction of structural contacts from protein sequence data and the description of neural activity patterns. However, inference of such models is a challenging computational problem that cannot be solved exactly. Here we describe the adaptive cluster expansion (ACE) method to quickly and accurately infer Ising or Potts models based on correlation data. ACE avoids overfitting by constructing a sparse network of interactions sufficient to reproduce the observed correlation data within the statistical error expected due to finite sampling. When convergence of the ACE algorithm is slow, we combine it with a Boltzmann Machine Learning algorithm (BML). We illustrate this method on a variety of biological and artificial data sets and compare it to state-of-the-art approximate methods such as Gaussian and pseudo-likelihood inference.We show that ACE accurately reproduces the true parameters of the underlying model when they are known, and yields accurate statistical descriptions of both biological and artificial data. Models inferred by ACE have substantially better statistical performance compared to those obtained from faster Gaussian and pseudo-likelihood methods, which only precisely recover the structure of the interaction network."}, {"title": "MetaR: simple, high-level languages for data analysis with the R ecosystem", "url": "https://www.biorxiv.org/content/early/2016/03/17/030254", "tag": "Bioinformatics", "abstract": "Data analysis tools have become essential to the study of biology. Here, we applied language workbench technology (LWT) to create data analysis languages tailored for biologists with a diverse range of experience: from beginners with no programming experience to expert bioinformaticians and statisticians. A key novelty of our approach is its ability to blend user interface with scripting in a single platform. This feature helps beginners and experts alike analyze data more productively. This new approach has several advantages over state of the art approaches currently popular for data analysis: experts can design simplified data analysis languages that require no programming experience, and behave like graphical user interfaces, yet have the advantages of scripting. We report on such a simple language, called MetaR, which we have used to teach complete beginners how to call differentially expressed genes and build heatmaps. We found that beginners can complete this task in less than 2 hours with MetaR, when more traditional teaching with R and its packages would require several training sessions (6-24hrs). Furthermore, MetaR seamlessly integrates with docker to enable reproducibility of analyses and simplified R package installations during training sessions. We used the same approach to develop the first composable R language. A composable language is a language that can be extended with micro-languages. We illustrate this capability with a Biomart micro-language designed to compose with R and help R programmers query Biomart interactively to assemble specific queries to retrieve data, (The same micro-language also composes with MetaR to help beginners query Biomart.) Our teaching experience suggests that language design with LWT can be a compelling approach for developing intelligent data analysis tools and can accelerate training for common data analysis task. LWT offers an interactive environment with the potential to promote exchanges between beginner and expert data analysts."}, {"title": "mixMC: a multivariate statistical framework to gain insight into Microbial Communities", "url": "https://www.biorxiv.org/content/early/2016/03/16/044206", "tag": "Bioinformatics", "abstract": "Culture independent techniques, such as shotgun metagenomics and 16S rRNA amplicon sequencing have dramatically changed the way we can examine microbial communities. Recently, changes in microbial community structure and dynamics have been associated with a growing list of human diseases. The identification and comparison of bacteria driving those changes requires the development of sound statistical tools, especially if microbial biomarkers are to be used in a clinical setting. We present mixMC, a novel multivariate data analysis framework for metagenomic biomarker discovery. mixMC accounts for the compositional nature of 16S data and enables detection of subtle differences when high inter-subject variability is present due to microbial sampling performed repeatedly on the same subjects but in multiple habitats. Through data dimension reduction the multivariate methods provide insightful graphical visualisations to characterise each type of environment in a detailed manner. We applied mixMC to 16S microbiome studies focusing on multiple body sites in healthy individuals, compared our results with existing statistical tools and illustrated added value of using multivariate methodologies to fully characterise and compare microbial communities."}, {"title": "Strawberry: fast and accurate genome-guided transcript reconstruction and quantification from RNA-seq", "url": "https://www.biorxiv.org/content/early/2016/03/16/043802", "tag": "Bioinformatics", "abstract": "We propose a novel method and computational tool, Strawberry, for transcript reconstruction and quantification from paired-end RNA-seq data under the guidance of genome alignment and independent of gene annotation. Strawberry achieves this through disentangling assembly and quantification in a sequential manner. The application of a fast flow network algorithm for assembly speeds up the construction of a parsimonious set of transcripts. The resulting reduced data representation improves the efficiency of expression-level quantification. Strawberry leverages the speed and accuracy of transcript assembly and quantification in such a way that processing 10 million simulated reads (after alignment) requires only 90 seconds using a single thread while achieving over 92% correlation with the ground truth, making it the state-of-the-art method. Strawberry outperforms Cufflinks and StringTie, the two other leading methods, in many aspects, including the number of corrected assembled transcripts and the correlation with the ground truth of simulated RNA-seq data. Availability: Strawberry is written in C++11, and is available as open source software at https://github.com/ruolin/Strawberry under the GPLv3 license."}, {"title": "iFORM: incorporating Find Occurrence of Regulatory Motifs", "url": "https://www.biorxiv.org/content/early/2016/03/16/044214", "tag": "Bioinformatics", "abstract": "Accurately identifying binding sites of transcription factors (TFs) is crucial to understand the mechanisms of transcriptional regulation and human disease. We present incorporating Find Occurrence of Regulatory Motifs (iFORM), an easy-to-use tool for scanning DNA sequence with TF motifs described as position weight matrices (PWMs). iFORM achieves higher accuracy and sensitivity by integrating the results from five classical motif discovery programs based on Fisher's combined probability test. We have used iFORM to provide accurate results on a variety of data in the ENCODE Project and the NIH Roadmap Epigenomics Project, and has demonstrated its utility to further understand individual roles of functional elements.iFORM can be freely accessed athttps://github.com/wenjiegroup/iFORM."}, {"title": "Hierarchical Association Coefficient Algorithm", "url": "https://www.biorxiv.org/content/early/2016/03/16/043844.1", "tag": "Bioinformatics", "abstract": "Suppose that members in a universal set are categorized based on observations, and that categories can be stratified based on the average of observations within each category. Two sorting extremes can be obtained from the perspective of arbitrariness of an order of observations. The first sorting extreme is an increasing order of observations on ascendingly stratified categories. The second sorting extreme is a decreasing order of observations on ascendingly stratified categories. Hierarchical association coefficient (HA-coefficient) algorithm is based on a principle that any order of observations in stratified categorization can be placed between the two sorting extremes. The algorithm produces a proportion of how much an order of observations in stratified categorization is close to the first sorting extreme, or how much an order of categorized observations is distant from the second sorting extreme. This paper introduces a theory about the HA-coefficient algorithm, and shows its applications with example data. In addition, proving a reliability of the algorithm is shown through a simulation."}, {"title": "Efficient detection of repeating sites to accelerate phylogenetic likelihood calculations", "url": "https://www.biorxiv.org/content/early/2016/03/16/035873", "tag": "Bioinformatics", "abstract": "The phylogenetic likelihood function is the major computational bottleneck in several applications of evolutionary biology such as phylogenetic inference, species delimitation, model selection and divergence times estimation. Given the alignment, a tree and the evolutionary model parameters, the likelihood function computes the conditional likelihood vectors for every node of the tree. Vector entries for which all input data are identical result in redundant likelihood operations which, in turn, yield identical conditional values. Such operations can be omitted for improving run-time and, using appropriate data structures, reducing memory usage. We present a fast, novel method for identifying and omitting such redundant operations in phylogenetic likelihood calculations, and assess the performance improvement and memory saving attained by our method. Using empirical and simulated data sets, we show that a prototype implementation of our method yields up to 10-fold speedups and uses up to 78% less memory than one of the fastest and most highly tuned implementations of the phylogenetic likelihood function currently available. Our method is generic and can seamlessly be integrated into any phylogenetic likelihood implementation."}, {"title": "Synlet: an R package for systemically analyzing synthetic lethal RNA interference screen data", "url": "https://www.biorxiv.org/content/early/2016/03/15/043570", "tag": "Bioinformatics", "abstract": "Summary: High-throughput synthetic lethal RNA interference (RNAi) screen experiments shed important insights on the filed of cancer researches and drug discovery, but a comprehensive software for analyzing the data was not available yet. We present synlet, an R package provided a complete pipeline to process the synthetic lethal RNAi screens data. Synlet provides several methods to access the screen quality, including Z' factor and data visualization. B-score and fraction of control or samples normalization methods are implemented in the package. More importantly, synlet facilitates the process of hits selection by implementing several algorithms, providing the possibility to identify high confidence targets. Availability: The source code is freely available in Bioconductor (http://bioconductor.org/). Contact: c.shao@Dkfz-Heidelberg.de"}, {"title": "Modeling methyl-sensitive transcription factor motifs with an expanded epigenetic alphabet", "url": "https://www.biorxiv.org/content/early/2016/03/15/043794", "tag": "Bioinformatics", "abstract": "Introduction. Many transcription factors initiate transcription only in specific sequence contexts, providing the means for sequence specificity of transcriptional control. A four-letter DNA alphabet only partially describes the possible diversity of nucleobases a transcription factor might encounter. For instance, cytosine is often present in a covalently modified form: 5-methylcytosine (5mC). 5mC can be successively oxidized to 5-hydroxymethylcytosine (5hmC), 5-formylcytosine (5fC), and 5-carboxylcytosine (5caC). Just as transcription factors distinguish one unmodified nucleobase from another, some have been shown to distinguish unmodified bases from these covalently modified bases. Modification-sensitive transcription factors provide a mechanism by which widespread changes in DNA methylation and hydroxymethylation can dramatically shift active gene expression programs. Methods. To understand the effect of modified nucleobases on gene regulation, we developed methods to discover motifs and identify transcription factor binding sites in DNA with covalent modifications. Our models expand the standard A/C/G/T alphabet, adding m (5mC) h (5hmC), f (5fC), and c (5caC). We additionally add symbols to encode guanine complementary to these modified cytosine nucleobases, as well as symbols to represent states of ambiguous modification. We adapted the well-established position weight matrix model of transcription factor binding affinity to an expanded alphabet. We developed a program, Cytomod, to create a modified sequence. We also enhanced the MEME Suite to be able to handle custom alphabets. These versions permit users to specify new alphabets, anticipating future alphabet expansions. Results. We created an expanded-alphabet sequence using whole-genome maps of 5mC and 5hmC in naive ex vivo mouse T cells. Using this sequence and ChIP-seq data from Mouse ENCODE and others, we identified modification-sensitive cis-regulatory modules. We elucidated various known methylation binding preferences, including the preference of ZFP57 and C/EBP\u03b2 for methylated motifs and the preference of c-Myc for unmethylated E-box motifs. We demonstrated that our method is robust to parameter perturbations, with transcription factors' sensitivities for methylated and hydroxymethylated DNA broadly conserved across a range of modified base calling thresholds. Hypothesis testing across different threshold values was used to determine cutoffs most suitable for further analyses. Using these known binding preferences to tune model parameters enables discovery of novel modified motifs. Discussion. Hypothesis testing of motif central enrichment provides a natural means of differentially assessing modified versus unmodified binding affinity, without most of the limitations of a de novo analysis. This approach can be readily extended to other DNA modifications, provided genome-wide single-base resolution data is available. As more high-resolution epigenomic data becomes available, we expect this method to continue to yield insights into altered transcription factor binding affinities across a variety of modifications."}, {"title": "RNA-seq based analysis of population structure within the maize inbred B73", "url": "https://www.biorxiv.org/content/early/2016/03/15/043513", "tag": "Bioinformatics", "abstract": "B73 is a variety of maize (Zea mays ssp. mays) widely used in genetic, genomic, and phenotypic research around the world. B73 was also served as the reference genotype for the original maize genome sequencing project. The advent of large-scale RNA-sequencing as a method of measuring gene expression presents a unique opportunity to assess the level of relatedness among individuals identified as variety B73. The level of haplotype conservation and divergence across the genome were assessed using 27 RNA-seq data sets from 20 independent research groups in three countries. Several clearly distinct clades were identified among putatively B73 samples. A number of these blocks were defined by the presence of clearly defined genomic blocks containing a haplotype which did not match the published B73 reference genome. In a number of cases the relationship among B73 samples generated by different research groups recapitulated mentor/mentee relationships within the maize genetics community. A number of regions with distinct, dissimilar, haplotypes were identified in our study. However, when considering the age of the B73 accession -- greater than 40 years -- and the challenges of maintaining isogenic lines of a naturally outcrossing species, a strikingly high overall level of conservation was exhibited among B73 samples from around the globe."}, {"title": "A little walk from physical to biological complexity: protein folding and stability", "url": "https://www.biorxiv.org/content/early/2016/03/15/043737", "tag": "Bioinformatics", "abstract": "As an example of topic where biology and physics meet, we present the issue of protein folding and stability, and the development of thermodynamics-based bioinformatics tools that predict the stability and thermal resistance of proteins and the change of these quantities upon amino acid substitutions. These methods are based on knowledge-driven statistical potentials, derived from experimental protein structures using the inverse Boltzmann law."}, {"title": "Pathway-Structured Predictive Model for Cancer Survival Prediction: A Two-Stage Approach", "url": "https://www.biorxiv.org/content/early/2016/03/15/043661", "tag": "Bioinformatics", "abstract": "Heterogeneity in terms of tumor characteristics, prognosis, and survival among cancer patients has been a persistent problem for many decades. Currently, prognosis and outcome predictions are made based on clinical factors and/or by incorporating molecular profiling data. However, inaccurate prognosis and prediction may result by using only clinical or molecular information directly. One of the main shortcomings of past studies is the failure to incorporate prior biological information into the predictive model, given strong evidence of pathway-based genetic nature of cancer, i.e. the potential for oncogenes to be grouped into pathways based on biological functions such as cell survival, proliferation and metastatic dissemination. To address this problem, we propose a two-stage procedure to incorporate pathway information into the prognostic modeling using large-scale gene expression data. In the first stage, we fit all predictors within each pathway using penalized Cox model (Lasso, Ridge and Elastic Net) and Bayesian hierarchical Cox model. In the second stage, we combine the cross-validated prognostic scores of all pathways obtained in the first stage as new predictors to build an integrated prognostic model for prediction. We apply the proposed method to analyze breast cancer data from The Cancer Genome Atlas (TCGA), predicting overall survival using clinical data and gene expression profiling. The data includes ~20000 genes mapped into 109 pathways for 505 patients. The results show that the proposed approach not only improves survival prediction compared with the alternative analysis that ignores the pathway information, but also identifies significant biological pathways."}, {"title": "Highly accessible AU-rich regions in 3\u2032 untranslated regions are hotspots for binding of proteins and miRNAs", "url": "https://www.biorxiv.org/content/early/2016/03/13/042986", "tag": "Bioinformatics", "abstract": "MicroRNAs (miRNAs) are endogenous short non-coding RNAs involved in the regulation of gene expression at the post-transcriptional level typically by promoting destabilization or translational repression of target RNAs. Sometimes this regulation is absent or different, which likely is the result of interactions with other post-transcriptional factors, particularly RNA-binding proteins (RBPs). Despite the importance of the interactions between RBPs and miRNAs, little is known about how they affect post-transcriptional regulation in a global scale. In this study, we have analyzed CLIP datasets of 49 RBPs in HEK293 cells with the aim of understanding the interplay between RBPs and miRNAs in post-transcriptional regulation. Our results show that RBPs bind preferentially in conserved regulatory hotspots that frequently contain miRNA target sites. This organization facilitates the competition and cooperation among RBPs and the regulation of miRNA target site accessibility. In some cases RBP enrichment on target sites correlates with miRNA expression, suggesting coordination between the regulatory factors. However, in most cases, competition among factors is the most plausible interpretation of our data. Upon AGO2 knockdown, transcripts that contain such hotspots that overlap target sites of expressed miRNAs in 3\u2032UTRs are significantly less up-regulated than transcripts without them, suggesting that RBP binding limits miRNA accessibility. In conclusion, we show that RBP binding is concentrated in regulatory hotspots in 3\u2032UTRs. The presence of these hotspots facilitates the interaction among post-transcriptional regulators, that interact or compete with each other under different conditions. These hotspots are enriched in genes with regulatory functions such as DNA binding and RNA binding. Taken together, our results suggest that hotspots are important regulatory regions that define an extra layer of auto-regulatory control of post-transcriptional regulation."}, {"title": "PCR-free library preparation greatly reduces stutter noise at short tandem repeats", "url": "https://www.biorxiv.org/content/early/2016/03/12/043448", "tag": "Bioinformatics", "abstract": "Over the past several decades, the forensic and population genetic communities have increasingly leveraged short tandem repeats (STRs) for a variety of applications. The advent of next-generation sequencing technologies and STR-specific bioninformatic tools has enabled the profiling of hundreds of thousands of STRs across the genome. Nonetheless, these genotypes remain error-prone, hindering their utility in downstream analyses. One of the primary drivers of STR genotyping errors are \u201cstutter\u201d artifacts arising during the PCR amplification step of library preparation that add or delete copies of the repeat unit in observed sequencing reads. Recently, Illumina developed the TruSeq PCR-free library preparation protocol which eliminates the PCR step and theoretically should reduce stutter error. Here, I compare two high coverage whole genome sequencing datasets prepared with and without the PCR-free protocol. I find that this protocol reduces the percent of reads due to stutter by more than four-fold and results in higher confidence STR genotypes. Notably, stutter at homopolymers was decreased by more than 6-fold, making these previously inaccessible loci amenable to STR calling. This technological improvement shows good promise for significantly increasing the feasibility of obtaining high quality STR genotypes from next-generation sequencing technologies."}, {"title": "CrispRVariants: precisely charting the mutation spectrum in genome engineering experiments", "url": "https://www.biorxiv.org/content/early/2016/03/10/034140", "tag": "Bioinformatics", "abstract": "CRISPR-Cas9 and related technologies efficiently alter genomic DNA at targeted positions and have far-reaching implications for functional screening and therapeutic gene editing. Understanding and unlocking this potential requires accurate evaluation of editing efficiency. We show that methodological decisions for analyzing sequencing data can significantly affect mutagenesis efficiency estimates and we provide a comprehensive R-based toolkit, CrispRVariants and accompanying web tool CrispRVariantsLite, that resolves and localizes individual mutant alleles with respect to the endonuclease cut site. CrispRVariants-enabled analyses of newly generated and existing genome editing datasets underscore how careful consideration of the full variant spectrum gives insight toward effective guide and amplicon design as well as the mutagenic process."}, {"title": "Crunch: Completely Automated Analysis of ChIP-seq Data", "url": "https://www.biorxiv.org/content/early/2016/03/09/042903", "tag": "Bioinformatics", "abstract": "Today experimental groups routinely apply ChIP-seq technology to quantitatively characterize the genome-wide binding patterns of any molecule associated with the DNA. Here we present Crunch, a completely automated procedure for ChIP-seq data analysis, starting from raw read quality control, through read mapping, peak detection and annotation, and including comprehensive DNA sequence motif analysis. Among Crunch's novel features are a Bayesian mixture model that automatically fits a noise model and infers significantly enriched genomic regions in parallel, as well as a Gaussian mixture model for decomposing enriched regions into individual binding peaks. Moreover, Crunch uses a combination of de novo motif finding with binding site prediction for a large collection of known regulatory motifs to model the observed ChIP-seq signal in terms of novel and known regulatory motifs, extensively characterizing the contribution of each motif to explaining the ChIP-seq signal, and annotating which combinations of motifs occur in each binding peak. To make Crunch easily available to all researchers, including those without bioinformatics expertise, Crunch has been implemented as a web server (crunch.unibas.ch) that only requires users to upload their raw sequencing data, providing all results within an interactive graphical web interface. To demonstrate Crunch's power we apply it to a collection of 128 ChIP-seq data-sets from the ENCODE project, showing that Crunch's de novo motifs often outperform existing motifs in explaining the ChIP-seq signal, and that Crunch successfully identifies binding partners of the proteins that were immuno-precipitated."}, {"title": "A fast and accurate method for detection of IBD shared haplotypes in genome-wide SNP data", "url": "https://www.biorxiv.org/content/early/2016/03/09/042879", "tag": "Bioinformatics", "abstract": "Identical by descent (IBD) segments are used to understand a number of fundamental issues in genetics. IBD segments are typically detected using long stretches of identical alleles between haplotypes in whole-genome SNP data. Phase or SNP call errors in genomic data can degrade accuracy of IBD detection and lead to false positive calls, false negative calls, and under- or overextension of true IBD segments. Furthermore, the number of comparisons increases quadratically with sample size, requiring high computational efficiency. We developed a new IBD segment detection program, FISHR (Find IBD Shared Haplotypes Rapidly), in an attempt to accurately detect IBD segments and to better estimate their endpoints using an algorithm that is fast enough to be deployed on the very large whole-genome SNP datasets. We compared the performance of FISHR to three leading IBD segment detection programs: GERMLINE, refinedIBD, and HaploScore. Using simulated and real genomic sequence data, we show that FISHR is slightly more accurate than all programs at detecting long (greater than 3 cM) IBD segments but slightly less accurate than refinedIBD at detecting short (1 cM) IBD segments. Moreover, FISHR outperforms all programs in determining the true endpoints of IBD segments, which is important for several reasons. FISHR takes two to four times longer than GERMLINE to run, whereas both GERMLINE and FISHR were orders of magnitude faster than refinedIBD and HaploScore. Overall, FISHR provides accurate IBD detection in unrelated individuals and is computationally efficient enough to be utilized on large SNP datasets greater than 20,000 individuals."}, {"title": "Discovery of Primary, Cofactor, and Novel Transcription Factor Binding Site Motifs by Recursive, Thresholded Entropy Minimization", "url": "https://www.biorxiv.org/content/early/2016/03/09/042853", "tag": "Bioinformatics", "abstract": "Data from ChIP-seq experiments can determine the genome-wide binding specificities of transcription factors (TFs) and other regulatory proteins. In the present study, we analyzed 745 ENCODE ChIP-seq peak datasets of 189 human TFs with a novel motif discovery method that is based on recursive, thresholded entropy minimization. This method is able to distinguish correct information models from noisy motifs, quantify the strengths of individual sites based on affinity, and detect adjacent cofactor binding sites that coordinate with primary TFs. We derived homogeneous and bipartite information models for 89 sequence-specific TFs, which enabled discovery of 24 cofactor motifs for 118 TFs, and revealed 6 high-confidence novel motifs. The reliability and accuracy of these models were determined via three independent quality control criteria, including the detection of experimentally proven binding sites, comparison with previously published motifs and statistical analyses. We also predict previously unreported TF cobinding interactions, and new components of known TF complexes. Because they are based on information theory, the derived models constitute a powerful tool for detecting and predicting the effects of variants in known binding sites, and predicting previously unrecognized binding sites and target genes."}, {"title": "Quantitative proteome-based guidelines for intrinsic disorder characterization", "url": "https://www.biorxiv.org/content/early/2016/03/08/032847", "tag": "Bioinformatics", "abstract": "Intrinsically disordered proteins fail to adopt a stable three-dimensional structure under physiological conditions. It is now understood that many disordered proteins are not dysfunctional, but instead engage in numerous cellular processes, including signaling and regulation. Disorder characterization from amino acid sequence relies on computational disorder prediction algorithms. While numerous large-scale investigations of disorder have been performed using these algorithms, and have offered valuable insight regarding the prevalence of protein disorder in many organisms, critical proteome-based descriptive statistical guidelines that would enable the objective assessment of intrinsic disorder in a protein of interest remain to be established. Here we present a quantitative characterization of numerous disorder features using a rigorous non-parametric statistical approach, providing expected values and percentile cutoffs for each feature in ten eukaryotic proteomes. Our estimates utilize multiple ab initio disorder prediction algorithms grounded on physicochemical principles. Furthermore, we present novel threshold values, specific to both the prediction algorithms and the proteomes, defining the longest primary sequence length in which the significance of a continuous disordered region can be evaluated on the basis of length alone. The guidelines presented here are intended to improve the interpretation of disorder content and continuous disorder predictions from the proteomic point of view."}, {"title": "mLDM: a new hierarchical Bayesian statistical model for sparse microbioal association discovery", "url": "https://www.biorxiv.org/content/early/2016/03/07/042630", "tag": "Bioinformatics", "abstract": "Interpretive analysis of metagenomic data depends on an understanding of the underlying associations among microbes from metagenomic samples.Although several statistical tools have been developed for metagenomic association studies, they suffer from compositional bias or fail to take into account environmental factors that directly affect the composition of a given microbial community. In this paper, we propose metagenomic Lognormal-Dirichlet-Multinomial (mLDM), a hierarchical Bayesian model with sparsity constraints to bypass compositional bias and discover new associations among microbes and between microbes and environmental factors. The mLDM model can 1) infer both conditionally dependent associations among microbes and direct associations between microbes and environmental factors; 2) consider both compositional bias and variance of metagenomic data; and 3) estimate absolute abundance for microbes. Thus, conditionally dependent association can capture direct relationship underlying microbial pairs and remove the indirect connections induced from other common factors. Empirical studies show the effectiveness of the mLDM model, using both synthetic data and the TARA Oceans eukaryotic data by comparing it with several state-of-the-art methodologies. Finally, mLDM is applied to western English Channel data and finds some interesting associations."}, {"title": "AnnoLnc: a web server for systematically annotating novel human lncRNAs", "url": "https://www.biorxiv.org/content/early/2016/03/07/042655", "tag": "Bioinformatics", "abstract": "Although the repertoire of human lncRNAs has rapidly expanded, their biological function and regulation remain largely elusive. Here, we present AnnoLnc (http://annolnc.cbi.pku.edu.cn), an online portal for systematically annotating newly identified human lncRNAs. AnnoLnc offers a full spectrum of annotations covering genomic location, RNA secondary structure, expression, transcriptional regulation, miRNA interaction, protein interaction, genetic association and evolution, as well as an abstraction-based text summary and various intuitive figures to help biologists quickly grasp the essentials. In addition to an intuitive and mobile-friendly Web interactive design, AnnoLnc supports batch analysis and provides JSON-based Web Service APIs for programmatic analysis. To the best of our knowledge, AnnoLnc is the first web server to provide on-the-fly and systematic annotation for newly identified human lncRNAs. Some case studies have shown the power of AnnoLnc to inspire novel hypotheses."}, {"title": "Exploring miRNAs as the key to understand symptoms induced by ZIKA virus infection through a collaborative database.", "url": "https://www.biorxiv.org/content/early/2016/03/06/042382", "tag": "Bioinformatics", "abstract": "In early 2015, a ZIKA Virus (ZIKV) infection outbreak was recognized in northeast Brazil, where concerns over its possible links with infant microcephaly have been discussed. Providing a definitive link between ZIKV infection and birth defects is still a big challenge. MicroRNAs (miRNAs), are small noncoding RNAs that regulating post-transcriptional gene expression by translational repression, and play important roles in viral pathogenesis and brain development. The potential for flavivirus-mediated miRNA signaling dysfunction in brain-tissue develop provides a compelling mechanism underlying perceived linked between ZIKV and microcephaly. Here, we provide novel evidences toward to understand the mechanism in which miRNAs can be linked to the congenital ZIKA syndrome symptoms. Moreover, following World Health Organization (WHO) recommendations, we have assembled a database to help target mechanistic investigations of this possible relationship between ZIKV symptoms and miRNA mediated human gene expression, helping to foster potential targets for therapy."}, {"title": "Rail-dbGaP: analyzing dbGaP-protected data in the cloud with Amazon Elastic MapReduce", "url": "https://www.biorxiv.org/content/early/2016/03/05/035287", "tag": "Bioinformatics", "abstract": "Motivation: Public archives contain thousands of trillions of bases of valuable sequencing data. More than 40% of the Sequence Read Archive is human data protected by provisions such as dbGaP. To analyze dbGaP-protected data, researchers must typically work with IT administrators and signing officials to ensure all levels of security are implemented at their institution. This is a major obstacle, impeding reproducibility and reducing the utility of archived data. Results: We present a protocol and software tool for analyzing protected data in a commercial cloud. The protocol, Rail-dbGaP, is applicable to any tool running on Amazon Web Services Elastic MapReduce. The tool, Rail-RNA v0.2, is a spliced aligner for RNA-seq data, which we demonstrate by running on 9,662 samples from the dbGaP-protected GTEx consortium dataset. The Rail-dbGaP protocol makes explicit for the first time the steps an investigator must take to develop Elastic MapReduce pipelines that analyze dbGaP-protected data in a manner compliant with NIH guidelines. Rail-RNA automates implementation of the protocol, making it easy for typical biomedical investigators to study protected RNA-seq data, regardless of their local IT resources or expertise. Availability: Rail-RNA is available from http://rail.bio. Technical details on the Rail-dbGaP protocol as well as an implementation walkthrough are available at https://github.com/nellore/rail-dbgap. Detailed instructions on running Rail-RNA on dbGaP-protected data using Amazon Web Services are available at http://docs.rail.bio/dbgap/."}, {"title": "The Ensembl Variant Effect Predictor", "url": "https://www.biorxiv.org/content/early/2016/03/04/042374", "tag": "Bioinformatics", "abstract": "The Ensembl Variant Effect Predictor (VEP) is a powerful toolset for the analysis, annotation and prioritization of genomic variants, including in non-coding regions. The VEP accurately predicts the effects of sequence variants on transcripts, protein products, regulatory regions and binding motifs by leveraging the high quality, broad scope, and integrated nature of the Ensembl databases. In addition, it enables comparison with a large collection of existing publicly available variation data within Ensembl to provide insights into population and ancestral genetics, phenotypes and disease. The VEP is open source and free to use. It is available via a simple web interface (http://www.ensembl.org/vep), a powerful downloadable package, and both Ensembl\u2019s Perl and REST application program interface (API) services."}, {"title": "Learning from heterogeneous data sources: an application in spatial proteomics", "url": "https://www.biorxiv.org/content/early/2016/03/04/022152", "tag": "Bioinformatics", "abstract": "Sub-cellular localisation of proteins is an essential post-translational regulatory mechanism that can be assayed using high-throughput mass spectrometry (MS). These MS-based spatial proteomics experiments enable to pinpoint the sub-cellular distribution of thousands of proteins in a specific system under controlled conditions. Recent advances in high-throughput MS methods have yielded a plethora of experimental spatial proteomics data for the cell biology community. Yet, there are many third-party data sources, such as immunofluorescence microscopy or protein annotations and sequences, which represent a rich and vast source of complementary information. We present a unique transfer learning classification framework that utilises a nearest neighbour or support vector machine system, to integrate heterogeneous data sources to considerably improve on the quantity and quality of sub-cellular protein assignment. We demonstrate the utility of our algorithms through evaluation of five experimental datasets, from four different species in conjunction with four different auxiliary data sources to classify proteins to tens of sub-cellular compartments with high generalisation accuracy. We further apply the method to a experiment on pluripotent mouse embryonic stem cells to classify a set of previously unknown proteins, and validate our findings against a recent high resolution map of the mouse stem cell proteome. The methodology is distributed as part of the open-source Bioconductor pRoloc suite for spatial proteomics data analysis."}, {"title": "BrowseVCF: a web-based application and workflow to quickly prioritise disease-causative variants in VCF files.", "url": "https://www.biorxiv.org/content/early/2016/03/04/034769", "tag": "Bioinformatics", "abstract": "As sequencing costs associated with fast advancing Next Generation Sequencing (NGS) technologies continue to decrease, variant discovery is becoming a more affordable and popular analysis method among research laboratories. Following variant calling and annotation, accurate variant filtering is a crucial step to extract meaningful biological information from sequencing data and to investigate disease etiology. However, the standard variant call file format (VCF) used to store this valuable information is not easy to handle without bioinformatics skills, thus preventing many investigators from directly analysing their data. Here, we present BrowseVCF, an easy-to-use stand-alone software that enables researchers to browse, query and filter millions of variants in a few seconds. Key features include the possibility to store intermediate search results, to query user-defined gene lists, to group samples for family or tumour/normal studies, to download a report of the filters applied, and to export the filtered variants in spreadsheet format. Additionally, BrowseVCF is suitable for any DNA variant analysis (exome, whole-genome and targeted sequencing), can be used also for non-diploid genomes, and is able to discriminate between Single Nucleotide Polymorphisms (SNPs), Insertions/Deletions (InDels), and Multiple Nucleotide Polymorphisms (MNPs). Owing to its portable implementation, BrowseVCF can be used either on personal computers or as part of automated analysis pipelines. The software can be initialised with a few clicks on any operating system without any special administrative or installation permissions. It is actively developed and maintained, and freely available for download from https://github.com/BSGOxford/BrowseVCF/releases/latest."}, {"title": "A simple analytical formula to compute the residual Mutual Information between pairs of data vectors", "url": "https://www.biorxiv.org/content/early/2016/03/02/041988", "tag": "Bioinformatics", "abstract": "The Mutual Information of pairs of data vectors, for example sequence alignment positions or gene expression profiles, is a quantitative measure of the interdependence between the data. However, data vectors based on a finite number of samples retain non-zero Mutual Information values even for completely random data, which is referred to as background or residual Mutual Information. Estimates of the residual Mutual Information have so far been obtained through heuristic or numerical approximations. Here we introduce a simple analytical formula for the computation of the residual Mutual Information that yields precise values and does not require the joint probabilities between the vector elements as input."}, {"title": "OncoScape: Exploring the cancer aberration landscape by genomic data fusion", "url": "https://www.biorxiv.org/content/early/2016/03/02/041962", "tag": "Bioinformatics", "abstract": "Although large-scale efforts for molecular profiling of cancer samples provide multiple data types for many samples, most approaches for finding candidate cancer genes rely on somatic mutations and DNA copy number only. We present a new method, OncoScape, which, for the first time, exploits five complementary data types across 11 cancer types to identify new candidate cancer genes. We find many rarely mutated genes that are strongly affected by other aberrations. We retrieve the majority of known cancer genes but also new candidates such as STK31 and MSRA with very high confidence. Several genes show a dual oncogene- and tumor suppressor-like behavior depending on the tumor type. Most notably, the well- known tumor suppressor RB1 shows strong oncogene-like signal in colon cancer. We applied OncoScape to cell lines representing ten cancer types, providing the most comprehensive comparison of aberrations in cell lines and tumor samples to date. This revealed that glioblastoma, breast and colon cancer show strong similarity between cell lines and tumors, while head and neck squamous cell carcinoma and bladder cancer, exhibit very little similarity between cell lines and tumors. To facilitate exploration of the cancer aberration landscape, we created a web portal enabling interactive analysis of OncoScape results."}, {"title": "Vcfanno: fast, flexible annotation of genetic variants", "url": "https://www.biorxiv.org/content/early/2016/03/02/041863", "tag": "Bioinformatics", "abstract": "Background: The integration of genome annotations and reference databases is critical to identifying genetic variants that may be of interest in studies of disease or other traits. However, comprehensive variant annotation with diverse file formats is difficult with existing methods. Results: We have developed vcfanno as a flexible toolset that simplifies the annotation of genetic variants in VCF format. Vcfanno can extract and summarize multiple attributes from one or more annotation files and append the resulting annotations to the INFO field of the original VCF file. Vcfanno also integrates the lua scripting language so that users can easily develop custom annotations and metrics. By leveraging a new parallel chromosome sweeping algorithm, it enables rapid annotation of both whole-exome and whole-genome datasets. We demonstrate this performance by annotating over 85.3 million variants in less than 17 minutes (>85,000 variants per second) with 50 attributes from 17 commonly used genome annotations. Conclusions: Vcfanno is a flexible software package that provides researchers with the ability to annotate genetic variation with a wide range of datasets and reference databases in diverse genomic formats. Availability: The vcfanno source code is available at https://github.com/brentp/vcfanno under the MIT license, and platform-specific binaries are available at https://github.com/brentp/vcfanno/releases. Detailed documentation is available at http://brentp.github.io/vcfanno/, and the code underlying the analyses presented can be found at https://github.com/brentp/vcfanno/tree/master/scripts/paper."}, {"title": "Diffusion pseudotime robustly reconstructs lineage branching", "url": "https://www.biorxiv.org/content/early/2016/03/02/041384", "tag": "Bioinformatics", "abstract": "Single-cell gene expression profiles of differentiating cells encode their intrinsic latent temporal order. We describe an efficient way to robustly estimate this order according to a diffusion pseudotime, which measures transitions on all length scales between cells using diffusion-like random walks. This allows us to identify cells that undergo branching decisions or are in metastable states, and thereby genes differentially regulated at these states."}, {"title": "SignaFish: a zebrafish-specific signaling pathway resource", "url": "https://www.biorxiv.org/content/early/2016/02/29/041822", "tag": "Bioinformatics", "abstract": "Understanding living systems requires an in depth knowledge of the signaling networks that drive cellular homeostasis, regulate intercellular communication and contribute to cell fates during development. Several resources exist to provide high-throughput datasets or manually curated interaction information from human or invertebrate model organisms. We previously developed SignaLink, a uniformly curated, multi-layered signaling resource containing information for human and for the model organisms nematode Caenorhabditis elegans and fruit fly Drosophila melanogaster. Until now, the use of the SignaLink database for zebrafish pathway analysis was limited. To overcome this limitation we created SignaFish (http://signafish.org), a fish-specific signaling resource, built using the concept of SignaLink. SignaFish contains more than 200 curation based signaling interactions, 132 further interactions listed in other resources, and it also lists potential miRNA based regulatory connections for 7 major signaling pathways. From the SignaFish website, users can reach other web resources, such as ZFIN. SignaFish provides signaling or signaling-related interactions that can be examined for each gene, or downloaded for each signaling pathway. We believe that the SignaFish resource will serve as a novel navigating point for experimental design and evaluation for the zebrafish community and for researchers focusing on non-model fish species, such as cyclids."}, {"title": "Assessment of Circulating Copy Number Variant Detection for Cancer Screening", "url": "https://www.biorxiv.org/content/early/2016/02/29/041848", "tag": "Bioinformatics", "abstract": "Current high-sensitivity cancer screening methods suffer from false positive rates that lead to numerous unnecessary procedures and questionable public health benefit overall. Detection of circulating tumor DNA (ctDNA) has the potential to transform cancer screening. Thus far, nearly all ctDNA studies have focused on detection of tumor-specific point mutations. However, ctDNA point mutation detection methods developed to date lack either the scope or sensitivity necessary to be useful for cancer screening, due to the extremely low (<1%) ctDNA fraction derived from early stage tumors. We suggest that tumor-derived copy number variant (CNV) detection is theoretically a superior means of ctDNA-based cancer screening for many tumor types, given that, relative to point mutations, each individual tumor CNV contributes a much larger number of ctDNA fragments to the overall pool of circulating DNA. Here we perform an in silico assessment of the potential for ctDNA CNV-based cancer screening across many common cancers."}, {"title": "An alternative class of targets for microRNAs containing CG dinucleotide", "url": "https://www.biorxiv.org/content/early/2016/02/28/026708", "tag": "Bioinformatics", "abstract": "MicroRNAs are endogenous ~23nt RNAs which regulate mRNA targets mainly through perfect pairing with their seed region (positions 2-7). Several instances of bulge UTR sequence can also be recognized by miRNA as their target. But such non-Watson-Crick base pairings are incompletely understood. We found a group of miRNAs which had very few conservative targets while potentially having a subclass of bulge message RNA targets. Compared with the canonical target, these bulge targets had a weaker negative correlation with the miRNA expression, and either were downregulated in the miRNA overexpression experiment or upregulated in the miRNA knock-down experiment."}, {"title": "Genome-Wide Prediction of cis-Regulatory Regions Using Supervised Deep Learning Methods", "url": "https://www.biorxiv.org/content/early/2016/02/28/041616", "tag": "Bioinformatics", "abstract": "Identifying active cis-regulatory regions in the human genome is critical for understanding gene regulation and assessing the impact of genetic variation on phenotype. Based on rich data resources such as the Encyclopedia of DNA Elements (ENCODE) and the Functional Annotation of the Mammalian Genome (FANTOM) projects, we introduce DECRES, the first supervised deep learning approach for the identification of enhancer and promoter regions in the human genome. Due to their ability to discover patterns in large and complex data, the introduction of deep learning methods enables a significant advance in our knowledge of the genomic locations of cis-regulatory regions. Using models for well-characterized cell lines, we identify key experimental features that contribute to the predictive performance. Applying DECRES, we delineate locations of 300,000 candidate enhancers genome wide (6.8% of the genome, of which 40,000 are supported by bidirectional transcription data) and 26,000 candidate promoters (0.6% of the genome)."}, {"title": "Does the choice of nucleotide substitution models matter topologically?", "url": "https://www.biorxiv.org/content/early/2016/02/26/041566", "tag": "Bioinformatics", "abstract": "Background: In the context of a master level programming practical at the computer science department of the Karlsruhe Institute of Technology, we developed and make available an open-source code for testing all 203 possible nucleotide substitution models in the Maximum Likelihood (ML) setting under the common Akaike, corrected Akaike, and Bayesian information criteria. We address the question if model selection matters topologically, that is, if conducting ML inferences under the optimal, instead of a standard General Time Reversible model, yields different tree topologies. We also assess, to which degree models selected and trees inferred under the three standard criteria (AIC, AICc, BIC) differ. Finally, we assess if the definition of the sample size (#sites versus #sites x #taxa) yields different models and, as a consequence, different tree topologies. Results: We find that, all three factors (by order of impact: nucleotide model selection, information criterion used, sample size definition) can yield topologically substantially different final tree topologies (topological difference exceeding 10%) for approximately 5% of the tree inferences conducted on the 39 empirical datasets used in our study. Conclusions: We find that, using the best-fit nucleotide substitution model may change the final ML tree topology compared to an inference under a default GTR model. The effect is less pronounced when comparing distinct information criteria. Nonetheless, in some cases we did obtain substantial topological differences."}, {"title": "VcfR: an R package to manipulate and visualize VCF format data", "url": "https://www.biorxiv.org/content/early/2016/02/26/041277", "tag": "Bioinformatics", "abstract": "Software to call single nucleotide polymorphisms or related genetic variants has converged on the variant call format (VCF) as the output format of choice. This has created a need for tools to work with VCF files. While an increasing number of software exists to read VCF data, many only extract the genotypes without including the data associated with each genotype that describes its quality. We created the R package vcfR to address this issue. We developed a VCF file exploration tool implemented in the R language because R provides an interactive experience and an environment that is commonly used for genetic data analysis. Functions to read and write VCF files into R as well as functions to extract portions of the data and to plot summary statistics of the data are implemented. VcfR further provides the ability to visualize how various parameterizations of the data affect the results. Additional tools are included to integrate sequence (FASTA) and annotation data (GFF) for visualization of genomic regions such as chromosomes. Conversion functions translate data from the vcfR data structure to formats used by other R genetics packages. Computationally intensive functions are implemented in C++ to improve performance. Use of these tools is intended to facilitate VCF data exploration, including intuitive methods for data quality control and easy export to other R packages for further analysis. VcfR thus provides essential, novel tools currently not available in R."}, {"title": "How reliable are ligand-centric methods for Target Fishing?", "url": "https://www.biorxiv.org/content/early/2016/02/25/032946", "tag": "Bioinformatics", "abstract": "Computational methods for Target Fishing permit the discovery of new targets of a drug, which may result in its reposition in a new indication or improving our current understanding of its efficacy and side effects. Being a relatively recent class of methods, there is still a need to improve their validation, which is technically difficult, often limited to a small part of the targets and not easily interpretable by the user. Here we propose a new validation approach and use it to assess the reliability of ligand-centric techniques, which by construction provide the widest coverage of the proteome. On average over approved drugs, we find that only five predicted targets will have to be tested in order to find at least two true targets with submicromolar potency, although a strong variability in performance is observed. Also, we identify an average of eight known targets in approved drugs, which suggests that polypharmacology is a common and strong event. In addition, we observe that many known targets of approved drugs are currently missed by these methods. Lastly, by using a control group of randomly-selected molecules, we discuss how the data generation process confounds this analysis and its implications for method validation."}, {"title": "4C-ker: A method to reproducibly identify genome-wide interactions captured by 4C-Seq experiments", "url": "https://www.biorxiv.org/content/early/2016/02/23/030569", "tag": "Bioinformatics", "abstract": "4C-Seq has proven to be a powerful technique to identify genome-wide interactions with a single locus of interest (or bait) that can be important for gene regulation. However, analysis of 4C-Seq data is complicated by the many biases inherent to the technique. An important consideration when dealing with 4C-Seq data is the differences in resolution of signal across the genome that result from differences in 3D distance separation from the bait. This leads to the highest signal in the region immediately surrounding the bait and increasingly lower signals in far-cis and trans. Another important aspect of 4C-Seq is the resolution, which is greatly influenced by the choice of restriction enzyme and the frequency at which it can cut the genome. Thus, it is important that a 4C-Seq analysis method is flexible enough to analyze data generated using different enzymes and to identify interactions across the entire genome. Current methods for 4C-Seq analysis only identify interactions in regions near the bait or in regions located in far-cis and trans, but no method comprehensively analyzes 4C signals of different length scales. In addition, some methods also fail in experiments where chromatin fragments are generated using frequent cutter restriction enzymes. Here, we describe 4C-ker, a Hidden-Markov Model based analysis that identifies regions throughout the genome that interact with the 4C bait locus. In addition we incorporate methods for the identification of differential interactions in multiple 4C-seq datasets collected from different genotypes or experimental conditions. Adaptive window sizes are used to correct for differences in signal coverage in near-bait regions, far-cis and trans chromosomes. Using several datasets, we demonstrate that 4C-ker outperforms all existing 4C-Seq pipelines in its ability to reproducibly identify interaction domains at all genomic ranges with different resolution enzymes."}, {"title": "Kronos: a workflow assembler for genome analytics and informatics", "url": "https://www.biorxiv.org/content/early/2016/02/22/040352", "tag": "Bioinformatics", "abstract": "The field of next generation sequencing informatics has matured to a point where algorithmic advances in sequence alignment and individual feature detection methods have stabilized. Practical and robust implementation of complex analytical workflows (where such tools are structured into \"best practices\" for automated analysis of NGS datasets) still requires significant programming investment and expertise. We present Kronos, a software platform for automating the development and execution of reproducible, auditable and distributable bioinformatics workflows. Kronos obviates the need for explicit coding of workflows by compiling a text configuration file into executable Python applications. The framework of each workflow includes a run manager to execute the encoded workflows locally (or on a cluster or cloud), parallelize tasks, and log all runtime events. Resulting workflows are highly modular and configurable by construction, facilitating flexible and extensible meta-applications which can be modified easily through configuration file editing. The workflows are fully encoded for ease of distribution and can be instantiated on external systems, promoting and facilitating reproducible research and comparative analyses. We introduce a framework for building Kronos components which function as shareable, modular nodes in Kronos workflows. The Kronos platform provides a standard framework for developers to implement custom tools, reuse existing tools, and contribute to the community at large. Kronos is shipped with both Docker and Amazon AWS machine images. It is free, open source and available through PyPI (Python Package Index) and https://github.com/jtaghiyar/kronos. Keywords: genomics; workflow; pipeline; reproducibility"}, {"title": "Enabling the democratization of the genomics revolution with a fully integrated web-based bioinformatics platform", "url": "https://www.biorxiv.org/content/early/2016/02/21/040477", "tag": "Bioinformatics", "abstract": "Continued advancements in sequencing technologies have fueled the development of new sequencing applications and promise to flood current databases with raw data. A number of factors prevent the seamless and easy use of these data, including the breadth of project goals, the wide array of tools that individually perform fractions of any given analysis, the large number of associated software/hardware dependencies, and the detailed expertise required to perform these analyses. To address these issues, we have developed an intuitive web-based environment with a wide assortment of integrated and cutting-edge bioinformatics tools. These preconfigured workflows provide even novice next-generation sequencing users with the ability to perform many complex analyses with only a few mouse clicks, and, within the context of the same environment, to visualize and further interrogate their results. This bioinformatics platform is an initial attempt at Empowering the Development of Genomics Expertise (EDGE) in a wide range of applications."}, {"title": "StrainSeeker: fast identification of bacterial strains from unassembled sequencing reads using user-provided guide trees.", "url": "https://www.biorxiv.org/content/early/2016/02/19/040261", "tag": "Bioinformatics", "abstract": "Background Fast, accurate and high-throughput detection of bacteria is in great demand. The present work was conducted to investigate the possibility of identifying both known and unknown bacterial strains from unassembled next-generation sequencing reads using custom-made guide trees. Results A program named StrainSeeker was developed that constructs a list of specific k-mers for each node of any given Newick-format tree and enables rapid identification of bacterial genomes within minutes. StrainSeeker has been tested and shown to successfully identify Escherichia coli strains from mixed samples in less than 5 minutes. StrainSeeker can also identify bacterial strains from highly diverse metagenomics samples. StrainSeeker is available at http://bioinfo.ut.ee/strainseeker. Conclusions Our novel approach can be useful for both clinical diagnostics and research laboratories because novel bacterial strains are constantly emerging and their fast and accurate detection is very important."}, {"title": "variancePartition: Interpreting drivers of variation in complex gene expression studies", "url": "https://www.biorxiv.org/content/early/2016/02/19/040170", "tag": "Bioinformatics", "abstract": "As genomics studies become more complex and consider multiple sources of biological and technical variation, characterizing these drivers of variation becomes essential to understanding disease biology and regulatory genetics. We describe a statistical and visualization framework, variancePartition, to prioritize drivers of variation with a genome-wide summary, and identify genes that deviate from the genome-wide trend. variancePartition enables rapid interpretation of complex gene expression studies and is applicable to many genomics assays."}, {"title": "MetaCycle: an integrated R package to evaluate periodicity in large scale data", "url": "https://www.biorxiv.org/content/early/2016/02/19/040345", "tag": "Bioinformatics", "abstract": "Summary: Detecting periodicity in large scale data remains a challenge. Different algorithms offer strengths and weaknesses in statistical power, sensitivity to outliers, ease of use, and sampling requirements. While efforts have been made to identify best of breed algorithms, relatively little research has gone into integrating these methods in a generalizable method. Here we present MetaCycle, an R package that incorporates ARSER, JTK_CYCLE, and Lomb-Scargle to conveniently evaluate periodicity in time-series data. Availability and implementation: MetaCycle package is available on the CRAN repository (https://cran.r-project.org/web/packages/MetaCycle/index.html) and GitHub (https://github.com/gangwug/MetaCycle). Contact: hogenesch@gmail.com Supplementary information: Supplementary data are available at Bioinformatics online."}, {"title": "Reconstruction of ancestral genomes in presence of gene gain and loss.", "url": "https://www.biorxiv.org/content/early/2016/02/18/040196", "tag": "Bioinformatics", "abstract": "Since most dramatic genomic changes are caused by genome rearrangements as well as gene duplications and gain/loss events, it becomes crucial to understand their mechanisms and reconstruct ancestral genomes of the given genomes. This problem was shown to be NP-complete even in the \"simplest\" case of three genomes, thus calling for heuristic rather than exact algorithmic solutions. At the same time, a larger number of input genomes may actually simplify the problem in practice as it was earlier illustrated with MGRA, a state-of-the-art software tool for reconstruction of ancestral genomes of multiple genomes. One of the key obstacles for MGRA and other similar tools is presence of breakpoint reuses when the same breakpoint region is broken by several different genome rearrangements in the course of evolution. Furthermore, such tools are often limited to genomes composed of the same genes with each gene present in a single copy in every genome. This limitation makes these tools inapplicable for many biological datasets and degrades the resolution of ancestral reconstructions in diverse datasets. We address these deficiencies by extending the MGRA algorithm to genomes with unequal gene contents. The developed next-generation tool MGRA2 can handle gene gain/loss events and shares the ability of MGRA to reconstruct ancestral genomes uniquely in the case of limited breakpoint reuse. Furthermore, MGRA2 employs a number of novel heuristics to cope with higher breakpoint reuse and process datasets inaccessible for MGRA. In practical experiments, MGRA2 shows superior performance for simulated and real genomes as compared to other ancestral genomes reconstruction tools. The MGRA2 tool is distributed as an open-source software and can be downloaded from GitHub repository http://github.com/ablab/mgra/. It is also available in the form of a web-server at http://mgra.cblab.org, which makes it readily accessible for inexperienced users."}, {"title": "Combating Chagas Disease Through Inhibition of Tiam1, a Rho GTPase Guanine Nucleotide Exchange Factor", "url": "https://www.biorxiv.org/content/early/2016/02/18/040121", "tag": "Bioinformatics", "abstract": "Chagas disease is a major cardiovascular affliction primarily endemic to Latin American countries, affecting some ten to twelve million people worldwide. The currently available drugs, Benznidazole and Nifurtimox, are ineffective in the chronic stages and induce severe side effects. In an attempt to improve this situation we use an in silico drug repurposing strategy to correlate drug-protein interactions with positive clinical outcomes. The strategy involves a protein functional site similarity search, along with computational docking studies and, given the findings, a phosphatidylinositol (PIP) strip test to determine the activity of Posaconazole, a recently developed antifungal triazole, in conjunction with Tiam1, a Rho GTPase Guanine Nucleotide Exchange Factor. The results from both computational and in vitro studies indicate possible inhibition of phosphoinositides via Posaconazole, preventing Rho GTPase-induced proliferation of T. cruzi, the etiological agent of Chagas Disease."}, {"title": "Succinct Colored de Bruijn Graphs", "url": "https://www.biorxiv.org/content/early/2016/02/18/040071", "tag": "Bioinformatics", "abstract": "Iqbal et al. (Nature Genetics, 2012) introduced the colored de Bruijn graph, a variant of the classic de Bruijn graph, which is aimed at \"detecting and genotyping simple and complex genetic variants in an individual or population\". Because they are intended to be applied to massive population level data, it is essential that the graphs be represented efficiently. Unfortunately, current succinct de Bruijn graph representations are not directly applicable to the colored de Bruijn graph, which require additional information to be succinctly encoded as well as support for non-standard traversal operations. Our data structure dramatically reduces the amount of memory required to store and use the colored de Bruijn graph, with some penalty to runtime, allowing it to be applied in much larger and more ambitious sequence projects than was previously possible."}, {"title": "Establishing evidenced-based best practice for the de novo assembly and evaluation of transcriptomes from non-model organisms", "url": "https://www.biorxiv.org/content/early/2016/02/18/035642", "tag": "Bioinformatics", "abstract": "Characterizing transcriptomes in both model and non-model organisms has resulted in a massive increase in our understanding of biological phenomena. This boon, largely made possible via high-throughput sequencing, means that studies of functional, evolutionary and population genomics are now being done by hundreds or even thousands of labs around the world. For many, these studies begin with a de novo transcriptome assembly, which is a technically complicated process involving several discrete steps. Each step may be accomplished in one of several different ways, using different software packages, each producing different results. This analytical complexity begs the question -- Which method(s) are optimal? Using reference and non-reference based evaluative methods, I propose a set of guidelines that aim to standardize and facilitate the process of transcriptome assembly. These recommendations include the generation of between 20 million and 40 million sequencing reads from single individual where possible, error correction of reads, gentle quality trimming, assembly filtering using Transrate and/or gene expression, annotation using dammit, and appropriate reporting. These recommendations have been extensively benchmarked and applied to publicly available transcriptomes, resulting in improvements in both content and contiguity. To facilitate the implementation of the proposed standardized methods, I have released a set of version controlled open-sourced code, The Oyster River Protocol for Transcriptome Assembly, available at http://oyster-river-protocol.rtfd.org/."}, {"title": "MetaPalette: A K-mer painting approach for metagenomic taxonomic profiling and quantification of novel strain variation", "url": "https://www.biorxiv.org/content/early/2016/02/17/039909", "tag": "Bioinformatics", "abstract": "Metagenomic profiling is challenging in part because of the highly uneven sampling of the tree of life by genome sequencing projects and the limitations imposed by performing phylogenetic inference at fixed taxonomic ranks. We present the algorithm MetaPalette which uses long k-mer sizes (k=30, 50) to fit a k-mer \"palette\" of a given sample to the k-mer palette of reference organisms. By modeling the k-mer palettes of unknown organisms, the method also gives an indication of the presence, abundance, and evolutionary relatedness of novel organisms present in the sample. The method returns a traditional, fixed-rank taxonomic profile which is shown on independently simulated data to be one of the most accurate to date. Tree figures are also returned that quantify the relatedness of novel organisms to reference sequences and the accuracy of such figures is demonstrated on simulated spike-ins and a metagenomic soil sample. The software implementing MetaPalette is available at: https://github.com/dkoslicki/MetaPalette Pre-trained databases are included for Archaea, Bacteria, Eukaryota, and viruses."}, {"title": "The divisible load balance problem with shared cost and its application to phylogenetic inference", "url": "https://www.biorxiv.org/content/early/2016/02/12/035840", "tag": "Bioinformatics", "abstract": "Motivated by load balance issues in parallel calculations of the phylogenetic likelihood function, we recently introduced an approximation algorithm for efficiently distributing partitioned alignment data to a given number of CPUs. The goal is to balance the accumulated number of sites per CPU, and, at the same time, to minimize the maximum number of unique partitions per CPU. The approximation algorithm assumes that likelihood calculations on individual alignment sites have identical runtimes and that likelihood calculation times on distinct sites are entirely independent from each other. However, a recently introduced optimization of the phylogenetic likelihood function, the so-called site repeats technique, violates both aforementioned assumptions. To this end, we modify our data distribution algorithm and explore 72 distinct heuristic strategies that take into account the additional restrictions induced by site repeats, to yield a 'good' parallel load balance. Our best heuristic strategy yields a reduction in required arithmetic operations that ranges between 2% and 92% with an average of 62% for all test datasets using 2, 4, 8, 16, 32, and 64 CPUs compared to the original site-repeat-agnostic data distribution algorithm."}, {"title": "Detecting Horizontal Gene Transfer by Mapping Sequencing Reads Across Species Boundaries", "url": "https://www.biorxiv.org/content/early/2016/02/11/039495", "tag": "Bioinformatics", "abstract": "Horizontal gene transfer (HGT) is a fundamental mechanism that enables organisms such as bacteria to directly transfer genetic material between distant species. This way, bacteria can acquire new traits such as antibiotic resistance or pathogenic toxins. Current bioinformatics approaches focus on the detection of past HGT events by exploring phylogenetic trees or genome composition inconsistencies. However, this normally requires the availability of finished and fully annotated genomes and of sufficiently large deviations that allow detection. Thus, these techniques are not widely applicable. Especially in an outbreak scenario where new HGT mediated pathogens emerge, there is need for fast and precise HGT detection. Next-generation sequencing (NGS) technologies can facilitate swift analysis of unknown pathogens but, to the best of our knowledge, so far no approach uses NGS data directly to detect HGTs. We present Daisy, a novel mapping-based tool for HGT detection directly from NGS data. Daisy determines HGT boundaries with split-read mapping and evaluates candidate regions relying on read pair and coverage information. Daisy can successfully detect HGT regions with base pair resolution in both simulated and real data, and outperforms alternative approaches using a genome assembly of the reads. We see our approach as a powerful complement for a comprehensive analysis of HGT in the context of NGS data. Daisy is freely available from http://github.com/ktrappe/daisy."}, {"title": "IMP: a pipeline for reproducible metagenomic and metatranscriptomic analyses", "url": "https://www.biorxiv.org/content/early/2016/02/10/039263", "tag": "Bioinformatics", "abstract": "We present IMP, an automated pipeline for reproducible integrated analyses of coupled metagenomic and metatranscriptomic data. IMP incorporates preprocessing, iterative co-assembly of metagenomic and metatranscriptomic data, analyses of microbial community structure and function as well as genomic signature-based visualizations. Complementary use of metagenomic and metatranscriptomic data improves assembly quality and enables the estimation of both population abundance and community activity while allowing the recovery and analysis of potentially important components, such as RNA viruses. IMP is containerized using Docker which ensures reproducibility. IMP is available at http://r3lab.uni.lu/web/imp/."}, {"title": "TRONCO: an R package for the inference of cancer progression models from heterogeneous genomic data", "url": "https://www.biorxiv.org/content/early/2016/02/10/027474", "tag": "Bioinformatics", "abstract": "Motivation: We introduce TRONCO (TRanslational ONCOlogy), an open-source R package that implements the state-of-the-art algorithms for the inference of cancer progression models from (epi)genomic mutational profiles. TRONCO can be used to extract population-level models describing the trends of accumulation of alterations in a cohort of cross-sectional samples, e.g., retrieved from publicly available databases, and individual-level models that reveal the clonal evolutionary history in single cancer patients, when multiple samples, e.g., multiple biopsies or single-cell sequencing data, are available. The resulting models can provide key hints in uncovering the evolutionary trajectories of cancer, especially for precision medicine or personalized therapy. Availability: TRONCO is released under the GPL license, it is hosted in the Software section at http://bimib.disco.unimib.it/ and archived also at bioconductor.org. Contact: tronco@disco.unimib.it"}, {"title": "Bacterial sequences detected in 99 out of 99 serum samples from Ebola patients", "url": "https://www.biorxiv.org/content/early/2016/02/09/039107", "tag": "Bioinformatics", "abstract": "Evolution and clinical manifestations of Ebola virus (EBOV) infection overlap with the pathologic processes that occur in sepsis1. Some viruses certainly compromise the immune system, leading to a breach in the integrity of the mucosal epithelial barrier, thus allowing bacterial translocation2, 3. Guided by these facts, we wondered if bacteria could be involved in the pathogenesis of some of the septic shock-like symptoms typical of EBOV infected patients, something that could have a dramatic impact on the design of new treatment approaches. We decided to search for bacteria in available EBOV patient sequence datasets. Given that EBOV is an RNA virus and that, hence, some NGS sequencing experiments carried out to sequence the EBOV genomes were RNA-Seq experiments, we thought that, if there were any bacteria in patient serum, at least some bacterial RNA might probably be detected in the sequenced material from Ebola patients. Thus, we searched for bacteria in a RNA-Seq public dataset from 99 Ebola samples from the last outbreak4, and surprisingly, in spite of the certainly suboptimal experimental conditions for bacterial RNA sequencing, we found bacteria in all of the 99 samples"}, {"title": "Partial derivatives meta-analysis: pooled analyses when individual participant data cannot be shared", "url": "https://www.biorxiv.org/content/early/2016/02/07/038893", "tag": "Bioinformatics", "abstract": "Joint analysis of data from multiple studies in collaborative efforts strengthens scientific evidence, with the gold standard approach being the pooling of individual participant data (IPD). However, sharing IPD often has legal, ethical, and logistic constraints for sensitive or high-dimensional data, such as in clinical trials, observational studies, and large-scale omics studies. Therefore, meta-analysis of study-level effect estimates is routinely done, but this compromises on statistical power, accuracy, and flexibility. Here we propose a novel meta-analytical approach, named partial derivatives meta-analysis, that is mathematically equivalent to using IPD, yet only requires the sharing of aggregate data. It not only yields identical results as pooled IPD analyses, but also allows post-hoc adjustments for covariates and stratification without the need for site-specific re-analysis. Thus, in case that IPD cannot be shared, partial derivatives meta-analysis still produces gold standard results, which can be used to better inform guidelines and policies on clinical practice."}, {"title": "Multiple goal pursuit -- to kill two birds with one stone or to fall between two stools ?", "url": "https://www.biorxiv.org/content/early/2016/02/07/038919", "tag": "Bioinformatics", "abstract": "We present the simple phenomenological (but -- analytic) model allowing to formalize description of multitasking, i.e. simultaneous performing several tasks. That process requires distribution of attention, and for great number of goals do not lead to success. Our consideration shows that simultaneous performing more than two tasks is, most likely, impossible."}, {"title": "CRISPResso: sequencing analysis toolbox for CRISPR genome editing", "url": "https://www.biorxiv.org/content/early/2016/02/07/031203", "tag": "Bioinformatics", "abstract": "Recent progress in genome editing technologies, in particular the CRISPR-Cas9 system, has provided new opportunities to investigate the biological functions of genomic sequences by targeted mutagenesis. Double strand breaks (DSBs) resulting from site-specific Cas9 cleavage can be resolved by endogenous DNA repair pathways such as non-homologous end joining (NHEJ) or homology-directed repair (HDR). Deep sequencing of amplified genomic regions allows for quantitative and sensitive detection of targeted mutations. However, no standard analytic tool to date has been developed to systematically enumerate and visualize these events, and to solve challenging issues such as amplification or sequencing errors, experimental variation in sequence quality, ambiguous alignment of variable length indels, and difficulty in deconvoluting mixed HDR/NHEJ outcomes. To address these issues we developed CRISPResso, a robust computational pipeline that enables accurate quantification and visualization of CRISPR-Cas9 outcomes as well as comprehensive evaluation of effects on coding sequences, noncoding elements and selected off-target sites."}, {"title": "SplAdder: Identification, quantification and testing of alternative splicing events from RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2016/02/05/017095", "tag": "Bioinformatics", "abstract": "Motivation: Understanding the occurrence and regulation of alternative splicing (AS) is a key task towards explaining the regulatory processes that shape the complex transcriptomes of higher eukaryotes. With the advent of high-throughput sequencing of RNA (RNA-Seq), the diversity of AS transcripts could be measured at an unprecedented depth. Although the catalog of known AS events has grown ever since, novel transcripts are commonly observed when working with less well annotated organisms, in the context of disease, or within large populations. Whereas an identification of complete transcripts is technically challenging and computationally expensive, focusing on single splicing events as a proxy for transcriptome characteristics is fruitful and sufficient for a wide range of analyses. Results: We present SplAdder, an alternative splicing toolbox, that takes RNA-Seq alignments and an annotation file as input to i) augment the annotation based on RNA-Seq evidence, ii) identify alternative splicing events present in the augmented annotation graph, iii) quantify and confirm these events based on the RNA-Seq data, and iv) test for significant quantitative differences between samples. Thereby, our main focus lies on performance, accuracy and usability. Availability: Source code and documentation are available for download at http://github.com/ratschlab/spladder. Example data, introductory information and a small tutorial are accessible via http://bioweb.me/spladder. Contact: andre.kahles@ratschlab.org, gunnar.ratsch@ratschlab.org"}, {"title": "Information-dependent Enrichment Analysis Reveals Time-dependent Transcriptional Regulation of the Estrogen Pathway of Toxicity", "url": "https://www.biorxiv.org/content/early/2016/02/04/038570", "tag": "Bioinformatics", "abstract": "The twenty-first century vision for toxicology involves a transition away from high-dose animal studies and into in vitro and computational models. This movement requires mapping pathways of toxicity through an understanding of how in vitro systems respond to chemical perturbation. Uncovering transcription factors responsible for gene expression patterns is essential for defining pathways of toxicity, and ultimately, for determining chemical mode of action, through which a toxicant acts. Traditionally this is achieved via chromatin immunoprecipitation studies and summarized by calculating, which transcription factors are statistically associated with the up- and down-regulated genes. These lists are commonly determined via statistical or fold-change cutoffs, a procedure that is sensitive to statistical power and may not be relevant to determining transcription factor associations. To move away from an arbitrary statistical or fold-change based cutoffs, we have developed in the context of the Mapping the Human Toxome project, a novel enrichment paradigm called Information Dependent Enrichment Analysis (IDEA) to guide identification of the transcription factor network. We used the test case of endocrine disruption of MCF-7 cells activated by 17\u03b2 estradiol (E2). Using this new approach, we were able to establish a time course for transcriptional and functional responses to E2. ER\u03b1 and ER\u03b2 are associated with short-term transcriptional changes in response to E2. Sustained exposure leads to the recruitment of an additional ensemble of transcription factors and alteration of cell-cycle machinery. TFAP2C and SOX2 were the transcription factors most highly correlated with dose. E2F7, E2F1 and Foxm1, which are involved in cell proliferation, were enriched only at 24h. IDEA is, therefore, a novel tool to identify candidate pathways of toxicity, clearly outperforming Gene-set Enrichment Analysis but with similar results as Weighted Gene Correlation Network Analysis, which helps to identify genes not annotated to pathways."}, {"title": "PoPoolationTE2: comparative population genomics of transposable elements using Pool-Seq", "url": "https://www.biorxiv.org/content/early/2016/02/03/038745", "tag": "Bioinformatics", "abstract": "The evolutionary dynamics of transposable elements (TEs) are still poorly understood. One reason is that TE abundance needs to be studied at the population level, and despite recent advances in sequencing technologies, characterizing TE abundance in multiple populations by sequencing individuals separately is still too expensive. While sequencing pools of individuals (Pool-Seq) dramatically reduces sequencing costs, a comparison of TE abundance between pooled samples has been difficult, if not impossible, due to various biases. Here, we introduce a novel bioinformatic tool, PoPoolationTE2, which is specifically tailored for the comparison of TE abundance among pooled population samples or different tissues. Using computer simulations we demonstrate that PoPoolationTE2 not only faithfully recovers TE insertion frequencies and positions but, by homogenizing the power to identify TEs acrosss samples, it provides an unbiased comparison of TE abundance between pooled population samples. We anticipate that PoPoolationTE2 will greatly facilitate the analysis of TE insertion patterns in a broad range of applications."}, {"title": "A novel approach to identifying marker genes and estimating the cellular composition of whole blood from gene expression profiles", "url": "https://www.biorxiv.org/content/early/2016/02/03/038794", "tag": "Bioinformatics", "abstract": "Measuring genome-wide changes in transcript abundance in circulating peripheral whole blood cells is a useful way to study disease pathobiology and may help elucidate biomarkers and molecular mechanisms of disease. The sensitivity and interpretability of analyses carried out in this complex tissue, however, are significantly affected by its dynamic heterogeneity. It is therefore desirable to quantify this heterogeneity, either to account for it or to better model interactions that may be present between the abundance of certain transcripts, some cell types and the indication under study. Accurate enumeration of the many component cell types that make up peripheral whole blood can be costly, however, and may further complicate the sample collection process. Many approaches have been developed to infer the composition of a sample from high-dimensional transcriptomic and, more recently, epigenetic data. These approaches rely on the availability of isolated expression profiles for the cell types to be enumerated. These profiles are platform-specific, suitable datasets are rare, and generating them is expensive. No such dataset exists on the Affymetrix Gene ST platform. We present a freely-available, and open source, multi-response Gaussian model capable of accurately predicting the composition of peripheral whole blood samples from Affymetrix Gene ST expression profiles. This model outperforms other current methods when applied to Gene ST data and could potentially be used to enrich the >10,000 Affymetrix Gene ST blood gene expression profiles currently available on GEO."}, {"title": "Predicting protein thermal stability changes upon point mutations using statistical potentials: Introducing HoTMuSiC", "url": "https://www.biorxiv.org/content/early/2016/02/02/038554", "tag": "Bioinformatics", "abstract": "The accurate prediction of the impact of an amino acid substitution on the thermal stability of a protein is a central issue in protein science, and is of key relevance for the rational optimization of various bioprocesses that use enzymes in unusual conditions. Here we present one of the first computational tools to predict the change in melting temperature \u0394Tm upon point mutations, given the protein structure and, when available, the melting temperature Tm of the wild-type protein. The key ingredients of our model structure are standard and temperature-dependent statistical potentials, which are combined with the help of an artificial neural network. The model structure was chosen on the basis of a detailed thermodynamic analysis of the system. The parameters of the model were identified on a set of more than 1,600 mutations with experimentally measured \u0394Tm. The performance of our method was tested using a strict 5-fold cross-validation procedure, and was found to be significantly superior to that of competing methods. We obtained a root mean square deviation between predicted and experimental \u0394Tm values of 4.2\u00b0C that reduces to 2.9\u00b0C when ten percent outliers are removed. A webserver-based tool is freely available for non-commercial use at soft.dezyme.com."}, {"title": "Application of new informatics tools for identifying allosteric lead ligands of the c-Src kinase", "url": "https://www.biorxiv.org/content/early/2016/02/02/038323", "tag": "Bioinformatics", "abstract": "Recent molecular dynamics (MD) simulations of the catalytic domain of the c-Src kinase revealed intermediate conformations with a potentially druggable allosteric pocket adjacent to the C-helix, bound by 8-anilino-1-naphthalene sulfonate. Towards confirming the existence of this pocket, we have developed a novel lead enrichment protocol using new target and lead enrichment software to identify sixteen allosteric lead ligands of the c-Src kinase. First, Markov State Models analysis was used to identify the most statistically significant c-Src target conformations from all MD-simulated conformations. The most statistically relevant candidate MSM targets were then prioritized by assessing how well each reproduced binding poses of ligands specific to the ATP-competitive and allosteric pockets. The top-performing MSM targets, identified by receiver-operating curve analysis, were then used to screen the ZINC library of 13 million \u2033clean, drug-like ligands\u2033, all of which prioritized based on their empirical scoring function, binding pose consistency across MSM targets, and strong hydrogen bonding and hydrophobic interactions with Src residues. The FragFEATURE knowledgebase of fragment-protein pocket interactions was then used to identify fragments specific to the ATP-competitive and allosteric pockets. This information was used to identify seven Type II and nine Type III lead ligands with binding poses supported by fragment predictions. Of these, Type II lead ligands, ZINC13037947 and ZINC09672647, and Type III lead ligands, ZINC12530852 and ZINC30012975, exhibited the most favorable fragment profiles and are recommended for further experimental testing for the existence of the allosteric pocket in Src."}, {"title": "MetaFlow: Metagenomic profiling based on whole-genome coverage analysis with min-cost flows", "url": "https://www.biorxiv.org/content/early/2016/01/29/038208", "tag": "Bioinformatics", "abstract": "High-throughput sequencing (HTS) of metagenomes is proving essential in understanding the environment and diseases. State-of-the-art methods for discovering the species and their abundances in an HTS metagenomic sample are based on genome-specific markers, which can lead to skewed results, especially at species level. We present MetaFlow, the first method based on coverage analysis across entire genomes that also scales to HTS samples. We formulated this problem as an NP-hard matching problem in a bipartite graph, which we solved in practice by min-cost flows. On synthetic data sets of varying complexity and similarity, MetaFlow is more precise and sensitive than popular tools such as MetaPhlAn, mOTU, GSMer and BLAST, and its abundance estimations at species level are two to four times better in terms of L1-norm. On a real human stool data set, MetaFlow identifies B.uniformis as most predominant, in line with previous human gut studies, whereas marker-based methods report it as rare. MetaFlow is freely available at http://cs.helsinki.fi/gsa/metaflow"}, {"title": "SNP-sites: rapid efficient extraction of SNPs from multi-FASTA alignments", "url": "https://www.biorxiv.org/content/early/2016/01/29/038190", "tag": "Bioinformatics", "abstract": "Rapidly decreasing genome sequencing costs have led to a proportionate increase in the number of samples used in prokaryotic population studies. Extracting single nucleotide polymorphisms (SNPs) from a large whole genome alignment is now a routine task, but existing tools have failed to scale efficiently with the increased size of studies. These tools are slow, memory inefficient and are installed through non-standard procedures. We present SNP-sites which can rapidly extract SNPs from a multi-FASTA alignment using modest resources and can output results in multiple formats for downstream analysis. SNPs can be extracted from a 8.3 GB alignment file (1,842 taxa, 22,618 sites) in 267 seconds using 59 MB of RAM and 1 CPU core, making it feasible to run on modest computers. It is easy to install through the Debian and Homebrew package managers, and has been successfully tested on more than 20 operating systems. SNP-sites is implemented in C and is available under the open source license GNU GPL version 3."}, {"title": "Seqping: Gene Prediction Pipeline for Plant Genomes using Self-Trained Gene Models and Transcriptomic Data", "url": "https://www.biorxiv.org/content/early/2016/01/27/038018", "tag": "Bioinformatics", "abstract": "Although various software are available for gene prediction, none of the currently available gene-finders have a universal Hidden Markov Models (HMM) that can perform gene prediction for all organisms equally well in an automatic fashion. Here, we report an automated pipeline that performs gene prediction using self-trained HMM models and transcriptomic data. The program processes the genome and transcriptome sequences of a target species through GlimmerHMM, SNAP, and AUGUSTUS training pipeline that ends with the program MAKER2 combining the predictions from the three models in association with the transcriptomic evidence. The pipeline generates species-specific HMMs and is able to predict genes that are not biased to other model organisms. Our evaluation of the program revealed that it performed better than the use of the closest related HMM from a standalone program."}, {"title": "Principal component of explained variance: an efficient and optimal data dimension reduction framework for association studies", "url": "https://www.biorxiv.org/content/early/2016/01/27/036566", "tag": "Bioinformatics", "abstract": "The genomics era has led to an increase in the dimensionality of the data collected to investigate biological questions. In this context, dimension-reduction techniques can be used to summarize high-dimensional signals into low-dimensional ones, to further test for association with one or more covariates of interest. This paper revisits one such approach, previously known as Principal Component of Heritability and renamed here as Principal Component of Explained Variance (PCEV). As its name suggests, the PCEV seeks a linear combination of outcomes in an optimal manner, by maximising the proportion of variance explained by one or several covariates of interest. By construction, this method optimises power but limited by its computational complexity, it has unfortunately received little attention in the past. Here, we propose a general analytical PCEV framework that builds on the assets of the original method, i.e. conceptually simple and free of tuning parameters. Moreover, our framework extends the range of applications of the original procedure by providing a computationally simple strategy for high-dimensional outcomes, along with exact and asymptotic testing procedures that drastically reduce its computational cost. We investigate the merits of the PCEV using an extensive set of simulations. Furthermore, the use of the PCEV approach will be illustrated using three examples taken from the epigenetics and brain imaging areas."}, {"title": "ME-plot: A QC package for bisulfite sequencing reads", "url": "https://www.biorxiv.org/content/early/2016/01/27/033696", "tag": "Bioinformatics", "abstract": "Summary: Bisulfite sequencing is the gold standard method for analyzing methylomes. However, the effect of quality control in bisulfite sequencing has not been studied extensively. We developed a package ME-Plot to detect the errors in bisulfite sequencing mapping data and produce higher quality methylation calls by trimming low quality portions of the reads. Our simulation results on both randomly generated reads where methylation status is known and real world data indicate that ME-plot can detect errors in methylation mapping procedures and suggest post-processing steps to reduce the errors. ME-Plot requires SAM/BAM files for its analysis. Availability: The python package is available at the following URL. https://github.com/joshuabhk/methylsuite"}, {"title": "pileup.js: a JavaScript library for interactive and in-browser visualization of genomic data", "url": "https://www.biorxiv.org/content/early/2016/01/26/036962", "tag": "Bioinformatics", "abstract": "pileup.js is a new browser-based genome viewer. It is designed to facilitate the investigation of evidence for genomic variants within larger web applications. It takes advantage of recent developments in the JavaScript ecosystem to provide a modular, reliable and easily embedded library."}, {"title": "The Northern Arizona SNP Pipeline (NASP): accurate, flexible, and rapid identification of SNPs in WGS datasets", "url": "https://www.biorxiv.org/content/early/2016/01/25/037267", "tag": "Bioinformatics", "abstract": "Whole genome sequencing (WGS) of bacteria is becoming standard practice in many laboratories. Applications for WGS analysis include phylogeography and molecular epidemiology, using single nucleotide polymorphisms (SNPs) as the unit of evolution. The Northern Arizona SNP Pipeline (NASP) was developed as a reproducible pipeline that scales well with the large amount of WGS data typically used in comparative genomics applications. In this study, we demonstrate how NASP compares to other tools in the analysis of two real bacterial genomics datasets and one simulated dataset. Our results demonstrate that NASP produces comparable, and often better, results to other pipelines, but is much more flexible in terms of data input types, job management systems, diversity of supported tools, and output formats. We also demonstrate differences in results based on the choice of the reference genome and choice of inferring phylogenies from concatenated SNPs or alignments including monomorphic positions. NASP represents a source-available, version-controlled, unit-tested method and can be obtained from tgennorth.github.io/NASP."}, {"title": "Reference-free deconvolution of DNA methylation data and mediation by cell composition effects", "url": "https://www.biorxiv.org/content/early/2016/01/23/037671", "tag": "Bioinformatics", "abstract": "We propose a simple method for reference-free deconvolution that provides both proportions of putative cell types defined by their underlying methylomes, the number of these constituent cell types, as well as a method for evaluating the extent to which the underlying methylomes reflect specific types of cells. We have demonstrated these methods in an analysis of 23 Infinium data sets from 13 distinct data collection efforts; these empirical evaluations show that our algorithm can reasonably estimate the number of constituent types, return cell proportion estimates that demonstrate anticipated associations with underlying phenotypic data; and methylomes that reflect the underlying biology of constituent cell types. Thus the methodology permits an explicit quantitation of the mediation of phenotypic associations with DNA methylation by cell composition effects. Although more work is needed to investigate functional information related to estimated methylomes, our proposed method provides a novel and useful foundation for conducting DNA methylation studies on heterogeneous tissues lacking reference data."}, {"title": "Features of ChIP-seq data peak calling algorithms with good operating characteristics", "url": "https://www.biorxiv.org/content/early/2016/01/21/037473", "tag": "Bioinformatics", "abstract": "Chromatin immunoprecipitation followed by sequencing (ChIP-seq) is an important tool for studying gene regulatory proteins, such as transcription factors and histones. Peak calling is one of the first steps in analysis of these data. Peak-calling consists of two sub-problems: identifying candidate peaks and testing candidate peaks for statistical significance. We surveyed 30 methods and identified 12 features of the two sub-problems that distinguish methods from each other. We picked six methods (GEM, MACS2, MUSIC, BCP, TM and ZINBA) that span this feature space and used a combination of 300 simulated ChIP-seq data sets, 3 real data sets and mathematical analyses to identify features of methods that allow some to perform better than others. We prove that methods that explicitly combine the signals from ChIP and input samples are less powerful than methods that do not. Methods that use windows of different sizes are more powerful than ones that do not. For statistical testing of candidate peaks, methods that use a Poisson test to rank their candidate peaks are more powerful than those that use a Binomial test. BCP and MACS2 have the best operating characteristics on simulated transcription factor binding data. GEM has the highest fraction of the top 500 peaks containing the binding motif of the immunoprecipitated factor, with 50% of its peaks within 10 base pairs (bp) of a motif. BCP and MUSIC perform best on histone data. These findings provide guidance and rationale for selecting the best peak caller for a given application."}, {"title": "High-throughput identification of C/D box snoRNA targets with CLIP and RiboMeth-seq.", "url": "https://www.biorxiv.org/content/early/2016/01/19/037259", "tag": "Bioinformatics", "abstract": "Identification of long and short RNAs, their processing and expression patterns have been greatly facilitated by high-throughput sequencing. Frequently, these RNAs act as guides for ribonucleoprotein complexes that regulate the expression or processing of target RNAs. However, to determine the targets of the many newly discovered regulatory RNAs in high-throughput remains a challenge. To globally assign guide small nucleolar RNAs to site of 2'-O-ribose methylation in human cells, we here developed novel computational methods for the analysis of data that was generated with protocols designed to capture direct small RNA-target interactions and to identify the sites of 2'-O-ribose methylation genome-wide. We thereby determined that many \"orphan\" snoRNAs appear to guide 2'-O-ribose methylation at sites that are targeted by other snoRNAs and that snoRNAs can be reliably captured in interaction with many mRNAs, in which a subsequent 2'-O-methylation cannot be detected. Our study provides a reliable approach to the comprehensive characterization of snoRNA-target interactions in species beyond those in which these interactions have been traditionally studied and contribute to the rapidly developing field of \"epitranscriptomics\"."}, {"title": "AdmixSim: A Forward-Time Simulator for Various and Complex Scenarios of Population Admixture", "url": "https://www.biorxiv.org/content/early/2016/01/18/037135", "tag": "Bioinformatics", "abstract": "Background: Population admixture has been a common phenomenon in human, animals and plants, and plays a very important role in shaping individual genetic architecture and population genetic diversity. Inference of population admixture, however, is challenging and typically relies on in silico simulation. We are aware of the lack of a computer tool for such a purpose, especially a simulator is not available for generating data under various and complex admixture scenarios. Results: Here we developed a forward-time simulator (AdmixSim) under standard Wright Fisher model, which can simulate admixed populations with: 1) multiple ancestral populations; 2) multiple waves of admixture events; 3) fluctuating population size; and 4) fluctuating admixture proportions. Results of analysis of the simulated data by AdmixSim show that our simulator can fast and accurately generate data resemble real one. We included in AdmixSim all possible parameters that allow users to modify and simulate any kinds of admixture scenarios easily so that it is very flexible. AdmixSim records recombination break points and trace of each chromosomal segment from different ancestral populations, with which users can easily do further analysis and comparative studies with empirical data. Conclusions: AdmixSim is expected to facilitate the study of population admixture by providing a simulation framework with flexible implementation of various admixture models and parameters."}, {"title": "Read-Based Phasing of Related Individuals", "url": "https://www.biorxiv.org/content/early/2016/01/18/037101", "tag": "Bioinformatics", "abstract": "Motivation: Read-based phasing deduces the haplotypes of an individual from sequencing reads that cover multiple variants, while genetic phasing takes only genotypes as input and applies the rules of Mendelian inheritance to infer haplotypes within a pedigree of individuals. Combining both into an approach that uses these two independent sources of information -- reads and pedigree -- has the potential to deliver results better than each individually. Results: We provide a theoretical framework combining read-based phasing with genetic haplotyping, and describe a fixed-parameter algorithm and its implementation for finding an optimal solution. We show that leveraging reads of related individuals jointly in this way yields more phased variants and at a higher accuracy than when phased separately, both in simulated and real data. Coverages as low as 2x for each member of a trio yield haplotypes that are as accurate as when analyzed separately at 15x coverage per individual."}, {"title": "Isoform-level gene expression patterns in single-cell RNA-sequencing data", "url": "https://www.biorxiv.org/content/early/2016/01/16/036988", "tag": "Bioinformatics", "abstract": "RNA-sequencing of single-cells enables characterization of transcriptional heterogeneity in seemingly homogenous cell populations. In this study we propose and apply a novel method, ISOform-Patterns (ISOP), based on mixture modeling, to characterize the expression patterns of pairs of isoforms from the same gene in single-cell isoform-level expression data. We define six principal patterns of isoform expression relationships and introduce the concept of differential pattern analysis. We applied ISOP for analysis of single-cell RNA-sequencing data from a breast cancer cell line, with replication in two independent datasets. In the primary dataset we detected and assigned pattern type of 16562 isoform-pairs from 4929 genes. Our results showed that 78% of the isoform pairs displayed a mutually exclusive expression pattern, 14% of the isoform pairs displayed bimodal isoform preference and 8% isoform pairs displayed isoform preference. 26% of the isoform-pair patterns were significant, while remaining isoform-pair patterns can be understood as effects of transcriptional bursting, drop-out and biological heterogeneity. 32% of genes discovered through differential pattern analysis were novel and not detected by differential expression analysis. ISOP provides a novel approach for characterization of isoform-level expression in single-cell populations. Our results reveal a common occurrence of isoform-level preference, commitment and heterogeneity in single-cell populations."}, {"title": "RapMap: A Rapid, Sensitive and Accurate Tool for Mapping RNA-seq Reads to Transcriptomes", "url": "https://www.biorxiv.org/content/early/2016/01/16/029652", "tag": "Bioinformatics", "abstract": "Motivation: The alignment of sequencing reads to a transcriptome is a common and important step in many RNA-seq analysis tasks. When aligning RNA-seq reads directly to a transcriptome (as is common in the de novo setting or when a trusted reference annotation is available), care must be taken to report the potentially large number of multi-mapping locations per read. This can pose a substantial computational burden for existing aligners, and can considerably slow downstream analysis. Results: We introduce a novel concept, quasi-mapping, and an efficient algorithm implementing this approach for mapping sequencing reads to a transcriptome. By attempting only to report the potential loci of origin of a sequencing read, and not the base-to-base alignment by which it derives from the reference, RapMap --- our tool implementing quasi-mapping --- is capable of mapping sequencing reads to a target transcriptome substantially faster than existing alignment tools. The algorithm we employ to implement quasi-mapping uses several efficient data structures and takes advantage of the special structure of shared sequence prevalent in transcriptomes to rapidly provide highly-accurate mapping information. We demonstrate how quasi-mapping can be successfully applied to the problems of transcript-level quantification from RNA-seq reads and the clustering of contigs from de novo assembled transcriptomes into biologically-meaningful groups. Availability: RapMap is implemented in C++11 and is available as open-source software, under GPL v3, at https://github.com/COMBINE-lab/RapMap."}, {"title": "Logic models to predict continuous outputs based on binary inputs with an application to personalized cancer therapy", "url": "https://www.biorxiv.org/content/early/2016/01/15/036970", "tag": "Bioinformatics", "abstract": "Mining large datasets using machine learning approaches often leads to models that are hard to interpret and not amenable to the generation of hypotheses that can be experimentally tested. Finding 'actionable knowledge' is becoming more important, but also more challenging as datasets grow in size and complexity. We present 'Logic Optimization for Binary Input to Continuous Output' (LOBICO), a computational approach that infers small and easily interpretable logic models of binary input features that explain a binarized continuous output variable. Although the continuous output variable is binarized prior to optimization, the continuous information is retained to find the optimal logic model. Applying LOBICO to a large cancer cell line panel, we find that logic combinations of multiple mutations are more predictive of drug response than single gene predictors. Importantly, we show that the use of the continuous information leads to robust and more accurate logic models. LOBICO is formulated as an integer programming problem, which enables rapid computation on large datasets. Moreover, LOBICO implements the ability to uncover logic models around predefined operating points in terms of sensitivity and specificity. As such, it represents an important step towards practical application of interpretable logic models."}, {"title": "Structural features of the fly chromatin colors revealed by automatic three-dimensional modeling.", "url": "https://www.biorxiv.org/content/early/2016/01/15/036764", "tag": "Bioinformatics", "abstract": "The sequence of a genome is insufficient to understand all genomic processes carried out in the cell nucleus. To achieve this, the knowledge of its three- dimensional architecture is necessary. Advances in genomic technologies and the development of new analytical methods, such as Chromosome Conformation Capture (3C) and its derivatives, now permit to investigate the spatial organization of genomes. However, inferring structures from raw contact data is a tedious process for shortage of available tools. Here we present TADbit, a computational framework to analyze and model the chromatin fiber in three dimensions. To illustrate the use of TADbit, we automatically modeled 50 genomic domains from the fly genome revealing differential structural features of the previously defined chromatin colors, establishing a link between the conformation of the genome and the local chromatin composition. More generally, TADbit allows to obtain three-dimensional models ready for visualization from 3C-based experiments and to characterize their relation to gene expression and epigenetic states. TADbit is open-source and available for download from http://www.3DGenomes.org."}, {"title": "Contrasting the genetic architecture of 30 complex traits from summary association data", "url": "https://www.biorxiv.org/content/early/2016/01/14/035907", "tag": "Bioinformatics", "abstract": "Variance components methods that estimate the aggregate contribution of large sets of variants to the heritability of complex traits have yielded important insights into the disease architecture of common diseases. Here, we introduce new methods that estimate the total variance in trait explained by a single locus in the genome (local heritability) from summary GWAS data while accounting for linkage disequilibrium (LD) among variants. We apply our new estimator to ultra large-scale GWAS summary data of 30 common traits and diseases to gain insights into their local genetic architecture. First, we find that common SNPs have a high contribution to the heritability of all studied traits. Second, we identify traits for which the majority of the SNP heritability can be confined to a small percentage of the genome. Third, we identify GWAS risk loci where the entire locus explains significantly more variance in the trait than the GWAS reported variants. Finally, we identify 55 loci that explain a large proportion of heritability across multiple traits."}, {"title": "Haplotag: software for haplotype-based genotyping-by-sequencing analysis", "url": "https://www.biorxiv.org/content/early/2016/01/14/031013", "tag": "Bioinformatics", "abstract": "Genotyping-by-sequencing (GBS) and related methods are based on high-throughput short-read sequencing of genomic complexity reductions followed by discovery of SNPs within sequence tags. This provides a powerful and economical approach to whole-genome genotyping, facilitating applications in genomics, diversity analysis, and molecular breeding. However, due to the complexity of analysing large data sets, applications of GBS may require substantial time, expertise and computational resources. Haplotag, the novel GBS software described here, is freely available and operates with minimal user-investment on widely-available computer platforms. Haplotag is unique in fulfilling the following set of criteria: (1) operates without a reference genome; (2) can be used in a polyploid species; (3) provides a discovery mode and a production mode; (4) discovers polymorphisms based on a model of tag-level haplotypes within sequenced tags; (5) reports SNPs as well as haplotype-based genotypes; (6) provides an intuitive visual passport for each inferred locus. Haplotag is optimized for use in a self-pollinating plant species."}, {"title": "Structural variation detection with read pair information --- An improved null-hypothesis reduces bias", "url": "https://www.biorxiv.org/content/early/2016/01/14/036707", "tag": "Bioinformatics", "abstract": "Abstract. Reads from paired-end and mate-pair libraries are often utilized to find structural variation in genomes, and one common approach is to use their fragment length for detection. After aligning read-pairs to the reference, read-pair distances are analyzed for statistically significant deviations. However, previously proposed methods are based on a simplified model of observed fragment lengths that does not agree with data. We show how this model limits statistical analysis of identifying variants and propose a new model, by adapting a model we have previously introduced for contig scaffolding, which agrees with data. From this model we derive an improved improved null hypothesis that, when applied in the variant caller CLEVER, reduces the number of false positives and corrects a bias that contributes to more deletion calls than insertion calls. A reference implementation is freely available at https://github.com/ksahlin/GetDistr."}, {"title": "Searching more genomic sequence with less memory for fast and accurate metagenomic profiling", "url": "https://www.biorxiv.org/content/early/2016/01/14/036681", "tag": "Bioinformatics", "abstract": "Software for rapid, accurate, and comprehensive microbial profiling of metagenomic sequence data on a desktop will play an important role in large scale clinical use of metagenomic data. Here we describe LMAT-ML (Livermore Metagenomics Analysis Toolkit-Marker Library) which can be run with 24 GB of DRAM memory, an amount available on many clusters, or with 16 GB DRAM plus a 24 GB low cost commodity flash drive (NVRAM), a cost effective alternative for desktop or laptop users. We compared results from LMAT with five other rapid, low-memory tools for metagenome analysis for 131 Human Microbiome Project samples, and assessed discordant calls with BLAST. All the tools except LMAT-ML reported overly specific or incorrect species and strain resolution of reads that were in fact much more widely conserved across species, genera, and even families. Several of the tools misclassified reads from synthetic or vector sequence as microbial or human reads as viral. We attribute the high numbers of false positive and false negative calls to a limited reference database with inadequate representation of known diversity. Our comparisons with real world samples show that LMAT-ML is the only tool tested that classifies the majority of reads, and does so with high accuracy."}, {"title": "Canvas: versatile and scalable detection of copy number variants", "url": "https://www.biorxiv.org/content/early/2016/01/13/036194", "tag": "Bioinformatics", "abstract": "Motivation: Increased throughput and diverse experimental designs of large-scale sequencing studies necessi-tate versatile, scalable and robust variant calling tools. In particular, identification of copy number changes re-mains a challenging task due to their complexity, susceptibility to sequencing biases, variation in coverage data and dependence on genome-wide sample properties, such as tumor polyploidy or polyclonality in cancer samples. Results: We have developed a new tool, Canvas, for identification of copy number changes from diverse se-quencing experiments including whole-genome matched tumor-normal and single-sample normal re-sequencing, as well as whole-exome matched and unmatched tumor-normal studies. In addition to variant calling, Canvas infers genome-wide parameters such as cancer ploidy, purity and heterogeneity. It provides fast and simple to execute workflows that can scale to thousands of samples and can be easily incorporated into existing variant calling pipelines. Availability: Canvas is distributed under an open source license and can be downloaded from https://github.com/Illumina/canvas."}, {"title": "Robust Lineage Reconstruction from High-Dimensional Single-Cell Data", "url": "https://www.biorxiv.org/content/early/2016/01/12/036533", "tag": "Bioinformatics", "abstract": "Single-cell gene expression data provide invaluable resources for systematic characterization of cellular hierarchy in multi-cellular organisms. However, cell lineage reconstruction is still often associated with significant uncertainty due to technological constraints. Such uncertainties have not been taken into account in current methods. We present ECLAIR, a novel computational method for the statistical inference of cell lineage relationships from single-cell gene expression data. ECLAIR uses an ensemble approach to improve the robustness of lineage predictions, and provides a quantitative estimate of the uncertainty of lineage branchings. We show that the application of ECLAIR to published datasets successfully reconstructs known lineage relationships and significantly improves the robustness of predictions. In conclusion, ECLAIR is a powerful bioinformatics tool for single-cell data analysis. It can be used for robust lineage reconstruction with quantitative estimate of prediction accuracy."}, {"title": "RiboDiff: Detecting Changes of Translation Efficiency from Ribosome Footprints", "url": "https://www.biorxiv.org/content/early/2016/01/11/017111", "tag": "Bioinformatics", "abstract": "Motivation: Deep sequencing based ribosome footprint profiling can provide novel insights into the regulatory mechanisms of protein translation. However, the observed ribosome profile is fundamentally confounded by transcriptional activity. In order to decipher principles of translation regulation, tools that can reliably detect changes in translation efficiency in case-control studies are needed. Results: We present a statistical framework and analysis tool, RiboDiff, to detect genes with changes in translation efficiency across experimental treatments. RiboDiff uses generalized linear models to estimate the over-dispersion of RNA-Seq and ribosome profiling measurements separately, and performs a statistical test for differential translation efficiency using both mRNA abundance and ribosome occupancy. Availability: Source code and documentation are available at http://github.com/ratschlab/ribodiff. Supplementary Material can be found at http://bioweb.me/ribo."}, {"title": "Regmex, Motif analysis in ranked lists of sequences", "url": "https://www.biorxiv.org/content/early/2016/01/11/035956", "tag": "Bioinformatics", "abstract": "Motif analysis has long been an important method to characterize biological functionality and the current growth of sequencing-based genomics experiments further extends its potential. These diverse experiments often generate sequence lists ranked by some functional property. There is therefore a growing need for motif analysis methods that can exploit this coupled data structure and be tailored for specific biological questions. Here, we present a motif analysis tool, Regmex (REGular expression Motif EXplorer), which offers several methods to identify overrepresented motifs in a ranked list of sequences. Regmex uses regular expressions to define motifs or families of motifs and embedded Markov models to calculate exact probabilities for motif observations in sequences. Motif enrichment is optionally evaluated using random walks, Brownian bridges, or modified rank based statistics. These features make Regmex well suited for a range of biological sequence analysis problems related to motif discovery. We demonstrate different usage scenarios including rank correlation of microRNA binding sites co-occurring with a U-rich motif. The method is available as an R package."}, {"title": "High-quality thermodynamic data on the stability changes of proteins upon single-site mutations", "url": "https://www.biorxiv.org/content/early/2016/01/10/036301", "tag": "Bioinformatics", "abstract": "We have set up and manually curated a dataset containing experimental information on the impact of amino acid substitutions in a protein on its thermal stability. It consists of a repository of experimentally measured melting temperatures (Tm) and their changes upon point mutations (\u2206Tm) for proteins having a well-resolved X-ray structure. This high-quality dataset is designed for being used for the training or benchmarking of in silico thermal stability prediction methods. It also reports other experimentally measured thermodynamic quantities when available, i.e. the folding enthalpy (\u2206H) and heat capacity (\u2206CP) of the wild type proteins and their changes upon mutations (\u2206\u2206H and \u2206\u2206CP ), as well as the change in folding free energy (\u2206\u2206G) at a reference temperature. These data are analyzed in view of improving our insights into the correlation between thermal and thermodynamic stabilities, the asymmetry between the number of stabilizing and destabilizing mutations, and the difference in stabilization potential of thermostable versus mesostable proteins."}, {"title": "Quantitative Relations in Protein and RNA Folding Deduced from Quantum Theory", "url": "https://www.biorxiv.org/content/early/2016/01/08/021782", "tag": "Bioinformatics", "abstract": "Quantitative relations in protein and RNA folding are deduced from the quantum folding theory of macromolecules. It includes: deduction of the law on the temperature-dependence of folding rate and its tests on protein dataset; study on the chain-length dependence of the folding rate for a large class of biomolecules; deduction of the statistical relation of folding free energy versus chain-length; and deduction of the statistical relation between folding rate and chain length and its test on protein and RNA dataset. In the above quantum approach the influence of the solvent environment factor on folding rate has been taken into account automatically. The successes of the deduction of these new relations from the first principle and their successful comparison with experimental data afford strong evidence on the possible existence of a common quantum mechanism in the conformational change of biomolecules."}, {"title": "Triplex Domain Finder: Detection of Triple Helix Binding Domains in Long Non-Coding RNAs", "url": "https://www.biorxiv.org/content/early/2016/01/08/020297", "tag": "Bioinformatics", "abstract": "Long non-coding RNAs (lncRNA) can act as a scaffold promoting the interaction of several proteins, RNA and DNA. Some lncRNAs interact with the DNA via a triple helix formation. Triple helices are formed by a single stranded RNA/DNA molecule, which binds to the major groove of a double helix following a canonical code. Recently, sequence analysis methods have been proposed to detect triple helices for a given RNA and DNA sequences. We propose the Triplex Domain Finder (TDF) to detect DNA binding domains in RNA molecules. For a candidate lncRNA and potential target DNA regions, i.e. promoter of genes differentially regulated after the knockdown of the lncRNA, TDF evaluates whether particular RNA regions are likely to form DNA binding domains (DBD). Moreover, the DNA binding sites from the predicted DBDs are used to indicate potential target DNA regions, i.e. genes with high binding site coverage in their promoter. The command line tool provides results on a user friendly and graphical html interface. A case study on FENDRR, an lncRNA known to form triple helices, demonstrates that TDF is able to recover both previously discovered DBDs and DNA binding sites. Source code, tutorial and case studies are available at www.regulatory-genomics.org/tdf."}, {"title": "Automating Assessment of the Undiscovered Biosynthetic Potential of Actinobacteria", "url": "https://www.biorxiv.org/content/early/2016/01/07/036087", "tag": "Bioinformatics", "abstract": "Background. Biosynthetic potential of Actinobacteria has long been the subject of theoretical estimates. Such an estimate is indeed important as a test of further exploitability of a taxon or group of taxa for new therapeutics. As neither a set of available genomes nor a set of bacterial cultivation methods are static, it makes sense to simplify as much as possible and to improve reproducibility of biosynthetic gene clusters similarity, diversity, and abundance estimations. Results. We have developed a command-line computational pipeline (available at https://bitbucket.org/qmentis/clusterscluster/) that assists in performing empirical (genome-based) assessment of microbial secondary metabolite gene clusters similarity and abundance, and applied it to a set of 208 complete and de-duplicated Actinobacteria genomes. After a brief overview of Actinobacteria biosynthetic potential as compared to other bacterial taxa, we use similarity thresholds derived from 4 pairs of known similar gene clusters to identify up to 40-48% of 3247 gene clusters in our set of genomes as unique. There is no saturation of the cumulative unique gene clusters curve within the examined dataset, and Heap's alpha is 0.129, suggesting an open pan-clustome. We identify and highlight pitfalls and possible improvements of genome-based gene cluster similarity measurements."}, {"title": "RASLseqTools: open-source methods for designing and analyzing RNA-mediated oligonucleotide Annealing, Selection, and, Ligation sequencing (RASL-seq) experiments", "url": "https://www.biorxiv.org/content/early/2016/01/07/036061", "tag": "Bioinformatics", "abstract": "RNA-mediated oligonucleotide Annealing, Selection, and Ligation (RASL-seq) is a method to measure the expression of hundreds of genes in thousands of samples for a fraction of the cost of competing methods. However, enzymatic inefficiencies of the original protocol and the lack of open source software to design and analyze RASL-seq experiments have limited its widespread adoption. We recently reported an Rnl2-based RASL-seq protocol (RRASL-seq) that offers improved ligation efficiency and a probe decoy strategy to optimize sequencing usage. Here, we describe an open source software package, RASLseqTools, that provides computational methods to design and analyze RASL-seq experiments. Furthermore, using data from a large RRASL-seq experiment, we demonstrate how normalization methods can be used for characterizing and correcting experimental, sequencing, and alignment error. We provide evidence that the three principal predictors of RRASL-seq reproducibility are barcode/probe sequence dissimilarity, sequencing read depth, and normalization strategy. Using dozens of technical and biological replicates across multiple 384-well plates, we find simple normalization strategies yield similar results to more statistically complex methods."}, {"title": "Systematic identification of cooperation between DNA binding proteins in 3D space", "url": "https://www.biorxiv.org/content/early/2016/01/07/036145", "tag": "Bioinformatics", "abstract": "Cooperation between DNA-binding proteins (DBPs) such as transcription factors and chromatin remodeling enzymes plays a pivotal role in regulating gene expression and other biological processes. Such cooperation is often via interaction between DBPs that bind to loci located distal in the linear genome but close in the 3D space, referred as trans-cooperation. Due to the lack of 3D chromosomal structure, identification of DBP cooperation has been limited to those binding to neighbor regions in the linear genome, referred as cis-cooperation. Here we present the first study that integrates protein ChIP-seq and Hi-C data to systematically identify both cis- and trans-cooperation between DBPs. We developed a new network model that allows identification of cooperation between multiple DBPs and reveals cell type specific or independent regulations. Particularly interesting, we have retrieved many known and previously unknown trans-cooperation between DBPs in the chromosomal loops that may be a key factor for influencing 3D chromosomal structure. The software is available at http://wanglab.ucsd.edu/star/DBPnet/index.html."}, {"title": "RTFBSDB: an integrated framework for transcription factor binding site analysis", "url": "https://www.biorxiv.org/content/early/2016/01/05/036053", "tag": "Bioinformatics", "abstract": "Transcription factors (TFs) regulate complex programs of gene transcription by binding to short DNA sequence motifs. Here we introduce rtfbsdb, a unified framework that integrates a database of more than 65,000 TF binding motifs with tools to easily and efficiently scan target genome sequences. Rtfbsdb clusters motifs with similar DNA sequence specificities and optionally integrates RNA-seq or PRO-seq data to restrict analyses to motifs recognized by TFs expressed in the cell type of interest. Our package allows common analyses to be performed rapidly in an integrated environment."}, {"title": "ShapeCluster: Applying parametric regression to analyse time-series gene expression data", "url": "https://www.biorxiv.org/content/early/2016/01/01/035782", "tag": "Bioinformatics", "abstract": "High-throughput technologies have made it possible to perform genome-scale analyses to investigate a variety of research areas. From these analyses, vast amounts of data are generated. However, these data can be noisy, which could obscure the underlying signal. Here, a high-throughput regression analysis approach was developed, where a variety of linear and nonlinear parametric models were fitted to gene expression profiles from time course experiments. These models include the logistic, Gompertz, exponential, critical exponential, linear+exponential, Gaussian and linear functions. The fitted parameters from these models reflect aspects of the model shape, and thus allowed for the interpretation of gene expression profiles in terms of the underlying biology, such as the time of initial gene expression. This provides a potentially more mechanistic ap-proach to studying the genetic responses to stimuli. Together with a cluster analysis, termed ShapeCluster, it was possible to group genes based on these aspects of the expression profiles. By investigating different combinations of parameters, this added flexibility to the analysis and allowed for the investigation of the data in multiple ways, including the identification of groups of genes that may be co-regulated, or participate in response to the biological stress in question. Clusters from these methods were assessed for significance through the use of over-represented annotation terms and motifs, and found to pro-duce biologically relevant sets of genes. The ShapeCluster package is available from https://sourceforge.net/projects/shapecluster/."}, {"title": "MyGene.info and MyVariant.info: Gene and Variant Annotation Query Services", "url": "https://www.biorxiv.org/content/early/2015/12/30/035667", "tag": "Bioinformatics", "abstract": "MyGene.info and MyVariant.info provide high-performance data APIs for querying gene and variant annotation information. They demonstrate a new model for organizing biological annotation information by utilizing a cloud-based scalable infrastructure. MyGene.info and MyVariant.info can be accessed at http://mygene.info and http://myvariant.info."}, {"title": "MPRAnator: a web-based tool for the design of Massively Parallel Reporter Assay experiments.", "url": "https://www.biorxiv.org/content/early/2015/12/28/035444", "tag": "Bioinformatics", "abstract": "DNA regulatory elements contain short motifs where transcription factors (TFs) can bind to modulate gene expression. Although the broad principles of TF regulation are well understood, the rules that dictate how combinatorial TF binding translates into transcriptional activity remain largely unknown. With the rapid advances in DNA synthesis and sequencing technologies and the continuing decline in the associated costs, high-throughput experiments can be performed to investigate the regulatory role of thousands of oligonucleotide sequences simultaneously. Nevertheless, designing high-throughput reporter assay experiments such as Massively Parallel Reporter Assays (MPRAs) and similar methods remains challenging. We introduce MPRAnator, a set of tools that facilitate rapid design of MPRA experiments. With MPRA Motif design, a set of variables provides fine control of how motifs are placed into sequences therefore allowing the user to investigate the rules that govern TF occupancy. MPRA SNP design can be used to investigate the functional effects of single or combinations of SNPs at regulatory sequences. Finally, the Transmutation tool allows for the design of negative controls by permitting scrambling, reversing, complementing or introducing multiple random mutations in the input sequences or motifs."}, {"title": "Privacy-preserving search for chemical compound databases", "url": "https://www.biorxiv.org/content/early/2015/12/28/013995", "tag": "Bioinformatics", "abstract": "Searching for similar compounds in a database is the most important process for in-silico drug screening. Since a query compound is an important starting point for the new drug, a query holder, who is afraid of the query being monitored by the database server, usually downloads all the records in the database and uses them in a closed network. However, a serious dilemma arises when the database holder also wants to output no information except for the search results, and such a dilemma prevents the use of many important data resources. In order to overcome this dilemma, we developed a novel cryptographic protocol that enables database searching while keeping both the query holder's privacy and database holder's privacy. Generally, the application of cryptographic techniques to practical problems is difficult because versatile techniques are computationally expensive while computationally inexpensive techniques can perform only trivial computation tasks. In this study, our protocol is successfully built only from an additive-homomorphic cryptosystem, which allows only addition performed on encrypted values but is computationally efficient compared with versatile techniques such as general purpose multi-party computation. In an experiment searching ChEMBL, which consists of more than 1,200,000 compounds, the proposed method was 36,900 times faster in CPU time and 12,000 times as efficient in communication size compared with general purpose multi-party computation. The proposed method, easily scaling for large-scale databases, may help to accelerate drug discovery research by making full use of unused but valuable data that includes sensitive information."}, {"title": "solarius: an R interface to SOLAR for variance component analysis in pedigrees", "url": "https://www.biorxiv.org/content/early/2015/12/25/035378", "tag": "Bioinformatics", "abstract": "The open source environment R is one of the most widely used software for statistical computing. It provides a variety of applications including statistical genetics. Most of the powerful tools for quantitative genetic analyses are stand-alone free programs developed by researchers in academia. SOLAR is the standard software program to perform linkage and association mappings of the quantitative trait loci (QTLs) in pedigrees of arbitrary size and complexity. solarius allows the user to exploit the variance component methods implemented in SOLAR. It automates such routine operations as formatting pedigree and phenotype data. It also parses the model output and contains summary and plotting functions for exploration of the results. In addition, solarius enables parallel computing of the linkage and association analyses, that makes the calculation of genome-wide scans more efficient. solarius is available on CRAN https://cran.r-project.org/package=solarius and on GitHub https://github.com/ugcd/solarius. See http://solar.txbiomedgenetics.org/ for more information about SOLAR."}, {"title": "Biological screens from linear codes: theory and tools", "url": "https://www.biorxiv.org/content/early/2015/12/25/035352", "tag": "Bioinformatics", "abstract": "Molecular biology increasingly relies on large screens where enormous numbers of specimens are systematically assayed in the search for a particular, rare outcome. These screens include the systematic testing of small molecules for potential drugs and testing the association between genetic variation and a phenotype of interest. While these screens are ``hypothesis-free,'' they can be wasteful; pooling the specimens and then testing the pools is more efficient. We articulate in precise mathematical ways the type of structures useful in combinatorial pooling designs so as to eliminate waste, to provide light weight, flexible, and modular designs. We show that Reed-Solomon codes, and more generally linear codes, satisfy all of these mathematical properties. We further demonstrate the power of this technique with Reed-Solomon-based biological experiments. We provide general purpose tools for experimentalists to construct and carry out practical pooling designs with rigorous guarantees for large screens."}, {"title": "Bibliometric and Geographical Analysis of Cell Death Related Literature", "url": "https://www.biorxiv.org/content/early/2015/12/24/035204", "tag": "Bioinformatics", "abstract": "Natural language processing continues to gain importance in a thriving scientific community that communicates its latest results in such a frequency that following up on the most recent developments even in a specific field cannot be managed by human readers alone. Here we summarize and compare the publishing activity of the previous years on a distinct topic across several countries, addressing not only publishing frequency and history, but also stylistic characteristics that are accessible by means of natural language processing. Though there are no profound differences in the sentence lengths or lexical diversity among different countries, writing styles approached by Part-Of-Speech tagging are similar among countries that share history or official language or those are spatially close."}, {"title": "De novo identification, differential analysis and functional annotation of SNPs from RNA-seq data in non-model species", "url": "https://www.biorxiv.org/content/early/2015/12/24/035238", "tag": "Bioinformatics", "abstract": "SNPs (Single Nucleotide Polymorphisms) are genetic markers whose precise identification is a prerequisite for association studies. Methods to identify them are currently well developed for model species, but rely on the availability of a (good) reference genome, and therefore cannot be applied to non-model species. They are also mostly tailored for whole genome (re-)sequencing experiments, whereas in many cases, transcriptome sequencing can be used as a cheaper alternative which already enables to identify SNPs located in transcribed regions. In this paper, we propose a method that identifies, quantifies and annotates SNPs without any reference genome, using RNA-seq data only. Individuals can be pooled prior to sequencing, if not enough material is available for sequencing from one individual. Using human RNA-seq data, we first compared the performance of our method with G<small>ATK</small>, a well established method that requires a reference genome. We showed that both methods predict SNPs with similar accuracy. We then validated experimentally the predictions of our method using RNA-seq data from two non-model species. The method can be used for any species to annotate SNPs and predict their impact on proteins. We further enable to test for the association of the identified SNPs with a phenotype of interest."}, {"title": "High-accuracy HLA type inference from whole-genome sequencing data", "url": "https://www.biorxiv.org/content/early/2015/12/24/035253", "tag": "Bioinformatics", "abstract": "Extensive hyperpolymorphism and sequence similarity between the HLA genes make HLA type inference from whole-genome sequencing data a challenging problem. We address these by representing sequences from over 10,000 known alleles in a reference graph structure, enabling accurate read mapping. HLA*PRG, our algorithm, outperforms existing methods by a wide margin and for the first time consistently achieves the accuracy of gold-standard reference methods with one error across 158 alleles tested."}, {"title": "Practical Guidelines for Secure Cloud Computing using Genomic Data", "url": "https://www.biorxiv.org/content/early/2015/12/20/034876", "tag": "Bioinformatics", "abstract": "Understanding Cloud security for genomics data is challenging but a critical need. It is our observation that security requirements are often inconsistent not only across datasets but also between on-premise solutions and Cloud for the same dataset. We attempt to summarize these security requirements across a wide range of regulatory bodies from government and private sources. While we expect the implementation to be different between Clouds and evolution in implementation methodology in time, we expect these guidelines to be applicable in foreseeable future. We also note that security does not necessary provide privacy and significant effort is needed to address privacy for a research centric platform."}, {"title": "Kaiju: Fast and sensitive taxonomic classification for metagenomics", "url": "https://www.biorxiv.org/content/early/2015/12/18/031229", "tag": "Bioinformatics", "abstract": "The constantly decreasing cost and increasing output of current sequencing technologies enable large scale metagenomic studies of microbial communities from diverse habitats. Therefore, fast and accurate methods for taxonomic classification are needed, which can operate on increasingly larger datasets and reference databases. Recently, several fast metagenomic classifiers have been developed, which are based on comparison of genomic k-mers. However, nucleotide comparison using a fixed k-mer length often lacks the sensitivity to overcome the evolutionary distance between sampled species and genomes in the reference database. Here, we present the novel metagenome classifier Kaiju for fast assignment of reads to taxa. Kaiju finds maximum exact matches on the protein-level using the Borrows-Wheeler transform, and can optionally allow amino acid substitutions in the search using a greedy heuristic. We show in a genome exclusion study that Kaiju can classify more reads with higher sensitivity and similar precision compared to fast k-mer based classifiers, especially in genera that are underrepresented in reference databases. We also demonstrate that Kaiju classifies more than twice as many reads in ten real metagenomes compared to programs based on genomic k-mers. Kaiju can process up to millions of reads per minute, and its memory footprint is below 6 GB of RAM, allowing the analysis on a standard PC. The program is available under the GPL3 license at: http://bioinformatics-centre.github.io/kaiju"}, {"title": "Inferring protein-protein interaction networks from inter-protein sequence co-evolution", "url": "https://www.biorxiv.org/content/early/2015/12/17/034744", "tag": "Bioinformatics", "abstract": "Interaction between proteins is a fundamental mechanism that underlies virtually all biological processes. Many important interactions are conserved across a large variety of species. The need to maintain interaction leads to a high degree of co-evolution between residues in the interface between partner proteins. The inference of protein-protein interaction networks from the rapidly growing sequence databases is one of the most formidable tasks in systems biology today. We propose here a novel approach based on the Direct-Coupling Analysis of the co-evolution between inter-protein residue pairs. We use ribosomal and trp operon proteins as test cases: For the small resp. large ribosomal subunit our approach predicts protein-interaction partners at a true-positive rate of 70% resp. 90% within the first 10 predictions, with areas of 0.69 resp. 0.81 under the ROC curves for all predictions. In the trp operon, it assigns the two largest interaction scores to the only two interactions experimentally known. On the level of residue interactions we show that for both the small and the large ribosomal subunit our approach predicts interacting residues in the system with a true positive rate of 60% and 85% in the first 20 predictions. We use artificial data to show that the performance of our approach depends crucially on the size of the joint multiple sequence alignments and analyze how many sequences would be necessary for a perfect prediction if the sequences were sampled from the same model that we use for prediction. Given the performance of our approach on the test data we speculate that it can be used to detect new interactions, especially in the light of the rapid growth of available sequence data."}, {"title": "SPECtre: a spectral coherence-based classifier of actively translated transcripts from ribosome profiling sequence data", "url": "https://www.biorxiv.org/content/early/2015/12/17/034777", "tag": "Bioinformatics", "abstract": "Active protein translation can be assessed and measured using ribosome profiling sequencing strategies. Existing approaches make use of sequence fragment length or frame occupancy to differentiate between active translation and background noise, however they do not consider additional characteristics inherent to the technology which limits their overall accuracy. Here, we present an analytical tool that models the overall tri-nucleotide periodicity of ribosomal occupancy using a classifier based on spectral coherence. Our software, SPECtre, examines the relationship of normalized ribosome profiling read coverage over a rolling series of windows along a transcript against an idealized reference signal. A comparison of SPECtre against current methods on existing and new data shows a marked improvement in accuracy for detecting active translation and exhibits overall high sensitivity at a low false discovery rate."}, {"title": "D3M: Detection of differential distributions of methylation levels", "url": "https://www.biorxiv.org/content/early/2015/12/16/023879", "tag": "Bioinformatics", "abstract": "DNA methylation is an important epigenetic modification related to a variety of diseases including cancers. One of the key issues of methylation analysis is to detect the differential methylation sites between case and control groups. Previous approaches describe data with simple summary statistics and kernel functions, and then use statistical tests to determine the difference. However, a summary statistics-based approach cannot capture complicated underlying structure, and a kernel functions-based approach lacks interpretability of results. We propose a novel method D3M, for detection of differential distribution of methylation, based on distribution-valued data. Our method can detect high-order moments, such as shapes of underlying distributions in methylation profiles, based on the Wasserstein metric. We test the significance of the difference between case and control groups and provide an interpretable summary of the results. The simulation results show that the proposed method achieves promising accuracy and outperforms previous methods. Glioblastoma multiforme and lower grade glioma data from The Cancer Genome Atlas and show that our method supports recent biological advances and suggests new insights."}, {"title": "Architecting a distributed bioinformatics platform with iRODS and iPlant Agave API", "url": "https://www.biorxiv.org/content/early/2015/12/15/034488", "tag": "Bioinformatics", "abstract": "Over the past few years, cloud-based platforms have been proposed to address storage, management, and computation of large-scale data, especially in the field of genomics. However, for collaboration efforts involving multiple institutes, data transfer and management, interoperability and standardization among different platforms have imposed new challenges. This paper proposes a distributed bioinformatics platform that can leverage local clusters with remote computational clusters for genomic analysis using the unified bioinformatics workflow. The platform is built with a data server configured with iRODS, a computation cluster authenticated with iPlant Agave system, and web server to interact with the platform. A Genome-Wide Association Study workflow is integrated to validate the feasibility of the proposed approach."}, {"title": "Event Extraction from Biomedical Literature", "url": "https://www.biorxiv.org/content/early/2015/12/15/034397", "tag": "Bioinformatics", "abstract": "The breadth and scope of the biomedical literature hinders a timely and thorough comprehension of its content. PubMed, the leading repository for biomedical literature, currently holds over 26 million records, and is growing at a rate of over 1.2 million records per year, with about 300 records added daily that mention `cancer' in the title or abstract. Natural language processing (NLP) can assist in accessing and interpreting this massive volume of literature, including its quality. NLP approaches to the automatic extraction of biomedical entities and relationships may assist the development of explanatory models that can comprehensively scan and summarize biomedical articles for end users. Users can also formulate structured queries against these entities, and their interactions, to mine the latest developments in related areas of interest. In this article, we explore the latest advances in automated event extraction methods in the biomedical domain, focusing primarily on tools participated in the Biomedical NLP (BioNLP) Shared Task (ST) competitions. We review the leading BioNLP methods, summarize their results, and their innovative contributions in this field."}, {"title": "Gene expression inference with deep learning", "url": "https://www.biorxiv.org/content/early/2015/12/15/034421", "tag": "Bioinformatics", "abstract": "Motivation: Large-scale gene expression profiling has been widely used to characterize cellular states in response to various disease conditions, genetic perturbations, etc. Although the cost of whole-genome expression profiles has been dropping steadily, generating a compendium of expression profiling over thousands of samples is still very expensive. Recognizing that gene expressions are often highly correlated, researchers from the NIH LINCS program have developed a cost- effective strategy of profiling only \u03031,000 carefully selected landmark genes and relying on computational methods to infer the expression of remaining target genes. However, the computational approach adopted by the LINCS program is currently based on linear regression, limiting its accuracy since it does not capture complex nonlinear relationship between expression of genes. Results: We present a deep learning method (abbreviated as D-GEX) to infer the expression of target genes from the expression of landmark genes. We used the microarray-based GEO dataset, consisting of 111K expression profiles, to train our model and compare its performance to those from other methods. In terms of mean absolute error averaged across all genes, deep learning significantly outperforms linear regression with 15.33% relative improvement. A gene-wise comparative analysis shows that deep learning achieves lower error than linear regression in 99.97% of the target genes. We also tested the performance of our learned model on an independent RNA-Seq-based GTEx dataset, which consists of 2,921 expression profiles. Deep learning still outperforms linear regression with 6.57% relative improvement, and achieves lower error in 81.31% of the target genes. Availability: D-GEX is available at https://github.com/uci-cbcl/D-GEX."}, {"title": "On the identifiability of transmission dynamic models for infectious diseases", "url": "https://www.biorxiv.org/content/early/2015/12/11/021972", "tag": "Bioinformatics", "abstract": "Understanding the transmission dynamics of infectious diseases is important for both biological research and public health applications. It has been widely demonstrated that statistical modeling provides a firm basis for inferring relevant epidemiological quantities from incidence and molecular data. However, the complexity of transmission dynamic models causes two challenges: Firstly, the likelihood function of the models is generally not computable and computationally intensive simulation-based inference methods need to be employed. Secondly, the model may not be fully identifiable from the available data. While the first difficulty can be tackled by computational and algorithmic advances, the second obstacle is more fundamental. Identifiability issues may lead to inferences which are more driven by the prior assumptions than the data themselves. We here consider a popular and relatively simple, yet analytically intractable model for the spread of tuberculosis based on classical IS6110 fingerprinting data. We report on the identifiability of the model, presenting also some methodological advances regarding the inference. Using likelihood approximations, it is shown that the reproductive value cannot be identified from the data available and that the posterior distributions obtained in previous work have likely been substantially dominated by the assumed prior distribution. Further, we show that the inferences are influenced by the assumed infectious population size which has generally been kept fixed in previous work. We demonstrate that the infectious population size can be inferred if the remaining epidemiological parameters are already known with sufficient precision."}, {"title": "chromPlot: visualization of genomic data in chromosomal context", "url": "https://www.biorxiv.org/content/early/2015/12/10/034108", "tag": "Bioinformatics", "abstract": "Visualizing genomic data in chromosomal context can help detecting errors in data generation or analysis and can suggest new hypotheses to be tested. Here we report a new tool for displaying large and diverse genomic data in idiograms of one or multiple chromosomes. The package is implemented in R so that visualization can be easily integrated with its numerous packages for processing genomic data. It supports simultaneous visualization of multiples tracks of data, each of potentially different nature. Large genomic regions such as QTLs or synteny tracts may be shown along histograms of number of genes, genetic variants, or any other type of genomic element. Tracks can also contain values for continuous or categorical variables and the user can choose among points, points connected by lines, line segments, barplots or histograms for representing data. chromPlot reads data from tables in BED format which are imported in R using its builtin functions. The information necessary to draw chromosomes for mouse and human is included with the package. Chromosomes for other organisms are downloaded automatically from the Ensembl website or can be provided by the user. We present common use cases here, and a full tutorial is included as the packages's vignette."}, {"title": "Combating the scientific decline effect with confidence (intervals)", "url": "https://www.biorxiv.org/content/early/2015/12/10/034074", "tag": "Bioinformatics", "abstract": "A symptom of the need for greater reproducibility in scientific practice is the \"decline effect,\" the fact that the size of many experimental effects decline with subsequent study or fail to replicate entirely. A simple way to combat this problem is for scientists to more routinely use confidence intervals (CIs) in their work. CIs provide frequentist bounds on the true size of an effect and can reveal when a statistically significant effect is possibly too small to be reliable or when a large effect might have been missed due to insufficient statistical power. CIs are often lacking in psychophysiological reports, likely due to the large number of dependent variables, which complicates deriving and visualizing CIs. In this article, I explain the value of CIs and show how to compute them for analyses involving multiple variables in various ways that adjust the intervals for the greater uncertainty induced by multiple statistical comparisons. The methods are illustrated using a basic visual oddball event-related potential (ERP) dataset and freely available Matlab software."}, {"title": "A framework for collaborative computational research", "url": "https://www.biorxiv.org/content/early/2015/12/09/033654", "tag": "Bioinformatics", "abstract": "Analysis of high troughput biological data often involves the use of many software packages and in-house written code. For each analysis step there are multiple options of software tools available, each with its own capabilities, limitations and assumptions on the input and output data. The development of bioinformatics pipelines involves a great deal of experimentation with different tools and parameters, considering how each would fit to the big picture and the practical implications of their use. Organizing data analysis could prove challenging. In this work we present a set of methods and tools that aim to enable the user to experiment extensively, while keeping analyses reproducible and organized. We present a framework based on simple principles that allow data analyses to be structured in a way that emphasizes reproducibility, organization and clarity, while being simple and intuitive so that adding and modifying analysis steps can be done naturally with little extra effort. The framework suppports version control of code, documentation and data, enabling collaboration between users."}, {"title": "Habitat variability does not generally promote metabolic network modularity in flies and mammals", "url": "https://www.biorxiv.org/content/early/2015/12/09/034033", "tag": "Bioinformatics", "abstract": "The evolution of species habitat range is an important topic over a wide range of research fields. In higher organisms, habitat range evolution is generally associated with genetic events such as gene duplication. However, the specific factors that determine habitat variability remain unclear at higher levels of biological organization (e.g., biochemical networks). One widely accepted hypothesis developed from both theoretical and empirical analyses is that habitat variability promotes network modularity; however, this relationship has not yet been directly tested in higher organisms. Therefore, I investigated the relationship between habitat variability and metabolic network modularity using compound and enzymatic networks in flies and mammals. Contrary to expectation, there was no clear positive correlation between habitat variability and network modularity. As an exception, the network modularity increased with habitat variability in the enzymatic networks of flies. However, the observed association was likely an artifact, and the frequency of gene duplication appears to be the main factor contributing to network modularity. These findings raise the question of whether or not there is a general mechanism for habitat range expansion at a higher level (i.e., above the gene scale). This study suggests that the currently widely accepted hypothesis for habitat variability should be reconsidered."}, {"title": "Factorbook Motif Pipeline: A de novo motif discovery and filtering web server for ChIP-seq peaks", "url": "https://www.biorxiv.org/content/early/2015/12/04/033670", "tag": "Bioinformatics", "abstract": "Summary: High-throughput sequencing technologies such as ChIP-seq have deepened our understanding in many biological processes. De novo motif search is one of the key downstream computational analysis following the ChIP-seq experiments and several algorithms have been proposed for this purpose. However, most web-based systems do not perform independent filtering or enrichment analyses to ensure the quality of the discovered motifs. Here, we developed a web server Factorbook Motif Pipeline based on an algorithm used in analyzing ENCODE consortium ChIP-seq datasets. It performs comprehensive analysis on the set of peaks detected from a ChIP-seq experiments: (i) de novo motif discovery; (ii) independent composition and bias analyses and (iii) matching to the annotated motifs. The statistical tests employed in our pipeline provide a reliable measure of confidence as to how significant are the motifs reported in the discovery step. Availability: Factorbook Motif Pipeline source code is accessible through the following URL. https://github.com/joshuabhk/factorbook-motif-pipeline"}, {"title": "Epigenomic co-localization and co-evolution reveal a key role for 5hmC as a communication hub in the chromatin network of ESCs", "url": "https://www.biorxiv.org/content/early/2015/12/03/008821", "tag": "Bioinformatics", "abstract": "Epigenetic communication through histone and cytosine modifications is essential for gene regulation and cell identity. Here, we propose a framework that is based on a chromatin communication model to get insight on the function of epigenetic modifications in ESCs. The epigenetic communication network was inferred from genome-wide location data plus extensive manual annotation. Notably, we found that 5-hydroxymethylcytosine (5hmC) is the most influential hub of this network, connecting DNA demethylation to nucleosome remodeling complexes and to key transcription factors of pluripotency. Moreover, an evolutionary analysis revealed a central role of 5hmC in the co-evolution of chromatin-related proteins. Further analysis of regions where 5hmC colocalizes with specific interactors shows that each interaction points to chromatin remodelling, stemness, differentiation or metabolism. Our results highlight the importance of cytosine modifications in the epigenetic communication of ESCs."}, {"title": "Cancer Classification by Correntropy-Based Sparse Compact Incremental Learning Machine", "url": "https://www.biorxiv.org/content/early/2015/12/03/028720", "tag": "Bioinformatics", "abstract": "Cancer prediction is of great importance and significance and it is crucial to provide researchers and scientists with novel, accurate and robust computational tools for this issue. Recent technologies such as Microarray and Next Generation Sequencing have paved the way for computational methods and techniques to play critical roles in this regard. Many important problems in cell biology require the dense nonlinear interactions between functional modules to be considered. The importance of computer simulation in understanding cellular processes is now widely accepted, and a variety of simulation algorithms useful for studying certain subsystems have been designed. In this article, a Sparse Compact Incremental Learning Machine (SCILM) is proposed for cancer classification problem on microarray gene expression data which take advantage of Correntropy cost that makes it robust against diverse noises and outliers. Moreover, since SCILM uses l1-norm of the weights, it has sparseness which can be applied for gene selection purposes as well. Finally, due to compact structure, the proposed method is capable of performing classification tasks in all of the cases with only one neuron in its hidden layer. The experimental analysis is performed on 26 well known microarray datasets regarding diverse kinds of cancers and the results show that the proposed method not only achieved significantly high accuracy but also because of its sparseness, final connectivity weights determined the value and effectivity of each gene regarding the corresponding cancer."}, {"title": "nihexporter: an R package for NIH funding data", "url": "https://www.biorxiv.org/content/early/2015/12/02/033456", "tag": "Bioinformatics", "abstract": "The National Institutes of Health (NIH) is the major source of federal funding for biomedical research in the United States. Analysis of past and current NIH funding can illustrate funding trends and identify productive research topics, but these analyses are conducted *ad hoc* by the institutes themselves and only provide a small glimpse of the available data. The NIH provides free access to funding data via NIH EXPORTER, but no tools have been developed to enable analysis of this data. We developed the `nihexporter` R package, which provides access to NIH EXPORTER (http://exporter.nih.gov/) data. We used the package to develop several analysis vignettes that show funding trends across NIH institutes over 15 years and highlight differences in how institutes change their funding profiles. Investigators and institutions can use the package to perform self-studies of their own NIH funding."}, {"title": "TopHat-Recondition: A post-processor for TopHat unmapped reads", "url": "https://www.biorxiv.org/content/early/2015/12/02/033530", "tag": "Bioinformatics", "abstract": "Summary: TopHat is a popular spliced junction mapper for RNA sequencing data, and writes files in the BAM format - the binary version of the Sequence Alignment/Map (SAM) format. BAM is the standard exchange format for aligned sequencing reads, thus correct format implementation is paramount for software interoperability and correct analysis. However, TopHat writes its unmapped reads in a way that is not compatible with other software that implements the SAM/BAM format. We have developed TopHat-Recondition, a post-processor for TopHat unmapped reads that restores read information in the proper format. TopHat-Recondition thus enables downstream software to process the plethora of BAM files written by TopHat. Availability and implementation: TopHat-Recondition is implemented in Python using the Pysam library and is freely available under a 2-clause BSD license on GitHub: https://github.com/cbrueffer/tophat-recondition. Contact: christian.brueffer@med.lu.se, lao.saal@med.lu.se"}, {"title": "iCOBRA: open, reproducible, standardized and live method benchmarking", "url": "https://www.biorxiv.org/content/early/2015/12/01/033431", "tag": "Bioinformatics", "abstract": "We present iCOBRA, a flexible general-purpose web-based application and accompanying R package to evaluate, compare and visualize the performance of methods for estimation or classification when ground truth is available. iCOBRA is interactive, can be run locally or remotely and generates customizable, publication-ready graphics. To facilitate open, reproducible and standardized method comparisons, expanding as new innovations are made, we encourage the community to provide benchmark results in a standard format."}, {"title": "A benchmark of gene expression tissue-specificity metrics", "url": "https://www.biorxiv.org/content/early/2015/12/01/027755", "tag": "Bioinformatics", "abstract": "One of the major properties of genes is their expression pattern. Notably, genes are often classified as tissue-specific or housekeeping. This property is of interest to molecular evolution as an explanatory factor of, e.g., evolutionary rate, as well as a functional feature which may in itself evolve. While many different methods of measuring tissue specificity have been proposed and used for such studies, there has been no comparison or benchmarking of these methods to our knowledge, and little justification of their use. In this study we compare nine measures of tissue-specificity. Most methods were established for ESTs and microarrays, and several were later adapted to RNA-seq. We analyze their capacity to distinguish gene categories, their robustness to the choice and number of tissues used, and their capture of evolutionary conservation signal."}, {"title": "An Improved Genome Assembly of Azadirachta indica A. Juss.", "url": "https://www.biorxiv.org/content/early/2015/11/30/033290", "tag": "Bioinformatics", "abstract": "Neem (Azadirachta indica A. Juss.), an evergreen tree of the Meliaceae family, is known for its medicinal, cosmetic, pesticidal and insecticidal properties. We had previously sequenced and published the draft genome of the plant, using mainly short read sequencing data. In this report, we present an improved genome assembly generated using additional short reads from Illumina and long reads from Pacific Biosciences SMRT sequencer. We assembled short reads and error corrected long reads using Platanus, an assembler designed to perform well for heterozygous genomes. The updated genome assembly (v2.0) yielded 3- and 3.5-fold increase in N50 and N75, respectively; 2.6-fold decrease in the total number of scaffolds; 1.25-fold increase in the number of valid transcriptome alignments; 13.4-fold less mis-assembly and 1.85-fold increase in the percentage repeat, over the earlier assembly (v1.0). The current assembly also maps better to the genes known to be involved in the terpenoid biosynthesis pathway. Together, the data represents an improved assembly of the A. indica genome. The raw data described in this manuscript are submitted to the NCBI Short Read Archive under the accession numbers SRX1074131, SRX1074132, SRX1074133, and SRX1074134 (SRP013453)."}, {"title": "Two Independent and Highly Efficient Open Source TKF91 Implementations", "url": "https://www.biorxiv.org/content/early/2015/11/29/033191", "tag": "Bioinformatics", "abstract": "In the context of a master level programming practical at the computer science department of the Karlsruhe Institute of Technology, we developed and make available two independent and highly optimized open-source implementations for the pair-wise statistical alignment model, also known as TKF91, that was developed by Thorne, Kishino, and Felsenstein in 1991. This paper has two parts. In the educational part, we cover teaching issues regarding the setup of the course and the practical and summarize student and teacher experiences. In the scientific part, the two student teams (Team I: Nikolai, Sebastian, Daniel; Team II: Sarah, Pierre) present their solutions for implementing efficient and numerically stable implementations of the TKF91 algorithm. The two teams worked independently on implementing the same algorithm. Hence, since the implementations yield identical results -with slight numerical deviations- we are confident that the implementations are correct. We describe the optimizations applied and make them available as open-source codes in the hope that our findings and software will be useful to the community as well as for similar programming practicals at other universities."}, {"title": "Sequenceserver: a modern graphical user interface for custom BLAST databases", "url": "https://www.biorxiv.org/content/early/2015/11/27/033142", "tag": "Bioinformatics", "abstract": "The dramatic drop in DNA sequencing costs has created many opportunities for novel biological research. These opportunities largely rest upon the ability to effectively compare newly obtained and previously known sequences. This is commonly done with BLAST, yet using BLAST directly on new datasets requires substantial technical skills or helpful colleagues. Furthermore, graphical interfaces for BLAST are challenging to install and largely mimic underlying computational processes rather than work patterns of researchers. We combined a user-centric design philosophy with sustainable software development approaches to create Sequenceserver (http://sequenceserver.com), a modern graphical user interface for BLAST. Sequenceserver substantially increases the efficiency of researchers working with sequence data. This is due first to innovations at three levels. First, our software can be installed and used on custom datasets extremely rapidly for personal and shared applications. Second, based on analysis of user input and simple algorithms, Sequenceserver reduces the amount of decisions the user must make, provides interactive visual feedback, and prevents common potential errors that would otherwise cause erroneous results. Finally, Sequenceserver provides multiple highly visual and text-based output options that mirror the requirements and work patterns of researchers. Together, these features greatly facilitate BLAST analysis and interpretation and thus substantially enhance researcher productivity."}, {"title": "Rich chromatin structure prediction from Hi-C data", "url": "https://www.biorxiv.org/content/early/2015/11/26/032953", "tag": "Bioinformatics", "abstract": "Recent studies involving the 3-dimensional conformation of chromatin have revealed the important role it has to play in different processes within the cell. These studies have also led to the discovery of densely interacting segments of the chromosome, called topologically associating domains. The accurate identification of these domains from Hi-C interaction data is an interesting and important computational problem for which numerous methods have been proposed. Unfortunately, most existing algorithms designed to identify these domains assume that they are non-overlapping whereas there is substantial evidence to believe a nested structure exists. We present an efficient methodology to predict hierarchical chromatin domains using chromatin conformation capture data. Our method predicts domains at different resolutions and uses these to construct a hierarchy that is based on intrinsic properties of the chromatin data. The hierarchy consists of a set of non-overlapping domains, that maximize intra-domain interaction frequencies, at each level. We show that our predicted structure is highly enriched for CTCF and various other chromatin markers. We also show that large-scale domains, at multiple resolutions within our hierarchy, are conserved across cell types and species. Our software, Matryoshka, is written in C++11 and licensed under GPL v3; it is available at https://github.com/COMBINE-lab/matryoshka."}, {"title": "AGOUTI: improving genome assembly and annotation using transcriptome data", "url": "https://www.biorxiv.org/content/early/2015/11/26/033019", "tag": "Bioinformatics", "abstract": "Current genome assemblies consist of thousands of contigs. These incomplete and fragmented assemblies lead to errors in gene identification, such that single genes spread across multiple contigs are annotated as separate gene models. We present AGOUTI (Annotated Genome Optimization Using Transcriptome Information), a tool that uses RNA-seq data to simultaneously combine contigs into scaffolds and fragmented gene models into single models. We show that AGOUTI improves both the contiguity of genome assemblies and the accuracy of gene annotation, providing updated versions of each as output."}, {"title": "DisGeNET-RDF: harnessing the innovative power of the Semantic Web to explore the genetic basis of diseases", "url": "https://www.biorxiv.org/content/early/2015/11/26/032961", "tag": "Bioinformatics", "abstract": "Motivation: DisGeNET-RDF makes available knowledge on the genetic basis of human diseases in the Semantic Web (SW). Gene-disease associations (GDAs) and their provenance metadata are published as human-readable and machine-processable web resources. The information on GDAs included in DisGeNET-RDF is interlinked to other biomedical databases to support the development of bioinformatics approaches for translational research through evidence-based exploitation of a rich and fully interconnected Linked Open Data (LOD)."}, {"title": "Validation of Illumina's Isaac variant calling workflow.", "url": "https://www.biorxiv.org/content/early/2015/11/26/031021", "tag": "Bioinformatics", "abstract": "As the pace of implementing personalized medicine concepts increases, high-throughput variant calling on hundreds of individual genomes per day is a reality that will likely be faced by sequencing facilities across the country in the near future. While the scientific best practices for human variant calling workflows have been well defined, they also pose serious computational challenges at this high scale. Therefore, efforts in both academia and the private sector have focused on developing alternative workflows that may substantially reduce the computational cost per individual genome. iSAAC is an \"ultra-fast\" variant calling workflow, designed by Illumina, Inc, and is claimed to be six times faster than BWA-GATK, with comparable sensitivity and specificity. This report is an independent review of iSAAC, mainly focused on the accuracy of variant calls. We note that iSAAC is indeed quite fast, and provide some benchmarks on a few hardware architectures. The overall conclusion from our analysis is that the iSAAC workflow has undergone substantial improvement from version 01.14.11.27 to iSAAC_2.0. The call accuracy is especially high on NA12878, however exomes and genomic data outside the Platinum sets tend to have a high fraction of false positive calls. We did not manage to reproduce the 99% sensitivity and specificity reported in the Illumina whitepaper, however that might be improved with further tweaking of the options. This report includes the information about some of the command-line parameters and documentation."}, {"title": "ProtAnnot: an App for Integrated Genome Browser to display how alternative splicing and transcription affect proteins", "url": "https://www.biorxiv.org/content/early/2015/11/26/025924", "tag": "Bioinformatics", "abstract": "Summary: One gene can produce multiple transcript variants en-coding proteins with different functions. To facilitate visual analysis of transcript variants, we developed ProtAnnot, which shows protein annotations in the context of genomic sequence. ProtAnnot searches InterPro and displays profile matches (protein annotations) alongside gene models, exposing how alternative promoters, splicing, and 3' end processing add, remove, or remodel functional motifs. To draw attention to these effects, ProtAnnot color-codes exons by frame and displays a cityscape graphic summarizing exonic sequence at each position. These techniques make visual analysis of alternative transcripts faster and more convenient for biologists. Availability and Implementation: ProtAnnot is a plug-in App for Integrated Genome Browser, an open source desktop genome browser available from http://www.bioviz.org."}, {"title": "An evaluation of methods correcting for cell type heterogeneity in DNA methylation studies", "url": "https://www.biorxiv.org/content/early/2015/11/25/032185", "tag": "Bioinformatics", "abstract": "Background: Many different methods exist to adjust for variability in cell-type mixture proportions when analyzing DNA methylation studies. Here we present the result of an extensive simulation study, built on cell-separated DNA methylation profiles from Illumina Infinium 450K methylation data, to compare the performance of 8 methods including the most commonly-used approaches. Results: We designed a rich multi-layered simulation containing a set of probes with true associations with either binary or continuous phenotypes, confounding by cell type, variability in means and standard deviations for population parameter, additional variability at the level of an individual cell-type-specific sample, and variability in the mixture proportions across samples. Performance varied quite substantially across methods and simulations. In particular, the false discovery rates (FDR) were sometimes unrealistically high, indicating limited ability to discriminate the true signals from those appearing significant through confounding. Methods that filtered probes had consequently poor power. QQ-plots of p-values across all tested probes showed that adjustments did not always improve the distribution. The same methods were used to examine associations between smoking and methylation data from a case-control study of colorectal cancer. Conclusions: We recommend surrogate variable analysis for cell-type mixture adjustment since performance was stable under all our simulated scenarios."}, {"title": "Quantifying polygenic effects in genome-wide association studies using generalized estimating equations", "url": "https://www.biorxiv.org/content/early/2015/11/25/032854", "tag": "Bioinformatics", "abstract": "Recently, LD Score regression1 has been proposed as a computationally fast method to contrast confounding biases with polygenicity and to quantify their contribution to the inflation of test statistics in GWAS. In this communication, we extend the LD Score regression approach by applying the generalized estimation equations (GEE) framework, which is capable of incorporating more external information from reference panels about the correlation structure of test statistics. We apply our GEE approach and LD Score regression to simulated and real data to compare their performance. We show that our proposed methodology obtains more efficient estimates while preserving the robustness and desired properties of LD Score regression."}, {"title": "G-quadruplex prediction in E.coli genome reveals a conserved putative G-quadruplex-Hairpin-Duplex switch", "url": "https://www.biorxiv.org/content/early/2015/11/24/032615", "tag": "Bioinformatics", "abstract": "Many studies show that short non-coding sequences are widely conserved among regulatory elements. More and more conserved sequences are being discovered since the development next generation sequencing technology. A common approach to identify conserved sequences with regulatory roles rely on the topological change such as hairpin formation on the DNA or RNA level. However G quadruplexes, a non-canonical nucleic acid topology with little established biological role, is rarely considered for conserved regulatory element discovery. Here we present the use of G-quadruplex prediction algorithm to identify putative G-quadruplex-forming and conserved elements in E.coli genome. Phylogenetic analysis of 52 G-quadruplex forming sequences revealed two conserved G-quadruplex motifs with potential regulatory role."}, {"title": "BioJS-HGV Viewer: Genetic Variation Visualizer", "url": "https://www.biorxiv.org/content/early/2015/11/23/032573", "tag": "Bioinformatics", "abstract": "Studying the pattern of genetic variants is a primary step in deciphering the basis of biological diversity, identifying key `driver variants' that affect disease states and evolution of a species. Catalogs of genetic variants contain vast numbers of variants and are growing at an exponential rate, but lack an interactive exploratory interface. We present BioJS-HGV Viewer, a BioJS component to represent and visualize genetic variants pooled from different sources. The tool displays sequences and variants at different levels of detail, facilitating representation of variant sites and annotations in a user friendly and interactive manner. Source code for BioJS-HGV Viewer is available at: https://github.com/saketkc/biojs-genetic-variation-viewer A demo is available at: http://saketkc.github.io/biojs-genetic-variation-viewer"}, {"title": "ASAP: A Machine-Learning Framework for Local Protein Properties", "url": "https://www.biorxiv.org/content/early/2015/11/21/032532", "tag": "Bioinformatics", "abstract": "Determining residue level protein properties, such as the sites for post-translational modifications (PTMs) are vital to understanding proteins at all levels of function. Experimental methods are costly and time-consuming, thus high confidence predictions become essential for functional knowledge at a genomic scale. Traditional computational methods based on strict rules (e.g. regular expressions) fail to annotate sites that lack substantial similarity. Thus, Machine Learning (ML) methods become fundamental in annotating proteins with unknown function. We present ASAP (Amino-acid Sequence Annotation Prediction), a universal ML framework for residue-level predictions. ASAP extracts efficiently and fast large set of window-based features from raw sequences. The platform also supports easy integration of external features such as secondary structure or PSSM profiles. The features are then combined to train underlying ML classifiers. We present a detailed case study for ASAP that was used to train CleavePred, a state-of-the-art protein precursor cleavage sites predictor. Protein cleavage is a fundamental PTM shared by a wide variety of protein groups with minimal sequence similarity. Current computational methods have high false positive rates, making them suboptimal for this task. CleavePred has a simple Python API, and is freely accessible via a web- based application. The high performance of ASAP toward the task of precursor cleavage is suited for analyzing new proteomes at a genomic scale. The tool is attractive to protein design, mass spectrometry search engines and the discovery of new peptide hormones. In summary, we illustrate ASAP as an entry point for predicting PTMs. The approach and flexibility of the platform can easily be extended for additional residue specific tasks. ASAP and CleavePred source code available at https://github.com/ddofer/asap."}, {"title": "BRANE Cut: Biologically-Related A priori Network Enhancement with Graph cuts for Gene Regulatory Network Inference", "url": "https://www.biorxiv.org/content/early/2015/11/20/032383", "tag": "Bioinformatics", "abstract": "Background: Inferring gene networks from high-throughput data constitutes an important step in the discovery of relevant regulatory relationships in organism cells. Despite the large number of available Gene Regulatory Network inference methods, the problem remains challenging: the underdetermination in the space of possible solutions requires additional constraints that incorporate a priori information on gene interactions. Methods: Weighting all possible pairwise gene relationships by a probability of edge presence, we formulate the regulatory network inference as a discrete variational problem on graphs. We enforce biologically plausible coupling between groups and types of genes by minimizing an edge labeling functional coding for a priori structures. The optimization is carried out with Graph cuts, an approach popular in image processing and computer vision. We compare the inferred regulatory networks to results achieved by the mutual-information-based Context Likelihood of Relatedness (CLR) method and by the state-of-the-art GENIE3, winner of the DREAM4 multifactorial challenge. Results: Our BRANE Cut approach infers more accurately the five DREAM4 in silico networks (with improvements from 6% to 11%). On a real Escherichia coli compendium, an improvement of 11.8% compared to CLR and 3% compared to GENIE3 is obtained in terms of Area Under Precision-Recall curve. Up to 48 additional verified interactions are obtained over GENIE3 for a given precision. On this dataset involving 4345 genes, our method achieves a performance similar to that of GENIE3, while being more than seven times faster. The BRANE Cut code is available at: http://www-syscom.univ-mlv.fr/~pirayre/Codes-GRN-BRANE-cut.html Conclusions: BRANE Cut is a weighted graph thresholding method. Using biologically sound penalties and data-driven parameters, it improves three state-of-the-art GRN inference methods. It is applicable as a generic network inference post-processing, due its computational efficiency."}, {"title": "From raw reads to trees: Whole genome SNP phylogenetics across the tree of life", "url": "https://www.biorxiv.org/content/early/2015/11/19/032250", "tag": "Bioinformatics", "abstract": "Next-generation sequencing is increasingly being used to examine closely related organisms. However, while genome-wide single nucleotide polymorphisms (SNPs) provide an excellent resource for phylogenetic reconstruction, to date evolutionary analyses have been performed using different ad hoc methods that are not often widely applicable across different projects. To facilitate the construction of robust phylogenies, we have developed a method for genome-wide identification/characterization of SNPs from sequencing reads and genome assemblies. Our phylogenetic and molecular evolutionary (PhaME) analysis software is unique in its ability to take reads and draft/complete genome(s) as input, derive core genome alignments, identify SNPs, construct phylogenies and perform evolutionary analyses. Several examples using genomes and read datasets for bacterial, eukaryotic and viral linages demonstrate the broad and robust functionality of PhaME. Furthermore, the ability to incorporate raw metagenomic reads from clinical samples with suspected infectious agents shows promise for the rapid phylogenetic characterization of pathogens within complex samples."}, {"title": "Wikidata as a semantic framework for the Gene Wiki initiative", "url": "https://www.biorxiv.org/content/early/2015/11/19/032144", "tag": "Bioinformatics", "abstract": "Open biological data is distributed over many resources making it challenging to integrate, to update and to disseminate quickly. Wikidata is a growing, open community database which can serve this purpose and also provides tight integration with Wikipedia. In order to improve the state of biological data, facilitate data management and dissemination, we imported all human and mouse genes, and all human and mouse proteins into Wikidata. In total, 59,530 human genes and 73,130 mouse genes have been imported from NCBI and 27,662 human proteins and 16,728 mouse proteins have been imported from the Swissprot subset of UniProt. As Wikidata is open and can be edited by anybody, our corpus of imported data serves as the starting point for integration of further data by scientists, the Wikidata community and citizen scientists alike. The first use case for this data is to populate Wikipedia Gene Wiki infoboxes directly from Wikidata with the data integrated above. This enables immediate updates of the Gene Wiki infoboxes as soon as the data in Wikidata is modified. Although Gene Wiki pages are currently only on the English language version of Wikipedia, the multilingual nature of Wikidata allows for a usage of the data we imported in all 280 different language Wikipedias. Apart from the Gene Wiki infobox use case, a powerful SPARQL endpoint and up to date exporting functionality (e.g. JSON, XML) enable very convenient further use of the data by scientists. In summary, we created a fully open and extensible data resource for human and mouse molecular biology and biochemistry data. This resource enriches all the Wikipedias with structured information and serves as a new linking hub for the biological semantic web."}, {"title": "XMRF: An R package to Fit Markov Networks to High-Throughput Genetics Data", "url": "https://www.biorxiv.org/content/early/2015/11/18/032219", "tag": "Bioinformatics", "abstract": "Motivation: Technological advances in medicine have led to a rapid proliferation of high-throughput \"omics\" data. Tools to mine this data and discover disrupted disease networks are needed as they hold the key to understanding complicated interactions between genes, mutations and aberrations, and epi-genetic markers. Results: We developed an R software package, XMRF, that can be used to fit Markov Networks to various types of high-throughput genomics data. Encoding the models and estimation techniques of the recently proposed exponential family Markov Random Fields (Yang et al., 2012), our software can be used to learn genetic networks from RNA-sequencing data (counts via Poisson graphical models), mutation and copy number variation data (categorical via Ising models), and methylation data (continuous via Gaussian graphical models). Availability: XMRF is available from the CRAN Project and Github at: https://github.com/zhandong/XMRF"}, {"title": "Testing Rare-Variant Association without Calling Genotypes Allows for Systematic Differences in Sequencing between Cases and Controls", "url": "https://www.biorxiv.org/content/early/2015/11/16/032037", "tag": "Bioinformatics", "abstract": "Next-generation sequencing of DNA provides an unprecedented opportunity to discover rare genetic variants associated with complex diseases and traits. However, when testing the association between rare variants and traits of interest, the current practice of first calling underlying genotypes and then treating the called values as known is prone to false positive findings, especially when genotyping errors are systematically different between cases and controls. This happens whenever cases and controls are sequenced at different depths or on different platforms. In this article, we provide a likelihood-based approach to testing rare variant associations that directly models sequencing reads without calling genotypes. We consider the (weighted) burden test statistic, which is the (weighted) sum of the score statistic for assessing effects of individual variants on the trait of interest. Because variant locations are unknown, we develop a simple, computationally efficient screening algorithm to estimate the loci that are variants. Because our burden statistic may not have mean zero after screening, we develop a novel bootstrap procedure for assessing the significance of the burden statistic. We demonstrate through extensive simulation studies that the proposed tests are robust to a wide range of differential sequencing qualities between cases and controls, and are at least as powerful as the standard genotype calling approach when the latter controls type I error. An application to the UK10K data reveals novel rare variants in gene BTBD18 associated with childhood onset obesity. The relevant software is freely available."}, {"title": "Learning structure in gene expression data using deep architectures, with an application to gene clustering", "url": "https://www.biorxiv.org/content/early/2015/11/16/031906", "tag": "Bioinformatics", "abstract": "Genes play a central role in all biological processes. DNA microarray technology has made it possible to study the expression behavior of thousands of genes in one go. Often, gene expression data is used to generate features for supervised and unsupervised learning tasks. At the same time, advances in the field of deep learning have made available a plethora of architectures. In this paper, we use deep architectures pre-trained in an unsupervised manner using denoising autoencoders as a preprocessing step for a popular unsupervised learning task. Denoising autoencoders (DA) can be used to learn a compact representation of input, and have been used to generate features for further supervised learning tasks. We propose that our deep architectures can be treated as empirical versions of Deep Belief Networks (DBNs). We use our deep architectures to regenerate gene expression time series data for two different data sets. We test our hypothesis on two popular datasets for the unsupervised learning task of clustering and find promising improvements in performance."}, {"title": "Wikidata: A platform for data integration and dissemination for the life sciences and beyond", "url": "https://www.biorxiv.org/content/early/2015/11/16/031971", "tag": "Bioinformatics", "abstract": "Wikidata is an open, Semantic Web-compatible database that anyone can edit. This \u2032data commons\u2032 provides structured data for Wikipedia articles and other applications. Every article on Wikipedia has a hyperlink to an editable item in this database. This unique connection to the world\u2032s largest community of volunteer knowledge editors could help make Wikidata a key hub within the greater Semantic Web. The life sciences, as ever, faces crucial challenges in disseminating and integrating knowledge. Our group is addressing these issues by populating Wikidata with the seeds of a foundational semantic network linking genes, drugs and diseases. Using this content, we are enhancing Wikipedia articles to both increase their quality and recruit human editors to expand and improve the underlying data. We encourage the community to join us as we collaboratively create what can become the most used and most central semantic data resource for the life sciences and beyond."}, {"title": "Mass graphs and their applications in top-down proteomics", "url": "https://www.biorxiv.org/content/early/2015/11/16/031997", "tag": "Bioinformatics", "abstract": "Although proteomics has made rapid progress in the past decade, researchers are still in the early stage of exploring the world of complex proteoforms, which are protein products with various primary structure alterations resulting from gene mutations, alternative splicing, post-translational modifications, and other biological processes. Proteoform identification is essential to mapping proteoforms to their biological functions as well as discovering novel proteoforms and new protein functions. Top-down mass spectrometry is the method of choice for identifying complex proteoforms because it provides a \"bird view\" of intact proteoforms. The combinatorial explosion of possible proteoforms, which may result in billions of possible proteoforms for one protein, makes proteoform identification a challenging computational problem. Here we propose a new data structure, called the mass graph, for efficiently representing proteoforms. In addition, we design mass graph alignment algorithms for proteoform identification by top-down mass spectrometry. Experiments on a histone H4 mass spectrometry data set showed that the proposed methods outperformed MS-Align-E in identifying complex proteoforms."}, {"title": "Towards extracting supporting information about predicted protein-protein interactions", "url": "https://www.biorxiv.org/content/early/2015/11/14/031591", "tag": "Bioinformatics", "abstract": "One of the goals of relation extraction is to identify protein-protein interactions (PPIs) in biomedical literature. Current systems are capturing binary relations and also the direction and type of an interaction. Besides assisting in the curation PPIs into databases, there has been little real-world application of these algorithms. We describe UPSITE, a text mining tool for extracting evidence in support of a hypothesized interaction. Given a predicted PPI, UPSITE uses a binary relation detector to check whether a PPI is found in abstracts in PubMed. If it is not found, UPSITE retrieves documents relevant to each of the two proteins separately, and extracts contextual information about biological events surrounding each protein, and calculates semantic similarity of the two proteins to provide evidential support for the predicted PPI. In evaluations, relation extraction achieved an Fscore of 0.88 on the HPRD50 corpus, and semantic similarity measured with angular distance was found to be statistically significant. With the development of PPI prediction algorithms, the burden of interpreting the validity and relevance of novel PPIs is on biologists. We suggest that presenting annotations of the two proteins in a PPI side-by-side and a score that quantifies their similarity lessens this burden to some extent."}, {"title": "DataJoint: managing big scientific data using MATLAB or Python", "url": "https://www.biorxiv.org/content/early/2015/11/14/031658", "tag": "Bioinformatics", "abstract": "The rise of big data in modern research poses serious challenges for data management: Large and intricate datasets from diverse instrumentation must be precisely aligned, annotated, and processed in a variety of ways to extract new insights. While high levels of data integrity are expected, research teams have diverse backgrounds, are geographically dispersed, and rarely possess a primary interest in data science. Here we describe DataJoint, an open-source toolbox designed for manipulating and processing scientific data under the relational data model. Designed for scientists who need a flexible and expressive database language with few basic concepts and operations, DataJoint facilitates multi-user access, efficient queries, and distributed computing. With implementations in both MATLAB and Python, DataJoint is not limited to particular file formats, acquisition systems, or data modalities and can be quickly adapted to new experimental designs. DataJoint and related resources are available at http://datajoint.github.com."}, {"title": "Evaluation of hybrid and non-hybrid methods for de novo assembly of nanopore reads", "url": "https://www.biorxiv.org/content/early/2015/11/13/030437", "tag": "Bioinformatics", "abstract": "Recent emergence of nanopore sequencing technology set a challenge for the established assembly methods not optimized for the combination of read lengths and high error rates of nanopore reads. In this work we assessed how existing de novo assembly methods perform on these reads. We benchmarked three non-hybrid (in terms of both error correction and scaffolding) assembly pipelines as well as two hybrid assemblers which use third generation sequencing data to scaffold Illumina assemblies. Tests were performed on several publicly available MinION and Illumina datasets of E. coli K-12, using several sequencing coverages of nanopore data (20x, 30x, 40x and 50x). We attempted to assess the quality of assembly at each of these coverages, to estimate the requirements for closed bacterial genome assembly. Results show that hybrid methods are highly dependent on the quality of NGS data, but much less on the quality and coverage of nanopore data and perform relatively well on lower nanopore coverages. Furthermore, when coverage is above 40x, all non-hybrid methods correctly assemble the E. coli genome, even a non-hybrid method tailored for Pacific Bioscience reads. While it requires higher coverage compared to a method designed particularly for nanopore reads, its running time is significantly lower."}, {"title": "Are all global alignment algorithms and implementations correct?", "url": "https://www.biorxiv.org/content/early/2015/11/12/031500", "tag": "Bioinformatics", "abstract": "While implementing the algorithm, we discovered two mathematical mistakes in Gotoh's paper that induce sub-optimal sequence alignments. First, there are minor indexing mistakes in the dynamic programming algorithm which become apparent immediately when implementing the procedure. Hence, we report on these for the sake of completeness. Second, there is a more profound problem with the dynamic programming matrix initialization. This initialization issue can easily be missed and find its way into actual implementations. This error is also present in standard text books. Namely, the widely used books by Gusfield and Waterman. To obtain an initial estimate of the extent to which this error has been propagated, we scrutinized freely available undergraduate lecture slides. We found that 8 out of 31 lecture slides contained the mistake, while 16 out of 31 simply omit parts of the initialization, thus giving an incomplete description of the algorithm. Finally, by inspecting ten source codes and running respective tests, we found that five implementations were incorrect. Note that, not all bugs we identified are due to the mistake in Gotoh's paper. Three implementations rely on additional constraints that limit generality. Thus, only two out of ten yield correct results. We show that the error introduced by Gotoh is straightforward to resolve and provide a correct open-source reference implementation. We do believe though, that raising the awareness about these errors is critical, since the impact of incorrect pairwise sequence alignments that typically represent one of the very first stages in any bioinformatics data analysis pipeline can have a detrimental impact on downstream analyses such as multiple sequence alignment, orthology assignment, phylogenetic analyses, divergence time estimates, etc."}, {"title": "Rapid Genotype Refinement for Whole-Genome Sequencing Data using Multi-Variate Normal Distributions", "url": "https://www.biorxiv.org/content/early/2015/11/12/031484", "tag": "Bioinformatics", "abstract": "Whole-genome low-coverage sequencing has been combined with linkage-disequilibrium (LD) based genotype refinement to accurately and cost-effectively infer genotypes in large cohorts of individuals. Most genotype refinement methods are based on hidden Markov models, which are accurate but computationally expensive. We introduce an algorithm that models LD using a simple multivariate Gaussian distribution. The key feature of our algorithm is its speed, it is hundreds of times faster than other methods on the same data set and its scaling behaviour is linear in the number of samples. We demonstrate the performance of the method on both low-coverage and high-coverage samples."}, {"title": "Large-scale non-targeted metabolomic profiling in three human population-based studies", "url": "https://www.biorxiv.org/content/early/2015/11/12/002782", "tag": "Bioinformatics", "abstract": "Non-targeted metabolomic profiling is used to simultaneously assess a large part of the metabolome in a biological sample. Here, we describe both the analytical and computational methods used to analyze a large UPLC-Q-TOF MS-based metabolomic profiling effort using plasma and serum samples from participants in three Swedish population-based studies of middle-aged and older human subjects: TwinGene, ULSAM and PIVUS. At present, more than 200 metabolites have been manually annotated in more than 3,600 participants using an in-house library of standards and publically available spectral databases. Data available at the Metabolights repository include individual raw unprocessed data, processed data, basic demographic variables and spectra of annotated metabolites. Additional phenotypical and genetic data is available upon request to cohort steering committees. These studies represent a unique resource to explore and evaluate how metabolic variability across individuals affects human diseases."}, {"title": "MicrobiomeGWAS: a tool for identifying host genetic variants associated with microbiome composition", "url": "https://www.biorxiv.org/content/early/2015/11/10/031187", "tag": "Bioinformatics", "abstract": "The microbiome is the collection of all microbial genes and can be investigated by sequencing highly variable regions of 16S ribosomal RNA (rRNA) genes. Evidence suggests that environmental factors and host genetics may interact to impact human microbiome composition. Identifying host genetic variants associated with human microbiome composition not only provides clues for characterizing microbiome variation but also helps to elucidate biological mechanisms of genetic associations, prioritize genetic variants, and improve genetic risk prediction. Since a microbiota functions as a community, it is best characterized by beta diversity, that is, a pairwise distance matrix. We develop a statistical framework and a computationally efficient software package, microbiomeGWAS, for identifying host genetic variants associated with microbiome beta diversity with or without interacting with an environmental factor. We show that score statistics have positive skewness and kurtosis due to the dependent nature of the pairwise data, which makes P-value approximations based on asymptotic distributions unacceptably liberal. By correcting for skewness and kurtosis, we develop accurate P-value approximations, whose accuracy was verified by extensive simulations. We exemplify our methods by analyzing a set of 147 genotyped subjects with 16S rRNA microbiome profiles from non-malignant lung tissues. Correcting for skewness and kurtosis eliminated the dramatic deviation in the quantile-quantile plots. We provided preliminary evidence that six established lung cancer risk SNPs were collectively associated with microbiome composition for both unweighted (P=0.0032) and weighted (P=0.011) UniFrac distance matrices. In summary, our methods will facilitate analyzing large-scale genome-wide association studies of the human microbiome."}, {"title": "Numerical simulation of population development using the function of the age distribution of birthrate", "url": "https://www.biorxiv.org/content/early/2015/11/09/031047", "tag": "Bioinformatics", "abstract": "Numerical model of population behaviour under conditions of full-fed existence of mortal persons in the personal areas, simulated with use the function of age distribution of birth rate in the form of normal (gaussian) distribution. Graphs of historical changes in the function of age distribution of population."}, {"title": "Fast principal components analysis reveals convergent evolution of ADH1B gene in Europe and East Asia", "url": "https://www.biorxiv.org/content/early/2015/11/09/018143", "tag": "Bioinformatics", "abstract": "Searching for genetic variants with unusual differentiation between subpopulations is an established approach for identifying signals of natural selection. However, existing methods generally require discrete subpopulations. We introduce a method that infers selection using principal components (PCs) by identifying variants whose differentiation along top PCs is significantly greater than the null distribution of genetic drift. To enable the application of this method to large data sets, we developed the FastPCA software, which employs recent advances in random matrix theory to accurately approximate top PCs while reducing time and memory cost from quadratic to linear in the number of individuals, a computational improvement of many orders of magnitude. We apply FastPCA to a cohort of 54,734 European Americans, identifying 5 distinct subpopulations spanning the top 4 PCs. Using the PC-based test for natural selection, we replicate previously known selected loci and identify three new genome-wide significant signals of selection, including selection in Europeans at the ADH1B gene. The coding variant rs1229984*T has previously been associated to a decreased risk of alcoholism and shown to be under selection in East Asians; we show that it is a rare example of independent evolution on two continents. We also detect new selection signals at IGFBP3 and IGH, which have also previously been associated to human disease."}, {"title": "chopBAI: BAM index reduction solves I/O bottlenecks in the joint analysis of large sequencing cohorts", "url": "https://www.biorxiv.org/content/early/2015/11/06/030825", "tag": "Bioinformatics", "abstract": "Summary: Advances in sequencing capacity have lead to the generation of unprecedented amounts of genomic data. The processing of this data frequently leads to I/O bottlenecks, e.g. when analyzing a small genomic region across a large number of samples. The largest I/O burden is, however, often not imposed by the amount of data needed for the analysis but rather by index files that help retrieving this data. We have developed chopBAI, a program that can chop a BAM index (BAI) file into small pieces. The program outputs a list of BAI files each indexing a specified genomic interval. The output files are much smaller in size but maintain compatibility with existing software tools. We show how preprocessing BAI files with chopBAI can lead to a reduction of I/O by more than 95 % during the analysis of 10 Kbp genomic regions, eventually enabling the joint analysis of more than 10,000 individuals. Availability and Implementation: The software is implemented in C++, GPL licensed and available at http://github.com/DecodeGenetics/chopBAI Contact: birte.kehr@decode.is"}, {"title": "ADAGE analysis of publicly available gene expression data collections illuminates Pseudomonas aeruginosa-host interactions", "url": "https://www.biorxiv.org/content/early/2015/11/05/030650", "tag": "Bioinformatics", "abstract": "The growth in genome-scale assays of gene expression for different species in publicly available databases presents new opportunities for computational methods that aid in hypothesis generation and biological interpretation of these data. Here, we present an unsupervised machine-learning approach, ADAGE (Analysis using Denoising Autoencoders of Gene Expression) and apply it to the interpretation of all of the publicly available gene expression data for Pseudomonas aeruginosa, an important opportunistic bacterial pathogen. In post-hoc positive control analyses using curated knowledge, the P. aeruginosa ADAGE model found that co-operonic genes often participated in similar processes and accurately predicted which genes had similar functions. By analyzing newly generated data and previously published microarray and RNA-seq data, the ADAGE model identified gene expression differences between strains, modeled the cellular response to low oxygen, and predicted the involvement of biological processes despite low level expression differences in directly involved genes. Comparison of ADAGE with PCA and ICA revealed that ADAGE extracts distinct signals. We provide the ADAGE model with analysis of all publicly available P. aeruginosa GeneChip experiments, and we provide open source code for use in other species and settings."}, {"title": "Evaluating disease similarity using latent Dirichlet allocation", "url": "https://www.biorxiv.org/content/early/2015/11/03/030593", "tag": "Bioinformatics", "abstract": "Measures of similarity between diseases have been used for applications from discovering drug-target interactions to identifying disease-gene relationships. It is challenging to quantitatively compare diseases because much of what we know about them is captured in free text descriptions. Here we present an application of Latent Dirichlet Allocation as a way to measure similarity between diseases using textual descriptions. We learn latent topic representations of text from Online Mendelian Inheritance in Man records and use them to compute similarity. We assess the performance of this approach by comparing our results to manually curated relationships from the Disease Ontology. Despite being unsupervised, our model recovers a record's curated Disease Ontology relations with a mean Receiver Operating Characteristic Area Under the Curve of 0.80. With low dimensional models, topics tend to represent higher level information about affected organ systems, while higher dimensional models capture more granular genetic and phenotypic information. We examine topic representations of diseases for mapping concepts between ontologies and for tagging existing text with concepts. We conclude topic modeling on disease text leads to a robust approach to computing similarity that does not depend on keywords or ontology."}, {"title": "GenOO: A Modern Perl Framework for High Throughput Sequencing analysis", "url": "https://www.biorxiv.org/content/early/2015/11/03/019265", "tag": "Bioinformatics", "abstract": "Background: High throughput sequencing (HTS) has become one of the primary experimental tools used to extract genomic information from biological samples. Bioinformatics tools are continuously being developed for the analysis of HTS data. Beyond some well-defined core analyses, such as quality control or genomic alignment, the consistent development of custom tools and the representation of sequencing data in organized computational structures and entities remains a challenging effort for bioinformaticians. Results: In this work, we present GenOO [jee-noo], an open-source; object-oriented (OO) Perl framework specifically developed for the design and implementation of HTS analysis tools. GenOO models biological entities such as genes and transcripts as Perl objects, and includes relevant modules, attributes and methods that allow for the manipulation of high throughput sequencing data. GenOO integrates these elements in a simple and transparent way which allows for the creation of complex analysis pipelines minimizing the overhead for the researcher. GenOO has been designed with flexibility in mind, and has an easily extendable modular structure with minimal requirements for external tools and libraries. As an example of the framework\u2019s capabilities and usability, we present a short and simple walkthrough of a custom use case in HTS analysis. Conclusions: GenOO is a tool of high software quality which can be efficiently used for advanced HTS analyses. It has been used to develop several custom analysis tools, leading to a number of published works. Using GenOO as a core development module can greatly benefit users, by reducing the overhead and complexity of managing HTS data and biological entities at hand."}, {"title": "repgenHMM: a dynamic programming tool to infer the rules of immune receptor generation from sequence data", "url": "https://www.biorxiv.org/content/early/2015/10/31/030403", "tag": "Bioinformatics", "abstract": "The diversity of the immune repertoire is initially generated by random rearrangements of the receptor gene during early T and B cell development. Rearrangement scenarios are composed of random events -- choices of gene templates, base pair deletions and insertions -- described by probability distributions. Not all scenarios are equally likely, and the same receptor sequence may be obtained in several different ways. Quantifying the distribution of these rearrangements is an essential baseline for studying the immune system diversity. Inferring the properties of the distributions from receptor sequences is a computationally hard problem, requiring enumerating every possible scenario for every sampled receptor sequence. We present a Hidden Markov model, which accounts for all plausible scenarios that can generate the receptor sequences. We developed and implemented a method based on the Baum-Welch algorithm that can efficiently infer the parameters for the different events of the rearrangement process. We tested our software tool on sequence data for both the alpha and beta chains of the T cell receptor. To test the validity of our algorithm, we also generated synthetic sequences produced by a known model, and confirmed that its parameters could be accurately inferred back from the sequences. The inferred model can be used to generate synthetic sequences, to calculate the probability of generation of any receptor sequence, as well as the theoretical diversity of the repertoire. We estimate this diversity to be \u22481023 for human T cells. The model gives a baseline to investigate the selection and dynamics of immune repertoires."}, {"title": "DBSIM: A Platform of Simulation Resources for Genetic Epidemiology Studies", "url": "https://www.biorxiv.org/content/early/2015/10/30/030353", "tag": "Bioinformatics", "abstract": "Computer simulations are routinely conducted to evaluate new statistical methods, to compare the properties among different methods, and to mimic the real data in genetic epidemiology studies. Conducting simulation studies can become a complicated task as several challenges can occur, such as the selection of an appropriate simulation tool and the specification of parameters in the simulation model. Although abundant simulated data have been generated for human genetic research, currently there is no public database designed specifically as a repository for these simulated data. With the lack of such database, for similar studies, similar simulations may have been repeated, which resulted in redundant works. We created an online platform, DBSIM, for simulation data sharing and discussion of simulation techniques for human genetic studies. DBSIM has a database containing simulation scripts, simulated data, and documentations from published manuscripts, as well as a discussion forum, which provides a platform for discussion of the simulated data and exchanging simulation ideas. DBSIM will be useful in three aspects. Moreover, summary statistics such as the simulation tools that are most commonly used and datasets that are most frequently downloaded are provided. The statistics will be very informative for researchers to choose an appropriate simulation tool or select a common dataset for method comparisons. DBSIM can be accessed at http://dbsim.nhri.org.tw."}, {"title": "PHYLUCE is a software package for the analysis of conserved genomic loci", "url": "https://www.biorxiv.org/content/early/2015/10/30/027904", "tag": "Bioinformatics", "abstract": "Summary: Targeted enrichment of conserved and ultraconserved genomic elements allows universal collection of phylogenomic data from hundreds of species at multiple time scales (< 5 Ma to > 300 Ma). Prior to downstream inference, data from these types of targeted enrichment studies must undergo pre-processing to assemble contigs from sequence data; identify targeted, enriched loci from the off-target background data; align enriched contigs representing conserved loci to one another; and prepare and manipulate these alignments for subsequent phylogenomic inference. PHYLUCE is an efficient and easy-to-install software package that accomplishes these tasks across hundreds of taxa and thousands of enriched loci. Availability and Implementation: PHYLUCE is written for Python 2.7. PHYLUCE is supported on OSX and Linux (RedHat/CentOS) operating systems. PHYLUCE source code is distributed under a BSD-style license from https://www.github.com/faircloth-lab/phyluce/. PHYLUCE is also available as a package (https://binstar.org/faircloth-lab/phyluce) for the Anaconda Python distribution that installs all dependencies, and users can request a PHYLUCE instance on iPlant Atmosphere (tag: phyluce). The software manual and a tutorial are available from http://phyluce.readthedocs.org/en/latest/ and test data are available from doi: 10.6084/m9.figshare.1284521."}, {"title": "Discrete Distributional Differential Expression (D3E) - A Tool for Gene Expression Analysis of Single-cell RNA-seq Data", "url": "https://www.biorxiv.org/content/early/2015/10/29/020735", "tag": "Bioinformatics", "abstract": "The advent of high throughput RNA-seq at the single-cell level has opened up new opportunities to elucidate the heterogeneity of gene expression. One of the most widespread applications of RNA-seq is to identify genes which are differentially expressed (DE) between two experimental conditions. Here, we present a discrete, distributional method for differential gene expression (D3E), a novel algorithm specifically designed for single-cell RNA-seq data. We use synthetic data to evaluate D3E, demonstrating that it can detect changes in expression, even when the mean level remains unchanged. D3E is based on an analytically tractable stochastic model, and thus it provides additional biological insights by quantifying biologically meaningful properties, such as the average burst size and frequency. We use D3E to investigate experimental data, and with the help of the underlying model, we directly test hypotheses about the driving mechanism behind changes in gene expression."}, {"title": "An evaluation of the accuracy and speed of metagenome analysis tools", "url": "https://www.biorxiv.org/content/early/2015/10/28/017830", "tag": "Bioinformatics", "abstract": "Metagenome studies are becoming increasingly widespread, yielding important insights into microbial communities covering diverse environments from terrestrial and aquatic ecosystems to human skin and gut. With the advent of high-throughput sequencing platforms, the use of large scale shotgun sequencing approaches is now commonplace. However, a thorough independent benchmark comparing state-of-the-art metagenome analysis tools is lacking. Here, we present a benchmark where the most widely used tools are tested on complex, realistic data sets. Our results clearly show that the most widely used tools are not necessarily the most accurate, that the most accurate tool is not necessarily the most time consuming, and that there is a high degree of variability between available tools. These findings are important as the conclusions of any metagenomics study are affected by errors in the predicted community composition. Data sets and results are freely available from http://www.ucbioinformatics.org/metabenchmark.html"}, {"title": "Effect of lossy compression of quality scores on variant calling", "url": "https://www.biorxiv.org/content/early/2015/10/26/029843", "tag": "Bioinformatics", "abstract": "Recent advancements in sequencing technology have led to a drastic reduction in the cost of genome sequencing. This development has generated an unprecedented amount of genomic data that must be stored, processed, and communicated. To facilitate this effort, compression of genomic files has been proposed. Specifically, lossy compression of quality scores is emerging as a natural candidate for reducing the growing costs of storage. A main goal of performing DNA sequencing in population studies and clinical settings is to identify genetic variation. Though the field agrees that smaller files are advantageous, the cost of lossy compression, in terms of variant discovery, is unclear. Bioinformatic algorithms to identify SNPs and INDELs from next-generation DNA sequencing data use base quality score information; here, we evaluate the effect of lossy compression of quality scores on SNP and INDEL detection. We analyze several lossy compressors introduced recently in the literature. Specifically, we investigate how the output of the variant caller when using the original data (uncompressed) differs from that obtained when quality scores are replaced by those generated by a lossy compressor. Using gold standard genomic datasets such as the GIAB (Genome In A Bottle) consensus sequence for NA12878 and simulated data, we are able to analyze how accurate the output of the variant calling is, both for the original data and that previously lossily compressed. We show that lossy compression can significantly alleviate the storage while maintaining variant calling performance comparable to that with the uncompressed data. Further, in some cases lossy compression can lead to variant calling performance which is superior to that using the uncompressed file. We envisage our findings and framework serving as a benchmark in future development and analyses of lossy genomic data compressors. The \\emph{Supplementary Data} can be found at \\url{http://web.stanford.edu/~iochoa/supplementEffectLossy.zip}."}, {"title": "AOP: An R Package For Sufficient Causal Analysis in Pathway-based Screening of Drugs and Chemicals for Adversity", "url": "https://www.biorxiv.org/content/early/2015/10/23/029694.1", "tag": "Bioinformatics", "abstract": "Summary: How can I quickly find the key events in a pathway that I need to monitor to predict that a/an beneficial/adverse event/outcome will occur? This is a key question when using signaling pathways for drug/chemical screening in pharmacology, toxicology and risk assessment. By identifying these sufficient causal key events, we have fewer events to monitor for a pathway, thereby decreasing assay costs and time, while maximizing the value of the information. I have developed the \"aop\" package which uses back-door analysis of causal networks to identify these minimal sets of key events that are sufficient for making causal predictions. Availability and Implementation: The source for the aop package is available online at Github at https://github.com/DataSciBurgoon/aop and can be installed using the R devtools package. The aop package runs within the R statistical environment. The package has functions that can take pathways (as directed graphs) formatted as a Cytoscape JSON file as input, or pathways can be represented as directed graphs using the R/Bioconductor \"graph\" package. The \"aop\" package has functions that can perform backdoor analysis to identify the minimal set of key events for making causal predictions. Contact: lyle.d.burgoon@usace.army.mil"}, {"title": "Robust Group Fused Lasso for Multisample CNV Detection under Uncertainty", "url": "https://www.biorxiv.org/content/early/2015/10/23/029769", "tag": "Bioinformatics", "abstract": "One of the most important needs in the post-genome era is providing the researchers with reliable and efficient computational tools to extract and analyze this huge amount of biological data, in which DNA copy number variation (CNV) is a vitally important one. Array-based comparative genomic hybridization (aCGH) is a common approach in order to detect CNVs. Most of methods for this purpose were proposed for one-dimensional profile. However, slightly this focus has moved from one- to multi-dimensional signals. In addition, since contamination of these profiles with noise is always an issue, it is highly important to have a robust method for analyzing multisample aCGH data. In this paper, we propose Robust Grouped Fused Lasso (RGFL) which utilizes the Robust Group Total Variations (RGTV). Instead of l2;1 norm, the l1-l2 M-estimator is used which is more robust in dealing with non-Gaussian noise and high corruption. More importantly, Correntropy (Welsch M-estimator) is also applied for fitting error. Extensive experiments indicate that the proposed method outperforms the state-of-the art algorithms and techniques under a wide range of scenarios with diverse noises."}, {"title": "Flowr: Robust and efficient pipelines using a simple language-agnostic approach", "url": "https://www.biorxiv.org/content/early/2015/10/22/029710", "tag": "Bioinformatics", "abstract": "Motivation: Bioinformatics analyses have become increasingly intensive computing processes, with lowering costs and increasing numbers of samples. Each laboratory spends time creating and maintaining a set of pipelines, which may not be robust, scalable, or efficient. Further, the existence of different computing environments across institutions hinders both collabo-ration and the portability of analysis pipelines. Results: Flowr is a robust and scalable framework for designing and deploying computing pipelines in an easy-to-use fashion. It implements a scatter-gather approach using computing clusters, simplifying the concept to the use of five simple terms (in submission and dependency types). Most importantly, it is flexible, such that customizing existing pipelines is easy, and since it works across several computing environments (LSF, SGE, Torque, and SLURM), it is portable. Availability: http://docs.flowr.space"}, {"title": "Combining Dependent P-values with an Empirical Adaptation of Brown's Method", "url": "https://www.biorxiv.org/content/early/2015/10/22/029637", "tag": "Bioinformatics", "abstract": "Combining P-values from multiple statistical tests is a common exercise in bioinformatics. However, this procedure is non-trivial for dependent P-values. Here we discuss an empirical adaptation of Brown's Method (an extension of Fisher's Method) for combining dependent P-values which is appropriate for the correlated data sets found in high-throughput biological experiments. We show that Fisher's Method is biased when used on dependent sets of P-values with both simulated data and gene expression data from The Cancer Genome Atlas (TCGA). When applied on the same data sets, the Empirical Brown's Method provides a better null distribution and a more conservative result. The Empirical Brown's Method is available in Python, R, and MATLAB and can be obtained from https://github.com/IlyaLab/CombiningDependentPvaluesUsingEBM."}, {"title": "Rawcopy: Improved copy number analysis with Affymetrix arrays", "url": "https://www.biorxiv.org/content/early/2015/10/22/027409", "tag": "Bioinformatics", "abstract": "Rawcopy is an R package for processing of Affymetrix CytoScan HD, CytoScan 750k and SNP 6.0 microarray raw intensities (CEL files). It uses data from a large number of reference samples to produce log ratio for total copy number analysis and B-allele frequency for allele-specific copy number and heterozygosity analysis. Rawcopy achieves higher signal-to-noise ratio than commonly used free and proprietary alternatives, leading to improved identification of copy number alterations. In addition, Rawcopy visualises each microarray sample for assessment of technical quality, patient identity and genome-wide absolute copy number states."}, {"title": "Design of the TRONCO BioConductor Package for TRanslational ONCOlogy", "url": "https://www.biorxiv.org/content/early/2015/10/22/027524", "tag": "Bioinformatics", "abstract": "Models of cancer progression provide insights on the order of accumulation of genetic alterations during cancer development. Algorithms to infer such models from the currently available mutational profiles collected from different cancer patiens (cross-sectional data) have been defined in the literature since late 90s. These algorithms differ in the way they extract a graphical model of the events modelling the progression, e.g., somatic mutations or copy-number alterations. TRONCO is an R package for TRanslational ONcology which provides a serie of functions to assist the user in the analysis of cross sectional genomic data and, in particular, it implements algorithms that aim to model cancer progression by means of the notion of selective advantage. These algorithms are proved to outperform the current state-of-the-art in the inference of cancer progression models. TRONCO also provides functionalities to load input cross-sectional data, set up the execution of the algorithms, assess the statistical confidence in the results and visualize the models. Availability. Freely available at http://www.bioconductor.org/ under GPL license; project hosted at http://bimib.disco.unimib.it/ and https://github.com/BIMIB-DISCo/TRONCO. Contact. tronco@disco.unimib.it"}, {"title": "Robust and Stable Gene Selection via Maximum-Minimum Correntropy Criterion", "url": "https://www.biorxiv.org/content/early/2015/10/21/029538", "tag": "Bioinformatics", "abstract": "One of the central challenges in cancer research is identifying significant genes among thousands of others on a microarray. Since preventing outbreak and progression of cancer is the ultimate goal in bioinformatics and computational biology, detection of genes that are most involved is vital and crucial. In this article, we propose a Maximum-Minimum Correntropy Criterion (MMCC) approach for selection of biologically meaningful genes from microarray data sets which is stable, fast and robust against diverse noise and outliers and competitively accurate in comparison with other algorithms. Moreover, via an evolutionary optimization process, the optimal number of features for each data set is determined. Through broad experimental evaluation, MMCC is proved to be significantly better compared to other well-known gene selection algorithms for 25 commonly used microarray data sets. Surprisingly, high accuracy in classification by Support Vector Machine (SVM) is achieved by less than 10 genes selected by MMCC in all of the cases."}, {"title": "A scalable permutation approach reveals replication and preservation patterns of gene coexpression modules", "url": "https://www.biorxiv.org/content/early/2015/10/21/029553", "tag": "Bioinformatics", "abstract": "Gene coexpression network modules provide a framework for identifying shared biological functions. Analysis of topological preservation of modules across datasets is important for assessing reproducibility, and can reveal common function between tissues, cell types, and species. Although module preservation statistics have been developed, heuristics have been required for significance testing. However, the scale of current and future analyses requires accurate and unbiased p-values, particularly to address the challenge of multiple testing. Here, we developed a rapid and efficient approach (NetRep) for assessing module preservation and show that module preservation statistics are typically non-normal, necessitating a permutation approach. Quantification of module preservation across brain, liver, adipose, and muscle tissues in a BxH mouse cross revealed complex patterns of multi-tissue preservation with 52% of modules showing unambiguous preservation in one or more tissues and 25% showing preservation in all four tissues. Phenotype association analysis uncovered a liver-derived gene module which harboured housekeeping genes and which also displayed adipose and muscle tissue specific association with body weight. Taken together, our study presents a rapid unbiased approach for testing preservation of gene network topology, thus enabling rigorous assessment of potentially conserved function and phenotype association analysis."}, {"title": "FASTmC: a suite of predictive models for non-reference-based estimations of DNA methylation", "url": "https://www.biorxiv.org/content/early/2015/10/20/029496", "tag": "Bioinformatics", "abstract": "We describe a suite of predictive models, coined FASTmC, for non-reference, cost-effective exploration and comparative analysis of context-specific DNA methylation levels. Accurate estimations of true DNA methylation levels can be obtained from as few as several thousand short-reads generated from whole genome bisulfite sequencing. These models make high-resolution time course or developmental, and large diversity studies practical regardless of species, genome size and availability of a reference genome."}, {"title": "Robust Classification of Protein Variation Using Structural Modeling and Large-Scale Data Integration", "url": "https://www.biorxiv.org/content/early/2015/10/16/029041", "tag": "Bioinformatics", "abstract": "Existing methods for interpreting protein variation focus on annotating mutation pathogenicity rather than detailed interpretation of variant deleteriousness and frequently use only sequence-based or structure-based information. We present VIPUR, a computational framework that seamlessly integrates sequence analysis and structural modeling (using the Rosetta protein modeling suite) to identify and interpret deleterious protein variants. To train VIPUR, we collected 9,477 protein variants with known effects on protein function from multiple organisms and curated structural models for each variant from crystal structures and homology models. VIPUR can be applied to mutations in any organism's proteome with improved generalized accuracy (AUROC .83) and interpretability (AUPR .87) compared to other methods. We demonstrate that VIPUR's predictions of deleteriousness match the biological phenotypes in ClinVar and provide a clear ranking of prediction confidence. We use VIPUR to interpret known mutations associated with inflammation and diabetes, demonstrating the structural diversity of disrupted functional sites and improved interpretation of mutations associated with human diseases. Lastly we demonstrate VIPUR's ability to highlight candidate genes associated with human diseases by applying VIPUR to de novo variants associated with autism spectrum disorders."}, {"title": "A pedagogical walkthrough of computational modeling and simulation of Wnt signaling pathway using static causal models in Matlab", "url": "https://www.biorxiv.org/content/early/2015/10/15/011064", "tag": "Bioinformatics", "abstract": "Insight, Innovation and Integration Simulation study involving computational experiments dealing with Wnt signaling pathways abound in literature but often lack a pedagogical perspective that might ease the understanding of beginner students and researchers in transition who intend to work on modeling of the pathway. This paucity might happen due to restrictive policies which enforce an unwanted embargo on the sharing of important scientific knowledge. The manuscript elucidates embedding of prior biological knowledge, integration of heterogeneous information, transformation of biological hypothesis into computational framework and design of experiments in a simple manner interleaved with aspects of Bayesian Network toolbox and Matlab code so as to help readers get a feel of a project related to modeling of the pathway. Abstract A tutorial introduction to computational modeling of Wnt signaling pathway in a human colorectal cancer dataset using static Bayesian network models is provided. The walkthrough might aid bio-logists/informaticians in understanding the design of computational experiments that is interleaved with exposition of the Matlab code and causal models from Bayesian Network toolbox. This is done in order to ease the understanding of beginner students and researchers in transition to computational signaling biology, who intend to work in the field of modeling of signaling pathways. The manuscript expounds the computational flow of the contents in advance article 1 via code development and takes the reader in a step by step process of how (1) the collection and the transformation of the available biological information from literature is done, (2) the integration of the heterogeneous data and prior biological knowledge in the network is achieved, (3) conditional probability tables for nodes in biologically inspired tables are estimated, (4) the simulation study is designed, (5) the hypothesis regarding a biological phenomena is transformed into computational framework, and (6) results and inferences drawn using d-connectivity/separability are reported. The manuscript finally ends with a programming assignment to help the readers get hands on experience of a perturbation project. Matlab code with dataset is made available under GNU GPL v3 license at google code project on https://code.google.com/p/ static-bn-for-wnt-signaling-pathway"}, {"title": "Differential transcript usage from RNA-seq data: isoform pre-filtering improves performance of count-based methods", "url": "https://www.biorxiv.org/content/early/2015/10/13/025387", "tag": "Bioinformatics", "abstract": "Large-scale sequencing of cDNA (RNA-seq) has been a boon to the quantitative analysis of transcriptomes. A notable application is the detection of changes in transcript usage between experimental conditions. For example, discovery of pathological alternative splicing may allow the development of new treatments or better management of patients. From an analysis perspective, there are several ways to approach RNA-seq data to unravel differential transcript usage, such as annotation-based exon-level counting, differential analysis of the `percent spliced in' measure or quantitative analysis of assembled transcripts. The goal of this research is to compare and contrast current state-of-the-art methods, as well as to suggest improvements to commonly used workflows. We assess the performance of representative workflows using synthetic data and explore the effect of using non-standard counting bin definitions as input to a state-of-the-art inference engine (DEXSeq). Although the canonical counting provided the best results overall, several non-canonical approaches were as good or better in specific aspects and most counting approaches outperformed the evaluated event- and assembly-based methods. We show that an incomplete annotation catalog can have a detrimental effect on the ability to detect differential transcript usage in transcriptomes with few isoforms per gene and that isoform-level pre-filtering can considerably improve false discovery rate (FDR) control. Count-based methods generally perform well in detection of differential transcript usage. Controlling the FDR at the imposed threshold is difficult, mainly in complex organisms, but can be improved by pre-filtering of the annotation catalog."}, {"title": "Coevolutionary landscape inference and the context-dependence of mutations in beta-lactamase TEM-1", "url": "https://www.biorxiv.org/content/early/2015/10/13/028902", "tag": "Bioinformatics", "abstract": "The quantitative characterization of mutational landscapes is a task of outstanding importance in evolutionary and medical biology: It is, e.g., of central importance for our understanding of the phenotypic effect of mutations related to disease and antibiotic drug resistance. Here we develop a novel inference scheme for mutational landscapes, which is based on the statistical analysis of large alignments of homologs of the protein of interest. Our method is able to capture epistatic couplings between residues, and therefore to assess the dependence of mutational effects on the sequence context where they appear. Compared recent large-scale mutagenesis data of the beta-lactamase TEM-1, a protein providing resistance against to beta-lactam antibiotics, our method leads to an increase of about 40% in explicative power as compared to approaches neglecting epistasis. We find that the informative sequence context extends to residues at native distances of about 20 Angstrom from the mutated site, reaching thus far beyond residues in direct physical contact."}, {"title": "Mat-aCGH: a Matlab toolbox for simultaneous multisample aCGH data analysis and visualization", "url": "https://www.biorxiv.org/content/early/2015/10/09/028761", "tag": "Bioinformatics", "abstract": "Mat-aCGH is an application toolbox for analysis and visualization of microarray-comparative genomic hybridization (array-CGH or aCGH) data which is based on Matlab. Full process of aCGH analysis, from denoising of the raw data to the visualization of the desired results, can be obtained via Mat-aCGH straight-forwardly. The main advantage of this toolbox is that it is collection of recent well-known statistical and information theoretic methods and algorithms for analyzing aCGH data. More importantly, the proposed toolbox is developed for multisample analysis which is one of the current challenges in this area. Mat-aCGH is convenient to apply for any format of data, robust against diverse noise and provides the users with valuable information in the form of diagrams and metrics. Therefore, it eliminates the needs of another software or package for multisample aCGH analysis."}, {"title": "HiFive: a tool suite for easy and efficient HiC and 5C data analysis", "url": "https://www.biorxiv.org/content/early/2015/10/09/009951", "tag": "Bioinformatics", "abstract": "The chromatin interaction assays 5C and HiC have advanced our understanding of genomic spatial organization but analysis approaches for these data are limited by usability and flexibility. The HiFive tool suite provides efficient data handling and a variety of normalization approaches for easy, fast analysis and method comparison. Integration of MPI-based parallelization allows scalability and rapid processing time. In addition to single-command analysis of an entire experiment from mapped reads to interaction values, HiFive has been integrated into the open-source, web-based platform Galaxy to connect users with computational resources and a graphical interface. HiFive is open-source software available from http://taylorlab.org/software/hifive/ and https://github.com/bxlab/hifive."}, {"title": "Distinct genomic and epigenomic features demarcate hypomethylated blocks in colon cancer", "url": "https://www.biorxiv.org/content/early/2015/10/09/028803", "tag": "Bioinformatics", "abstract": "Background. Large mega base-pair genomic regions show robust alterations in DNA methylation levels in multiple cancers, a vast majority of which are hypo-methylated in cancers. These regions are generally bounded by CpG islands, overlap with Lamin Associated Domains and Large organized chromatin lysine modifications, and are associated with stochastic variability in gene expression. Given the size and consistency of hypo-methylated blocks (HMB) across cancer types, their immediate causes are likely to be encoded in the genomic region near HMB boundaries, in terms of specific genomic or epigenomic signatures. However, a detailed characterization of the HMB boundaries has not been reported. Method. Here, we focused on ~13k HMBs, encompassing approximately half the genome, identified in colon cancer. We analyzed a number of distinguishing features at the HMB boundaries including transcription factor (TF) binding motifs, various epigenomic marks, and chromatin structural features. Result. We found that the classical promoter epigenomic mark, H3K4me3, is highly enriched at HMB boundaries, as are CTCF bound sites. HMB boundaries harbor distinct combinations of TF motifs. Our Random Forest model based on TF motifs can accurately distinguish boundaries not only from regions inside and outside HMBs, but surprisingly, from active promoters as well. Interestingly, the distinguishing TFs and their interacting proteins are involved in chromatin modification. Finally, HMB boundaries significantly coincide with the boundaries of Topologically Associating Domains of the chromatin. Conclusion. Our analyses suggest that the overall architecture of HMBs is guided by pre-existing chromatin architecture, and are associated with aberrant activity of promoter-like sequences at the boundary."}, {"title": "Heterogeneity of Transcription Factor binding specificity models within and across cell lines", "url": "https://www.biorxiv.org/content/early/2015/10/09/028787", "tag": "Bioinformatics", "abstract": "Complex gene expression patterns are mediated by binding of transcription factors (TF) to specific genomic loci. The in vivo occupancy of a TF is, in large part, determined by the TFs DNA binding interaction partners, motivating genomic context based models of TF occupancy. However, the approaches thus far have assumed a uniform binding model to explain genome wide bound sites for a TF in a cell-type and as such heterogeneity of TF occupancy models, and the extent to which binding rules underlying a TFs occupancy are shared across cell types, has not been investigated. Here, we develop an ensemble based approach (TRISECT) to identify heterogeneous binding rules of cell-type specific TF occupancy and analyze the inter-cell-type sharing of such rules. Comprehensive analysis of 23 TFs, each with ChIP-Seq data in 4-12 cell-types, shows that by explicitly capturing the heterogeneity of binding rules, TRISECT accurately identifies in vivo TF occupancy (93%) substantially improving upon previous methods. Importantly, many of the binding rules derived from individual cell-types are shared across cell-types and reveal distinct yet functionally coherent putative target genes in different cell-types. Closer inspection of the predicted cell-type-specific interaction partners provides insights into context-specific functional landscape of a TF. Together, our novel ensemble-based approach reveals, for the first time, a widespread heterogeneity of binding rules, comprising interaction partners within a cell-type, many of which nevertheless transcend cell-types. Notably, the putative targets of shared binding rules in different cell-types, while distinct, exhibit significant functional coherence."}, {"title": "destiny \u2013 diffusion maps for large-scale single-cell data in R", "url": "https://www.biorxiv.org/content/early/2015/10/09/023309", "tag": "Bioinformatics", "abstract": "Diffusion maps are a spectral method for non-linear dimension reduction and have recently been adapted for the visualization of single cell expression data. Here we present destiny, an efficient R implementation of the diffusion map algorithm. Our package includes a single-cell specific noise model allowing for missing and censored values. In contrast to previous implementations, we further present an efficient nearest-neighbour approximation that allows for the processing of hundreds of thousands of cells and a functionality for projecting new data on existing diffusion maps. We exemplarily apply destiny to a recent time-resolved mass cytometry dataset of cellular reprogramming."}, {"title": "clonotypeR--high throughput analysis of T cell antigen receptor sequences", "url": "https://www.biorxiv.org/content/early/2015/10/08/028696", "tag": "Bioinformatics", "abstract": "Motivation The T cell receptors are expressed as millions of different rearrangements. Amplified as a complex mixture of PCR products, they can be sequenced directly on next-generation instruments without the need for cloning. This method is increasingly used to characterize, quantify and study these highly diverse receptors. Results We present here clonotypeR, a software package to identify and analyze antigen receptors from high-throughput sequence libraries. ClonotypeR is designed to process, organize and analyze very large numbers of sequences, in the order of millions, typically produced by Roche 454 or Illumina instruments, and is made of two parts. The first contains shell scripts and reference segment sequences to produce a data file where each line represents a the detection of a clonotype in a sequence read. The second part is a R module available from Bioconductor, to load and filter the data, and prepare clonotype abundance tables ready for analysis with third-party tools for differential representation analysis, sample clustering, etc. To analyze clonotype data at the nucleotide level, we introduce unique clonotype identifiers based on those developed by Yassai et al. (2009), that we corrected to avoid identifier collisions. Availability http://clonotyper.branchable.com (CC0 license)."}, {"title": "Integrative tissue-specific functional annotations in the human genome provide novel insights on many complex traits and improve signal prioritization in genome wide association studies", "url": "https://www.biorxiv.org/content/early/2015/10/06/028464", "tag": "Bioinformatics", "abstract": "Extensive efforts have been made to understand genomic function through both experimental and computational approaches, yet proper annotation still remains challenging, especially in non-coding regions. In this manuscript, we introduce GenoSkyline, an unsupervised learning framework to predict tissue-specific functional regions through integrating high-throughput epigenetic annotations. GenoSkyline successfully identified a variety of non-coding regulatory machinery including enhancers, regulatory miRNA, and hypomethylated transposable elements in extensive case studies. Integrative analysis of GenoSkyline annotations and results from genome-wide association studies (GWAS) led to novel biological insights on the etiologies of a number of human complex traits. We also explored using tissue-specific functional annotations to prioritize GWAS signals and predict relevant tissue types for each risk locus. Brain and blood-specific annotations led to better prioritization performance for schizophrenia than standard GWAS p-values and non-tissue-specific annotations. As for coronary artery disease, heart-specific functional regions was highly enriched of GWAS signals, but previously identified risk loci were found to be most functional in other tissues, suggesting a substantial proportion of still undetected heart-related loci. In summary, GenoSkyline annotations can guide genetic studies at multiple resolutions and provide valuable insights in understanding complex diseases. GenoSkyline is available at http://genocanyon.med.yale.edu/GenoSkyline."}, {"title": "3D RNA from evolutionary couplings", "url": "https://www.biorxiv.org/content/early/2015/10/06/028456", "tag": "Bioinformatics", "abstract": "Non-protein-coding RNAs are ubiquitous in cell physiology, with a diverse repertoire of known functions. In fact, the majority of the eukaryotic genome does not code for proteins, and thousands of conserved long non-protein-coding RNAs of currently unkown function have been identified. When available, knowledge of their 3D structure is very helpful in elucidating the function of these RNAs. However, despite some outstanding structure elucidation of RNAs using X-ray crystallography, NMR and cryoEM, learning RNA 3D structures remains low-throughput. RNA structure prediction in silico is a promising alternative approach and works well for double-helical stems, but full 3D structure determination requires tertiary contacts outside of secondary structures that are difficult to infer from sequence information. Here, based only on information from RNA multiple sequence alignments, we use a global statistical sequence probability model of co-variation in a pairs of nucleotide positions to detect 3D contacts, in analogy to recently developed breakthrough methods for computational protein folding. In blinded tests on 22 known RNA structures ranging in size from 65 to 1800 nucleotides, the predicted contacts matched physical nucleotide interactions with 65-95% true positive prediction accuracy. Importantly, we infer many long-range tertiary contacts, including non-Watson-Crick interactions, where secondary structure elements assemble in 3D. When used as restraints in molecular dynamics simulations, the inferred contacts improve RNA 3D structure prediction to a coordinate error as low as 6 to 10 angstrom rmsd deviation in atom positions, with potential for further refinement by molecular dynamics. These contacts include functionally important interactions, such as those that distinguish the active and inactive conformations of four riboswitches. In blind prediction mode, we present evolutionary couplings suitable for folding simulations for 180 RNAs of unknown structure, available at https://marks.hms.harvard.edu/ev_rna/. We anticipate that this approach can help shed light on the structure and function of non-protein-coding RNAs as well as 3D-structured mRNAs."}, {"title": "Characterising Complex Enzyme Reaction Data", "url": "https://www.biorxiv.org/content/early/2015/10/06/028142", "tag": "Bioinformatics", "abstract": "The relationship between enzyme-catalysed reactions and the Enzyme Commission (EC) number, the widely accepted classification scheme used to characterise enzyme activity, is complex and with the rapid increase in our knowledge of the reactions catalysed by enzymes needs revisiting. We present a manual and computational analysis to investigate this complexity and found that almost one-third of all known EC numbers are linked to more than one reaction in the secondary reaction databases (e.g. KEGG). Although this complexity is often resolved by defining generic, alternative and partial reactions, we have also found individual EC numbers with more than one reaction catalysing different types of bond changes. This analysis adds a new dimension to our understanding of enzyme function and might be useful for the accurate annotation of the function of enzymes and to study the changes in enzyme function during evolution."}, {"title": "Investigating the importance of anatomical homology for cross-species phenotype comparisons using semantic similarity.", "url": "https://www.biorxiv.org/content/early/2015/10/05/028449", "tag": "Bioinformatics", "abstract": "There is growing use of ontologies for the measurement of cross-species phenotype similarity. Such similarity measurements contribute to diverse applications, such as identifying genetic models for human diseases, transferring knowledge among model organisms, and studying the genetic basis of evolutionary innovations. Two organismal features, whether genes, anatomical parts, or any other inherited feature, are considered to be homologous when they are evolutionarily derived from a single feature in a common ancestor. A classic example is the homology between the paired fins of fishes and vertebrate limbs. Anatomical ontologies that model the structural relations among parts may fail to include some known anatomical homologies unless they are deliberately added as separate axioms. The consequences of neglecting known homologies for applications that rely on such ontologies has not been well studied. Here, we examine how semantic similarity is affected when external homology knowledge is included. We measure phenotypic similarity between orthologous and non-orthologous gene pairs between humans and either mouse or zebrafish, and compare the inclusion of real with faux homology axioms. Semantic similarity was preferentially increased for orthologs when using real homology axioms, but only in the more divergent of the two species comparisons (human to zebrafish, not human to mouse), and the relative increase was less than 1% to non-orthologs. By contrast, inclusion of both real and faux random homology axioms preferentially increased similarities between genes that were initially more dissimilar in the other comparisons. Biologically meaningful increases in semantic similarity were seen for a select subset of gene pairs. Overall, the effect of including homology axioms on cross-species semantic similarity was modest at the levels of divergence examined here, but our results hint that it may be greater for more distant species comparisons."}, {"title": "Resolving Complex Structural Genomic Rearrangements using a Randomized Approach", "url": "https://www.biorxiv.org/content/early/2015/10/04/028217", "tag": "Bioinformatics", "abstract": "Complex chromosomal rearrangements consist of structural genomic alterations involving multiple instances of deletions, duplications, inversions, or translocations that co-occur either on the same chromosome or represent different overlapping events on homologous chromosomes. We present SVelter, an algorithm that first identifies regions of the genome suspected to harbor a complex event and then iteratively rearranges the local genome structure, in a randomized fashion, with each structure scored against characteristics of the observed sequencing data. We show that SVelter is able to accurately reconstruct these regions when compared to well-characterized genomes that have been deep sequenced with both short and long read technologies."}, {"title": "QuicK-mer: A rapid paralog sensitive CNV detection pipeline", "url": "https://www.biorxiv.org/content/early/2015/10/02/028225", "tag": "Bioinformatics", "abstract": "QuicK-mer is a unified pipeline for estimating genome copy-number from high-throughput Illumina sequencing data. QuicK-mer utilizes the Jellyfish application to efficiently tabulate counts of predefined sets of k-mers. The program performs GC-normalization using defined control regions and reports paralog-specific estimates of copy-number suitable for downstream analysis. The package is freely available at https://github.com/KiddLab/QuicK-mer"}, {"title": "SinaPlot: an enhanced chart for simple and truthful representation of single observations over multiple classes", "url": "https://www.biorxiv.org/content/early/2015/10/02/028191", "tag": "Bioinformatics", "abstract": "Recent developments in data driven science, in particular computational biology, have led scientists to integrate data from several sources, over diverse experimental procedures, or databases. This alone poses a major challenge in truthfully visualising data, especially when the amount of data points varies between classes. To aid the presentation of datasets with differing sample size we have developed a new type of plot overcoming limitations of current standard visualization charts. Plots like bar charts, violin plots, strip charts or box-and-whiskers plots may provide visual information about mean/median, variance of the data, number of data points or density distribution of data; still, only a combination of these plots may provide all relevant information. We have designed a new and simple plot inspired by the strip chart and the violin plot that operates by letting the normalized density of points restrict the jitter along the x-axis. \u03a4he plot displays the same contour as a violin plot, but resembles a simple strip chart for small number of data points. In this way the plot conveys information of both the number of data points, the density distribution, outliers and data spread in a very simple, comprehensible and condensed format. The package for producing the plots is available for R through the CRAN network (https://cran.r-project.org/web/packages/sinaplot/index.html). In order to aid users without experience in R we also provide access to a web-server accepting excel sheets to produce the plots (http://servers.binf.ku.dk:8890/sinaplot/) ."}, {"title": "Indel variant analysis of short-read sequencing data with Scalpel", "url": "https://www.biorxiv.org/content/early/2015/10/01/028050", "tag": "Bioinformatics", "abstract": "As the second most common type of variations in the human genome, insertions and deletions (indels) have been linked to many diseases, but indels of more than a few bases are still challenging to discover from short-read sequencing data. Scalpel (http://scalpel.sourceforge.net) is open-source software for reliable indel detection based on the micro-assembly technique. To date, it has been successfully used to discover mutations in novel candidate genes for autism, and is extensively used in other large-scale studies of human diseases. This protocol gives an overview of the algorithm and describes how to use Scalpel to perform highly accurate indel calling from whole genome and exome sequencing data. We provide detailed instructions for an exemplary family-based de novo study, but we also characterize the other two supported modes of operation for single sample and somatic analysis. Indel normalization, visualization, and annotation of the mutations are also illustrated. Using a standard server, indel discovery and characterization in the exonic regions of the example sequencing data can be finished in ~6 hours after read mapping."}, {"title": "Sphinx: modeling transcriptional heterogeneity in single-cell RNA-Seq", "url": "https://www.biorxiv.org/content/early/2015/09/30/027870", "tag": "Bioinformatics", "abstract": "The significance of single cell transcription resides not only in the cumulative expression strength of the cell population but also in its heterogeneity. We propose a new model that improves the detection of changes in the transcriptional heterogeneity pattern of RNAseq data using two heterogeneity parameters: \"burst proportion\" and \"burst magnitude\", whose changes are validated using RNA FISH. Transcriptional \"cobursting\" governed by distinct mechanisms during myoblast proliferation and differentiation, is described here."}, {"title": "One Codex: A Sensitive and Accurate Data Platform for Genomic Microbial Identification", "url": "https://www.biorxiv.org/content/early/2015/09/28/027607", "tag": "Bioinformatics", "abstract": "High-throughput sequencing (HTS) is increasingly being used for broad applications of microbial characterization, such as microbial ecology, clinical diagnosis, and outbreak epidemiology. However, the analytical task of comparing short sequence reads against the known diversity of microbial life has proved to be computationally challenging. The One Codex data platform was created with the dual goals of analyzing microbial data against the largest possible collection of microbial reference genomes, as well as presenting those results in a format that is consumable by applied end-users. One Codex identifies microbial sequences using a \"k-mer based\" taxonomic classification algorithm through a web-based data platform, using a reference database that currently includes approximately 40,000 bacterial, viral, fungal, and protozoan genomes. In order to evaluate whether this classification method and associated database provided quantitatively different performance for microbial identification, we created a large and diverse evaluation dataset containing 50 million reads from 10,639 genomes, as well as sequences from six organisms novel species not be included in the reference databases of any of the tested classifiers. Quantitative evaluation of several published microbial detection methods shows that One Codex has the highest degree of sensitivity and specificity (AUC = 0.97, compared to 0.82-0.88 for other methods), both when detecting well-characterized species as well as newly sequenced, \"taxonomically novel\" organisms."}, {"title": "MG7: Configurable and scalable 16S metagenomics data analysis", "url": "https://www.biorxiv.org/content/early/2015/09/28/027714", "tag": "Bioinformatics", "abstract": "As part of the Cambrian explosion of omics data, metagenomics brings to the table a specific, defining trait: its social essence. The *meta* prefix exerts its influence, with multitudes manifesting themselves everywhere; from samples to data analysis, from actors involved to (present and future) applications. Of these dimensions, data analysis is where needs lay further from what current tools provide. Key features are, among others, scalability, reproducibility, data provenance and distribution, process identity and versioning. These are the goals guiding our work in MG7, a 16S metagenomics data analysis system. The basic principle is a new approach to data analysis, where configuration, processes, or data locations are static, type-checked and subject to the standard evolution of a well-maintained software project. Cloud computing, in its Amazon Web Services incarnation, when coupled with these ideas, produces a robust, safely configurable, scalable tool. Processes, data, machine behaviors and their dependencies are expressed using a set of libraries which bring as much as possible checking and validation to the type level, without sacrificing expressiveness. Together they form a toolkit for defining scalable cloud-based workflows composed of stateless computations, with a static reproducible specification of dependencies, behavior and wiring of all steps. The modeling of taxonomy data is done using Bio4j, where the new paradigm of graph databases allows for both a simple expression of taxonomic assignment tasks and the calculation of taxa abundance values considering the hierarchic structure of the taxonomy tree. MG7 includes a new 16S reference database, *16S-DB7*, built with a flexible and sustainable update system, and the possibility of project-driven personalization. The first and second authors contributed equally to this work."}, {"title": "SARTools: a DESeq2- and edgeR-based R pipeline for comprehensive differential analysis of RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2015/09/26/021741", "tag": "Bioinformatics", "abstract": "Background Several R packages exist for the detection of differentially expressed genes from RNA-Seq data. The analysis process includes three main steps, namely normalization, dispersion estimation and test for differential expression. Quality control steps along this process are recommended but not mandatory, and failing to check the characteristics of the dataset may lead to spurious results. In addition, normalization methods and statistical models are not exchangeable across the packages without adequate transformations the users are often not aware of. Thus, dedicated analysis pipelines are needed to include systematic quality control steps and prevent errors from misusing the proposed methods. Results SARTools is an R pipeline for differential analysis of RNA-Seq count data. It can handle designs involving two or more conditions of a single biological factor with or without a blocking factor (such as a batch effect or a sample pairing). It is based on DESeq2 and edgeR and is composed of an R package and two R script templates (for DESeq2 and edgeR respectively). Tuning a small number of parameters and executing one of the R scripts, users have access to the full results of the analysis, including lists of differentially expressed genes and a HTML report that (i) displays diagnostic plots for quality control and model hypotheses checking and (ii) keeps track of the whole analysis process, parameter values and versions of the R packages used. Conclusions SARTools provides systematic quality controls of the dataset as well as diagnostic plots that help to tune the model parameters. It gives access to the main parameters of DESeq2 and edgeR and prevents untrained users from misusing some functionalities of both packages. By keeping track of all the parameters of the analysis process it fits the requirements of reproducible research."}, {"title": "Generalised empirical Bayesian methods for discovery of differential data in high-throughput biology", "url": "https://www.biorxiv.org/content/early/2015/09/25/011890", "tag": "Bioinformatics", "abstract": "Motivation: High-throughput data are now commonplace in biological research. Rapidly changing technologies and application mean that novel methods for detecting differential behaviour that account for a 'large P , small n' setting are required at an increasing rate. The development of such methods is, in general, being done on an ad hoc basis, requiring further development cycles and a lack of standardization between analyses. Results: We present here a generalised method for identifying differential behaviour within high-throughput biological data through empirical Bayesian methods. This approach is based on our baySeq algorithm for identification of differential expression in RNA-seq data based on a negative binomial distribution, and in paired data based on a beta-binomial distribution. Here we show how the same empirical Bayesian approach can be applied to any parametric distribution, removing the need for lengthy development of novel methods for differently distributed data. Comparisons with existing methods developed to address specific problems in high-throughput biological data show that these generic methods can achieve equivalent or better performance. A number of enhancements to the basic algorithm are also presented to increase flexibility and reduce computational costs. Availability: The methods are implemented in the R baySeq (v2) package, available on Bioconductor http://www.bioconductor.org/packages/release/bioc/html/baySeq.html."}, {"title": "Minimum Information for Reporting Next Generation Sequence Genotyping (MIRING): Guidelines for Reporting HLA and KIR Genotyping via Next Generation Sequencing", "url": "https://www.biorxiv.org/content/early/2015/09/24/015230", "tag": "Bioinformatics", "abstract": "The development of next-generation sequencing (NGS) technologies for HLA and KIR genotyping is rapidly advancing knowledge of genetic variation of these highly polymorphic loci. NGS genotyping is poised to replace older methods for clinical use, but standard methods for reporting and exchanging these new, high quality genotype data are needed. The Immunogenomic NGS Consortium, a broad collaboration of histocompatibility and immunogenetics clinicians, researchers, instrument manufacturers and software developers, has developed the Minimum Information for Reporting Immunogenomic NGS Genotyping (MIRING) reporting guidelines. MIRING is a checklist that specifies the content of NGS genotyping results as well as a set of messaging guidelines for reporting the results. A MIRING message includes five categories of structured information \u2013 message annotation, reference context, full genotype, consensus sequence and novel polymorphism \u2013 and references to three categories of accessory information \u2013 NGS platform documentation, read processing documentation and primary data. These eight categories of information ensure the long-term portability and broad application of this NGS data for all current histocompatibility and immunogenetics use cases. In addition, MIRING can be extended to allow the reporting of genotype data generated using pre-NGS technologies. Because genotyping results reported using MIRING are easily updated in accordance with reference and nomenclature databases, MIRING represents a bold departure from previous methods of reporting HLA and KIR genotyping results, which have provided static and less-portable data. More information about MIRING can be found online at miring.immunogenomics.org."}, {"title": "Toward high-throughput predictive modeling of protein binding/unbinding kinetics", "url": "https://www.biorxiv.org/content/early/2015/09/24/024513", "tag": "Bioinformatics", "abstract": "One of the unaddressed challenges in drug discovery is that drug potency determined in vitro is not a reliable indicator of drug efficacy and toxicity in humans. Accumulated evidences suggest that the in vivo activity is more strongly correlated with the binding/unbinding kinetics than the equilibrium thermodynamics of protein-ligand interactions (PLI) in many cases. However, existing experimental and computational techniques are both insufficient in studying the molecular details of kinetics process of PLI. Consequently, we not only have limited mechanistic understanding of the kinetic process but also lack a practical platform for the high-throughput screening and optimization of drug leads based on their kinetic properties. Here we address this unmet need by integrating energetic and conformational dynamic features derived from molecular modeling with multi-task learning. To test our method, HIV-1 protease drug complexes are used as a model system. Our integrated model provides us with new insights into the molecular determinants of the kinetics of PLI. We find that the coherent coupling of conformational dynamics between protein and ligand may play a critical role in determining the kinetic rate constants of PLI. Furthermore, we demonstrated that Normal Mode Analysis (NMA) is an efficient method to capture conformational dynamics of the binding/unbinding kinetics. Coupled with the multi-task learning, we can predict combined kon and koff accurately with an accuracy of 74.35%. Thus, it is possible to screen and optimize compounds based on their kinetic property. Further development of such computational tools will bridge one of the critical missing links between in vitro drug screening and in vivo drug efficacy and toxicity."}, {"title": "Haplotype synthesis analysis in public reference data reveals functional variants underlying known genome-wide associated susceptibility loci", "url": "https://www.biorxiv.org/content/early/2015/09/21/027276", "tag": "Bioinformatics", "abstract": "The functional mechanisms underlying disease association identified by Genome-wide Association Studies remain unknown for susceptibility loci located outside gene coding regions. In addition to the regulation of gene expression, synthesis of effects from multiple surrounding functional variants has been suggested as an explanation of hard-to-interpret associations. Here, we define filter criteria based on linkage disequilibrium measures and allele frequencies which reflect expected properties of synthesizing variant sets. For eligible candidate sets we search for those haplotypes that are highly correlated with the risk alleles of a genome-wide associated variant. We applied our methods to 1,000 Genomes reference data and confirmed Crohn's Disease and Type 2 Diabetes susceptibility loci. Of these, a proportion of 32% allowed explanation by three-variant-haplotypes carrying at least two functional variants, as compared to a proportion of 16% for random variants (P=2.9 x t10^-6). More importantly, we detected examples of known loci whose association can fully be explained by surrounding missense variants: three missense variants from MUC19 synthesize rs11564258 (LOC105369736/MUC19, intron; Crohn's Disease). Next, rs2797685 (PER3, intron; Crohn's Disease) is synthesized by a 57 kilobase haplotype defined by five missense variants from PER3 and three missense variants from UTS2. Finally, the association of rs7178572 (HMG20A, intron; Type 2 Diabetes) can be explained by the synthesis of eight haplotypes, each carrying at least one missense variant in either PEAK1, TBC1D2B, CHRNA5 or ADAMTS7. In summary, application of our new methods highlights the potential of synthesis analysis to guide functional follow-up investigation of findings from association studies."}, {"title": "WU-CRISPR: characteristics of functional guide RNAs for the CRISPR/Cas9 system", "url": "https://www.biorxiv.org/content/early/2015/09/18/026971", "tag": "Bioinformatics", "abstract": "The CRISPR/Cas9 system has been rapidly adopted for genome editing. However, one major issue with this system is the lack of robust bioinformatics tools for design of single guide RNA (sgRNA), which determines the efficacy and specificity of genome editing. To address this pressing need, we analyze CRISPR RNA-seq data and identify many novel features that are characteristic of highly potent sgRNAs. These features are used to develop a bioinformatics tool for genome-wide design of sgRNAs with improved efficiency. These sgRNAs as well as the design tool are freely accessible via a web server, WU-CRISPR (http://crispr.wustl.edu)."}, {"title": "Genomic variant calling: Flexible tools and a diagnostic data set", "url": "https://www.biorxiv.org/content/early/2015/09/18/027227", "tag": "Bioinformatics", "abstract": "The accurate identification of low-frequency variants in tumors remains an unsolved problem. To support characterization of the issues in a realistic setting, we have developed software tools and a reference dataset for diagnosing variant calling pipelines. The dataset contains millions of variants at frequencies ranging from 0.05 to 1.0. To generate the dataset, we performed whole-genome sequencing of a mixture of two Corriel cell lines, NA19240 and NA12878, the mothers of YRI (Y) and CEU (C) HapMap trios, respectively. The cells were mixed in three different proportions, 10Y/90C, 50Y/50C and 90Y/10C, in an effort to simulate the heterogeneity found in tumor samples. We sequenced three biological replicates for each mixture, yielding approximately 1.4 billion reads per mixture for an average of 64X coverage. Using the published genotypes as our reference, we evaluate the performance of a general variant calling algorithm, constructed as a demonstration of our flexible toolset, and make comparisons to a standard GATK pipeline. We estimate the overall FDR to be 0.028 and the FNR (when coverage exceeds 20X) to be 0.019 in the 50Y/50C mixture. Interestingly, even with these relatively well studied individuals, we predict over 475,000 new variants, validating in well-behaved coding regions at a rate of 0.97, that were not included in the published genotypes."}, {"title": "Laplacian eigenmaps and principal curves for high resolution pseudotemporal ordering of single-cell RNA-seq profiles", "url": "https://www.biorxiv.org/content/early/2015/09/18/027219", "tag": "Bioinformatics", "abstract": "Advances in RNA-seq technologies provide unprecedented insight into the variability and heterogeneity of gene expression at the single-cell level. However, such data offers only a snapshot of the transcriptome, whereas it is often the progression of cells through dynamic biological processes that is of interest. As a result, one outstanding challenge is to infer such progressions by ordering gene expression from single cell data alone, known as the cell ordering problem. Here, we introduce a new method that constructs a low-dimensional non-linear embedding of the data using laplacian eigenmaps before assigning each cell a pseudotime using principal curves. We characterise why on a theoretical level our method is more robust to the high levels of noise typical of single-cell RNA-seq data before demonstrating its utility on two existing datasets of differentiating cells."}, {"title": "A pathway-centric view of spatial proximity in the 3D nucleome across cell lines", "url": "https://www.biorxiv.org/content/early/2015/09/17/027045", "tag": "Bioinformatics", "abstract": "Spatial organization of the genome is critical for condition-specific gene expression. Previous studies have shown that functionally related genes tend to be spatially proximal. However, these studies have not been extended to multiple human cell types, and the extent to which context-specific spatial proximity of a pathway is related to its context-specific activity is not known. We report the first pathway-centric analyses of spatial proximity in six human cell lines. We find that spatial proximity of genes in a pathway tends to be context-specific, in a manner consistent with the pathway's context-specific expression and function; housekeeping genes are ubiquitously proximal to each other, and cancer-related pathways such as p53 signaling are uniquely proximal in hESC. Intriguingly, we find a correlation between the spatial proximity of genes and interactions of their protein products, even after accounting for the propensity of co-pathway proteins to interact. Related pathways are also often spatially proximal to one another, and housekeeping genes tend to be proximal to several other pathways suggesting their coordinating role. Further, the spatially proximal genes in a pathway tend to be the drivers of the pathway activity and are enriched for transcription, splicing and transport functions. Overall, our analyses reveal a pathway-centric organization of the 3D nucleome whereby functionally related and interacting genes, particularly the initial drivers of pathway activity, but also genes across multiple related pathways, are in spatial proximity in a context-specific way. Our results provide further insights into the role of differential spatial organization in cell type-specific pathway activity."}, {"title": "Improved metagenome assemblies and taxonomic binning using long-read circular consensus sequence data", "url": "https://www.biorxiv.org/content/early/2015/09/16/026922", "tag": "Bioinformatics", "abstract": "DNA assembly is a core methodological step in metagenomic pipelines used to study the structure and function within microbial communities. Here we investigate the utility of Pacific Biosciences long and high accuracy circular consensus sequencing (CCS) reads for metagenomics projects. We compared the application and performance of both PacBio CCS and Illumina HiSeq data with assembly and taxonomic binning algorithms using metagenomic samples representing a complex microbial community. Eight SMRT cells produced approximately 94 Mb of CCS reads from a biogas reactor microbiome sample, which averaged 1319 nt in length and 99.7 % accuracy. CCS data assembly generated a comparative number of large contigs greater than 1 kb, to those assembled from a ~190x larger HiSeq dataset (~18 Gb) produced from the same sample (i.e approximately 62 % of total contigs). Hybrid assemblies using PacBio CCS and HiSeq contigs produced improvements in assembly statistics, including an increase in the average contig length and number of large contigs. The incorporation of CCS data produced significant enhancements in taxonomic binning and genome reconstruction of two dominant phylotypes, which assembled and binned poorly using HiSeq data alone. Collectively these results illustrate the value of PacBio CCS reads in certain metagenomics applications."}, {"title": "Diffantom: whole-brain diffusion MRI phantoms derived from real datasets of the Human Connectome Project", "url": "https://www.biorxiv.org/content/early/2015/09/15/026898", "tag": "Bioinformatics", "abstract": "Diffantom is a whole-brain digital phantom generated from a dataset from the Human Connectome Project. Diffantom is presented here to be openly and freely distributed along with the diffantomizer workflow to generate new diffantoms. We encourage the neuroimage community to contribute with their own diffantoms and share them openly."}, {"title": "IDENTIFICATION OF GENOMIC REGIONS CARRYING A CAUSAL MUTATION IN UNORDERED GENOMES", "url": "https://www.biorxiv.org/content/early/2015/09/15/026856", "tag": "Bioinformatics", "abstract": "Whole genome sequencing using high-throughput sequencing (HTS) technologies offers powerful opportunities to study genetic variation. Mapping the mutations responsible for different phenotypes is generally an involved and time-consuming process so researchers have developed user-friendly tools for mapping-by-sequencing, yet they are not applica- ble to organisms with non-sequenced genomes. We introduce SDM (SNP Distribution Method), a reference independent method for rapid discovery of mutagen-induced muta- tions in typical forward genetic screens. SDM aims to order a disordered collection of HTS reads or contigs such that the fragment carrying the causative mutation can be identified. SDM uses typical distributions of homozygous SNPs that are linked to a phenotype-altering SNP in a non-recombinant region as a model to order the fragments. To implement and test SDM, we created model genomes with an idealised SNP density based on Arabidop- sis thaliana chromosome 1 and analysed fragments with size distribution similar to reads or contigs assembled from HTS sequencing experiments. SDM groups the contigs by their normalised SNP density and arranges them to maximise the fit to the expected SNP distribution. We tested the procedure in existing datasets by examining SNP distributions in recent out-cross and back-cross experiments in Arabidopsis thaliana backgrounds. In all the examples we analysed, homozygous SNPs were normally distributed around the causal mutation. We used the real SNP densities obtained from these experiments to prove the efficiency and accuracy of SDM. The algorithm was able to successfully identify small sized (10-100 kb) genomic regions containing the causative mutation."}, {"title": "Bayesian Gaussian Process Latent Variable Models for pseudotime inference in single-cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2015/09/15/026872", "tag": "Bioinformatics", "abstract": "Single-cell genomics has revolutionised modern biology while requiring the development of advanced computational and statistical methods. Advances have been made in uncovering gene expression heterogeneity, discovering new cell types and novel identification of genes and transcription factors involved in cellular processes. One such approach to the analysis is to construct pseudotime orderings of cells as they progress through a particular biological process, such as cell-cycle or differentiation. These methods assign a score - known as the pseudotime - to each cell as a surrogate measure of progression. However, all published methods to date are purely algorithmic and lack any way to give uncertainty to the pseudotime assigned to a cell. Here we present a method that combines Gaussian Process Latent Variable Models (GP-LVM) with a recently published electroGP prior to perform Bayesian inference on the pseudotimes. We go on to show that the posterior variability in these pseudotimes leads to nontrivial uncertainty in the pseudo-temporal ordering of the cells and that pseudotimes should not be thought of as point estimates."}, {"title": "Integrated Genome Browser: visual analytics platform for genomics", "url": "https://www.biorxiv.org/content/early/2015/09/15/026351", "tag": "Bioinformatics", "abstract": "Motivation: Genome browsers that support fast navigation and interactive visual analytics can help scientists achieve deeper insight into large-scale genomic data sets more quickly, thus accelerating the discovery process. Toward this end, we developed Integrated Genome Browser (IGB), a highly configurable, interactive and fast open source desktop genome browser. Results: Here we describe multiple updates to IGB, including all-new capability to display and interact with data from high-throughput sequencing experiments. To demonstrate, we describe example visualizations and analyses of data sets from RNA-Seq, ChIP-Seq, and bisulfite sequencing experiments. Understanding results from genome-scale experiments requires viewing the data in the context of reference genome annotations and other related data sets. To facilitate this, we enhanced IGB's ability to consume data from diverse sources, including Galaxy, Distributed Annotation, and IGB-specific Quickload servers. To support future visualization needs as new genome-scale assays enter wide use, we transformed the IGB codebase into a modular, extensible platform for developers to create and deploy all-new visualizations of genomic data. Availability: IGB is open source and is freely available from http://bioviz.org/igb."}, {"title": "Using Cell line and Patient samples to improve Drug Response Prediction", "url": "https://www.biorxiv.org/content/early/2015/09/15/026534", "tag": "Bioinformatics", "abstract": "Recent advances in high-throughput technologies have facilitated the profiling of large panels of cancer cell lines with responses measured for thousands of drugs. The computational challenge is now to realize the potential of these data in predicting patients responses to these drugs in the clinic. We address this issue by examining the spectrum of prediction models of patient response: models predicting directly from cell lines, those predicting directly from patients, and those trained on cell lines and patients at the same time. We tested 21 classification models on four drugs (bortezomib, erlotinib, docetaxel and epirubicin) for which clinical trial data were available. Our integrative models consistently outperform cell line-based predictors, indicating that there are limitations to the predictive potential of in vitro data alone. Furthermore, these integrative models achieve better predictive accuracy and require substantially fewer patients than would be the case if only patient data were available. Altogether our results support the relevance of preclinical data for therapy prediction in clinical trials, enabling more efficient and cost-effective trial design."}, {"title": "Revisiting inconsistency in large pharmacogenomic studies", "url": "https://www.biorxiv.org/content/early/2015/09/11/026153", "tag": "Bioinformatics", "abstract": "Background: In 2012, two large pharmacogenomic studies, the Genomics of Drug Sensitivity in Cancer (GDSC) and Cancer Cell Line Encyclopedia (CCLE), were published, each reported gene expression data and measures of drug response for a large number of drugs and hundreds of cell lines. In 2013, we published a comparative analysis that reported gene expression profiles for the 471 cell lines profiled in both studies and dose response measurements for the 15 drugs characterized in the common cell lines by both studies. While we found good concordance in gene expression profiles, there was substantial inconsistency in the drug responses reported by the GDSC and CCLE projects. Our paper was widely discussed and we received extensive feedback on the comparisons that we performed. This feedback, along with the release of new data, prompted us to revisit our initial analysis. Here we present a new analysis using these expanded data in which we address the most significant suggestions for improvements on our published analysis: that drugs with different response characteristics should have been treated differently, that targeted therapies and broad cytotoxic drugs should have been treated differently in assessing consistency, that consistency of both molecular profiles and drug sensitivity measurements should both be compared across cell lines to accurately assess differences in the studies, that we missed some biomarkers that are consistent between studies, and that the software analysis tools we provided with our analysis should have been easier to run, particularly as the GDSC and CCLE released additional data. Methods: For each drug, we used published sensitivity data from the GDSC and CCLE to separately estimate drug dose-response curves. We then used two statistics, the area between drug dose-response curves (ABC) and the Matthews correlation coefficient (MCC), to robustly estimate the consistency of continuous and discrete drug sensitivity measures, respectively. We also used recently released RNA-seq data together with previously published gene expression microarray data to assess inter-platform reproducibility of cell line gene expression profiles. Results: This re-analysis supports our previous finding that gene expression data are significantly more consistent than drug sensitivity measurements. The use of new statistics to assess data consistency allowed us to identify two broad effect drugs -- 17-AAG and PD-0332901 -- and three targeted drugs -- PLX4720, nilotinib and crizotinib -- with moderate to good consistency in drug sensitivity data between GDSC and CCLE. Not enough sensitive cell lines were screened in both studies to robustly assess consistency for three other targeted drugs, PHA-665752, erlotinib, and sorafenib. Concurring with our published results, we found evidence of inconsistencies in pharmacological phenotypes for the remaining eight drugs. Further, to discover \"consistency\" between studies required the use of multiple statistics and the selection of specific measures on a case-by-case basis. Conclusion: Our results reaffirm our initial findings of an inconsistency in drug sensitivity measures for eight of fifteen drugs screened both in GDSC and CCLE, irrespective of which statistical metric was used to assess correlation. Taken together, our findings suggest that the phenotypic data on drug response in the GDSC and CCLE continue to present challenges for robust biomarker discovery. This re-analysis provides additional support for the argument that experimental standardization and validation of pharmacogenomic response will be necessary to advance the broad use of large pharmacogenomic screens."}, {"title": "Pyvolve: a flexible Python module for simulating sequences along phylogenies", "url": "https://www.biorxiv.org/content/early/2015/09/10/020214", "tag": "Bioinformatics", "abstract": "We introduce Pyvolve, a flexible Python module for simulating genetic data along a phylogeny using continuous-time Markov models of sequence evolution. Easily incorporated into Python bioinformatics pipelines, Pyvolve can simulate sequences according to most standard models of nucleotide, amino-acid, and codon sequence evolution. All model parameters are fully customizable. Users can additionally specify custom evolutionary models, with custom rate matrices and/or states to evolve. This flexibility makes Pyvolve a convenient framework not only for simulating sequences under a wide variety of conditions, but also for developing and testing new evolutionary models. Pyvolve is an open-source project under a FreeBSD license, and it is available for download, along with a detailed user-manual and example scripts, from http://github.com/sjspielman/pyvolve."}, {"title": "pcaReduce: Hierarchical Clustering of Single Cell Transcriptional Profiles", "url": "https://www.biorxiv.org/content/early/2015/09/08/026385", "tag": "Bioinformatics", "abstract": "Advances in single cell genomics provides a way of routinely generating transcriptomics data at the single cell level. A frequent requirement of single cell expression experiments is the identification of novel patterns of heterogeneity across single cells that might explain complex cellular states or tissue composition. To date, classical statistical analysis tools have being routinely applied to single cell data, but there is considerable scope for the development of novel statistical approaches that are better adapted to the challenges of inferring cellular hierarchies. Here, we present a novel integration of principal components analysis and hierarchi- cal clustering to create a framework for characterising cell state identity. Our methodology uses agglomerative clustering to generate a cell state hierarchy where each cluster branch is associated with a principal component of variation that can be used to differentiate two cellular states. We demonstrate that using real single cell datasets this approach allows for consistent clustering of single cell transcriptional profiles across multiple scales of interpretation. An R implementation of pcaReduce algorithm is available from https://github. com/JustinaZ/pcaReduce"}, {"title": "Evolution of genes neighborhood within reconciled phylogenies: an ensemble approach", "url": "https://www.biorxiv.org/content/early/2015/09/08/026310", "tag": "Bioinformatics", "abstract": "Context: The reconstruction of evolutionary scenarios for whole genomes in terms of genome rearrangements is a fundamental problem in evolutionary and comparative genomics. The DeCo algorithm, recently introduced by B erard et al., computes parsimonious evolutionary scenarios for gene adjacencies, from pairs of reconciled gene trees. However, as for many combinatorial optimization algorithms, there can exist many co-optimal, or slightly sub-optimal, evolutionary scenarios that deserve to be considered. Contribution: We extend the DeCo algorithm to sample evolutionary scenarios from the whole solution space under the Boltzmann distribution, and also to compute Boltzmann probabilities for specific ancestral adjacencies. Results: We apply our algorithms to a dataset of mammalian gene trees and adjacencies, and observe a significant reduction of the number of syntenic conflicts observed in the resulting ancestral gene adjacencies."}, {"title": "Privacy-Preserving Microbiome Analysis Using Secure Computation", "url": "https://www.biorxiv.org/content/early/2015/09/03/025999", "tag": "Bioinformatics", "abstract": "Motivation: Developing targeted therapeutics and identifying biomarkers relies on large amounts of patient data. Beyond human DNA, researchers now investigate the DNA of micro-organisms inhabiting the human body. An individual's collection of microbial DNA consistently identifies that person and could be used to link a real-world identity to a sensitive attribute in a research dataset. Unfortunately, the current suite of DNA-specific privacy-preserving analysis tools does not meet the requirements for microbiome sequencing studies. Results: We augment an existing categorization of genomic-privacy attacks to incorporate microbiome sequencing and provide an implementation of metagenomic analyses using secure computation. Our implementation allows researchers to perform analysis over combined data without revealing individual patient attributes. We implement three metagenomic analyses and perform an evaluation on real datasets for comparative analysis. We use our implementation to simulate sharing data between four policy-domains and measure the increase in significant discoveries. Additionally, we describe an application of our implementation to form patient pools of data to allow drug companies to query against and compensate patients for the analysis. Availability: The software is freely available for download at: http://cbcb.umd.edu/\\textapprox{}hcorrada/projects/secureseq.html"}, {"title": "A Comparison of Methods: Normalizing High-Throughput RNA Sequencing Data", "url": "https://www.biorxiv.org/content/early/2015/09/03/026062", "tag": "Bioinformatics", "abstract": "As RNA-Seq and other high-throughput sequencing grow in use and remain critical for gene expression studies, technical variability in counts data impedes studies of differential expression studies, data across samples and experiments, or reproducing results. Studies like Dillies et al. (2013) compare several between-lane normalization methods involving scaling factors, while Hansen et al. (2012) and Risso et al. (2014) propose methods that correct for sample-specific bias or use sets of control genes to isolate and remove technical variability. This paper evaluates four normalization methods in terms of reducing intra-group, technical variability and facilitating differential expression analysis or other research where the biological, inter-group variability is of interest. To this end, the four methods were evaluated in differential expression analysis between data from Pickrell et al. (2010) and Montgomery et al. (2010) and between simulated data modeled on these two datasets. Though the between-lane scaling factor methods perform worse on real data sets, they are much stronger for simulated data. We cannot reject the recommendation of Dillies et al. to use TMM and DESeq normalization, but further study of power to detect effects of different size under each normalization method is merited."}, {"title": "Teaser: Individualized benchmarking and optimization of read mapping results for NGS data", "url": "https://www.biorxiv.org/content/early/2015/09/01/025858", "tag": "Bioinformatics", "abstract": "Mapping reads to a genome remains challenging, especially for non-model organisms with poorer quality assemblies, or for organisms with higher rates of mutations. While most research has focused on speeding up the mapping process, little attention has been paid to optimize the choice of mapper and parameters for a user\u2019s dataset. Here we present Teaser, which assists in these choices through rapid automated benchmarking of different mappers and parameter settings for individualized data. Within minutes, Teaser completes a quantitative evaluation of an ensemble of mapping algorithms and parameters. Using Teaser, we demonstrate how Bowtie2 can be optimized for different data."}, {"title": "Modelizing Drosophila melanogaster longevity curves using a new discontinuous 2-Phases of Aging model", "url": "https://www.biorxiv.org/content/early/2015/09/01/025411", "tag": "Bioinformatics", "abstract": "Aging is commonly described as being a continuous process affecting progressively organisms as time passes. This process results in a progressive decrease in individuals fitness through a wide range of both organismal \u2013 decreased motor activity, fertility, resistance to stress \u2013 and molecular phenotypes \u2013 decreased protein and energy homeostasis, impairment of insulin signaling. In the past 20 years, numerous genes have been identified as playing a major role in the aging process, yet little is known about the events leading to that loss of fitness. We recently described an event characterized by a dramatic increase of intestinal permeability to a blue food dye in aging flies committed to die within a few days. Importantly, flies showing this so called \u2018Smurf\u2019 phenotype are the only ones, among a population, to show various age-related changes and exhibit a high-risk of impending death whatever their chronological age. Thus, these observations suggest that instead of being one continuous phenomenon, aging may be a discontinuous process well described by at least two distinguishable phases. In this paper we addressed this hypothesis by implementing a new 2-Phases of Aging mathematiCal model (2PAC model) to simulate longevity curves based on the simple hypothesis of two consecutive phases of lifetime presenting different properties. We first present a unique equation for each phase and discuss the biological significance of the 3 associated parameters. Then we evaluate the influence of each parameter on the shape of survival curves. Overall, this new mathematical model, based on simple biological observations, is able to reproduce many experimental longevity curves, supporting the existence of 2-phases of aging exhibiting specific properties and separated by a dramatic transition that remains to be characterized. Moreover, it indicates that Smurf survival can be approximated by one single constant parameter for a broad range of genotypes that we have tested under our environmental conditions."}, {"title": "Genome scaffolding with PE-contaminated mate-pair libraries", "url": "https://www.biorxiv.org/content/early/2015/08/28/025650", "tag": "Bioinformatics", "abstract": "Scaffolding is often an essential step in a genome assembly process,in which contigs are ordered and oriented using read pairs from a combination of paired-ends libraries and longer-range mate-pair libraries. Although a simple idea, scaffolding is unfortunately hard to get right in practice. One source of problem is so-called PE-contamination in mate-pair libraries, in which a non-negligible fraction of the read pairs get the wrong orientation and a much smaller insert size than what is expected. This contamination has been discussed in previous work on integrated scaffolders in end-to-end assemblers such as Allpaths-LG and MaSuRCA but the methods relies on the fact that the orientation is observable, \\emph{e.g.}, by finding the junction adapter sequence in the reads. This is not always the case, making orientation and insert size of a read pair stochastic. Furthermore, work on modeling PE-contamination has so far been disregarded in stand-alone scaffolders and the effect that PE-contamination has on scaffolding quality has not been examined before. We have addressed PE-contamination in an update of our scaffolder BESST. We formulate the problem as an Integer Linear Program (ILP) and use characteristics of the problem, such as contig lengths and insert size, to efficiently solve the ILP using a linear amount (with respect to the number of contigs) of Linear Programs. Our results show significant improvement over both integrated and standalone scaffolders. The impact of modeling PE-contamination is quantified by comparison with the previous BESST model. We also show how other scaffolders are vulnerable to PE-contaminated libraries, resulting in increased number of misassemblies, more conservative scaffolding, and inflated assembly sizes. The model is implemented in BESST. Source code and usage instructions are found at https://github.com/ksahlin/BESST. BESST can also be downloaded using PyPI."}, {"title": "Modeling of RNA-seq fragment sequence bias reduces systematic errors in transcript abundance estimation", "url": "https://www.biorxiv.org/content/early/2015/08/28/025767", "tag": "Bioinformatics", "abstract": "RNA-seq technology is widely used in biomedical and basic science research. These studies rely on complex computational methods that quantify expression levels for observed transcripts. We find that current computational methods can lead to hundreds of false positive results related to alternative isoform usage. This flaw in the current methodology stems from a lack of modeling sample-specific bias that leads to drops in coverage and is related to sequence features like fragment GC content and GC stretches. By incorporating features that explain this bias into transcript expression models, we greatly increase the specificity of transcript expression estimates, with more than a four-fold reduction in the number of false positives for reported changes in expression. We introduce alpine, a method for estimation of bias-corrected transcript abundance. The method is available as a Bioconductor package that includes data visualization tools useful for bias discovery."}, {"title": "Simultaneously inferring T cell fate and clonality from single cell transcriptomes", "url": "https://www.biorxiv.org/content/early/2015/08/28/025676", "tag": "Bioinformatics", "abstract": "The heterodimeric T cell receptor (TCR) comprises two protein chains that pair to determine the antigen specificity of each T lymphocyte. The enormous sequence diversity within TCR repertoires allows specific TCR sequences to be used as lineage markers for T cells that derive from a common progenitor. We have developed a computational method, called TraCeR, to reconstruct full\u00adlength, paired TCR sequences from T lymphocyte single\u00adcell RNA\u00adseq by combining existing assembly and alignment programs with a \u201csynthetic genome\u201d library comprising all possible TCR sequences. We validate this method with PCR to quantify its accuracy and sensitivity, and compare to other TCR sequencing methods. Our inferred TCR sequences reveal clonal relationships between T cells, which we put into the context of each cell\u2019s functional state from the complete transcriptional landscape quantified from the remaining RNA\u00adseq data. This provides a powerful tool to link T cell specificity with functional response in a variety of normal and pathological conditions. We demonstrate this by determining the distribution of members of expanded T cell clonotypes in response to Salmonella infection in the mouse. We show that members of the same clonotype span early activated CD4+ T cells, as well as mature effector and memory cells."}, {"title": "OEFinder: A user interface to identify and visualize ordering effects in single-cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2015/08/25/025437", "tag": "Bioinformatics", "abstract": "A recent paper identified an artifact in multiple single-cell RNA-seq (scRNA-seq) data sets generated by the Fluidigm C1 platform. Specifically, Leng* et al. showed significantly increased gene expression in cells captured from sites with small or large plate output IDs. We refer to this artifact as an ordering effect (OE). Including OE genes in downstream analyses could lead to biased results. To address this problem, we developed a statistical method and software called OEFinder to identify a sorted list of OE genes. OEFinder is available as an R package along with user-friendly graphical interface implementations that allows users to check for potential artifacts in scRNA-seq data generated by the Fluidigm C1 platform."}, {"title": "dbSUPER: a database of super-enhancers in mouse and human genome", "url": "https://www.biorxiv.org/content/early/2015/08/22/014803", "tag": "Bioinformatics", "abstract": "Super-enhancers are the clusters of transcriptional enhancers that can drive cell-type-specific gene expression and also crucial in cell identity. Many disease-associated sequence variations are enriched in super-enhancer regions of disease-relevant cell types. Thus, super-enhancers can be used as potential biomarkers for disease diagnosis and therapeutics. Current studies have identified super-enhancers in more than 100 cell types and demonstrated their functional importance. However, no centralized resource to integrate all these findings is available yet. We developed dbSUPER (http://bioinfo.au.tsinghua.edu.cn/dbsuper/), the first integrated and interactive database of super-enhancers, with the primary goal of providing a resource for assistance in further studies related to transcriptional control of cell identity and disease. dbSUPER provides a responsive and user-friendly web interface to facilitate efficient and comprehensive search and browsing. The data can be easily sent to Galaxy instances, GREAT and Cistrome web servers for downstream analysis, and can be visualized in UCSC genome browser while custom tracks added automatically. The data can be downloaded and exported in variety of formats. Further, dbSUPER lists genes associated with the super-enhancers and links to various other databases such as GeneCards, UniProt and Entrez. dbSUPER also provides an overlap analysis tool, to annotate user defined regions. We believe dbSUPER is a valuable resource for the biologists and genetic research communities."}, {"title": "Fasta-O-Matic: a tool to sanity check and if needed reformat FASTA files", "url": "https://www.biorxiv.org/content/early/2015/08/21/024448", "tag": "Bioinformatics", "abstract": "As the shear volume of bioinformatic sequence data increases the only way to take advantage of this content is to more completely automate robust analysis workflows. Analysis bottlenecks are often mundane and overlooked processing steps. Idiosyncrasies in reading and/or writing bioinformatics file formats can halt or impair analysis workflows by interfering with the transfer of data from one informatics tools to another. Fasta-O-Matic automates handling of common but minor format issues that otherwise may halt pipelines. The need for automation must be balanced by the need for manual confirmation that any formatting error is actually minor rather than indicative of a corrupt data file. To that end Fasta-O-Matic reports any issues detected to the user with optionally color coded and quiet or verbose logs. Fasta-O-Matic can be used as a general pre-processing tool in bioinformatics workflows (e.g. to automatically wrap FASTA files so that they can be read by BioPerl). It was also developed as a sanity check for bioinformatic core facilities that tend to repeat common analysis steps on FASTA files received from disparate sources. Fasta-O-Matic can be set with format requirements specific to downstream tools as a first step in a larger analysis workflow. Fasta-O-Matic is available free of charge to academic and non-profit institutions at https://github.com/i5K-KINBRE-script-share/read-cleaning-format-conversion/tree/master/KSU_bioinfo_lab/fasta-o-matic."}, {"title": "A Graph Theoretical Approach to Data Fusion", "url": "https://www.biorxiv.org/content/early/2015/08/21/025262", "tag": "Bioinformatics", "abstract": "The rapid development of high throughput experimental techniques has resulted in a growing diversity of genomic datasets being produced and requiring analysis. A variety of computational techniques allow us to analyse such data and to model the biological processes behind them. However, it is increasingly being recognised that we can gain deeper understanding by combining the insights obtained from multiple, diverse datasets. We therefore require scalable computational approaches for data fusion. We propose a novel methodology for scalable unsupervised data fusion. Our technique exploits network representations of the data in order to identify (and quantify) similarities among the datasets. We may work within the Bayesian formalism, using Bayesian nonparametric approaches to model each dataset; or (for fast, approximate, and massive scale data fusion) can naturally switch to more heuristic modelling techniques. An advantage of the proposed approach is that each dataset can initially be modelled independently (and therefore in parallel), before applying a fast post-processing step in order to perform data fusion. This allows us to incorporate new experimental data in an online fashion, without having to rerun all of the analysis. The methodology can be applied to genomic scale datasets and we demonstrate its applicability on examples from the literature, using a broad range of genomic datasets, and also on a recent gene expression dataset from Sporadic inclusion body myositis."}, {"title": "SFS_CODE: More Efficient and Flexible Forward Simulations", "url": "https://www.biorxiv.org/content/early/2015/08/20/025064", "tag": "Bioinformatics", "abstract": "SUMMARY: Modern implementations of forward population genetic simulations are efficient and flexible, enabling the exploration of complex models that may otherwise be intractable. Here we describe an updated version of SFS_CODE, which has increased efficiency and includes many novel features. Among these features is an arbitrary model of dominance, the ability to simulate partial and soft selective sweeps, as well as track the trajectories of mutations and/or ancestries across multiple populations under complex models that are not possible under a coalescent framework. We also release sfs_coder, a Python wrapper to SFS_CODE allowing the user to easily generate command lines for common models of demography, selection, and human genome structure, as well as parse and simulate phenotypes from SFS_CODE output. Availability and Implementation: Our open source software is written in C and Python, and are available under the GNU General Public License at http://sfscode.sourceforge.net. Contact: ryan.hernandez@ucsf.edu Supplementary information: Detailed usage information is available from the project website at http://sfscode.sourceforge.net."}, {"title": "Statistical Inference of a Convergent Antibody Repertoire Response to Influenza Vaccine", "url": "https://www.biorxiv.org/content/early/2015/08/20/025098", "tag": "Bioinformatics", "abstract": "Background: Vaccines dramatically affect an individual\u2019s adaptive immune system, and thus provide an excellent means to study human immunity. Upon vaccination, the B cells that express antibodies (Abs) that happen to bind the vaccine are stimulated to proliferate and undergo mutagenesis at their Ab locus. This process may alter the composition of B cell lineages within an individual, which are known collectively as the antibody repertoire (AbR). Antibodies are also highly expressed in whole blood, potentially enabling unbiased RNA sequencing technologies to query this diversity. Less is known about the diversity of AbR responses across individuals to a given vaccine and if individuals tend to yield a similar response to the same antigenic stimulus. Methods: Here we implement a bioinformatic pipeline that extracts the AbR information from a time-series RNA-seq dataset of 5 patients who were administered a seasonal trivalent influenza vaccine (TIV). We harness the detailed time-series nature of this dataset and use methods based in functional data analysis (FDA) to identify the B cell lineages that respond to the vaccine. We then design and implement rigorous statistical tests in order to ask whether or not these patients exhibit a convergent AbR response to the same TIV. Results: We find that high-resolution time-series data can be used to help identify the Ab lineages that respond to an antigenic stimulus, and that this response can exhibit a convergent nature across patients inoculated with the same vaccine. However, correlations in AbR diversity among individuals prior to inoculation can confound inference of a convergent signal unless it is taken into account. Conclusions: We developed a framework to identify the elements of an AbR that respond to an antigen. This information could be used to understand the diversity of different immune responses in different individuals, as well as to gauge the effectiveness of the immune response to a given stimulus within an individual. We also present a framework for testing a convergent hypothesis between AbRs; a hypothesis that is more difficult to test than previously appreciated. Our discovery of a convergent signal suggests that similar epitopes do select for antibodies with similar sequence characteristics."}, {"title": "The impact of partitioning on phylogenomic accuracy", "url": "https://www.biorxiv.org/content/early/2015/08/19/023978", "tag": "Bioinformatics", "abstract": "Several strategies have been proposed to assign substitution models in phylogenomic datasets, or partitioning. The accuracy of these methods, and most importantly, their impact on phylogenetic estimation has not been thoroughly assessed using computer simulations. We simulated multiple partitioning scenarios to benchmark two a priori partitioning schemes (one model for the whole alignment, one model for each data block), and two statistical approaches (hierarchical clustering and greedy) implemented in PartitionFinder and in our new program, PartitionTest. Most methods were able to identify optimal partitioning schemes closely related to the true one. Greedy algorithms identified the true partitioning scheme more frequently than the clustering algorithms, but selected slightly less accurate partitioning schemes and tended to underestimate the number of partitions. PartitionTest was several times faster than PartitionFinder, with equal or better accuracy. Importantly, maximum likelihood phylogenetic inference was very robust to the partitioning scheme. Best-fit partitioning schemes resulted in optimal phylogenetic performance, without appreciable differences compared to the use of the true partitioning scheme. However, accurate trees were also obtained by a ?simple? strategy consisting of assigning independent GTR+G models to each data block. On the contrary, leaving the data unpartitioned always diminished the quality of the trees inferred, to a greater or lesser extent depending on the simulated scenario. The analysis of empirical data confirmed these trends, although suggesting a stronger influence of the partitioning scheme. Overall, our results suggests that statistical partitioning, but also the a priori assignment of independent GTR+G models, maximize phylogenomic performance."}, {"title": "The LUX Score: A Metric for Lipidome Homology", "url": "https://www.biorxiv.org/content/early/2015/08/19/013847", "tag": "Bioinformatics", "abstract": "Motivation: We propose a method for estimating lipidome homologies analogous to the ones used in sequence analysis and phylogenetics. Results: Algorithms were developed to quantify the structural similarity between lipids and to compute chemical space models of sets of lipid structures and lipidomes. When all lipid molecules of the LIPIDMAPS structure database were mapped in such a chemical space, they automatically formed clusters corresponding to conventional chemical families. Homologies between the lipidomes of four yeast strains based on our LUX score reflected the genetic relationship, although the score is based solely on lipid structures. Source code availability: www.lux.fz-borstel.de"}, {"title": "Genome ARTIST: a robust, high-accuracy aligner tool for mapping transposon insertions and self-insertions", "url": "https://www.biorxiv.org/content/early/2015/08/18/024976", "tag": "Bioinformatics", "abstract": "A critical topic of insertional mutagenesis experiments performed on model organisms is mapping the hits of artificial transposons (ATs) at nucleotide level accuracy. Obviously, mapping errors may occur when sequencing artifacts or mutations as SNPs and small indels are present very close to the junction between a genomic sequence and a transposon inverted repeat (TIR). Another particular item of insertional mutagenesis is mapping of the transposon self-insertions and, to our best knowledge, there is no publicly available mapping tool designed to analyze such molecular events. We developed Genome ARTIST, a pairwise gapped aligner tool which works out both issues by means of an original, robust mapping strategy. Genome ARTIST is not designed to use NGS data but to analyze ATs insertions obtained in small to medium-scale mutagenesis experiments. Genome ARTIST employs a heuristic approach to find DNA sequence similarities and harnesses a multi-step implementation of a Smith-Waterman adapted algorithm to compute the mapping alignments. The experience is enhanced by easily customizable parameters and a user-friendly interface that describes the genomic landscape surrounding the insertion. Genome ARTIST deals with many genomes of bacteria and eukaryotes available in Ensembl and GenBank repositories. Our tool specifically harnesses/exploits the sequence annotation data provided by FlyBase for Drosophila melanogaster (the fruit fly), which enables mapping of insertions relative to various genomic features such as natural transposons. Genome ARTIST was tested against other alignment tools using relevant query sequences derived from the D. melanogaster and Mus musculus (mouse) genomes. Real and simulated query sequences were also comparatively inquired, revealing that Genome ARTIST is a very robust solution for mapping transposon insertions. Genome ARTIST is a stand-alone user-friendly application, designed for high-accuracy mapping of transposon insertions and self-insertions. The tool is also useful for routine aligning assessments like detection of SNPs or checking the specificity of primers and probes. Genome ARTIST is an open source software and is available for download at www.genomeartist.ro and at www.bioinformatics.org."}, {"title": "Tools and pipelines for BioNano data: molecule assembly pipeline and FASTA super scaffolding tool", "url": "https://www.biorxiv.org/content/early/2015/08/18/020966", "tag": "Bioinformatics", "abstract": "Background: Genome assembly remains an unsolved problem. Assembly projects face a range of hurdles that confound assembly. Thus a variety of tools and approaches are needed to improve draft genomes. Results: We used a custom assembly workflow to optimize consensus genome map assembly, resulting in an assembly equal to the estimated length of the Tribolium castaneum genome and with an N50 of more than 1 Mb. We used this map for super scaffolding the T. castaneum sequence assembly, more than tripling its N50 with the program Stitch. Conclusions: In this article we present software that leverages consensus genome maps assembled from extremely long single molecule maps to increase the contiguity of sequence assemblies. We report the results of applying these tools to validate and improve a 7x Sanger draft of the T. castaneum genome."}, {"title": "WMP: A novel comprehensive wheat miRNA database, including related bioinformatics software", "url": "https://www.biorxiv.org/content/early/2015/08/18/024893", "tag": "Bioinformatics", "abstract": "MicroRNAs (miRNAs) are emerging as important post-transcriptional regulators that may regulate key plant genes responsible for agronomic traits such as grain yield and stress tolerance. Several studies identified species and clades specific miRNA families associated with plant stress regulated genes. Here, we propose a novel resource that provides data related to the expression of abiotic stress responsive miRNAs in wheat, one of the most important staple food crops. This database allows the query of small RNA libraries, including in silico predicted wheat miRNA sequences and the expression profiles of small RNAs identified from those libraries. Our database also provides a direct access to online miRNA prediction software tuned to de novo miRNA detection in wheat, in monocotyledon clades, as well as in other plant species. These data and software will facilitate multiple comparative analyses and reproducible studies on small RNAs and miRNA families in plants. Our web-portal is available at: http://wheat.bioinfo.uqam.ca."}, {"title": "Data science identifies novel drug interactions that prolong the QT interval", "url": "https://www.biorxiv.org/content/early/2015/08/17/024745", "tag": "Bioinformatics", "abstract": "Drug-induced prolongation of the QT interval on the electrocardiogram (long QT syndrome, LQTS) can lead to a potentially fatal ventricular arrhythmia called Torsades de Pointes (TdP). 180 drugs with both cardiac and non-cardiac indications have been found to increase risk for TdP, but drug-drug interactions contributing to LQTS (QT-DDIs) remain poorly characterized. Traditional methods for mining observational healthcare data are poorly equipped to detect QT-DDI signals due to low reporting numbers and a lack of direct evidence for LQTS. In this study we present an integrative data science pipeline that effectively circumvents these limitations by identifying latent signals for QT-DDIs in the FDA?s Adverse Event Reporting System and retrospectively validating these predictions using electrocardiogram data in electronic health records. We present 26 novel QT-DDIs flagged using this method that warrant further investigation."}, {"title": "Cookiecutter: a tool for kmer-based read filtering and extraction", "url": "https://www.biorxiv.org/content/early/2015/08/16/024679", "tag": "Bioinformatics", "abstract": "Motivation: Kmer-based analysis is a powerful method used in read error correction and implemented in various genome assembly tools. A number of read processing routines include extracting or removing sequence reads from the results of highthroughput sequencing experiments prior to further analysis. Here we present a new approach to sorting or filtering of raw reads based on a provided list of kmers. Results: We developed Cookiecutter \u2014 a computational tool for rapid read extraction or removing according to a provided list of k-mers generated from a FASTA file. Cookiecutter is based on the implementation of the Aho-Corasik algorithm and is useful in routine processing of high-throughput sequencing datasets. Cookiecutter can be used for both removing undesirable reads and read extraction from a user-defined region of interest. Availability: The open-source implementation with user instructions can be obtained from GitHub: https://github.com/ ad3002/Cookiecutter"}, {"title": "Rail-RNA: Scalable analysis of RNA-seq splicing and coverage", "url": "https://www.biorxiv.org/content/early/2015/08/11/019067", "tag": "Bioinformatics", "abstract": "RNA sequencing (RNA-seq) experiments now span hundreds to thousands of samples. Current spliced alignment software is designed to analyze each sample separately. Consequently, no information is gained from analyzing multiple samples together, and it is difficult to reproduce the exact analysis without access to original computing resources. We describe Rail-RNA, a cloud-enabled spliced aligner that analyzes many samples at once. Rail-RNA eliminates redundant work across samples, making it more efficient as samples are added. For many samples, Rail-RNA is more accurate than annotation-assisted aligners. We use Rail-RNA to align 667 RNA-seq samples from the GEUVADIS project on Amazon Web Services in under 16 hours for US$0.91 per sample. Rail-RNA produces alignments and base-resolution bigWig coverage files, ready for use with downstream packages for reproducible statistical analysis. We identify expressed regions in the GEUVADIS samples and show that both annotated and unannotated (novel) expressed regions exhibit consistent patterns of variation across populations and with respect to known confounders. Rail-RNA is open-source software available at http://rail.bio."}, {"title": "Fuzzy-FishNet: A highly precise distribution-free network approach for feature selection in clinical proteomics", "url": "https://www.biorxiv.org/content/early/2015/08/11/024430", "tag": "Bioinformatics", "abstract": "Network-based analysis methods can help resolve coverage and inconsistency issues in proteomics data. Previously, it was demonstrated that a suite of rank-based network approaches (RBNAs) provides unparalleled consistency and reliable feature selection. However, reliance on the t-statistic/t-distribution and hypersensitivity (coupled to a relatively flat p-value distribution) makes feature prioritization for validation difficult. To address these concerns, a refinement based on the fuzzified Fisher exact test, Fuzzy-FishNet was developed. Fuzzy-FishNet is highly precise (providing probability values that allows exact ranking of features). Furthermore, feature ranks are stable, even in small sample size scenario. Comparison of features selected by genomics and proteomics data respectively revealed that in spite of relative feature stability, cross-platform overlaps are extremely limited, suggesting that networks may not be the answer towards bridging the proteomics-genomics divide."}, {"title": "Manta: Rapid detection of structural variants and indels for clinical sequencing applications", "url": "https://www.biorxiv.org/content/early/2015/08/10/024232", "tag": "Bioinformatics", "abstract": "Summary: We describe Manta, a method to discover structural variants and indels from next generation sequencing data. Manta is optimized for rapid clinical analysis, calling structural variants, medium-sized indels and large insertions on standard compute hardware in less than a tenth of the time that comparable methods require to identify only subsets of these variant types: for example NA12878 at 50x genomic coverage is analyzed in less than 20 minutes. Manta can discover and score variants based on supporting paired and split-read evidence, with scoring models optimized for germline analysis of diploid individuals and somatic analysis of tumor-normal sample pairs. Call quality is similar to or better than comparable methods, as determined by pedigree consistency of germline calls and comparison of somatic calls to COSMIC database variants. Manta consistently assembles a higher fraction of its calls to basepair resolution, allowing for improved downstream annotation and analysis of clinical significance. We provide Manta as a community resource to facilitate practical and routine structural variant analysis in clinical and research sequencing scenarios. Availability: Manta source code and Linux binaries are available from http://github.com/Illumina/manta. Contact: csaunders@illumina.com"}, {"title": "Fast and efficient QTL mapper for thousands of molecular phenotypes", "url": "https://www.biorxiv.org/content/early/2015/08/07/022301", "tag": "Bioinformatics", "abstract": "Motivation: In order to discover quantitative trait loci (QTLs), multi-dimensional genomic data sets combining DNA-seq and ChiP-/RNA-seq require methods that rapidly correlate tens of thousands of molecular phenotypes with millions of genetic variants while appropriately controlling for multiple testing. Results: We have developed FastQTL, a method that implements a popular cis-QTL mapping strategy in a user- and cluster-friendly tool. FastQTL also proposes an efficient permutation procedure to control for multiple testing. The outcome of permutations is modeled using beta distributions trained from a few permutations and from which adjusted p-values can be estimated at any level of significance with little computational cost. The Geuvadis & GTEx pilot data sets can be now easily analyzed an order of magnitude faster than previous approaches. Availability: Source code, binaries and comprehensive documentation of FastQTL are freely available to download at http://fastqtl.sourceforge.net/"}, {"title": "DADA2: High resolution sample inference from amplicon data", "url": "https://www.biorxiv.org/content/early/2015/08/06/024034", "tag": "Bioinformatics", "abstract": "Microbial communities are commonly characterized by amplifying and sequencing target genes, but errors limit the precision of amplicon sequencing. We present DADA2, a software package that models and corrects amplicon errors. DADA2 identified more real variants than other methods in Illumina-sequenced mock communities, some differing by a single nucleotide, while outputting fewer spurious sequences. DADA2 analysis of vaginal samples revealed a diversity of Lactobacillus crispatus strains undetected by OTU methods."}, {"title": "Correcting bias from stochastic insert size in read pair data \u2014 applications to structural variation detection and genome assembly", "url": "https://www.biorxiv.org/content/early/2015/08/04/023929", "tag": "Bioinformatics", "abstract": "Insert size distributions from paired read protocols are used for inference in bioinformatic applications such as genome assembly and structural variation detection. However, many of the models that are being used are subject to bias. This bias arises when we assume that all insert sizes within a distribution are equally likely to be observed, when in fact, size matters. These systematic errors exist in popular software even when the assumptions made about data are true. We have previously shown that bias occurs for scaffolders in genome assembly. Here, we generalize the theory and demonstrate that it is applicable in other contexts. We provide examples of bias in state-of the-art software and improve them using our model. One key application of our theory is structural variation detection using read pairs. We show that an incorrect null-hypothesis is commonly used in popular tools and can be corrected using our theory. Furthermore, we approximate the smallest size of indels that are possible to discover given an insert size distribution. Two other applications are inference of insert size distribution on \\emph{de novo} genome assemblies and error correction of genome assemblies using mated reads. Our theory is implemented in a tool called GetDistr (\\url{https://github.com/ksahlin/GetDistr})."}, {"title": "Perturbative formulation of general continuous-time Markov model of sequence evolution via insertions/deletions, Part III: Algorithm for first approximation", "url": "https://www.biorxiv.org/content/early/2015/08/04/023614", "tag": "Bioinformatics", "abstract": "Background Insertions and deletions (indels) account for more nucleotide differences between two related DNA sequences than substitutions do, and thus it is imperative to develop a stochastic evolutionary model that enables us to reliably calculate the probability of the sequence evolution through indel processes. In a separate paper (Ezawa, Graur and Landan 2015a), we established an ab initio perturbative formulation of a continuous-time Markov model of the evolution of an entire sequence via insertions and deletions. And we showed that, under a certain set of conditions, the ab initio probability of an alignment can be factorized into the product of an overall factor and contributions from regions (or local alignments) separated by gapless columns. Moreover, in another separate paper (Ezawa, Graur and Landan 2015b), we performed concrete perturbation analyses on all types of local pairwise alignments (PWAs) and some typical types of local multiple sequence alignments (MSAs). The analyses indicated that even the fewest-indel terms alone can quite accurately approximate the probabilities of local alignments, as long as the segments and the branches in the tree are of modest lengths. Results To examine whether or not the fewest-indel terms alone can well approximate the alignment probabilities of more general types of local MSAs as well, and as a first step toward the automatic application of our ab initio perturbative formulation, we developed an algorithm that calculates the first approximation of the probability of a given MSA under a given parameter setting including a phylogenetic tree. The algorithm first chops the MSA into gapped and gapless segments, second enumerates all parsimonious indel histories potentially responsible for each gapped segment, and finally calculates their contributions to the MSA probability. We performed validation analyses using more than ten million local MSAs. The results indicated that even the first approximation can quite accurately estimate the probability of each local MSA, as long as the gaps and tree branches are at most moderately long. Conclusions The newly developed algorithm, called LOLIPOG, brought our ab initio perturbation formulation at least one step closer to a practically useful method to quite accurately calculate the probability of a MSA under a given biologically realistic parameter setting. [This paper and three other papers (Ezawa, Graur and Landan 2015a,b,c) describe a series of our efforts to develop, apply, and extend the ab initio perturbative formulation of a general continuous-time Markov model of indels.]"}, {"title": "On enhancing variation detection through pan-genome indexing", "url": "https://www.biorxiv.org/content/early/2015/08/03/021444", "tag": "Bioinformatics", "abstract": "Detection of genomic variants is commonly conducted by aligning a set of reads sequenced from an individual to the reference genome of the species and analyzing the resulting read pileup. Typically, this process finds a subset of variants already reported in databases and additional novel variants characteristic to the sequenced individual. Most of the effort in the literature has been put to the alignment problem on a single reference sequence, although our gathered knowledge on species such as human is pan-genomic: We know most of the common variation in addition to the reference sequence. There have been some efforts to exploit pan-genome indexing, where the most widely adopted approach is to build an index structure on a set of reference sequences containing observed variation combinations. The enhancement in alignment accuracy when using pan-genome indexing has been demonstrated in experiments, but so far the above multiple references pan-genome indexing approach has not been tested on its final goal, that is, in enhancing variation detection. This is the focus of this article: We study a generic approach to add variation detection support on top of the multiple references pan-genomic indexing approach. Namely, we study the read pileup on a multiple alignment of reference genomes, and propose a heaviest path algorithm to extract a new recombined reference sequence. This recombined reference sequence can then be utilized in any standard read alignment and variation detection workflow. We demonstrate that the approach enhances variation detection on realistic data sets."}, {"title": "Comparing Variant Call Files for Performance Benchmarking of Next-Generation Sequencing Variant Calling Pipelines", "url": "https://www.biorxiv.org/content/early/2015/08/03/023754", "tag": "Bioinformatics", "abstract": "To evaluate and compare the performance of variant calling methods and their confidence scores, comparisons between a test call set and a ?gold standard? need to be carried out. Unfortunately, these comparisons are not straightforward with the current Variant Call Files (VCF), which are the standard output of most variant calling algorithms for high-throughput sequencing data. Comparisons of VCFs are often confounded by the different representations of indels, MNPs, and combinations thereof with SNVs in complex regions of the genome, resulting in misleading results. A variant caller is inherently a classification method designed to score putative variants with confidence scores that could permit controlling the rate of false positives (FP) or false negatives (FN) for a given application. Receiver operator curves (ROC) and the area under the ROC (AUC) are efficient metrics to evaluate a test call set versus a gold standard. However, in the case of VCF data this also requires a special accounting to deal with discrepant representations. We developed a novel algorithm for comparing variant call sets that deals with complex call representation discrepancies and through a dynamic programing method that minimizes false positives and negatives globally across the entire call sets for accurate performance evaluation of VCFs."}, {"title": "Inverting proteomics analysis provides powerful insight into the peptide/protein conundrum", "url": "https://www.biorxiv.org/content/early/2015/07/31/023515", "tag": "Bioinformatics", "abstract": "In proteomics, a large proportion of mass spectrometry (MS) data is ignored due to the lack of, or insufficient statistical evidence for mappable peptides. In reality, only a small fraction of features are expected to be differentially relevant anyway. Mapping spectra to peptides and subsequently, proteins, produces uncertainty at several levels. We propose it is better to analyze proteomic profiling data directly at MS level, and then relate these features to peptides/proteins. In a renal cancer data comprising 12 normal and 12 cancer subjects, we demonstrate that a simple rule-based binning approach can give rise to informative features. We note that the peptides associated with significant spectral bins gave rise to better class separation than the corresponding proteins, suggesting a loss of signal in the peptide-to-protein transition. Additionally, the binning approach sharpens focus on relevant protein splice forms rather than just canonical sequences. Taken together, the inverted raw spectra analysis paradigm, which is realised by the MZ-Bin method described in this article, provides new possibilities and insights, in how MS-data can be interpreted."}, {"title": "Elucidating Variants in Systemic JIA from Analysis of Illumina & Ion Torrent Exome Data", "url": "https://www.biorxiv.org/content/early/2015/07/30/023580", "tag": "Bioinformatics", "abstract": "Children with systemic juvenile idiopathic arthritis (SJIA) are affected by a wide-range of complications. Partially arising from the difficulty of diagnosis due to the idiopathic nature of the indication. There may be a genetic basis for SJIA, which could help in both diagnosis, and treatment. Two mutations in the Fc epsilon RI pathway, including PIK3CD, were detected in low-coverage Ion Torrent data. Variants of unknown significance were detected within HLA regions on standard Illumina exomes. CSF2RA, which could account for pulmonary observations, had insignificant coverage on both datasets."}, {"title": "Dissecting the roles of local packing density and longer-range effects in protein sequence evolution", "url": "https://www.biorxiv.org/content/early/2015/07/30/023499", "tag": "Bioinformatics", "abstract": "What are the structural determinants of protein sequence evolution? A number of site-specific structural characteristics have been proposed, most of which are broadly related to either the density of contacts or the solvent accessibility of individual residues. Most importantly, there has been disagreement in the literature over the relative importance of solvent accessibility and local packing density for explaining site-specific sequence variability in proteins. We show here that this discussion has been confounded by the definition of local packing density. The most commonly used measures of local packing, such as the contact number and the weighted contact number, represent by definition the combined effects of local packing density and longer-range effects. As an alternative, we here propose a truly local measure of packing density around a single residue, based on the Voronoi cell volume. We show that the Voronoi cell volume, when calculated relative to the geometric center of amino-acid side chains, behaves nearly identically to the relative solvent accessibility, and both can explain, on average, approximately 34\\% of the site-specific variation in evolutionary rate in a data set of 209 enzymes. An additional 10\\% of variation can be explained by non-local effects that are captured in the weighted contact number. Consequently, evolutionary variation at a site is determined by the combined action of the immediate amino-acid neighbors of that site and of effects mediated by more distant amino acids. We conclude that instead of contrasting solvent accessibility and local packing density, future research should emphasize the relative importance of immediate contacts and longer-range effects on evolutionary variation."}, {"title": "Circlator: automated circularization of genome assemblies using long sequencing reads", "url": "https://www.biorxiv.org/content/early/2015/07/30/023408", "tag": "Bioinformatics", "abstract": "The assembly of DNA sequence data into finished genomes is undergoing a renaissance thanks to emerging technologies producing reads of tens of kilobases. Assembling complete bacterial and small eukaryotic genomes is now possible, but the final step of circularizing sequences remains unsolved. Here we present Circlator, the first tool to automate assembly circularization and produce accurate linear representations of circular sequences. Using Pacific Biosciences and Oxford Nanopore data, Circlator correctly circularized 26 of 27 circularizable sequences, comprising 11 chromosomes and 12 plasmids from bacteria, the apicoplast and mitochondrion of Plasmodium falciparum and a human mitochondrion. Circlator is available at http://sanger-pathogens.github.io/circlator/."}, {"title": "Ancestral gene synteny reconstruction improves extant species scaffolding", "url": "https://www.biorxiv.org/content/early/2015/07/23/023085", "tag": "Bioinformatics", "abstract": "We exploit the methodological similarity between ancestral genome reconstruction and extant genome scaffolding. We present a method, called \\decopp\\ that constructs neighborhood relationships between genes or contigs, in both ancestral and extant genomes, in a phylogenetic context. It is able to handle dozens of complete genomes, including genes with complex histories, by using gene phylogenies reconciled with a species tree, that is, annotated with speciation, duplication and loss events. Reconstructed ancestral or extant synteny comes with a support computed from an exhaustive exploration of the solution space. We compare our method with a previously published one that follows the same goal on a small number of genomes with universal unicopy genes. Then we test it on the whole Ensembl database, by proposing partial ancestral genome structures, as well as a more complete scaffolding for many partially assembled genomes on 69 eukaryote species. We carefully analyze a couple of extant adjacencies proposed by our method, and show that they are indeed real links in the extant genomes, that were missing in the current assembly. On a reduced data set of 39 eutherian mammals, we estimate the precision and sensitivity of \\decopp\\ by simulating a fragmentation in some well assembled genomes, and measure how many adjacencies are recovered. We find a very high precision, while the sensitivity depends on the quality of the data and on the proximity of closely related genomes."}, {"title": "CUA: a Flexible and Comprehensive Codon Usage Analyzer", "url": "https://www.biorxiv.org/content/early/2015/07/19/022814", "tag": "Bioinformatics", "abstract": "Codon usage bias (CUB) is pervasive in genomes. Studying its patterns and causes is fundamental for understanding genome evolution. Rapidly emerging large-scale RNA and DNA sequences make studying CUB in many species feasible. Existing software however is limited in incorporating the new data resources. Therefore, I release the software CUA which can compute all popular CUB metrics, including CAI, tAI, Fop, ENC. More importantly, CUA allows users to incorporate user-specific data, such as tRNA abundance and highly expressed genes from considered tissues; this flexibility enables computing CUB metrics for any species with improved accuracy. In sum, CUA eases codon usage studies and establishes a platform for incorporating new metrics in future. CUA is available at http://search.cpan.org/dist/Bio-CUA/ with help documentation and tutorial."}, {"title": "MuCor: Mutation Aggregation and Correlation", "url": "https://www.biorxiv.org/content/early/2015/07/19/022780", "tag": "Bioinformatics", "abstract": "Motivation: There are many tools for variant calling and effect prediction, but little to tie together large sample groups. Aggregating, sorting, and summarizing variants and effects across a cohort is often done with ad hoc scripts that must be re-written for every new project. In response, we have written MuCor, a tool to gather variants from a variety of input formats (including multiple files per sample), perform database lookups and frequency calculations, and write many report types. In addition to use in large studies with numerous samples, MuCor can also be employed to directly compare variant calls from the same sample across two or more platforms, parameters, or pipelines. A companion utility, DepthGauge, measures coverage at regions of interest to increase confidence in calls. Availability: Source code is freely available at https://github.com/blachlylab Contact: james.blachly@osumc.edu Supplementary data: Supplementary data, including detailed documentation, are available online."}, {"title": "Ensembler: Enabling high-throughput molecular simulations at the superfamily scale", "url": "https://www.biorxiv.org/content/early/2015/07/18/018036", "tag": "Bioinformatics", "abstract": "The rapidly expanding body of available genomic and protein structural data provides a rich resource for understanding protein dynamics with biomolecular simulation. While computational infrastructure has grown rapidly, simulations on an omics scale are not yet widespread, primarily because software infrastructure to enable simulations at this scale has not kept pace. It should now be possible to study protein dynamics across entire (super)families, exploiting both available structural biology data and conformational similarities across homologous proteins. Here, we present a new tool for enabling high-throughput simulation in the genomics era. Ensembler takes any set of sequences - from a single sequence to an entire superfamily - and shepherds them through various stages of modeling and refinement to produce simulation-ready structures. This includes comparative modeling to all relevant PDB structures (which may span multiple conformational states of interest), reconstruction of missing loops, addition of missing atoms, culling of nearly identical structures, assignment of appropriate protonation states, solvation in explicit solvent, and refinement and filtering with molecular simulation to ensure stable simulation. The output of this pipeline is an ensemble of structures ready for subsequent molecular simulations using computer clusters, supercomputers, or distributed computing projects like Folding@home. Ensembler thus automates much of the time-consuming process of preparing protein models suitable for simulation, while allowing scalability up to entire superfamilies. A particular advantage of this approach can be found in the construction of kinetic models of conformational dynamics - such as Markov state models (MSMs) - which benefit from a diverse array of initial configurations that span the accessible conformational states to aid sampling. We demonstrate the power of this approach by constructing models for all catalytic domains in the human tyrosine kinase family, using all available kinase catalytic domain structures from any organism as structural templates. Ensembler is free and open source software licensed under the GNU General Public License (GPL) v2. It is compatible with Linux and OS X. The latest release can be installed via the conda package manager, and the latest source can be downloaded from https://github.com/choderalab/ensembler."}, {"title": "Scalable multi whole-genome alignment using recursive exact matching", "url": "https://www.biorxiv.org/content/early/2015/07/17/022715", "tag": "Bioinformatics", "abstract": "The emergence of third generation sequencing technologies has brought near perfect de-novo genome assembly within reach. This clears the way towards reference-free detection of genomic variations. In this paper, we introduce a novel concept for aligning whole-genomes which allows the alignment of multiple genomes. Alignments are constructed in a recursive manner, in which alignment decisions are statistically supported. Computational performance is achieved by splitting an initial indexing data structure into a multitude of smaller indices. We show that our method can be used to detect high resolution structural variations between two human genomes, and that it can be used to obtain a high quality multiple genome alignment of at least nineteen Mycobacterium tuberculosis genomes. An implementation of the outlined algorithm called REVEAL is available on: https://github.com/jasperlinthorst/REVEAL"}, {"title": "metaCCA: Summary statistics-based multivariate meta-analysis of genome-wide association studies using canonical correlation analysis", "url": "https://www.biorxiv.org/content/early/2015/07/16/022665", "tag": "Bioinformatics", "abstract": "A dominant approach to genetic association studies is to perform univariate tests between genotype-phenotype pairs. However, analysing related traits together increases statistical power, and certain complex associations become detectable only when several variants are tested jointly. Currently, modest sample sizes of individual cohorts and restricted availability of individual-level genotype-phenotype data across the cohorts limit conducting multivariate tests. We introduce metaCCA, a computational framework for summary statistics-based analysis of a single or multiple studies that allows multivariate representation of both genotype and phenotype. It extends the statistical technique of canonical correlation analysis to the setting where original individual-level records are not available, and employs a covariance shrinkage algorithm to achieve robustness. Multivariate meta-analysis of two Finnish studies of nuclear magnetic resonance metabolomics by metaCCA, using standard univariate output from the program SNPTEST, shows an excellent agreement with the pooled individual-level analysis of original data. Motivated by strong multivariate signals in the lipid genes tested, we envision that multivariate association testing using metaCCA has a great potential to provide novel insights from already published summary statistics from high-throughput phenotyping technologies."}, {"title": "Oxford Nanopore Sequencing, Hybrid Error Correction, and de novo Assembly of a Eukaryotic Genome", "url": "https://www.biorxiv.org/content/early/2015/07/15/013490", "tag": "Bioinformatics", "abstract": "Monitoring the progress of DNA molecules through a membrane pore has been postulated as a method for sequencing DNA for several decades. Recently, a nanopore-based sequencing instrument, the Oxford Nanopore MinION, has become available that we used for sequencing the S. cerevisiae genome. To make use of these data, we developed a novel open-source hybrid error correction algorithm Nanocorr (https://github.com/jgurtowski/nanocorr) specifically for Oxford Nanopore reads, as existing packages were incapable of assembling the long read lengths (5-50kbp) at such high error rate (between ~5 and 40% error). With this new method we were able to perform a hybrid error correction of the nanopore reads using complementary MiSeq data and produce a de novo assembly that is highly contiguous and accurate: the contig N50 length is more than ten-times greater than an Illumina-only assembly (678kb versus 59.9kbp), and has greater than 99.88% consensus identity when compared to the reference. Furthermore, the assembly with the long nanopore reads presents a much more complete representation of the features of the genome and correctly assembles gene cassettes, rRNAs, transposable elements, and other genomic features that were almost entirely absent in the Illumina-only assembly."}, {"title": "Accelerating Protein Structure Prediction using Particle Swarm Optimization on GPU", "url": "https://www.biorxiv.org/content/early/2015/07/13/022434", "tag": "Bioinformatics", "abstract": "Protein tertiary structure prediction (PSP) is one of the most challenging problems in bioinformatics. Different methods have been introduced to solve this problem so far, but PSP is computationally intensive and belongs to the NP-hard class. One of the best solutions to accelerate PSP is the use of a massively parallel processing architecture, such graphical processing unit (GPU), which is used to parallelize computational algorithms. In this paper, we have proposed a parallel architecture to accelerate PSP. A bio-inspired method, particle swarm optimization (PSO) has been used as the optimization method to solve PSP. We have also performed a comprehensive study on implementing different topologies of PSO on GPU to consider the acceleration rate. Our solution belongs to ab-initio category which is based on the dihedral angles and calculates the energy-levels to predict the tertiary structure. Indeed, we have studied the search space of a protein to find the best pair of angles that gives the minimum free energy. A profile-level knowledge-based force field based on PSI-BLAST multiple sequence alignment has been applied as a fitness function to calculate the energy values. Different topologies and variations of PSO are considered here and the experimental results show that the speedup gain using GPU is about 34 times faster than CPU implementation of the algorithm with an acceptable precision. The energy values of predicted structures confirm the robustness of the algorithm."}, {"title": "A max-margin model for predicting residue-base contacts in protein-RNA interactions", "url": "https://www.biorxiv.org/content/early/2015/07/13/022459", "tag": "Bioinformatics", "abstract": "Motivation: Protein-RNA interactions (PRIs) are essential for many biological processes, so understanding aspects of the sequence and structure in PRIs is important for understanding those processes. Due to the expensive and time-consuming processes required for experimental determination of complex protein-RNA structures, various computational methods have been developed to predict PRIs. However, most of these methods focus on predicting only RNA-binding regions in proteins or only protein-binding motifs in RNA. Methods for predicting entire residue-base contacts in PRIs have not yet achieved sufficient accuracy. Furthermore, some of these methods require 3D structures or homologous sequences, which are not available for all protein and RNA sequences. Results: We propose a prediction method for residue-base contacts between proteins and RNAs using only sequence information and structural information predicted from only sequences. The method can be applied to any protein-RNA pair, even when rich information such as 3D structure is not available. Residue-base contact prediction is formalized as an integer programming problem. We predict a residue-base contact map that maximizes a scoring function based on sequence-based features such as k-mer of sequences and predicted secondary structure. The scoring function is trained by a max-margin framework from known PRIs with 3D structures. To verify our method, we conducted several computational experiments. The results suggest that our method, which is based on only sequence information, is comparable with RNA-binding residue prediction methods based on known binding data."}, {"title": "Automated and accurate estimation of gene family abundance from shotgun metagenomes", "url": "https://www.biorxiv.org/content/early/2015/07/10/022335", "tag": "Bioinformatics", "abstract": "Shotgun metagenomic DNA sequencing is a widely applicable tool for characterizing the functions that are encoded by microbial communities. Several bioinformatic tools can be used to functionally annotate metagenomes, allowing researchers to draw inferences about the functional potential of the community and to identify putative functional biomarkers. However, little is known about how decisions made during annotation affect the reliability of the results. Here, we use statistical simulations to rigorously assess how to optimize annotation accuracy and speed, given parameters of the input data like read length and library size. We identify best practices in metagenome annotation and use them to guide the development of the Shotgun Metagenome Annotation Pipeline (ShotMAP). ShotMAP is an analytically flexible, end-to-end annotation pipeline that can be implemented either on a local computer or a cloud compute cluster. We use ShotMAP to assess how different annotation databases impact the interpretation of how marine metagenome and metatranscriptome functional capacity changes across seasons. We also apply ShotMAP to data obtained from a clinical microbiome investigation of inflammatory bowel disease. This analysis finds that gut microbiota collected from Crohn's disease patients are functionally distinct from gut microbiota collected from either ulcerative colitis patients or healthy controls, with differential abundance of metabolic pathways related to host-microbiome interactions that may serve as putative biomarkers of disease."}, {"title": "EC-PSI: Associating Enzyme Commission Numbers with Pfam Domains", "url": "https://www.biorxiv.org/content/early/2015/07/10/022343", "tag": "Bioinformatics", "abstract": "Abstract With the growing number of protein structures in the protein data bank (PDB), there is a need to annotate these structures at the domain level in order to relate protein structure to protein function. Thanks to the SIFTS database, many PDB chains are now cross-referenced with Pfam domains and enzyme commission (EC) numbers. However, these annotations do not include any explicit relationship between individual Pfam domains and EC numbers. This article presents a novel statistical training-based method called EC-PSI that can automatically infer high confidence associations between EC numbers and Pfam domains directly from EC-chain associations from SIFTS and from EC-sequence associations from the SwissProt, and TrEMBL databases. By collecting and integrating these existing EC-chain/sequence annotations, our approach is able to infer a total of 8,329 direct EC-Pfam associations with an overall F-measure of 0.819 with respect to the manually curated InterPro database, which we treat here as a \u201cgold standard\u201d reference dataset. Thus, compared to the 1,493 EC-Pfam associations in InterPro, our approach provides a way to find over six times as many high quality EC-Pfam associations completely automatically."}, {"title": "Protein binding and methylation on looping chromatin accurately predict distal regulatory interactions", "url": "https://www.biorxiv.org/content/early/2015/07/09/022293", "tag": "Bioinformatics", "abstract": "Identifying the gene targets of distal regulatory sequences is a challenging problem with the potential to illuminate the causal underpinnings of complex diseases. However, current experimental methods to map enhancer-promoter interactions genome-wide are limited by their cost and complexity. We present TargetFinder, a computational method that reconstructs a cell's three-dimensional regulatory landscape from two-dimensional genomic features. TargetFinder achieves outstanding predictive accuracy across diverse cell lines with a false discovery rate up to fifteen times smaller than common heuristics, and reveals that distal regulatory interactions are characterized by distinct signatures of protein interactions and epigenetic marks on the DNA loop between an active enhancer and targeted promoter. Much of this signature is shared across cell types, shedding light on the role of chromatin organization in gene regulation and establishing TargetFinder as a method to accurately map long-range regulatory interactions using a small number of easily acquired datasets."}, {"title": "HTS-IBIS: fast and accurate inference of binding site motifs from HT-SELEX data", "url": "https://www.biorxiv.org/content/early/2015/07/08/022277", "tag": "Bioinformatics", "abstract": "Recent technological advancements enable measuring the binding of a transcription factor to thousands of DNA sequences, in order to infer its binding preferences. High-throughput-SELEX measures protein-DNA binding by deep sequencing over several cycles of enrichment. We devised a new algorithm called HTS-IBIS for the inference task. HTS-IBIS corrects for technological biases, selects the cycle and k, and builds a motif starting from a consensus k-mer in that cycle. In large scale tests, HTS-IBIS outperformed the extant automatic algorithm for the motif finding task on both in vitro and in vivo binding prediction."}, {"title": "SPARTA: Simple Program for Automated reference-based bacterial RNA-seq Transcriptome Analysis", "url": "https://www.biorxiv.org/content/early/2015/07/08/021915", "tag": "Bioinformatics", "abstract": "Summary: SPARTA is a reference-based bacterial RNA-seq analysis workflow application for single-end Illumina reads. SPARTA is turnkey software that simplifies the process of analyzing RNA-seq data sets, making bacterial RNA-seq analysis a routine process that can be undertaken on a personal computer or in the classroom. The easy-to-install, complete workflow processes whole transcriptome shotgun sequencing data files by trimming reads and removing adapters, mapping reads to a reference, counting gene features, calculating differential gene expression, and, importantly, checking for potential batch effects within the data set. SPARTA outputs quality analysis reports, gene feature counts and differential gene expression tables and scatterplots. The workflow is implemented in Python for file management and sequential execution of each analysis step and is available for Mac OS X, Microsoft Windows, and Linux. To promote the use of SPARTA as a teaching platform, a web-based tutorial is available explaining how RNA-seq data are processed and analyzed by the software. Availability and Implementation: Tutorial and workflow can be found at sparta.readthedocs.org. Teaching materials are located at sparta-teaching.readthedocs.org. Source code can be downloaded at www.github.com/abramovitchMSU/, implemented in Python and supported on Mac OS X, Linux, and MS Windows. Contact: Robert B. Abramovitch (abramov5@msu.edu) Supplemental Information: Supplementary data are available online"}, {"title": "Chromatin interactions correlate with local transcriptional activity in Saccharomyces cerevisiae", "url": "https://www.biorxiv.org/content/early/2015/07/06/021725", "tag": "Bioinformatics", "abstract": "Genome organization is crucial for efficiently responding to DNA damage and regulating transcription. In this study, we relate the genome organization of Saccharomyces cerevisiae (budding yeast) to its transcription activity by analyzing published circularized chromosome conformation capture (4C) data in conjunction with eight separate datasets describing genome-wide transcription rate or RNA polymerase II (Pol II) occupancy. We find that large chromosome segments are more likely to interact in areas that have high transcription rate or Pol II occupancy. Additionally, we find that groups of genes with similar transcription rates or similar Pol II occupancy are more likely to have higher numbers of chromosomal interactions than groups of random genes. We hypothesize that transcription localization occurs around sets of genes with similar transcription rates, and more often around genes that are highly transcribed, in order to produce more efficient transcription. Our analysis cannot discern whether gene co-localization occurs because of similar transcription rates or whether similar transcription rates are a consequence of co-localization."}, {"title": "GERV: A Statistical Method for Generative Evaluation of Regulatory Variants for Transcription Factor Binding", "url": "https://www.biorxiv.org/content/early/2015/07/04/017392", "tag": "Bioinformatics", "abstract": "The majority of disease-associated variants identified in genome-wide association studies (GWAS) reside in noncoding regions of the genome with regulatory roles. Thus being able to interpret the functional consequence of a variant is essential for identifying causal variants in the analysis of GWAS studies. We present GERV (Generative Evaluation of Regulatory Variants), a novel computational method for predicting regulatory variants that affect transcription factor binding. GERV learns a k-mer based generative model of transcription factor binding from ChIP-seq and DNase-seq data, and scores variants by computing the change of predicted ChIP-seq reads between the reference and alternate allele. The k-mers learned by GERV capture more sequence determinants of transcription factor binding than a motif-based approach alone, including both a transcription factor's canonical motif as well as associated co-factor motifs. We show that GERV outperforms existing methods in predicting SNPs associated with allele-specific binding. GERV correctly predicts a validated causal variant among linked SNPs, and prioritizes the variants previously reported to modulate the binding of FOXA1 in breast cancer cell lines. Thus, GERV provides a powerful approach for functionally annotating and prioritizing causal variants for experimental follow-up analysis."}, {"title": "EVfold.org: Evolutionary Couplings and Protein 3D Structure Prediction", "url": "https://www.biorxiv.org/content/early/2015/07/02/021022", "tag": "Bioinformatics", "abstract": "Recently developed maximum entropy methods infer evolutionary constraints on protein function and structure from the millions of protein sequences available in genomic databases. The EVfold web server (at EVfold.org) makes these methods available to predict functional and structural interactions in proteins. The key algorithmic development has been to disentangle direct and indirect residue-residue correlations in large multiple sequence alignments and derive direct residue-residue evolutionary couplings (EVcouplings or ECs). For proteins of unknown structure, distance constraints obtained from evolutionarily couplings between residue pairs are used to de novo predict all-atom 3D structures, often to good accuracy. Given sufficient sequence information in a protein family, this is a major advance toward solving the problem of computing the native 3D fold of proteins from sequence information alone. Availability: EVfold server at http://evfold.org/ Contact: evfoldtest@gmail.com"}, {"title": "SubClonal Hierarchy Inference from Somatic Mutations: automatic reconstruction of cancer evolutionary trees from multi-region next generation sequencing", "url": "https://www.biorxiv.org/content/early/2015/07/01/011833", "tag": "Bioinformatics", "abstract": "Recent improvements in deep next-generation sequencing of tumor samples and the ability to identify somatic mutations at low allelic fractions have opened the way for new approaches to model the evolution of individual cancers. The power and utility of these models is increased when tumor samples from multiple sites are sequenced. Temporal ordering of the samples may provide insight into the etiology of both primary and metastatic lesions and rationalizations for tumor recurrence and therapeutic failures. Additional insights may be provided by temporal ordering of evolving subclones ? cellular subpopulations with unique mutational profiles. Current methods for subclone hierarchy inference tightly couple the problem of temporal ordering with that of estimating the fraction of cancer cells harboring each mutation. Here, we present a new framework that includes a rigorous statistical hypothesis test and a collection of tools that make it possible to decouple these problems, which we believe will enable substantial progress in the field of subclone hierarchy inference. The methods presented here can be flexibly combined with methods developed by others addressing either of these problems. We provide tools to interpret hypothesis test results, which inform phylogenetic tree construction, and we introduce the first genetic algorithm designed for this purpose. The utility of our framework is systematically demonstrated in simulations. For most tested combinations of tumor purity, sequencing coverage, and tree complexity, good power (\u2265 0.8) can be achieved and Type 1 error is well controlled when at least three tumor samples are available from a patient. Using data from three published multi-region tumor sequencing studies of (murine) small cell lung cancer, acute myeloid leukemia, and chronic lymphocytic leukemia, in which the authors reconstructed subclonal phylogenetic trees by manual expert curation, we show how different configurations of our tools can identify either a single tree in agreement with the authors, or a small set of trees, which include the authors? preferred tree. Our results have implications for improved modeling of tumor evolution and the importance of multi-region tumor sequencing."}, {"title": "Clustering of mRNA-Seq data for detection of alternative splicing patterns", "url": "https://www.biorxiv.org/content/early/2015/06/30/021733", "tag": "Bioinformatics", "abstract": "Current sequencing of mRNA can provide estimates of the levels of individual isoforms within the cell, where isoforms are the different distinct mRNA products or proteins created by a gene. It remains to adapt many standard statistical methods commonly used for analyzing gene expression levels to take advantage of this additional information. One novel question is whether we can find groupings or clusters of samples that are distinguished not by their gene expression but by their isoform usage. Such clusters in tumors, for example, could be the result of shared disruption to the splicing system that creates the different isoforms. We propose a novel approach to clustering mRNA-Seq data that identifies clusters of samples with common isoform usage. We show via simulation that our methods are more sensitive to finding clusters of similar alternative splicing patterns than standard clustering techniques applied directly to the estimates of isoform levels. We further demonstrate that clustering on isoform usage is more accurate than clustering directly on isoform levels by examining real data that contains a technical artifact that resulted in different batches having different isoform usage patterns."}, {"title": "TransRate: reference free quality assessment of de-novo transcriptome assemblies", "url": "https://www.biorxiv.org/content/early/2015/06/27/021626", "tag": "Bioinformatics", "abstract": "TransRate is a tool for reference-free quality assessment of de novo transcriptome assemblies. Using only sequenced reads as the input, TransRate measures the quality of individual contigs and whole assemblies, enabling assembly optimization and comparison. TransRate can accurately evaluate assemblies of conserved and novel RNA molecules of any kind in any species. We show that it is more accurate than comparable methods and demonstrate its use on a variety of data."}, {"title": "Modeling small RNA competition in C. elegans", "url": "https://www.biorxiv.org/content/early/2015/06/26/021576", "tag": "Bioinformatics", "abstract": "Small RNAs have been determined to have an essential role in gene regulation. However, competition between small RNAs is a poorly understood aspect of small RNA dynamics. Recent evidence has suggested that competition between small RNA pathways arises from a scarcity of common resources essential for small RNA activity. In order to understand how competition affects small RNAs in C. elegans, a system of differential equations was used. The model recreates normal behavior of small RNAs and uses random sampling in order to determine the coefficients of competition for each small RNA class. The model includes endogenous small-interfering RNAs (endo-siRNA), exogenous small-interfering RNAs (exo-siRNA), and microRNAs (miRNA). The model predicts that exo-siRNAs is dominated by competition between endo-siRNAs and miRNAs. Furthermore, the model predicts that competition is required for normal levels of endogenous small RNAs to be maintained. Although the model makes several assumptions about cell dynamics, the model is still useful in order to understand competition between small RNA pathways."}, {"title": "In Silico Predictive Modeling of CRISPR/Cas9 guide efficiency", "url": "https://www.biorxiv.org/content/early/2015/06/26/021568", "tag": "Bioinformatics", "abstract": "The CRISPR/Cas9 system provides unprecedented genome editing capabilities; however, several facets of this system are under investigation for further characterization and optimization, including the choice of guide RNA that directs Cas9 to target DNA. In particular, given that one would like to target the protein-coding region of a gene, hundreds of guides satisfy the basic constraints of the CRISPR/Cas9 Protospacer Adjacent Motif sequence (PAM); however, not all of these guides actually generate gene knockouts with equal efficiency. Leveraging a broad set of experimental measurements of guide knockout efficiency, we introduce a state-of-the art in silico modeling approach to identify guides that will lead to more effective gene knockout. We first investigated which guide and gene features are critical for prediction (e.g., single- and di-nucleotide identity of the gene target), which are helpful (e.g., thermodynamics), and which are predictive but redundant (e.g., microhomology). We also investigated evaluation measures for comparing predictive models in the present context, suggesting that Area Under the Receiver Operating Curve is not ideal. Finally, we explored a variety of different model classes and found that use of gradient-boosted regression trees produced the best predictive performance. Pointers to our open-source software, code, and prediction server will be available at http://research.microsoft.com/en-us/projects/azimuth."}, {"title": "SSCM: A method to analyze and predict the pathogenicity of sequence variants", "url": "https://www.biorxiv.org/content/early/2015/06/26/021527", "tag": "Bioinformatics", "abstract": "The advent of cost-effective DNA sequencing has provided clinics with high-resolution information about patients' genetic variants, which has resulted in the need for efficient interpretation of this genomic data. Traditionally, variant interpretation has been dominated by many manual, time-consuming processes due to the disparate forms of relevant information in clinical databases and literature. Computational techniques promise to automate much of this, and while they currently play only a supporting role, their continued improvement for variant interpretation is necessary to tackle the problem of scaling genetic sequencing to ever larger populations. Here, we present SSCM-Pathogenic, a genome-wide, allele-specific score for predicting variant pathogenicity. The score, generated by a semi-supervised clustering algorithm, shows predictive power on clinically relevant mutations, while also displaying predictive ability in noncoding regions of the genome."}, {"title": "Shrinkage of dispersion parameters in the binomial family, with application to differential exon skipping", "url": "https://www.biorxiv.org/content/early/2015/06/22/012823", "tag": "Bioinformatics", "abstract": "The prevalence of sequencing experiments in genomics has led to an increased use of methods for count data in analyzing high-throughput genomic data to perform analyses. The importance of shrinkage methods in improving the performance of statistical methods remains. A common example is that of gene expression data, where the counts per gene are often modeled as some form of an over-dispersed Poisson. In this case, shrinkage estimates of the per-gene dispersion parameter have led to improved estimation of dispersion in the case of a small number of samples. We address a different count setting introduced by the use of sequencing data: comparing differential proportional usage via an over-dispersed binomial model. This is motivated by our interest in testing for differential exon skipping in mRNA-Seq experiments. We introduce a novel method that is developed by modeling the dispersion based on the double binomial distribution proposed by Efron (1986). Our method (WEB-Seq) is an empirical bayes strategy for producing a shrunken estimate of dispersion and effectively detects differential proportional usage, and has close ties to the weighted-likelihood strategy of edgeR developed for gene expression data (Robinson and Smyth, 2007; Robinson et al., 2010). We analyze its behavior on simulated data sets as well as real data and show that our method is fast, powerful and gives accurate control of the FDR compared to alternative approaches. We provide implementation of our methods in the R package DoubleExpSeq available on CRAN."}, {"title": "TESS: Bayesian inference of lineage diversification rates from (incompletely sampled) molecular phylogenies in R", "url": "https://www.biorxiv.org/content/early/2015/06/19/021238", "tag": "Bioinformatics", "abstract": "Many fundamental questions in evolutionary biology entail estimating rates of lineage diversification (speciation\u2013extinction). We develop a flexible Bayesian framework for specifying an effectively infinite array of diversification models\u2014where rates are constant, vary continuously, or change episodically through time\u2014and implement numerical methods to estimate parameters of these models from molecular phylogenies, even when species sampling is incomplete. Additionally we provide robust methods for comparing the relative and absolute fit of competing branching-process models to a given tree, thereby providing rigorous tests of biological hypotheses regarding patterns and processes of lineage diversification."}, {"title": "Inference under a Wright-Fisher model using an accurate beta approximation", "url": "https://www.biorxiv.org/content/early/2015/06/19/021261", "tag": "Bioinformatics", "abstract": "The large amount and high quality of genomic data available today enables, in principle, accurate inference of evolutionary history of observed populations. The Wright-Fisher model is one of the most widely used models for this purpose. It describes the stochastic behavior in time of allele frequencies and the influence of evolutionary pressures, such as mutation and selection. Despite its simple mathematical formulation, exact results for the distribution of allele frequency (DAF) as a function of time are not available in closed analytic form. Existing approximations build on the computationally intensive diffusion limit, or rely on matching moments of the DAF. One of the moment-based approximations relies on the beta distribution, which can accurately describe the DAF when the allele frequency is not close to the boundaries (zero and one). Nonetheless, under a Wright-Fisher model, the probability of being on the boundary can be positive, corresponding to the allele being either lost or fixed. Here, we introduce the beta with spikes, an extension of the beta approximation, which explicitly models the loss and fixation probabilities as two spikes at the boundaries. We show that the addition of spikes greatly improves the quality of the approximation. We additionally illustrate, using both simulated and real data, how the beta with spikes can be used for inference of divergence times between populations, with comparable performance to existing state-of-the-art method."}, {"title": "Hi-Cpipe: a pipeline for high-throughput chromosome capture", "url": "https://www.biorxiv.org/content/early/2015/06/18/020636", "tag": "Bioinformatics", "abstract": "Hi-Cpipe is a bioinformatics pipeline for the automated analysis of data generated by high-throughput chromatin conformation capture (HiC). The analysis workflow comprises steps of data formatting, genome alignment, quality control and filtering, identification of genome-wide chromatin interactions, visualization and statistics. An interactive browser enables visual inspection of interaction data and results."}, {"title": "PaxtoolsR: Pathway Analysis in R Using Pathway Commons", "url": "https://www.biorxiv.org/content/early/2015/06/18/021105", "tag": "Bioinformatics", "abstract": "Purpose: PaxtoolsR package enables access to pathway data represented in the BioPAX format and made available through the Pathway Commons webservice for users of the R language. Features include the extraction, merging, and validation of pathway data represented in the BioPAX format. This package also provides novel pathway datasets and advanced querying features for R users through the Pathway Commons webservice allowing users to query, extract, and retrieve data and integrate this data with local BioPAX datasets. Availability: The PaxtoolsR package is compatible with R 3.1.1 on Windows, Mac OS X, and Linux using Bioconductor 3.0 and is available through the Bioconductor R package repository along with source code and a tutorial vignette describing common tasks, such as data visualization and gene set enrichment analysis. Source code and documentation are at http://bioconductor.org/packages/release/bioc/html/paxtoolsr.html. This plugin is free, open-source and licensed under the GNU Lesser General Public License (LGPL) v3.0."}, {"title": "Biographika: rich interactive data visualizations on the web for the research community", "url": "https://www.biorxiv.org/content/early/2015/06/17/021063", "tag": "Bioinformatics", "abstract": "When visualizing scientific data one of the current bottlenecks is the lack of interactivity. There already exist many options to build static data visualizations such as R, Matlab or Microsoft Excel among others. On the other hand, we can also find many different pieces of software with a broader or more specific aim that however must be installed locally. There is therefore a gap that is not covered by any of these latter two worlds. Here is where Biographika comes in handy since it provides scientists in general, and more specifically bioinformaticians, with a way of being able to use interactive rich visualizations on the web as part of their daily research. This first version of Biographika includes a set of charts combining diverse approaches that are thought to give users different perspectives of their research data. For the sake of interoperability and expressivity among other reasons we are using D3.js, the de facto standard visualization JavaScript library for manipulating documents based on data. But not only that, we incorporate new approaches as the fact of having fully interactive 3D charts that can be easily integrated with the rest of visualizations; providing the possibility of analyzing multidimensional data in a way that could otherwise be difficult to tackle. For that we use X3DOM, an open-source framework and runtime for 3D graphics on the Web that eases the integration of HTML5 and declarative 3D content. And last but not least, Biographika is also conceived as an effort to provide the data visualization layer for Bio4j that researchers have been lacking in the past few years."}, {"title": "Sequence evidence for common ancestry of eukaryotic endomembrane coatomers", "url": "https://www.biorxiv.org/content/early/2015/06/16/020990", "tag": "Bioinformatics", "abstract": "Eukaryotic cells are defined by compartments through which the trafficking of macromolecules is mediated by large complexes, such as the nuclear pore, transport vesicles and intraflagellar transport. The assembly and maintenance of these complexes is facilitated by endomembrane coatomers, long suspected to be divergently related on the basis of structural and more recently phylogenomic analysis. By performing supervised walks in sequence space across coatomer superfamilies, we uncover subtle sequence patterns that have remained elusive to date, ultimately unifying eukaryotic coatomers by divergent evolution. The conserved residues shared by 3,502 endomembrane coatomer components are mapped onto the solenoid superhelix of nucleoporin and COPII protein structures, thus determining the invariant elements of coatomer architecture. This ancient structural motif can be considered as a universal signature connecting eukaryotic coatomers involved in multiple cellular processes across cell physiology and human disease."}, {"title": "Automation and Evaluation of the SOWH Test with SOWHAT", "url": "https://www.biorxiv.org/content/early/2015/06/15/005264", "tag": "Bioinformatics", "abstract": "The Swofford-Olsen-Waddell-Hillis (SOWH) test evaluates statistical support for incongruent phylogenetic topologies. It is commonly applied to determine if the maximum likelihood tree in a phylogenetic analysis is significantly different than an alternative hypothesis. The SOWH test compares the observed difference in likelihood between two topologies to a null distribution of differences in likelihood generated by parametric resampling. The test is a well-established phylogenetic method for topology testing, but is is sensitive to model misspecification, it is computationally burdensome to perform, and its implementation requires the investigator to make multiple decisions that each have the potential to affect the outcome of the test. We analyzed the effects of multiple factors using seven datasets to which the SOWH test was previously applied. These factors include bootstrap sample size, likelihood software, the introduction of gaps to simulated data, the use of distinct models of evolution for data simulation and likelihood inference, and a suggested test correction wherein an unresolved \"zero-constrained\" tree is used to simulate sequence data. In order to facilitate these analyses and future applications of the SOWH test, we wrote SOWHAT, a program that automates the SOWH test. We find that inadequate bootstrap sampling can change the outcome of the SOWH test. The results also show that using a zero-constrained tree for data simulation can result in a wider null distribution and higher p-values, but does not change the outcome of the SOWH test for most datasets. These results will help others implement and evaluate the SOWH test and allow us to provide recommendation for future applications of the SOWH test. SOWHAT is available for download from https://github.com/josephryan/SOWHAT."}, {"title": "Automated Contamination Detection in Single-Cell Sequencing", "url": "https://www.biorxiv.org/content/early/2015/06/15/020859", "tag": "Bioinformatics", "abstract": "Novel methods for the sequencing of single-cell DNA offer tremendous opportunities. However, many techniques are still in their infancy and a major obstacle is given by sample contamination with foreign DNA. In this contribution, we present a pipeline that allows for fast, automated detection of contaminated samples by the use of modern machine learning methods. First, a vectorial representation of the genomic data is obtained using oligonucleotide signatures. Using non-linear subspace projections, data is transformed to be suitable for automatic clustering. This allows for the detection of one vs. more genomes (clusters) in a sample. As clustering is an ill-posed problem, the pipeline relies on a thorough choice of all involved methods and parameters. We give an overview of the problem and evaluate techniques suitable for this task."}, {"title": "MAST: A flexible statistical framework for assessing transcriptional changes and characterizing heterogeneity in single-cell RNA-seq data.", "url": "https://www.biorxiv.org/content/early/2015/06/15/020842", "tag": "Bioinformatics", "abstract": "Single-cell transcriptomic profiling enables the unprecedented interrogation of gene expression heterogeneity in rare cell populations that would otherwise be obscured in bulk RNA sequencing experiments. The stochastic nature of transcription is revealed in the bimodality of single-cell transcriptomic data, a feature shared across single-cell expression platforms. There is, however, a paucity of computational tools that take advantage of this unique characteristic. We present a new methodology to analyze single-cell transcriptomic data that models this bimodality within a coherent generalized linear modeling framework. We propose a two-part, generalized linear model that allows one to characterize biological changes in the proportions of cells that are expressing each gene, and in the positive mean expression level of that gene. We introduce the cellular detection rate, the fraction of genes turned on in a cell, and show how it can be used to simultaneously adjust for technical variation and so-called \u201cextrinsic noise\u201d at the single-cell level without the use of control genes. Our model permits direct inference on statistics formed by collections of genes, facilitating gene set enrichment analysis. The residuals defined by such models can be manipulated to interrogate cellular heterogeneity and gene-gene correlation across cells and conditions, providing insights into the temporal evolution of networks of co-expressed genes at the single-cell level. Using two single-cell RNA-seq datasets, including newly generated data from Mucosal Associated Invariant T (MAIT) cells, we show how model residuals can be used to identify significant changes across biologically relevant gene sets that are missed by other methods and characterize cellular heterogeneity in response to stimulation."}, {"title": "Overcoming analytical reliability issues in clinical proteomics using rank-based network approaches", "url": "https://www.biorxiv.org/content/early/2015/06/15/020867", "tag": "Bioinformatics", "abstract": "Proteomics is poised to play critical roles in clinical research. However, due to limited coverage and high noise, integration with powerful analysis algorithms is necessary. In particular, network-based algorithms can improve selection of reproducible features in spite of incomplete proteome coverage, technical inconsistency or high inter-sample variability. We define analytical reliability on three benchmarks --- precision/recall rates, feature-selection stability and cross-validation accuracy. Using these, we demonstrate the insufficiencies of commonly used Student???s t-test and Hypergeometric enrichment. Given advances in sample sizes, quantitation accuracy and coverage, we are now able to introduce and evaluate Ranked-Based Network Approaches (RBNAs) for the first time in proteomics. These include SNET (SubNETwork), FSNET (FuzzySNET), PFSNET (PairedFSNET). We also introduce for the first time, PPFSNET(samplePairedPFSNET), which is a paired-sample variant of PFSNET. RBNAs (particularly PFSNET and PPFSNET) excelled on all three benchmarks and can make consistent and reproducible predictions even in the small-sample size scenario (n=4). Given these qualities, RBNAs represent an important advancement in network biology, and is expected to see practical usage, particularly in clinical biomarker and drug target prediction."}, {"title": "ZIFA: Dimensionality reduction for zero-inflated single cell gene expression analysis", "url": "https://www.biorxiv.org/content/early/2015/06/14/019141", "tag": "Bioinformatics", "abstract": "Single cell RNA-seq data allows insight into normal cellular function and diseases including cancer through the molecular characterisation of cellular state at the single-cell level. Dimensionality reduction of such high-dimensional datasets is essential for visualization and analysis, but single-cell RNA-seq data is challenging for classical dimensionality reduction methods because of the prevalence of dropout events leading to zero-inflated data. Here we develop a dimensionality reduction method, (Z)ero (I)nflated (F)actor (A)nalysis (ZIFA), which explicitly models the dropout characteristics, and show that it improves performance on simulated and biological datasets."}, {"title": "Excess False Positive Rates in Methods for Differential Gene Expression Analysis using RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2015/06/11/020784", "tag": "Bioinformatics", "abstract": "Motivation: An important property of a valid method for testing for differential expression is that the false positive rate should at least roughly correspond to the p-value cutoff, so that if 10,000 genes are tested at a p-value cutoff of 10\u22124, and if all the null hypotheses are true, then there should be only about 1 gene declared to be significantly differentially expressed. We tested this by resampling from existing RNA-Seq data sets and also by matched negative binomial simulations. Results: Methods we examined, which rely strongly on a negative binomial model, such as edgeR, DESeq, and DESeq2, show large numbers of false positives in both the resampled real-data case and in the simulated negative binomial case. This also occurs with a negative binomial generalized linear model function in R. Methods that use only the variance function, such as limma-voom, do not show excessive false positives, as is also the case with a variance stabilizing transformation followed by linear model analysis with limma. The excess false positives are likely caused by apparently small biases in estimation of negative binomial dispersion and, perhaps surprisingly, occur mostly when the mean and/or the dis-persion is high, rather than for low-count genes."}, {"title": "WaspAtlas: A Nasonia vitripennis gene database", "url": "https://www.biorxiv.org/content/early/2015/06/08/020669", "tag": "Bioinformatics", "abstract": "Summary: WaspAtlas is a new integrated gene database for the emerging model organism Nasonia vitripennis, which combines annotation data from all available annotation releases with original analyses to form the most comprehensive N. vitripennis resource to date. WaspAtlas allows users to browse and search for gene information in a clear and co-ordinated fashion providing detailed illustrations and easy to understand summaries. The database provides a platform for integrating gene expression and DNA methylation data. WaspAtlas also functions as an archive for empirical data relating to genes, allowing users to easily browse published data relating to their gene(s) of interest. Availability: Freely available on the web at http://waspatlas.com. Website implemented in Catalyst, MySQL and Apache, with all major browsers supported."}, {"title": "DISSECT: A new tool for analyzing extremely large genomic datasets", "url": "https://www.biorxiv.org/content/early/2015/06/05/020453", "tag": "Bioinformatics", "abstract": "Computational tools are quickly becoming the main bottleneck to analyze large-scale genomic and genetic data. This big-data problem, affecting a wide range of fields, is becoming more acute with the fast increase of data available. To address it, we developed DISSECT, a new, easy to use, and freely available software able to exploit the parallel computer architectures of supercomputers to perform a wide range of genomic and epidemiologic analyses which currently can only be carried out on reduced sample sizes or in restricted conditions. We showcased our new tool by addressing the challenge of predicting phenotypes from genotype data in human populations using Mixed Linear Model analysis. We analyzed simulated traits from half a million individuals genotyped for 590,004 SNPs using the combined computational power of 8,400 processor cores. We found that prediction accuracies in excess of 80% of the theoretical maximum could be achieved with large numbers of training individuals."}, {"title": "Segmenting Microarrays with Deep Neural Networks", "url": "https://www.biorxiv.org/content/early/2015/06/03/020404", "tag": "Bioinformatics", "abstract": "Microarray images consist of thousands of spots, each of which corresponds to a different biological material. The microarray segmentation problem is to work out which pixels belong to which spots, even in presence of noise and corruption. We propose a solution based on deep neural networks, which achieves excellent results both on simulated and experimental data. We have made the source code for our solution available on Github under a permissive license."}, {"title": "Cpipe: a shared variant detection pipeline designed for diagnostic settings", "url": "https://www.biorxiv.org/content/early/2015/06/03/020388", "tag": "Bioinformatics", "abstract": "The benefits of implementing high throughput sequencing in the clinic are quickly becoming apparent. However, few freely available bioinformatics pipelines have been built from the ground up with clinical genomics in mind. Here we present Cpipe, a pipeline designed specifically for clinical genetic disease diagnostics. Cpipe was developed by the Melbourne Genomics Health Alliance, an Australian initiative to promote common approaches to genomics across healthcare institutions. As such, Cpipe has been designed to provide fast, effective and reproducible analysis, while also being highly flexible and customisable to meet the individual needs of diverse clinical settings. Cpipe is being shared with the clinical sequencing community as an open source project and is available at http://cpipeline.org."}, {"title": "A semi-supervised approach uncovers thousands of intragenic enhancers differentially activated in human cells", "url": "https://www.biorxiv.org/content/early/2015/06/03/020362", "tag": "Bioinformatics", "abstract": "Background Transcriptional enhancers are generally known to regulate gene transcription from afar. Their activation involves a series of changes in chromatin marks and recruitment of protein factors. These enhancers may also occur inside genes, but how many may be active in human cells and their effects on the regulation of the host gene remains unclear. Results We describe a novel semi-supervised method based on the relative enrichment of chromatin signals between 2 conditions to predict active enhancers. We applied this method to the tumoral K562 and the normal GM12878 cell lines to predict enhancers that are differentially active in one cell type. These predictions show enhancer-like properties according to positional distribution, correlation with gene expression and production of enhancer RNAs. Using this model, we predict 10,365 and 9,777 intragenic active enhancers in K562 and GM12878, respectively, and relate the differential activation of these enhancers to expression and splicing differences of the host genes. Conclusions We propose that the activation or silencing of intragenic transcriptional enhancers modulate the regulation of the host gene by means of a local change of the chromatin and the recruitment of enhancer-related factors that may interact with the RNA directly or through the interaction with RNA binding proteins. Predicted enhancers are available at http://regulatorygenomics.upf.edu/Projects/enhancers.html"}, {"title": "The next 20 years of genome research", "url": "https://www.biorxiv.org/content/early/2015/06/02/020289", "tag": "Bioinformatics", "abstract": "The last 20 years have been a remarkable era for biology and medicine. One of the most significant achievements has been the sequencing of the first human genomes, which has laid the foundation for profound insights into human genetics, the intricacies of regulation and development, and the forces of evolution. Incredibly, as we look into the future over the next 20 years, we see the very real potential for sequencing more than one billion genomes, bringing with it even deeper insights into human genetics as well as the genetics of millions of other species on the planet. Realizing this great potential, though, will only be achieved through the integration and development of highly scalable computational and quantitative approaches can keep pace with the rapid improvements to biotechnology. In this perspective, we aim to chart out these future technologies, anticipate the major themes of research, and call out the challenges ahead. One of the largest shifts will be in the training used to prepare the class of 2035 for their highly interdisciplinary world."}, {"title": "A simple model-based approach to inferring and visualizing cancer mutation signatures", "url": "https://www.biorxiv.org/content/early/2015/06/01/019901", "tag": "Bioinformatics", "abstract": "Recent advances in sequencing technologies have enabled the production of massive amounts of data on somatic mutations from cancer genomes. These data have led to the detection of characteristic patterns of somatic mutations or ``mutation signatures'' at an unprecedented resolution, with the potential for new insights into the causes and mechanisms of tumorigenesis. Here we present new methods for modelling, identifying and visualizing such mutation signatures. Our methods greatly simplify mutation signature models compared with existing approaches, reducing the number of parameters by orders of magnitude even while increasing the contextual factors (e.g. the number of flanking bases) that are accounted for. This improves both sensitivity and robustness of inferred signatures. We also provide a new intuitive way to visualize the signatures, analogous to the use of sequence logos to visualize transcription factor binding sites. We illustrate our new method on somatic mutation data from urothelial carcinoma of the upper urinary tract, and a larger dataset from 30 diverse cancer types. The results illustrate several important features of our methods, including the ability of our new visualization tool to clearly highlight the key features of each signature, the improved robustness of signature inferences from small sample sizes, and more detailed inference of signature characteristics such as strand biases and sequence context effects at the base two positions 5' to the mutated site. The overall framework of our work is based on probabilistic models that are closely connected with ``mixed-membership models'' which are widely used in population genetic admixture analysis, and in machine learning for document clustering. We argue that recognizing these relationships should help improve understanding of mutation signature extraction problems, and suggests ways to further improve the statistical methods. Our methods are implemented in an R package pmsignature (https://github.com/friend1ws/pmsignature) and a web application available at https://friend1ws.shinyapps.io/pmsignature_shiny/."}, {"title": "Improved Algorithms for Finding Edit Distance Based Motifs", "url": "https://www.biorxiv.org/content/early/2015/05/31/020131", "tag": "Bioinformatics", "abstract": "Motif search is an important step in extracting meaningful patterns from biological data. Since the general problem of motif search is intractable, there is a pressing need to develop efficient exact and approximation algorithms to solve this problem. We design novel algorithms for solving the Edit-distance-based Motif Search (EMS) problem: given two integers l,d and n biological strings, find all strings of length l that appear in each input strings with at most d substitutions, insertions and deletions. These algorithms have been evaluated on several challenging instances. Our algorithm solves a moderately hard instance (11,3) in a couple of minutes and the next difficult instance (14,3) in a couple of hours whereas the best previously known algorithm, EMS1, solves (11,3) in a few hours and does not solve (13,4) even after 3 days. This significant improvement is due to a novel and provably efficient neighborhood generation technique introduced in this paper. This efficient approach can be used in other edit distance based applications in Bioinformatics, such as k-spectrum based sequence error correction algorithms. We also use a trie based data structure to efficiently store the candidate motifs in the neighbourhood and to output the motifs in a sorted order."}, {"title": "Learning quantitative sequence-function relationships from high-throughput biological data", "url": "https://www.biorxiv.org/content/early/2015/05/31/020172", "tag": "Bioinformatics", "abstract": "Understanding the transcriptional regulatory code, as well as other types of information encoded within biomolecular sequences, will require learning biophysical models of sequence-function relationships from high-throughput data. Controlling and characterizing the noise in such experiments, however, is notoriously difficult. The unpredictability of such noise creates problems for standard likelihood-based methods in statistical learning, which require that the quantitative form of experimental noise be known precisely. However, when this unpredictability is properly accounted for, important theoretical aspects of statistical learning which remain hidden in standard treatments are revealed. Specifically, one finds a close relationship between the standard inference method, based on likelihood, and an alternative inference method based on mutual information. Here we review and extend this relationship. We also describe its implications for learning sequence-function relationships from real biological data. Finally, we detail an idealized experiment in which these results can be demonstrated analytically."}, {"title": "Optimizing error correction of RNAseq reads", "url": "https://www.biorxiv.org/content/early/2015/05/29/020123", "tag": "Bioinformatics", "abstract": "Motivation: The correction of sequencing errors contained in Illumina reads derived from genomic DNA is a common pre-processing step in many de novo genome assembly pipelines, and has been shown to improved the quality of resultant assemblies. In contrast, the correction of errors in transcriptome sequence data is much less common, but can potentially yield similar improvements in mapping and assembly quality. This manuscript evaluates several popular read-correction tool's ability to correct sequence errors commonplace to transcriptome derived Illumina reads. Results: I evaluated the efficacy of correction of transcriptome derived sequencing reads using using several metrics across a variety of sequencing depths. This evaluation demonstrates a complex relationship between the quality of the correction, depth of sequencing, and hardware availability which results in variable recommendations depending on the goals of the experiment, tolerance for false positives, and depth of coverage. Overall, read error correction is an important step in read quality control, and should become a standard part of analytical pipelines. Availability: Results are non-deterministically repeatable using AMI:ami-3dae4956 (MacManes EC 2015) and the Makefile available here: https://goo.gl/oVIuE0"}, {"title": "SAM/BAM format v1.5 extensions for de novo assemblies", "url": "https://www.biorxiv.org/content/early/2015/05/29/020024", "tag": "Bioinformatics", "abstract": "Summary: The plain text Sequence Alignment/Map (SAM) file format and its companion binary form (BAM) are a generic alignment format for storing read alignments against reference sequences (and unmapped reads) together with structured meta-data (Li et al., 2009). Driven by the needs of the 1000 Genomes Project which sequenced many individual human genomes, early SAM/BAM usage focused on pairwise alignments of reads to a reference. However, through the CIGAR P operator multiple sequence alignments can also be preserved. Herein we describe clarifications and additions in version 1.5 of the specification to facilitate storing de novo sequence alignments: Padded reference sequences (with gap characters), annotation of reads or regions of the reference, and the option of embedding the reference sequence within the file. Availability: The latest public release of the specification is at http://samtools.sourceforge.net/SAM1.pdf, with in development drafts at https://github.com/samtools/hts-specs/ under version control."}, {"title": "The GL Service: Web Service to Exchange GL String Encoded HLA & KIR Genotypes With Complete and Accurate Allele and Genotype Ambiguity", "url": "https://www.biorxiv.org/content/early/2015/05/29/020099", "tag": "Bioinformatics", "abstract": "Genotype List (GL) Strings use a set of hierarchical character delimiters to represent allele and genotype ambiguity in HLA and KIR genotypes in a complete and accurate fashion. A RESTful web service called Genotype List Service was created to allow users to register a GL String and receive a unique identifier for that string in the form of a URI. By exchanging URIs and dereferencing them through the GL Service, users can easily transmit HLA genotypes in a variety of useful formats. The GL Service was developed to be secure, scalable, and persistent. An instance of the GL Service is configured with a nomenclature and can be run in strict or non-strict modes. Strict mode requires alleles used in the GL String to be present in the allele database using the fully qualified nomenclature. Non-strict mode allows any GL String to be registered as long as it is syntactically correct. The GL Service source code is free and open source software, distributed under the GNU Lesser General Public License (LGPL) version 3 or later."}, {"title": "Re-Annotator: Annotation Pipeline for Microarrays", "url": "https://www.biorxiv.org/content/early/2015/05/21/019596", "tag": "Bioinformatics", "abstract": "Background: Microarray technologies are established approaches for high throughput gene expression, methylation and genotyping analysis. An accurate mapping of the array probes is essential to generate reliable biological findings. Manufacturers typically provide incomplete and outdated annotation tables, which often rely on older genome and transcriptome versions differing substantially from up-to-date sequence databases. Results: Here, we present the Re-Annotator, a re-annotation pipeline for microarrays. It is primarily designed for gene expression microarrays but can be adapted to other types of microarrays. The Re-Annotator is based on a custom-built mRNA reference, used to identify the positions of gene expression array probe sequences. A comparison of our re-annotation of the Human-HT12-v4 microarray to the manufacturer's annotation led to over 25% differently interpreted probes. Conclusions: A thorough re-annotation of probe information is crucial to any microarray analysis. The Re-Annotator pipeline consists of Perl and Shell scripts, freely available at (http://sourceforge.net/projects/reannotator). Re-annotation files for Illumina microarrays Human HT-12 v3/v4 and MouseRef-8 v2 are available as well."}, {"title": "GenoWAP: Post-GWAS Prioritization Through Integrated Analysis of Genomic Functional Annotation", "url": "https://www.biorxiv.org/content/early/2015/05/20/019539", "tag": "Bioinformatics", "abstract": "Genome-wide association study (GWAS) has been a great success in the past decade. However, significant challenges still remain in both identifying new risk loci and interpreting results. Bonferroni-corrected significance level is known to be conservative, leading to insufficient statistical power when the effect size is moderate at risk locus. Complex structure of linkage disequilibrium also makes it challenging to separate causal variants from nonfunctional ones in large haplotype blocks. We describe GenoWAP, a post-GWAS prioritization method that integrates genomic functional annotation and GWAS test statistics. The effectiveness of GenoWAP is demonstrated through its applications to Crohn\u2019s disease and schizophrenia using the largest studies available, where highly ranked loci show substantially stronger signals in the whole dataset after prioritization based on a subset of samples. At the single nucleotide polymorphism (SNP) level, top ranked SNPs after prioritization have both higher replication rates and consistently stronger enrichment of eQTLs. Within each risk locus, GenoWAP is also able to distinguish functional sites from groups of correlated SNPs. GenoWAP is freely available on the web at http://genocanyon.med.yale.edu/GenoWAP"}, {"title": "Diagnosis of coronary heart diseases using gene expression profiling; stable coronary artery disease, cardiac ischemia with and without myocardial necrosis", "url": "https://www.biorxiv.org/content/early/2015/05/19/019505", "tag": "Bioinformatics", "abstract": "Cardiovascular disease including coronary artery disease and myocardial infarction is one of the leading causes of death in Europe, and is influenced by both environmental and genetic factors. With the advancements in genomic tools and technologies there is potential to predict and diagnose heart disease using molecular data from analysis of blood cells. We analyzed gene expression data from blood samples taken from normal people (n=21), non-significant coronary artery disease (n=93), patients with unstable angina (n=16), stable coronary artery disease (n=14) and myocardial infarction (MI; n=207). We used a feature selection approach to identify a set of gene expression variables which successfully differentiate different cardiovascular diseases. The initial features were discovered by fitting a linear model for each probe set across all arrays of normal individuals and patients with myocardial infarction. Three different feature optimisation algorithms were devised which identified two most discriminating sets of genes one using MI and normal controls (total genes=8) and another one using MI and unstable angina patients (total genes=17). The results proved the diagnostic robustness of the final feature sets in discriminating not only patients with myocardial infraction from healthy controls but also from patients with clinical symptoms of cardiac ischemia with myocardial necrosis and stable coronary artery disease despite the influence of batch effects and different microarray gene chips and platforms."}, {"title": "A de novo DNA Sequencing and Variant Calling Algorithm for Nanopores", "url": "https://www.biorxiv.org/content/early/2015/05/19/019448", "tag": "Bioinformatics", "abstract": "The single-molecule accuracy of nanopore sequencing has been an area of rapid academic and commercial advancement, but remains insufficient for the de novo analysis of genomes. We introduce here a novel algorithm for the error correction of nanopore data, utilizing statistical models of the physical system in order to obtain high accuracy de novo sequences at a range of coverage depths. We demonstrate the technique by sequencing M13 bacteriophage DNA to 99% accuracy at moderate coverage as well as its use in an assembly pipeline by sequencing \u03bb DNA at a range of coverages. We also show the algorithm\u2019s ability to accurately classify sequence variants at far lower coverage than existing methods."}, {"title": "ReproPhylo: An Environment for Reproducible Phylogenomics", "url": "https://www.biorxiv.org/content/early/2015/05/18/019349", "tag": "Bioinformatics", "abstract": "The reproducibility of experiments is key to the scientific process, and particularly necessary for accurate reporting of analyses in data-rich fields such as phylogenomics. We present ReproPhylo, a phylogenomic analysis environment developed to ensure experimental reproducibility, to facilitate the handling of large-scale data, and to assist methodological experimentation. Reproducibility, and instantaneous repeatability, is built in to the ReproPhylo system, and does not require user intervention or configuration because it stores the experimental workflow as a single, serialized Python object containing explicit provenance and environment information. This ?single file? approach ensures the persistence of provenance across iterations of the analysis, with changes automatically managed by the version control program Git. ReproPhylo produces an extensive human-readable report, and generates a comprehensive experimental archive file, both of which are suitable for submission with publications. The system facilitates thorough experimental exploration of both parameters and data. ReproPhylo is a platform independent CC0 python module, and is easily installed as a Docker image, with an Jupyter GUI, or as a slimmer version in a Galaxy distribution."}, {"title": "Using Mixtures of Biological Samples as Process Controls for RNA-sequencing experiments", "url": "https://www.biorxiv.org/content/early/2015/05/15/015107", "tag": "Bioinformatics", "abstract": "Genome-scale ?-omics? measurements are challenging to benchmark due to the enormous variety of unique biological molecules involved. Mixtures of previously-characterized samples can be used to benchmark repeatability and reproducibility using component proportions as truth for the measurement. We describe and evaluate experiments characterizing the performance of RNA-sequencing (RNA-Seq) measurements, and discuss cases where mixtures can serve as effective process controls. We apply a linear model to total RNA mixture samples in RNA-seq experiments. This model provides a context for performance benchmarking. The parameters of the model fit to experimental results can be evaluated to assess bias and variability of the measurement of a mixture. A linear model describes the behavior of mixture expression measures and provides a context for performance benchmarking. Residuals from fitting the model to experimental data can be used as a metric for evaluating the effect that an individual step in an experimental process has on the linear response function and precision of the underlying measurement while identifying signals affected by interference from other sources. Effective benchmarking requires well-defined mixtures, which for RNA-Seq requires knowledge of the messenger RNA (mRNA) content of the individual total RNA components. We demonstrate and evaluate an experimental method suitable for use in genome-scale process control and lay out a method utilizing spike-in controls to determine mRNA content of total RNA in samples. Genome-scale process controls can be derived from mixtures. These controls relate prior knowledge of individual components to a complex mixture, allowing assessment of measurement performance. The mRNA fraction accounts for differential enrichment of mRNA from varying total RNA samples. Spike-in controls can be utilized to measure this relationship between mRNA content and input total RNA. Our mixture analysis method also enables estimation of the proportions of an unknown mixture, even when component-specific markers are not previously known, whenever pure components are measured alongside the mixture."}, {"title": "Roary: Rapid large-scale prokaryote pan genome analysis", "url": "https://www.biorxiv.org/content/early/2015/05/13/019315", "tag": "Bioinformatics", "abstract": "A typical prokaryote population sequencing study can now consist of hundreds or thousands of isolates. Interrogating these datasets can provide detailed insights into the genetic structure of of prokaryotic genomes. We introduce Roary, a tool that rapidly builds large-scale pan genomes, identifying the core and dispensable accessory genes. Roary makes construction of the pan genome of thousands of prokaryote samples possible on a standard desktop without compromising on the accuracy of results. Using a single CPU Roary can produce a pan genome consisting of 1000 isolates in 4.5 hours using 13 GB of RAM, with further speedups possible using multiple processors."}, {"title": "FourCSeq: Analysis of 4C sequencing data", "url": "https://www.biorxiv.org/content/early/2015/05/12/009548", "tag": "Bioinformatics", "abstract": "Abstract Motivation: Circularized Chromosome Conformation Capture (4C) is a powerful technique for studying the spatial interactions of a specific genomic region called the ?view- point? with the rest of the genome, both in a single condition or comparing different experimental conditions or cell types. Observed ligation frequencies show a strong, regular dependence on genomic distance from the viewpoint, on top of which specific interaction peaks are superimposed. Here, we address the computational task to find these specific interactions and to detect changes between interaction profiles of different conditions. Results: We model the overall trend of decreasing interaction frequency with genomic distance by fitting a smooth monotonously decreasing function to suitably trans- formed count data. Based on the fit, z-scores are calculated from the residuals, with high z scores being interpreted as peaks providing evidence for specific interactions. To compare different conditions, we normalize fragment counts between samples, and call for differential contact frequencies using the statisti- cal method DESeq2 adapted from RNA-Seq analysis. Availability and Implementation: A full end-to-end analysis pipeline is implemented in the R package FourCSeq available at www.bioconductor.org."}, {"title": "Improving the Power of Structural Variation Detection by Augmenting the Reference", "url": "https://www.biorxiv.org/content/early/2015/05/08/019109", "tag": "Bioinformatics", "abstract": "The uses of the Genome Reference Consortium's human reference sequence can be roughly categorized into three related but distinct categories: as a representative species genome, as a coordinate system for identifying variants, and as an alignment reference for variation detection algorithms. However, the use of this reference sequence as simultaneously a representative species genome and as an alignment reference leads to unnecessary artifacts for structural variation detection algorithms and limits their accuracy. We show how decoupling these two references and developing a separate alignment reference can significantly improve the accuracy of structural variation detection, lead to improved genotyping of disease related genes, and decrease the cost of studying polymorphism in a population."}, {"title": "Sharing and specificity of co-expression networks across 35 human tissues", "url": "https://www.biorxiv.org/content/early/2014/10/29/010843.1", "tag": "Bioinformatics", "abstract": "To understand the regulation of tissue-specific gene expression, the GTEx Consortium generated RNA-seq expression data for more than thirty distinct human tissues. This data provides an opportunity for deriving shared and tissue-specific gene regulatory networks on the basis of co-expression between genes. However, a small number of samples are available for a majority of the tissues, and therefore statistical inference of networks in this setting is highly underpowered. To address this problem, we infer tissue-specific gene co-expression networks for 35 tissues in the GTEx dataset using a novel algorithm, GNAT, that uses a hierarchy of tissues to share data between related tissues. We show that this transfer learning approach increases the accuracy with which networks are learned. Analysis of these networks reveals that tissue-specific transcription factors are hubs that preferentially connect to genes with tissue-specific functions. Additionally, we observe that genes with tissue-specific functions lie at the peripheries of our networks. We identify numerous modules enriched for Gene Ontology functions, and show that modules conserved across tissues are especially likely to have functions common to all tissues, while modules that are upregulated in a particular tissue are often instrumental to tissue-specific function. Finally, we provide a web tool, available at mostafavilab.stat.ubc.ca/GNAT, which allows exploration of gene function and regulation in a tissue-specific manner."}, {"title": "Removing unwanted variation in a differential methylation analysis of Illumina HumanMethylation450 array data", "url": "https://www.biorxiv.org/content/early/2015/05/07/019042", "tag": "Bioinformatics", "abstract": "Due to their relatively low-cost per sample and broad, gene-centric coverage of CpGs across the human genome, Illumina's 450k arrays are widely used in large scale differential methylation studies. However, by their very nature, large studies are particularly susceptible to the effects of unwanted variation. The effects of unwanted variation have been extensively documented in gene expression array studies and numerous methods have been developed to mitigate these effects. However, there has been much less research focused on the appropriate methodology to use for accounting for unwanted variation in methylation array studies. Here we present a novel 2-stage approach using RUV-inverse in a differential methylation analysis of 450k data and show that it outperforms existing methods."}, {"title": "CAPRI: Efficient Inference of Cancer Progression Models from Cross-sectional Data", "url": "https://www.biorxiv.org/content/early/2015/05/07/008110", "tag": "Bioinformatics", "abstract": "We devise a novel inference algorithm to effectively solve the cancer progression model reconstruction problem. Our empirical analysis of the accuracy and convergence rate of our algorithm, CAncer PRogression Inference (CAPRI), shows that it outperforms the state-of-the-art algorithms addressing similar problems. Motivation: Several cancer-related genomic data have become available (e.g., The Cancer Genome Atlas, TCGA) typically involving hundreds of patients. At present, most of these data are aggregated in a cross-sectional fashion providing all measurements at the time of diagnosis. Our goal is to infer cancer ?progression? models from such data. These models are represented as directed acyclic graphs (DAGs) of collections of ?selectivity? relations, where a mutation in a gene A ?selects? for a later mutation in a gene B. Gaining insight into the structure of such progressions has the potential to improve both the stratification of patients and personalized therapy choices. Results: The CAPRI algorithm relies on a scoring method based on a probabilistic theory developed by Suppes, coupled with bootstrap and maximum likelihood inference. The resulting algorithm is efficient, achieves high accuracy, and has good complexity, also, in terms of convergence properties. CAPRI performs especially well in the presence of noise in the data, and with limited sample sizes. Moreover CAPRI, in contrast to other approaches, robustly reconstructs different types of confluent trajectories despite irregularities in the data. We also report on an ongoing investigation using CAPRI to study atypical Chronic Myeloid Leukemia, in which we uncovered non trivial selectivity relations and exclusivity patterns among key genomic events."}, {"title": "CIDER: a pipeline for detecting waves of coordinated transcriptional regulation in gene expression time-course data", "url": "https://www.biorxiv.org/content/early/2015/05/06/012518", "tag": "Bioinformatics", "abstract": "Cell adaptability to environmental changes is conferred by complex transcriptional regulatory networks, which respond to external stimuli by modulating the expression dynamics of each gene. Hence, deciphering the network of transcriptional regulation is remarkably important, but proves to be extremely challenging, mainly due to the unfavorable ratio between the number of available observations and the number of parameters to estimate. Most of the existing computational methods for the inference of transcriptional networks consider steady-state gene expression datasets, and produce models of transcriptional regulation best explaining the observed static gene expression. Gene expression time-courses are an emergent typology of gene expression data, paving the way to the characterization of the time-dependent dynamics of transcriptional regulation. In this work we introduce the Complexity Invariant Dynamic Time Warping motif EnRichment (CIDER) analysis, a novel computational pipeline to identify the prominent waves of coordinated gene transcription induced in cells by external stimuli, and determine which TFs are involved in the coordination of gene transcription. The CIDER pipeline combines unsupervised time series clustering and motif enrichment analysis to first detect transcriptional expression patterns, and then identify the TFs over-represented in the promoter regions of gene sets with similar expression dynamics. The ability of CIDER to correctly identify regulatory interactions is assessed on a realistic synthetic dataset of gene expression time-courses, generated by simulating the effects of knock-out perturbations on the E. coli regulatory network. The CIDER source code and the validation datasets are available on request from the corresponding author."}, {"title": "GO-PCA: An Unsupervised Method to Explore Biological Heterogeneity Based on Gene Expression and Prior Knowledge", "url": "https://www.biorxiv.org/content/early/2015/05/05/018705", "tag": "Bioinformatics", "abstract": "Genome-wide expression profiling is a cost-efficient and widely used method to characterize heterogeneous populations of cells, tissues, biopsies, or other biological specimen. The exploratory analysis of such datasets typically relies on generic unsupervised methods, e.g. principal component analysis or hierarchical clustering. However, generic methods fail to exploit the significant amount of knowledge that exists about the molecular functions of genes. Here, I introduce GO-PCA, an unsupervised method that incorporates prior knowledge about gene functions in the form of gene ontology (GO) annotations. GO-PCA aims to discover and represent biological heterogeneity along all major axes of variation in a given dataset, while suppressing heterogeneity due to technical biases. To this end, GO-PCA combines principal component analysis (PCA) with nonparametric GO enrichment analysis, and uses the results to generate expression signatures based on small sets of functionally related genes. I first applied GO-PCA to expression data from diverse lineages of the human hematopoietic system, and obtained a small set of signatures that captured known cell characteristics for most lineages. I then applied the method to expression profiles of glioblastoma (GBM) tumor biopsies, and obtained signatures that were strongly associated with multiple previously described GBM subtypes. Surprisingly, GO-PCA discovered a cell cycle-related signature that exhibited significant differences between the Proneural and the prognostically favorable GBM CpG Island Methylator (G-CIMP) subtypes, suggesting that the G-CIMP subtype is characterized in part by lower mitotic activity. Previous expression-based classifications have failed to separate these subtypes, demonstrating that GO-PCA can detect heterogeneity that is missed by other methods. My results show that GO-PCA is a powerful and versatile expression-based method that facilitates exploration of large-scale expression data, without requiring additional types of experimental data. The low-dimensional representation generated by GO-PCA lends itself to interpretation, hypothesis generation, and further analysis."}, {"title": "NCBI BLAST+ integrated into Galaxy", "url": "https://www.biorxiv.org/content/early/2015/05/04/014043", "tag": "Bioinformatics", "abstract": "Background: The NCBI BLAST suite has become ubiquitous in modern molecular biology, used for small tasks like checking capillary sequencing results of single PCR products through to genome annotation or even larger scale pan-genome analyses. For early adopters of the Galaxy web-based biomedical data analysis platform, integrating BLAST was a natural step for sequence comparison workflows. Findings: The command line NCBI BLAST+ tool suite was wrapped for use within Galaxy, defining appropriate datatypes as needed, with the goal of making common BLAST tasks easy, and advanced tasks possible. Conclusions: This effort has been come an informal international collaborative effort, and is deployed and used on Galaxy servers worldwide. Several example use-cases are described herein."}, {"title": "Replication is Recursion; or, Lambda: the Biological Imperative", "url": "https://www.biorxiv.org/content/early/2015/05/01/018804", "tag": "Bioinformatics", "abstract": "There is a striking visual symmetry between the 'paradoxical combinator' (which implements recursion in the mathematical theory of computation) and the biological replication fork (which implements reproduction in the living cell). Can this mean anything? Living organisms are information processors \u2013 the genome encodes instructions for the process of life much as computer programs encode instructions for a computer. Replication in biological systems is intuitively similar to recursion in computational systems. The following discussion will present enough of the mathematics to allow biologists to make sense of the symmetries in the logo. Mathematicians will learn nothing new about biology, but may be encouraged to look at biological processes from a new perspective."}, {"title": "Using the Phenoscape Knowledgebase to relate genetic perturbations to phenotypic evolution", "url": "https://www.biorxiv.org/content/early/2015/05/01/018853", "tag": "Bioinformatics", "abstract": "The abundance of phenotypic diversity among species can enrich our knowledge of development and genetics beyond the limits of variation that can be observed in model organisms. The Phenoscape Knowledgebase (KB) is designed to enable exploration and discovery of phenotypic variation among species. Because phenotypes in the KB are annotated using standard ontologies, evolutionary phenotypes can be compared with phenotypes from genetic perturbations in model organisms. To illustrate the power of this approach, we review the use of the KB to find taxa showing evolutionary variation similar to that of a query gene. Matches are made between the full set of phenotypes described for a gene and an evolutionary profile, the latter of which is defined as the set of phenotypes that are variable among the daughters of any node on the taxonomic tree. Phenoscape\u2019s semantic similarity interface allows the user to assess the statistical significance of each match and flags matches that may only result from differences in annotation coverage between genetic and evolutionary studies. Tools such as this will help meet the challenge of relating the growing volume of genetic knowledge in model organisms to the diversity of phenotypes in nature. The Phenoscape KB is available at http://kb.phenoscape.org."}, {"title": "Fine-mapping cellular QTLs with RASQUAL and ATAC-seq", "url": "https://www.biorxiv.org/content/early/2015/04/30/018788", "tag": "Bioinformatics", "abstract": "When cellular traits are measured using high-throughput DNA sequencing quantitative trait loci (QTLs) manifest at two levels: population level differences between individuals and allelic differences between cis-haplotypes within individuals. We present RASQUAL (Robust Allele Specific QUAntitation and quality controL), a novel statistical approach for association mapping that integrates genetic effects and robust modelling of biases in next generation sequencing (NGS) data within a single, probabilistic framework. RASQUAL substantially improves causal variant localisation and sensitivity of association detection over existing methods in RNA-seq, DNaseI-seq and ChIP-seq data. We illustrate how RASQUAL can be used to maximise association detection by generating the first map of chromatin accessibility QTLs (caQTLs) in a European population using ATAC-seq. Despite a modest sample size, we identified 2,706 independent caQTLs (FDR 10%) and illustrate how RASQUAL's improved causal variant localisation provides powerful information for fine-mapping disease-associated variants. We also map \u201cmultipeak\u201d caQTLs, identical genetic associations found across multiple, independent open chromatin regions and illustrate how genetic signals in ATAC-seq data can be used to link distal regulatory elements with gene promoters. Our results highlight how joint modelling of population and allele-specific genetic signals can improve functional interpretation of noncoding variation."}, {"title": "The \"Gini index\" in genetics: measuring genetic architecture complexity of quantitative traits", "url": "https://www.biorxiv.org/content/early/2015/04/29/018713", "tag": "Bioinformatics", "abstract": "Genetic architecture is a general terminology used and discussed very often in complex traits genetics. It is related to the number of functional loci involved in explaining variation of a complex trait and the distribution of genetic effects across these loci. Understanding the complexity level of the genetic architecture of complex traits is essential for evaluating the potential power of mapping functional loci and prediction of complex traits. However, there has been no quantitative measurement of the genetic architecture complexity, which makes it difficult to link results from genetic data analysis to such terminology. Inspired by the \"Gini index\" for measuring income distribution in economics, I develop a genetic architecture score (\"GA score\") to measure genetic architecture complexity. Simulations indicate that the GA score is an effective measurement of the complexity level of complex traits genetic architecture."}, {"title": "Controlling False Positive Rates in Methods for Differential Gene Expression Analysis using RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2015/04/29/018739", "tag": "Bioinformatics", "abstract": "We review existing methods for the analysis of RNA-Seq data and place them in a common framework of a sequence of tasks that are usually part of the process. We show that many existing methods produce large numbers of false positives in cases where the null hypothesis is true by construction and where actual data from RNA-Seq studies are used, as opposed to simulations that make specific assumptions about the nature of the data. We show that some of those mathematical assumptions about the data likely are one of the causes of the false positives, and define a general structure that is not apparently subject to these problems. The best performance was shown by limma-voom and by some simple methods composed of easily understandable steps."}, {"title": "Discovery of large genomic inversions using pooled clone sequencing", "url": "https://www.biorxiv.org/content/early/2015/04/22/015156", "tag": "Bioinformatics", "abstract": "There are many different forms of genomic structural variation that can be broadly classified as copy number variation (CNV) and balanced rearrangements. Although many algorithms are now available in the literature that aim to characterize CNVs, discovery of balanced rearrangements (inversions and translocations) remains an open problem. This is mainly because the breakpoints of such events typically lie within segmental duplications and common repeats, which reduce the mappability of short reads. The 1000 Genomes Project spearheaded the development of several methods to identify inversions, however, they are limited to relatively short inversions, and there are currently no available algorithms to discover large inversions using high throughput sequencing technologies (HTS). Here we propose to use a sequencing method (Kitzman et al., 2011) originally developed to improve haplotype resolution to characterize large genomic inversions. This method, called pooled clone sequencing, merges the advantages of clone based sequencing approach with the speed and cost efficiency of HTS technologies. Using data generated with pooled clone sequencing method, we developed a novel algorithm, dipSeq, to discover large inversions (>500 Kbp). We show the power of dipSeq first on simulated data, and then apply it to the genome of a HapMap individual (NA12878). We were able to accurately discover all previously known and experimentally validated large inversions in the same genome. We also identified a novel inversion, and confirmed using fluorescent in situ hybridization. Availability: Implementation of the dipSeq algorithm is available at https://github.com/BilkentCompGen/dipseq"}, {"title": "Bandage: interactive visualisation of de novo genome assemblies", "url": "https://www.biorxiv.org/content/early/2015/04/21/018333", "tag": "Bioinformatics", "abstract": "Summary: While de novo assembly graphs contain assembled contigs (nodes), the connections between those contigs (edges) are difficult for users to access. Bandage (a Bioinformatics Application for Navigating De novo Assembly Graphs Easily) is a tool for visualising assembly graphs with connections. Users can zoom in to specific areas of the graph and interact with it by moving nodes, adding labels, changing colours and extracting sequences. BLAST searches can be performed within the Bandage GUI and the hits are displayed as highlights in the graph. By displaying connections between contigs, Bandage presents new possibilities for analysing de novo assemblies that are not possible through investigation of contigs alone. Availability and implementation: Source code and binaries are freely available at https://github.com/rrwick/Bandage. Bandage is implemented in C++ and supported on Linux, OS X and Windows. Contact: rrwick@gmail.com Supplementary information: A full feature list and screenshots are available at Bioinformatics online and http://rrwick.github.io/Bandage."}, {"title": "BioWardrobe: an integrated platform for analysis of epigenomics and transcriptomics data", "url": "https://www.biorxiv.org/content/early/2015/04/20/012799", "tag": "Bioinformatics", "abstract": "Development of next-generation sequencing has revolutionized molecular biology by enhancing the ability to perform genome-wide studies. However, due to the need for bioinformatic expertise and the size of resulting datasets, use of these technologies is still beyond the capabilities of many laboratories. Herein, we present the Wardrobe Experiment Management System, which allows users to store, visualize and analyze epigenomic and transcriptomic next-generation sequencing data using a biologist-friendly, web-based graphical user interface without the need for programming expertize. Wardrobe can be installed on consumer-class hardware within an institutional local network. Analysis capabilities include predefined pipelines that allow the user to download data from either institutional core facilities or public databases, perform quality control, map reads and visualize data on a built-in mirror of the University of California, Santa Cruz (UCSC) genome browser. Reads per kilobase of transcript per million reads mapped (RPKMs) are calculated for RNA sequencing (RNA-Seq), and islands of enrichment are identified for chromatin immunoprecipitation sequencing (ChIP-Seq) and similar datasets. Advanced analysis capabilities include analyzing differential gene expression and binding and creating average tag density profiles and heatmaps. The Wardrobe package and documentation is available at https://biowardrobe.com. A limited functionality demo-version is available at http://demo.biowardrobe.com"}, {"title": "Do count-based differential expression methods perform poorly when genes are expressed in only one condition?", "url": "https://www.biorxiv.org/content/early/2015/04/20/017673", "tag": "Bioinformatics", "abstract": "A correspondence with respect to: Comprehensive evaluation of differential gene expression analysis methods for RNA-seq data Rapaport F, Khanin R, Liang Y, Pirun M, Krek A, Zumbo P, Mason CE, Socci ND and Betel D, Genome Biol 2013, 14:R95"}, {"title": "benchNGS : An approach to benchmark short reads alignment tools", "url": "https://www.biorxiv.org/content/early/2015/04/18/018234", "tag": "Bioinformatics", "abstract": "In the last decade a number of algorithms and associated software were developed to align next generation sequencing (NGS) reads to relevant reference genomes. The results of these programs may vary significantly, especially when the NGS reads are contain mutations not found in the reference genome. Yet there is no standard way to compare these programs and assess their biological relevance. We propose a benchmark to assess accuracy of the short reads mapping based on the precomputed global alignment of closely related genome sequences. In this paper we outline the method and also present a short report of an experiment performed on five popular alignment tools."}, {"title": "benchmarkR: an R package for benchmarking genome-scale methods", "url": "https://www.biorxiv.org/content/early/2015/04/17/018200", "tag": "Bioinformatics", "abstract": "benchmarkR is an R package designed to assess and visualize the performance of statistical methods for datasets that have an independent truth (e.g., simulations or datasets with large-scale validation), in particular for methods that claim to control false discovery rates (FDR). We augment some of the standard performance plots (e.g., receiver operating characteristic, or ROC, curves) with information about how well the methods are calibrated (i.e., whether they achieve their expected FDR control). For example, performance plots are extended with a point to highlight the power or FDR at a user-set threshold (e.g., at a method's estimated 5% FDR). The package contains general containers to store simulation results (SimResults) and methods to create graphical summaries, such as receiver operating characteristic curves (rocX), false discovery plots (fdX) and power-to-achieved FDR plots (powerFDR); each plot is augmented with some form of calibration information. We find these plots to be an improved way to interpret relative performance of statistical methods for genomic datasets where many hypothesis tests are performed. The strategies, however, are general and will find applications in other domains."}, {"title": "Measuring the Contribution of Genomic Predictors to Improving Estimator Precision in Randomized trials", "url": "https://www.biorxiv.org/content/early/2015/04/16/018168", "tag": "Bioinformatics", "abstract": "The use of genomic data in the clinic has not been as widespread as was envisioned when sequencing and genomic analysis became common techniques. An underlying difficulty is the direct assessment of how much additional information genomic data are providing beyond standard clinical measurements. This is hard to quantify in the clinical setting where laboratory tests based on genomic signatures are fairly new and there are not sufficient data collected to determine how valuable these tests have been in practice. Here we focus on the potential precision gain from using the popular MammaPrint genomic signature in a covariate-adjusted, randomized clinical trial. We describe how adjustment of an estimator for the average treatment effect using baseline measurements can improve precision. This precision gain can be translated directly into sample size reduction and corresponding cost savings. We conduct a simulation study using genomic and clinical data gathered for breast cancer patients and find that adjusting for clinical factors alone provides a gain in precision of 5-6%, adjusting for genomic factors alone provides a similar gain (5%), and combining the two yields a 2-3% additional gain over only adjusting for clinical covariates."}, {"title": "A Statistical Framework to Predict Functional Non-Coding Regions in the Human Genome Through Integrated Analysis of Annotation Data", "url": "https://www.biorxiv.org/content/early/2015/04/15/018093", "tag": "Bioinformatics", "abstract": "Identifying functional regions in the human genome is a major goal in human genetics. Great efforts have been made to functionally annotate the human genome either through computational predictions, such as genomic conservation, or high-throughput experiments, such as the ENCODE project. These efforts have resulted in a rich collection of functional annotation data of diverse types that need to be jointly analyzed for integrated interpretation and annotation. Here we present GenoCanyon, a whole-genome annotation method that performs unsupervised statistical learning using 22 computational and experimental annotations thereby inferring the functional potential of each position in the human genome. With GenoCanyon, we are able to predict many of the known functional regions. The ability of predicting functional regions as well as its generalizable statistical framework makes GenoCanyon a unique and powerful tool for whole-genome annotation. The GenoCanyon web server is available at http://genocanyon.med.yale.edu"}, {"title": "A multi-method approach for proteomic network inference in 11 human cancers", "url": "https://www.biorxiv.org/content/early/2015/04/15/015214", "tag": "Bioinformatics", "abstract": "Protein expression and post-translational modification levels are tightly regulated in neoplastic cells to maintain cellular processes known as ?cancer hallmarks?. The first Pan-Cancer initiative of The Cancer Genome Atlas (TCGA) Research Network has aggregated protein expression profiles for 3,467 patient samples from 11 tumor types using the antibody based reverse phase protein array (RPPA) technology. The resultant proteomic data can be utilized to computationally infer protein-protein interaction (PPI) networks and to study the commonalities and differences across tumor types. In this study, we compare the performance of 13 established network inference methods in their capacity to retrieve literature-curated pathway interactions from RPPA data. We observe that no single method has the best performance in all tumor types, but a group of six methods, including diverse techniques such as correlation, mutual information, and regression, consistently rank highly among the tested methods. A consensus network from this high-performing group reveals that signal transduction events involving receptor tyrosine kinases (RTKs), the RAS/MAPK pathway, and the PI3K/AKT/mTOR pathway, as well as innate and adaptive immunity signaling, are the most significant PPIs shared across all tumor types. Our results illustrate the utility of the RPPA platform as a tool to study proteomic networks in cancer."}, {"title": "Interrogating conserved elements of diseases using Boolean combinations of orthologous phenotypes", "url": "https://www.biorxiv.org/content/early/2015/04/13/017947", "tag": "Bioinformatics", "abstract": "Conserved genetic programs often predate the homologous structures and phenotypes to which they give rise; eyes, for example, have evolved several dozen times, but their development seems to involve a common set of conserved genes. Recently, the concept of orthologous phenotypes (or phenologs) offered a quantitative way to describe this property. Phenologs are phenotypes or diseases from separate species who share an unexpectedly large set of their associated gene orthologs. It has been shown that the phenotype pairs which make up a phenolog are mutually predictive in terms of the genes involved. Recently, we demonstrated the ranking of gene\u2013phenotype association predictions using multiple phenologs from an array of species. In this work, we demonstrate a computational method which provides a more targeted view of the conserved pathways which give rise to diseases. Our approach involves the generation of synthetic pseudo-phenotypes made up of Boolean combinations (union, intersection, and difference) of the gene sets for phenotypes from our database. We search for diseases that overlap significantly with these Boolean phenotypes, and find a number of highly predictive combinations. While set unions produce less specific predictions (as expected), intersection and difference-based combinations appear to offer insights into extremely specific aspects of target diseases. For example, breast cancer is predicted by zebrafish methylmercury response minus metal ion response, with predictions MT-COI, JUN, SOD2, GADD45B, and BAX all involved in the pro-apoptotic response to reactive oxygen species, thought to be a key player in cancer. We also demonstrate predictions from Arabidopsis Boolean phenotypes for increased brown adipose tissue in mouse (salt stress response's intersection with sucrose stimulus response); and for human myopathy (red light response minus water deprivation response). We demonstrate the ranking of predictions for human holoprosencephaly from the set intersections between each pair of a variety of closely-related zebrafish phenotypes. Our results suggest that Boolean phenolog combinations may provide a more informed insight into the conserved pathways underlying diseases than either regular phenologs or the na\u00efve Bayes approach."}, {"title": "Bayesian Modeling of Epigenetic Variation in Multiple Human Cell Types", "url": "https://www.biorxiv.org/content/early/2015/04/13/018028", "tag": "Bioinformatics", "abstract": "With high-throughput sequencing data generated for multiple epigenetic features in many cell types, a chief challenge is to explain the dynamics in multiple epigenomes that lead to differential regulation and phenotypes. We introduce a Bayesian framework for jointly annotating multiple epigenomes and detecting differential regulation among multiple cell types. Our method, IDEAS (integrative and discriminative epigenome annotation system), achieves superior power by modeling both position and cell type specific epigenetic activities. Using ENCODE data sets in 6 cell types, we identified epigenetic variation strongly associated with differential gene expression. The detected regions are significantly enriched in disease genetic variants with much stronger enrichment scores than achievable by existing methods, and the enriched phenotypes are highly relevant to the corresponding cell types. IDEAS is a powerful tool for integrative epigenome annotation and detection of variation, which could be of important utility in elucidating the interplay between genetics, gene regulation and diseases."}, {"title": "CIDANE: Comprehensive isoform discovery and abundance estimation", "url": "https://www.biorxiv.org/content/early/2015/04/12/017939", "tag": "Bioinformatics", "abstract": "We present CIDANE, a novel framework for genome-based transcript reconstruction and quantification from RNA-seq reads. CIDANE assembles transcripts with significantly higher sensitivity and precision than existing tools, while competing in speed with the fastest methods. In addition to reconstructing transcripts ab initio, the algorithm also allows to make use of the growing annotation of known splice sites, transcription start and end sites, or full-length transcripts, which are available for most model organisms. CIDANE supports the integrated analysis of RNA-seq and additional gene-boundary data and recovers splice junctions that are invisible to other methods. CIDANE is available at http://ccb.jhu.edu/software/cidane/."}, {"title": "Choosing subsamples for sequencing studies by minimizing the average distance to the closest leaf", "url": "https://www.biorxiv.org/content/early/2015/04/09/017822", "tag": "Bioinformatics", "abstract": "Imputation of genotypes in a study sample can make use of sequenced or densely genotyped external reference panels consisting of individuals that are not from the study sample. It can also employ internal reference panels, incorporating a subset of individuals from the study sample itself. Internal panels offer an advantage over external panels, as they can reduce imputation errors arising from genetic dissimilarity between a population of interest and a second, distinct population from which the external reference panel has been constructed. As the cost of next-generation sequencing decreases, internal reference panel selection is becoming increasingly feasible. However, it is not clear how best to select individuals to include in such panels. We introduce a new method for selecting an internal reference panel???minimizing the average distance to the closest leaf (ADCL)???and compare its performance relative to an earlier algorithm: maximizing phylogenetic diversity (PD). Employing both simulated data and sequences from the 1000 Genomes Project, we show that ADCL provides a significant improvement in imputation accuracy, especially for imputation of sites with low-frequency alleles. This improvement in imputation accuracy is robust to changes in reference panel size, marker density, and length of the imputation target region."}, {"title": "MEGSA: A powerful and flexible framework for analyzing mutual exclusivity of tumor mutations", "url": "https://www.biorxiv.org/content/early/2015/04/09/017731", "tag": "Bioinformatics", "abstract": "The central challenge in tumor sequencing studies is to identify driver genes and pathways, investigate their functional relationships and nominate drug targets. The efficiency of these analyses, particularly for infrequently mutated genes, is compromised when patients carry different combinations of driver mutations. Mutual exclusivity analysis helps address these challenges. To identify mutually exclusive gene sets (MEGS), we developed a powerful and flexible analytic framework based on a likelihood ratio test and a model selection procedure. Extensive simulations demonstrated that our method outperformed existing methods for both statistical power and the capability of identifying the exact MEGS, particularly for highly imbalanced MEGS. Our method can be used for de novo discovery, pathway-guided searches or for expanding established small MEGS. We applied our method to the whole exome sequencing data for fourteen cancer types from The Cancer Genome Atlas (TCGA). We identified multiple previously unreported non-pairwise MEGS in multiple cancer types. For acute myeloid leukemia, we identified a novel MEGS with five genes (FLT3, IDH2, NRAS, KIT and TP53) and a MEGS (NPM1, TP53 and RUX1) whose mutation status was strongly associated with survival (P=6.7\u00d710-4). For breast cancer, we identified a significant MEGS consisting of TP53 and four infrequently mutated genes (ARID1A, AKT1, MED23 and TBL1XR1), providing support for their role as cancer drivers. Keywords: Mutual exclusivity, oncogenic pathways, driver genes, tumor sequencing"}, {"title": "Phylogenetic tree inference from local gene content", "url": "https://www.biorxiv.org/content/early/2015/04/08/017699", "tag": "Bioinformatics", "abstract": "Abstract Background: Complete genome sequences provide many new characters suitable for studying phylogenetic relationships. The limitations of the single sequence-based phylogenetic reconstruction prompted the efforts to build trees based on genome-wide properties, such as the fraction of shared orthologous genes or conservation of adjoining gene pairs. Gene content-based phylogenies, however, have their own biases: most notably, differential losses and horizontal transfers of genes interfere with phylogenetic signal, each in their own way, and special measures need to be taken to eliminate these types of noise. Results: We expand the repertoire of genome-wide traits available for phylogeny building, by developing a practical approach for measuring local gene conservation in two genomes. We counted the number of orthologous genes shared by chromosomal neighborhoods (\u201cbins\u201d), and built the phylogeny of 63 prokaryotic genomes on this basis. The tree correctly resolved all well-established clades, and also suggested the monophyly of firmicutes, which tend to be split in other genome-based trees. Conclusions: Our measure of local gene order conservation extracts strong phylogenetic signal. This new measure appears to be substantially resistant to the observed instances of gene loss and horizontal transfer, two evolutionary forces which can cause systematic biases in the genome-based phylogenies."}, {"title": "The Impact of High-Performance Computing Best Practice Applied to Next-Generation Sequencing Workflows", "url": "https://www.biorxiv.org/content/early/2015/04/07/017665", "tag": "Bioinformatics", "abstract": "High Performance Computing (HPC) Best Practice offers opportunities to implement lessons learned in areas such as computational chemistry and physics in genomics workflows, specifically Next-Generation Sequencing (NGS) workflows. In this study we will briefly describe how distributed-memory parallelism can be an important enhancement to the performance and resource utilization of NGS workflows. We will illustrate this point by showing results on the parallelization of the Inchworm module of the Trinity RNA-Seq pipeline for de novo transcriptome assembly. We show that these types of applications can scale to thousands of cores. Time scaling as well as memory scaling will be discussed at length using two RNA-Seq datasets, targeting the Mus musculus (mouse) and the Axolotl (Mexican salamander). Details about the efficient MPI communication and the impact on performance will also be shown. We hope to demonstrate that this type of parallelization approach can be extended to most types of bioinformatics workflows, with substantial benefits. The efficient, distributed-memory parallel implementation eliminates memory bottlenecks and dramatically accelerates NGS analysis. We further include a summary of programming paradigms available to the bioinformatics community, such as C++/MPI."}, {"title": "Index-based map-to-sequence alignment in large eukaryotic genomes", "url": "https://www.biorxiv.org/content/early/2015/04/06/017194", "tag": "Bioinformatics", "abstract": "Resolution of complex repeat structures and rearrangements in the assembly and analysis of large eukaryotic genomes is often aided by a combination of high-throughput sequencing and mapping technologies (e.g. optical restriction mapping). In particular, mapping technologies can generate sparse maps of large DNA fragments (150 kbp--2 Mbp) and thus provide a unique source of information for disambiguating complex rearrangements in cancer genomes. Despite their utility, combining high-throughput sequencing and mapping technologies has been challenging due to the lack of efficient and freely available software for robustly aligning maps to sequences. Here we introduce two new map-to-sequence alignment algorithms that efficiently and accurately align high-throughput mapping datasets to large, eukaryotic genomes while accounting for high error rates. In order to do so, these methods (OPTIMA for glocal and OPTIMA-Overlap for overlap alignment) exploit the ability to create efficient data structures that index continuous-valued mapping data while accounting for errors. We also introduce an approach for evaluating the significance of alignments that avoids expensive permutation-based tests while being agnostic to technology-dependent error rates. Our benchmarking results suggest that OPTIMA and OPTIMA-Overlap outperform state-of-the-art approaches in sensitivity (1.6--2X improvement) while simultaneously being more efficient (170--200%) and precise in their alignments (99% precision). These advantages are independent of the quality of the data, suggesting that our indexing approach and statistical evaluation are robust and provide improved sensitivity while guaranteeing high precision."}, {"title": "An integrative somatic mutation analysis to identify pathways linked with survival outcomes across 19 cancer types", "url": "https://www.biorxiv.org/content/early/2015/04/06/017582", "tag": "Bioinformatics", "abstract": "Identification of altered pathways that are clinically relevant across human cancers is a key challenge in cancer genomics. We developed a network-based algorithm to integrate somatic mutation data with gene networks and pathways, in order to identify pathways altered by somatic mutations across cancers. We applied our approach to The Cancer Genome Atlas (TCGA) dataset of somatic mutations in 4,790 cancer patients with 19 different types of malignancies. Our analysis identified cancer-type-specific altered pathways enriched with known cancer-relevant genes and drug targets. Consensus clustering using gene expression datasets that included 4,870 patients from TCGA and multiple independent cohorts confirmed that the altered pathways could be used to stratify patients into subgroups with significantly different clinical outcomes. Of particular significance, certain patient subpopulations with poor prognosis were identified because they had specific altered pathways for which there are available targeted therapies. These findings could be used to tailor and intensify therapy in these patients, for whom current therapy is suboptimal."}, {"title": "Isoform-level Ribosome Occupancy Estimation Guided by Transcript Abundance with Ribomap", "url": "https://www.biorxiv.org/content/early/2015/04/04/017509", "tag": "Bioinformatics", "abstract": "Ribosome profiling is a recently developed high-throughput sequencing technique that captures approximately 30 bp long ribosome-protected mRNA fragments during translation. Because of alternative splicing and repetitive sequences, a ribosome-protected read may map to many places in the transcriptome, leading to discarded or arbitrary mappings when standard approaches are used. We present a technique and software that addresses this problem by assigning reads to potential origins proportional to estimated transcript abundance. This yields a more accurate estimate of ribosome profiles compared with a na\u00efve mapping. Ribomap is available as open source at http://www.cs.cmu.edu/\u223cckingsf/software/ribomap."}, {"title": "FinisherSC : A repeat-aware tool for upgrading de-novo assembly using long reads", "url": "https://www.biorxiv.org/content/early/2015/04/04/010215", "tag": "Bioinformatics", "abstract": "We introduce FinisherSC, which is a repeat-aware and scalable tool for upgrading de-novo assembly using long reads. Experiments with real data suggest that FinisherSC can provide longer and higher quality contigs than existing tools while maintaining high concordance."}, {"title": "Un-complicating protein complex prediction.", "url": "https://www.biorxiv.org/content/early/2015/04/01/017376", "tag": "Bioinformatics", "abstract": "Identification of protein complexes from proteomic experiments is crucial to understand not only their function but also the principles of cellular organization. Advances in experimental techniques have enabled the construction of large-scale protein-protein interaction networks, and computational methods have been developed to analyze high-throughput data. In most cases several parameters are introduced that have to be trained before application. But how do we select the parameter values when there are no training data available? How many data do we need to properly train a method. How is the performance of a method affected when we incorrectly select the parameter values? The above questions, although important to determine the applicability of a method, are most of the time overlooked. We highlight the importance of such an analysis by investigating how limited knowledge, in the form of incomplete training data, affects the performance of parametric protein-complex prediction algorithms. Furthermore, we develop a simple non-parametric method that does not rely on the existence of training data and we compare it with the parametric alternatives. Using datasets from yeast and fly we demonstrate that parametric methods trained with limited data provide sub-optimal predictions, while our non-parametric method performs better or is on par with the parametric alternatives. Overall, our analysis questions, at least for the specific problem, whether parametric methods provide significantly better results than non-parametric ones to justify the additional effort for applying them."}, {"title": "Conservation of expression regulation throughout the animal kingdom", "url": "https://www.biorxiv.org/content/early/2015/04/01/007252", "tag": "Bioinformatics", "abstract": "Following the increase in available sequenced genomes, tissue-specific transcriptomes are being determined for a rapidly growing number of highly diverse species. Traditionally, only the transcriptomes of related species with equivalent tissues have been compared. Such an analysis is much more challenging over larger evolutionary distances when complementary tissues cannot readily be defined. Here, we present a method for the cross-species mapping of tissue-specific and developmental gene expression patterns across a wide range of animals, including many non-model species. Our approach maps gene expression patterns between species without requiring the definition of homologous tissues. With the help of this mapping, gene expression patterns can be compared even across distantly related species. In our survey of 36 datasets across 27 species, we detected conserved expression programs on all taxonomic levels, both within animals and between the animals and their closest unicellular relatives, the choanoflagellates. We found that the rate of change in tissue expression patterns is a property of gene families. Our findings open new avenues of study for the comparison and transfer of knowledge between different species."}, {"title": "b\u00edogo: a simple high-performance bioinformatics toolkit for the Go language", "url": "https://www.biorxiv.org/content/early/2015/03/27/005033", "tag": "Bioinformatics", "abstract": "b\u00edogo is a framework designed to ease development and maintenance of computationally intensive bioinformatics applications. The library is written in the Go programming language, a garbage-collected, strictly typed compiled language with built in support for concurrent processing, and performance comparable to C and Java. It provides a variety of data types and utility functions to facilitate manipulation and analysis of large scale genomic and other biological data. b\u00edogo uses a concise and expressive syntax, lowering the barriers to entry for researchers needing to process large data sets with custom analyses while retaining computational safety and ease of code review. We believe b\u00edogo provides an excellent environment for training and research in computational biology because of its combination of strict typing, simple and expressive syntax, and high performance."}, {"title": "MMR: A Tool for Read Multi-Mapper Resolution", "url": "https://www.biorxiv.org/content/early/2015/03/26/017103", "tag": "Bioinformatics", "abstract": "Motivation: Mapping high throughput sequencing data to a reference genome is an essential step for most analysis pipelines aiming at the computational analysis of genome and transcriptome sequencing data. Breaking ties between equally well mapping locations poses a severe problem not only during the alignment phase, but also has significant impact on the results of downstream analyses. We present the multimapper resolution (MMR) tool that infers optimal mapping locations from the coverage density of other mapped reads. Results: Filtering alignments with MMR can significantly improve the performance of downstream analyses like transcript quantitation and differential testing. We illustrate that the accuracy (Spearman correlation) of transcript quantification increases by 17% when using reads of length 51. In addition, MMR decreases the alignment file sizes by more than 50% and this leads to a reduced running time of the quantification tool. Our efficient implementation of the MMR algorithm is easily applicable as a post-processing step to existing alignment files in BAM format. Its complexity scales linearly with the number of alignments and requires no further inputs. Supplementary Material: Source code and documentation are available for download at http://github.com/ratschlab/mmr. Supplementary text and figures, comprehensive testing results and further information can be found at http://bioweb.me/mmr."}, {"title": "Large-Scale Search of Transcriptomic Read Sets with Sequence Bloom Trees", "url": "https://www.biorxiv.org/content/early/2015/03/26/017087", "tag": "Bioinformatics", "abstract": "Enormous databases of short-read RNA-seq sequencing experiments such as the NIH Sequence Read Archive (SRA) are now available. However, these collections remain difficult to use due to the inability to search for a particular expressed sequence. A natural question is which of these experiments contain sequences that indicate the expression of a particular sequence such as a gene isoform, lncRNA, or uORF. However, at present this is a computationally demanding question at the scale of these databases. We introduce an indexing scheme, the Sequence Bloom Tree (SBT), to support sequence-based querying of terabase-scale collections of thousands of short-read sequencing experiments. We apply SBT to the problem of finding conditions under which query transcripts are expressed. Our experiments are conducted on a set of 2652 publicly available RNA-seq experiments contained in the NIH for the breast, blood, and brain tissues, comprising 5 terabytes of sequence. SBTs of this size can be queried for a 1000 nt sequence in 19 minutes using less than 300 MB of RAM, over 100 times faster than standard usage of SRA-BLAST and 119 times faster than STAR. SBTs allow for fast identification of experiments with expressed novel isoforms, even if these isoforms were unknown at the time the SBT was built. We also provide some theoretical guidance about appropriate parameter selection in SBT and propose a sampling-based scheme for potentially scaling SBT to even larger collections of files. While SBT can handle any set of reads, we demonstrate the effectiveness of SBT by searching a large collection of blood, brain, and breast RNA-seq files for all 214,293 known human transcripts to identify tissue-specific transcripts. The implementation used in the experiments below is in C++ and is available as open source at http://www.cs.cmu.edu/~ckingsf/software/bloomtree."}, {"title": "Software for the analysis and visualization of deep mutational scanning data", "url": "https://www.biorxiv.org/content/early/2015/03/25/013623", "tag": "Bioinformatics", "abstract": "Background Deep mutational scanning is a technique to estimate the impacts of mutations on a gene by using deep sequencing to count mutations in a library of variants before and after imposing a functional selection. The impacts of mutations must be inferred from changes in their counts after selection. Results I describe a software package, dms_tools, to infer the impacts of mutations from deep mutational scanning data using a likelihood-based treatment of the mutation counts. I show that dms_tools yields more accurate inferences on simulated data than simply calculating ratios of counts pre- and post-selection. Using dms_tools, one can infer the preference of each site for each amino acid given a single selection pressure, or assess the extent to which these preferences change under different selection pressures. The preferences and their changes can be intuitively visualized with sequence-logo-style plots created using an extension to weblogo. Conclusions dms_tools implements a statistically principled approach for the analysis and subsequent visualization of deep mutational scanning data."}, {"title": "Bio4j: a high-performance cloud-enabled graph-based data platform", "url": "https://www.biorxiv.org/content/early/2015/03/20/016758", "tag": "Bioinformatics", "abstract": "Background. Next Generation Sequencing and other high-throughput technologies have brought a revolution to the bioinformatics landscape, by offering sheer amounts of data about previously unaccessible domains in a cheap and scalable way. However, fast, reproducible, and cost-effective data analysis at such scale remains elusive. A key need for achieving it is being able to access and query the vast amount of publicly available data, specially so in the case of knowledge-intensive, semantically rich data: incredibly valuable information about proteins and their functions, genes, pathways, or all sort of biological knowledge encoded in ontologies remains scattered, semantically and physically fragmented. Methods and Results. Guided by this, we have designed and developed Bio4j. It aims to offer a platform for the integration of semantically rich biological data using typed graph models. We have modeled and integrated most publicly available data linked with proteins into a set of interdependent graphs. Data querying is possible through a data model aware Domain Specific Language implemented in Java, letting the user write typed graph traversals over the integrated data. A ready to use cloud-based data distribution, based on the Titan graph database engine is provided; generic data import code can also be used for in-house deployment. Conclusion. Bio4j represents a unique resource for the current Bioinformatician, providing at once a solution for several key problems: data integration; expressive, high performance data access; and a cost-effective scalable cloud deployment model."}, {"title": "LINKS: Scaffolding genome assemblies with kilobase-long nanopore reads", "url": "https://www.biorxiv.org/content/early/2015/03/20/016519", "tag": "Bioinformatics", "abstract": "Owing to the complexity of the assembly problem, we do not yet have complete genome sequences. The difficulty in assembling reads into finished genomes is exacerbated by sequence repeats and the inability of short reads to capture sufficient genomic information to resolve those problematic regions. Established and emerging long read technologies show great promise in this regard, but their current associated higher error rates typically require computational base correction and/or additional bioinformatics pre-processing before they could be of value. We present LINKS, the Long Interval Nucleotide K-mer Scaffolder algorithm, a solution that makes use of the information in error-rich long reads, without the need for read alignment or base correction. We show how the contiguity of an ABySS E. coli K-12 genome assembly could be increased over five-fold by the use of beta-released Oxford Nanopore Ltd. (ONT) long reads and how LINKS leverages long-range information in S. cerevisiae W303 ONT reads to yield an assembly with less than half the errors of competing applications. Re-scaffolding the colossal white spruce assembly draft (PG29, 20 Gbp) and how LINKS scales to larger genomes is also presented. We expect LINKS to have broad utility in harnessing the potential of long reads in connecting high-quality sequences of small and large genome assembly drafts. Availability: http://www.bcgsc.ca/bioinfo/software/links"}, {"title": "largeQvalue: A program for calculating FDR estimates with large datasets", "url": "https://www.biorxiv.org/content/early/2015/03/18/010074", "tag": "Bioinformatics", "abstract": "This is an implementation of the R statistical software qvalue package (Alan Dabney, John D. Storey and with assistance from Gregory R. Warnes (). qvalue: Q-value estimation for false discovery rate control. R package version 1.34.0.), designed for use with large datasets where memory or computation time is limiting. In addition to estimating p values adjusted for multiple testing, the software outputs a script which can be pasted into R to produce diagnostic plots and report parameter estimates. This program runs almost 30 times faster and requests substantially less memory than the qvalue package when analysing 10 million p values on a high performance cluster. The software has been used to control for the multiple testing of 390 million tests when analysing a full cis scan of RNA-seq exon level gene expression from the Eurobats project. The source code and links to executable files for linux and Mac OSX can be found here: https://github.com/abrown25/qvalue. Help for the package can be found by running ./largeQvalue --help."}, {"title": "Distributed Bayesian Networks Reconstruction on the Whole Genome Scale", "url": "https://www.biorxiv.org/content/early/2015/03/17/016683", "tag": "Bioinformatics", "abstract": "Background: Bayesian networks are directed acyclic graphical models widely used to represent the probabilistic relationships between random variables. Recently, they have been applied in various biological contexts, including gene regulatory networks and protein-protein interactions inference. Generally, learning Bayesian networks from experimental data is NP-hard, leading to widespread use of heuristic search methods giving suboptimal results. However, in cases when the acyclicity of the graph can be ensured, it is possible to find the optimal network in polynomial time. While our previously developed tool BNFinder implements polynomial time algorithm, reconstructing networks with the large amount of experimental data still leads to numerous days of the computations given single CPU. Results: In the present paper we propose parallelized algorithm designed for multi-core and distributed systems and its implementation in the improved version of BNFinder - our tool for learning optimal Bayesian networks. The new algorithm has been tested on simulated datasets as well as different experimental data showing that it has much better efficiency of parallelization than the previous version. When tested on the DREAM datasets in comparison with other methods, BNFinder gives consistently the best results in terms of the area under the ROC curve as well as in the number of positive predictions at the top of the prediction ranking. The latter is especially important for the purposes of the future experimental validation of the predictions. Conclusions: We show that the new method can be used to reconstruct networks in the size range of thousands of genes making it practically applicable to whole genome datasets of prokaryotic systems and large components of eukaryotic genomes. Our benchmarking results on realistic datasets indicate that the tool should be useful to wide audience of researchers interested in discovering dependencies in their large-scale transcriptomic datasets. Keywords: Bayesian networks learning; gene regulatory networks inference; distributed computing; DREAM challenge"}, {"title": "regionReport: Interactive reports for region-based analyses", "url": "https://www.biorxiv.org/content/early/2015/03/17/016659", "tag": "Bioinformatics", "abstract": "regionReport is a R package for generating detailed interactive reports from regions of the genome. The report includes quality-control checks, an overview of the results, an interactive table of the genomic regions, and reproducibility information. regionReport can easily be expanded with report templates for other specialized analyses. In particular, regionReport has an extensive report template for exploring derfinder results from annotation-agnostic RNA-seq differential expression analyses. regionReport is freely available via Bioconductor at bioconductor.org/packages/release/bioc/html/regionReport.html ."}, {"title": "Latent epistatic interaction model identifies loci associated with human working memory", "url": "https://www.biorxiv.org/content/early/2015/03/17/016576", "tag": "Bioinformatics", "abstract": "Epistatic interactions among genomic loci are expected to explain a large fraction of the heritability of complex diseases and phenotypic traits of living organisms. Although epistasis detection methods are continually being developed, the current state of the art is exhaustive search methods, which become infeasible when the number of analyzed loci is large. We develop a novel latent interaction-based selection method for polymorphic loci as the first stage of a two-stage epistasis detection approach. Given a continuous phenotype and a single-nucleotide polymorphism (SNP), we rank the SNPs according to their interaction potential. When tested on simulated datasets and compared to standard marginal association and exhaustive search methods, our procedure significantly outperforms main-effect heuristics, especially in the presence of linkage disequilibrium (LD), which is explicitly accounted for in our model. Applied to real human genotype data, we prioritized several SNP pairs as candidates for epistatic interactions that influence human working memory performance, some of which are known to be connected to this phenotype. The proposed method improves two-stage epistasis detection. Its linear runtime and increased statistical power contribute to reducing the computational complexity and to addressing some of the statistical challenges associated with the genome-wide search for epistatic loci."}, {"title": "SomaticSignatures: Inferring Mutational Signatures from Single Nucleotide Variants", "url": "https://www.biorxiv.org/content/early/2015/03/16/010686", "tag": "Bioinformatics", "abstract": "Mutational signatures are patterns in the occurrence of somatic single nucleotide variants (SNVs) that can reflect underlying mutational processes. The SomaticSignatures package provides flexible, interoperable, and easy-to-use tools that identify such signatures in cancer sequencing data. It facilitates large-scale, cross-dataset estimation of mutational signatures, implements existing methods for pattern decomposition, supports extension through user-defined methods and integrates with Bioconductor workflows. The R package SomaticSignatures is available as part of the Bioconductor project (R Core Team, 2014; Gentleman et al., 2004). Its documentation provides additional details on the methodology and demonstrates applications to biological datasets."}, {"title": "A complete bacterial genome assembled de novo using only nanopore sequencing data", "url": "https://www.biorxiv.org/content/early/2015/03/11/015552", "tag": "Bioinformatics", "abstract": "A method for de novo assembly of data from the Oxford Nanopore MinION instrument is presented which is able to reconstruct the sequence of an entire bacterial chromosome in a single contig. Initially, overlaps between nanopore reads are detected. Reads are then subjected to one or more rounds of error correction by a multiple alignment process employing partial order graphs. After correction, reads are assembled using the Celera assembler. Finally, the assembly is polished using signal-level data from the nanopore employing a novel hidden Markov model. We show that this method is able to assemble nanopore reads from Escherichia coli K-12 MG1655 into a single contig of length 4.6Mb permitting a full reconstruction of gene order. The resulting draft assembly has 98.4% nucleotide identity compared to the finished reference genome. After polishing the assembly with our signal-level HMM, the nucleotide identity is improved to 99.4%. We show that MinION sequencing data can be used to reconstruct genomes without the need for a reference sequence or data from other sequencing platforms."}, {"title": "Metassembler: Merging and optimizing de novo genome assemblies", "url": "https://www.biorxiv.org/content/early/2015/03/10/016352", "tag": "Bioinformatics", "abstract": "Genome assembly projects typically run multiple algorithms in an attempt to find the single best assembly, although those assemblies often have complementary, if untapped, strengths and weaknesses. We present our metassembler algorithm that merges multiple assemblies of a genome into a single superior sequence. We apply it to the four genomes from the Assemblathon competitions and show it consistently and substantially improves the contiguity and quality of each assembly. We also develop guidelines for metassembly by systematically evaluating 120 permutations of merging the top 5 assemblies of the first Assemblathon competition. The software is open-source at http://metassembler.sourceforge.net."}, {"title": "ISMapper: Identifying insertion sequences in bacterial genomes from short read sequence data", "url": "https://www.biorxiv.org/content/early/2015/03/10/016345", "tag": "Bioinformatics", "abstract": "Background Insertion sequences (IS) are small transposable elements, commonly found in bacterial genomes. Identifying the location of IS in bacterial genomes can be useful for a variety of purposes including epidemiological tracking and predicting antibiotic resistance. However IS are commonly present in multiple copies in a single genome, which complicates genome assembly and the identification of IS insertion sites. Here we present ISMapper, a mapping-based tool for identification of the site and orientation of IS insertions in bacterial genomes, direct from paired-end short read data. Results ISMapper was validated using three types of short read data: (i) simulated reads from a variety of species, (ii) Illumina reads from 5 isolates for which finished genome sequences were available for comparison, and (iii) Illumina reads from 7 Acinetobacter baumannii isolates for which predicted IS locations were tested using PCR. A total of 20 genomes, including 13 species and 32 distinct IS, were used for validation. ISMapper correctly identified 96% of known IS insertions in the analysis of simulated reads, and 98% in real Illumina reads. Subsampling of real Illumina reads to lower depths indicated ISMapper was reliable for average genome-wide read depths >20x. All ISAba1 insertions identified by ISMapper in the A. baumannii genomes were confirmed by PCR. In each A. baumannii genome, ISMapper successfully identified an IS insertion upstream of the ampC beta-lactamase that could explain phenotypic resistance to third-generation cephalosporins. The utility of ISMapper was further demonstrated by profiling genome-wide IS6110 insertions in 138 publicly available Mycobacterium tuberculosis genomes, revealing lineage-specific insertions and multiple insertion hotspots. Conclusions ISMapper provides a rapid and robust method for identifying IS insertion sites direct from short read data, with a high degree of accuracy demonstrated across a wide range of bacteria."}, {"title": "Revisiting the Structure/Function Relationships of H/ACA(-like) RNAs: A Unified Model for Euryarchaea and Crenarchaea", "url": "https://www.biorxiv.org/content/early/2015/03/10/016246", "tag": "Bioinformatics", "abstract": "A structural and functional classification of H/ACA and H/ACA-like motifs is proposed from the analysis of the H/ACA guide RNAs which have been identified previously in the genomes of Euryarchaea (Pyrococcus) and Crenarchaea (Pyrobaculum). A unified structure/function model is proposed based on the common structural determinants shared by H/ACA and H/ACA-like motifs in both Euryarchaea and Crenarchaea. Using a computational approach, structural and energetic rules for the guide-target RNA-RNA interactions are derived from structural and functional data on the H/ACA RNP particles. H/ACA(-like) motifs found in Pyrococcus are evaluated through the classification and their biological relevance is discussed. Extra-ribosomal targets found in both Pyrococcus and Pyrobaculum are presented as testable gene candidates which might support the hypothesis of a gene regulation mediated by H/ACA(-like) guide RNAs."}, {"title": "Differential expression analysis of RNA sequencing data by incorporating non-exonic mapped reads", "url": "https://www.biorxiv.org/content/early/2015/03/07/016196", "tag": "Bioinformatics", "abstract": "Background RNA sequencing (RNA-seq) is a powerful tool for genome-wide expression profiling of biological samples with the advantage of high-throughput and high resolution. There are many existing algorithms nowadays for quantifying expression levels and detecting differential gene expression, but none of them takes the misaligned reads that are mapped to non-exonic regions into account. We developed a novel algorithm, XBSeq, where a statistical model was established based on the assumption that observed signals are the convolution of true expression signals and sequencing noises. The mapped reads in non-exonic regions are considered as sequencing noises, which follows a Poisson distribution. Given measureable observed and noise signals from RNA-seq data, true expression signals, assuming governed by the negative binomial distribution, can be delineated and thus the accurate detection of differential expressed genes. Results We implemented our novel XBSeq algorithm and evaluated it by using a set of simulated expression datasets under different conditions, using a combination of negative binomial and Poisson distributions with parameters derived from real RNA-seq data. We compared the performance of our method with other commonly used differential expression analysis algorithms. We also evaluated the changes in true and false positive rates with variations in biological replicates, differential fold changes, and expression levels in non-exonic regions. We also tested the algorithm on a set of real RNA-seq data where the common and different detection results from different algorithms were reported. Conclusions In this paper, we proposed a novel XBSeq, a differential expression analysis algorithm for RNA-seq data that takes non-exonic mapped reads into consideration. When background noise is at baseline level, the performance of XBSeq and DESeq are mostly equivalent. However, our method surpasses DESeq and other algorithms with the increase of non-exonic mapped reads. Only in very low read count condition XBSeq had a slightly higher false discovery rate, which may be improved by adjusting the background noise effect in this situation. Taken together, by considering non-exonic mapped reads, XBSeq can provide accurate expression measurement and thus detect differential expressed genes even in noisy conditions."}, {"title": "Interactive analysis and quality assessment of single-cell copy-number variations", "url": "https://www.biorxiv.org/content/early/2015/03/06/011346", "tag": "Bioinformatics", "abstract": "We present an open-source visual-analytics web platform, Ginkgo (http://qb.cshl.edu/ginkgo), for the interactive analysis and quality assessment of single-cell copy-number alterations. Ginkgo automatically constructs copy-number profiles of individual cells from mapped reads, as well as constructing phylogenetic trees of related cells. We validate Ginkgo by reproducing the results of five major studies and examine the data characteristics of three commonly used single-cell amplification techniques to conclude DOP-PCR to be the most consistent for CNV analysis."}, {"title": "Similarity Estimation Between DNA Sequences Based on Local Pattern Histograms of Binary Images", "url": "https://www.biorxiv.org/content/early/2015/03/05/016089", "tag": "Bioinformatics", "abstract": "Graphical representation of DNA sequences is one of the most popular techniques of alignment-free sequence comparison. In this article, we propose a new method for extracting features of DNA sequences represented by binary images, in which we estimate the similarity between DNA sequences by the frequency histograms of local bitmap patterns on the images. Our method has linear time complexity for the length of DNA sequences, which is practical even for comparison of long sequences. We tested five distance measures to estimate sequence similarities and found that histogram intersection and Manhattan distance are most appropriate for our method among them."}, {"title": "Modeling Linkage Disequilibrium Increases Accuracy of Polygenic Risk Scores", "url": "https://www.biorxiv.org/content/early/2015/03/04/015859", "tag": "Bioinformatics", "abstract": "Polygenic risk scores have shown great promise in predicting complex disease risk, and will become more accurate as training sample sizes increase. The standard approach for calculating risk scores involves LD-pruning markers and applying a P-value threshold to association statistics, but this discards information and may reduce predictive accuracy. We introduce a new method, LDpred, which infers the posterior mean causal effect size of each marker using a prior on effect sizes and LD information from an external reference panel. Theory and simulations show that LDpred outperforms the pruning/thresholding approach, particularly at large sample sizes. Accordingly, prediction R2 increased from 20.1% to 25.3% in a large schizophrenia data set and from 9.8% to 12.0% in a large multiple sclerosis data set. A similar relative improvement in accuracy was observed for three additional large disease data sets and when predicting in non-European schizophrenia samples. The advantage of LDpred over existing methods will grow as sample sizes increase."}, {"title": "BamHash: a checksum program for verifying the integrity of sequence data", "url": "https://www.biorxiv.org/content/early/2015/03/03/015867", "tag": "Bioinformatics", "abstract": "Summary: Large resequencing projects require a significant amount of storage for raw sequences, as well as alignment files. Since the raw sequences are redundant once the alignment has been generated, it is possible to keep only the alignment files. We present BamHash, a checksum based method to ensure that the read pairs in FASTQ files match exactly the read pairs stored in BAM files, regardless of the ordering of reads. BamHash can be used to verify the integrity of the files stored and discover any discrepancies. Thus, BamHash can be used to determine if it is safe to delete the FASTQ files storing raw sequencing reads after alignment, without the loss of data. Availability and Implementation: The software is implemented in C++, GPL licensed and available at https://github.com/DecodeGenetics/BamHash Contact pmelsted@hi.is"}, {"title": "Leveraging transcript quantification for fast computation of alternative splicing profiles", "url": "https://www.biorxiv.org/content/early/2015/02/26/008763", "tag": "Bioinformatics", "abstract": "Alternative splicing plays an essential role in many cellular processes and bears major relevance in the understanding of multiple diseases, including cancer. High-throughput RNA sequencing allows genome-wide analyses of splicing across multiple conditions. However, the increasing number of available datasets represents a major challenge in terms of computation time and storage requirements. We describe SUPPA, a computational tool to calculate relative inclusion values of alternative splicing events, exploiting fast transcript quantification. SUPPA accuracy is comparable and sometimes superior to standard methods using simulated as well as real RNA sequencing data compared to experimentally validated events. We assess the variability in terms of the choice of annotation and provide evidence that using complete transcripts rather than more transcripts per gene provides better estimates. Moreover, SUPPA coupled with de novo transcript reconstruction methods does not achieve accuracies as high as using quantification of known transcripts, but remains comparable to existing methods. Finally, we show that SUPPA is more than 1000 times faster than standard methods. Coupled with fast transcript quantification, SUPPA provides inclusion values at a much higher speed than existing methods without compromising accuracy, thereby facilitating the systematic splicing analysis of large datasets with limited computational resources. The software is implemented in Python 2.7 and is available under the MIT license at https://bitbucket.org/regulatorygenomicsupf/suppa"}, {"title": "Quality assessment for different haplotyping methods and GWAS sensitivity to phasing errors", "url": "https://www.biorxiv.org/content/early/2015/02/24/015669", "tag": "Bioinformatics", "abstract": "In this report we present a multimarker association tool (Flash) based on a novel algorithm to generate haplotypes from raw genotype data. It belongs to the entropy minimization class of methods and is composed of a two stage deterministic - heuristic part and of a optional stochastic optimization. This algorithm is able to scale up well to handle huge datasets with faster performance than the competing technologies such as BEAGLE and MACH while maintaining a comparable accuracy. A quality assessment of the results is carried out by comparing the switch error. Finally, the haplotypes are used to perform a haplotype-based Genome-wide Association Study (GWAS). The association results are compared with a multimarker and a single SNP association test performed with Plink. Our experiments confirm that the multimarker association test can be more powerful than the single SNP one as stated in the literature. Moreover, Flash and Plink show similar results for the multimarker association test but Flash speeds up the computation time of about an order of magnitude using 5 SNP size haplotypes."}, {"title": "The In Silico Genotyper (ISG): an open-source pipeline to rapidly identify and annotate nucleotide variants for comparative genomics applications", "url": "https://www.biorxiv.org/content/early/2015/02/20/015578", "tag": "Bioinformatics", "abstract": "The identification and annotation of nucleotide variants, including insertions/deletions and single nucleotide polymorphisms (SNPs), from whole genome sequence data is important for studies of bacterial evolution, comparative genomics, and phylogeography. The in silico Genotyper (ISG) represents a parallel, tested, open source tool that can perform these functions and scales well to thousands of bacterial genomes. ISG is written in Java and requires MUMmer (Delcher, et al., 2003), BWA (Li and Durbin, 2009), and GATK (McKenna, et al., 2010) for full functionality. The source code and compiled binaries are freely available from https://github.com/TGenNorth/ISGPipeline under a GNU General Public License. Benchmark comparisons demonstrate that ISG is faster and more flexible than comparable tools."}, {"title": "The Use of Distributions in SBML Models", "url": "https://www.biorxiv.org/content/early/2015/02/20/015503", "tag": "Bioinformatics", "abstract": "In this technical note we describe modifications to Antimony (Biochemical model specification language) that allows modelers to use the SBML distributions package. In addition the article describes best practice for using distributions, including when they should and should not be used."}, {"title": "SHAPE directed RNA folding", "url": "https://www.biorxiv.org/content/early/2015/02/20/015537", "tag": "Bioinformatics", "abstract": "Summary: Chemical mapping experiments allow for nucleotide resolution assessment of RNA structure. We demonstrate that different strategies of integrating probing data with thermodynamics- based RNA secondary structure prediction algorithms can be implemented by means of soft constraints. This amounts to incorporating suitable pseudo-energies into the standard energy model for RNA secondary structures. As a showcase application for this new feature of the ViennaRNA Package we compare three distinct, previously published strategies to utilize SHAPE reactivities for structure prediction. The new tool is benchmarked on a set of RNAs with known reference structure. Availability and implementation: The capability for SHAPE directed RNA folding is part of the upcoming release of the ViennaRNA Package 2.2, for which a preliminary release is already freely available at http://www.tbi.univie.ac.at/RNA."}, {"title": "Differential Evolution Approach to Detect Recent Admixture", "url": "https://www.biorxiv.org/content/early/2015/02/19/015446", "tag": "Bioinformatics", "abstract": "The genetic structure of human populations is extraordinarily complex and of fundamental importance to studies of anthropology, evolution, and medicine. As increasingly many individuals are of mixed origin, there is an unmet need for tools that can infer multiple origins. Misclassification of such individuals can lead to incorrect and costly misinterpretations of genomic data, primarily in disease studies and drug trials. We present an advanced tool to infer ancestry that can identify the biogeographic origins of highly mixed individuals. reAdmix can incorporate individual's knowledge of ancestors (e.g. having some ancestors from Turkey or a Scottish grandmother). reAdmix is an online tool available at http://chcb.saban-chla.usc.edu/reAdmix/."}, {"title": "Flaw or discovery? Calculating exact p-values for genome-wide association studies in inbred populations", "url": "https://www.biorxiv.org/content/early/2015/02/17/015339", "tag": "Bioinformatics", "abstract": "Motivation: Genome-wide association studies have been conducted in inbred populations where the sample size is small. The ordinary association p-values and multiple testing correction therefore become questionable, as the detected genetic effect may or may not be due to chance, depending on the minor allele frequency distribution across the genome. Instead of permutation testing, marker-specific false positive rate can be analytically calculated in inbred populations without heterozygotes. Results: Solutions of exact p-values for genome-wide association studies in inbred populations were derived and implemented. An example is presented to illustrate that the marker-specific experiment-wise p-value varies as the genome-wide minor allele frequency distribution changes. A simulation using real Arabidopsis thaliana genome indicates that the use of exact p-values improves detection power and reduces inflation due to population structure. An analysis of a defense-related case-control phenotype using the exact p-values revealed the causal locus, where markers with higher MAFs had smaller p-values than the top variants with lower MAFs in ordinary genome-wide association analysis. Availability and Implementation: Project URL: https://r-forge.r-project.org/projects/statomics/. The R package p.exact: https://r-forge.r-project.org/R/?group_id=2030."}, {"title": "A GENE FEATURE ENUMERATION APPROACH FOR DESCRIBING HLA ALLELE POLYMORPHISM", "url": "https://www.biorxiv.org/content/early/2015/02/15/015222", "tag": "Bioinformatics", "abstract": "HLA genotyping via next generation sequencing (NGS) poses challenges for the use of HLA allele names to analyze and discuss sequence polymorphism. NGS will identify many new synonymous and non- coding HLA sequence variants. Allele names identify the types of nucleotide polymorphism that define an allele (non-synonymous, synonymous and non-coding changes), but do not describe how polymorphism is distributed among the individual features (the flanking untranslated regions, exons and introns) of a gene. Further, HLA alleles cannot be named in the absence of antigen-recognition domain (ARD) encoding exons. Here, a system for describing HLA polymorphism in terms of HLA gene features (GFs) is proposed. This system enumerates the unique nucleotide sequences for each GF in an HLA gene, and records these in a GF enumeration notation that allows both more granular dissection of allele-level HLA polymorphism, and the discussion and analysis of GFs in the absence of ARD-encoding exon sequences."}, {"title": "ViennaNGS: A toolbox for building efficient next-generation sequencing analysis pipelines", "url": "https://www.biorxiv.org/content/early/2015/02/13/013011", "tag": "Bioinformatics", "abstract": "Recent achievements in next-generation sequencing (NGS) technologies lead to a high demand for reuseable software components to easily compile customized analysis workflows for big genomics data. We present ViennaNGS, an integrated collection of Perl modules focused on building efficient pipelines for NGS data processing. It comes with functionality for extracting and converting features from common NGS file formats, computation and evaluation of read mapping statistics, as well as normalization of RNA abundance. Moreover, ViennaNGS provides software components for identification and characterization of splice junctions from RNA-seq data, parsing and condensing sequence motif data, automated construction of Assembly and Track Hubs for the UCSC genome browser, as well as wrapper routines for a set of commonly used NGS command line tools."}, {"title": "Learning Immune-Defectives Graph through Group Tests", "url": "https://www.biorxiv.org/content/early/2015/02/11/015149", "tag": "Bioinformatics", "abstract": "This paper deals with an abstraction of a unified problem of drug discovery and pathogen identification. Here, the ``lead compounds'' are abstracted as inhibitors, pathogenic proteins as defectives, and the mixture of ``ineffective'' chemical compounds and non-pathogenic proteins as normal items. A defective could be immune to the presence of an inhibitor in a test. So, a test containing a defective is positive iff it does not contain its ``associated'' inhibitor. The goal of this paper is to identify the defectives, inhibitors, and their ``associations'' with high probability, or in other words, learn the Immune Defectives Graph (IDG). We propose a probabilistic non-adaptive pooling design, a probabilistic two-stage adaptive pooling design and decoding algorithms for learning the IDG. For the two-stage adaptive-pooling design, we show that the sample complexity of the number of tests required to guarantee recovery of the inhibitors, defectives and their associations with high probability, i.e., the upper bound, exceeds the proposed lower bound by a logarithmic multiplicative factor in the number of items. For the non-adaptive pooling design, in the large inhibitor regime, we show that the upper bound exceeds the proposed lower bound by a logarithmic multiplicative factor in the number of inhibitors."}, {"title": "Relationship between tumor grade and geometrical complexity in prostate cancer", "url": "https://www.biorxiv.org/content/early/2015/02/08/015016", "tag": "Bioinformatics", "abstract": "Prostate cancer exhibits high mathematical complexity due to the disruption of tissue architecture. An important part of the diagnostic of prostate tumor samples is the histological evaluation of cellular and glandular organization. The Gleason grade and score, a commonly used prognostic indicator of patient outcome, is based on the match of glandular architectural patterns with standard patterns. Unfortunately, the subjective nature of visual grading leads to variations in scoring by different pathologists. We proposed the fractal dimension of the lumen and the Lempel-Zip complexity of the histopathological patterns as useful descriptors aiding pathologist to standardize histological classification and thus prognosis and therapy planning."}, {"title": "Assembly by Reduced Complexity (ARC): a hybrid approach for targeted assembly of homologous sequences.", "url": "https://www.biorxiv.org/content/early/2015/02/07/014662", "tag": "Bioinformatics", "abstract": "Analysis of High-throughput sequencing (HTS) data is a difficult problem, especially in the context of non-model organisms where comparison of homologous sequences may be hindered by the lack of a close reference genome. Current mapping-based methods rely on the availability of a highly similar reference sequence, whereas de novo assemblies produce anonymous (unannotated) contigs that are not easily compared across samples. Here, we present Assembly by Reduced Complexity (ARC) a hybrid mapping and assembly approach for targeted assembly of homologous sequences. ARC is an open-source project (http://ibest.github.io/ARC/) implemented in the Python language and consists of the following stages: 1) align sequence reads to reference targets, 2) use alignment results to distribute reads into target specific bins, 3) perform assemblies for each bin (target) to produce contigs, and 4) replace previous reference targets with assembled contigs and iterate. We show that ARC is able to assemble high quality, unbiased mitochondrial genomes seeded from 11 progressively divergent references, and is able to assemble full mitochondrial genomes starting from short, poor quality ancient DNA reads. We also show ARC compares favorably to de novo assembly of a large exome capture dataset for CPU and memory requirements; assembling 7,627 individual targets across 55 samples, completing over 1.3 million assemblies in less than 78 hours, while using under 32 Gb of system memory. ARC breaks the assembly problem down into many smaller problems, solving the anonymous contig and poor scaling inherent in some de novo assembly methods and reference bias inherent in traditional read mapping."}, {"title": "Histoimmunogenetics Markup Language 1.0: Reporting Next Generation Sequencing-based HLA and KIR Genotyping", "url": "https://www.biorxiv.org/content/early/2015/02/06/014951", "tag": "Bioinformatics", "abstract": "We present an electronic format for exchanging data for HLA and KIR genotyping with extensions for next-generation sequencing (NGS). This format addresses NGS data exchange by refining the Histoimmunogenetics Markup Language (HML) to conform to the proposed Minimum Information for Reporting Immunogenomic NGS Genotyping (MIRING) reporting guidelines (miring.immunogenomics.org). Our refinements of HML include two major additions. First, NGS is supported by new XML structures to capture additional NGS data and metadata required to produce a genotyping result, including analysis-dependent (dynamic) and method-dependent (static) components. A full genotype, consensus sequence, and the surrounding metadata are included directly, while the raw sequence reads and platform documentation are externally referenced. Second, genotype ambiguity is fully represented by integrating Genotype List Strings, which use a hierarchical set of delimiters to represent allele and genotype ambiguity in a complete and accurate fashion. HML also continues to enable the transmission of legacy methods (e.g. site-specific oligonucleotide, sequence-specific priming, and sequence based typing (SBT)), adding features such as allowing multiple group-specific sequencing primers, and fully leveraging techniques that combine multiple methods to obtain a single result, such as SBT integrated with NGS."}, {"title": "PDMQ - Protein Digestion Multi Query software tool to perform in silico digestion of protein/peptide sequences", "url": "https://www.biorxiv.org/content/early/2015/02/02/014019", "tag": "Bioinformatics", "abstract": "Motivation: In silico enzymatic digestion tools mostly can be used for digestion of single sequence query, which means a significant limitation in their utility when a number of sequences need to be processed. The other limitation of these applications is the selection options of restriction enzymes that are usually allow only simultaneous digestion. Non-conventional proteins such as cereal prolamins require multi-enzyme multi step digestion, and for cereal proteomics experts this type of application is missing. Results: PDMQ, Protein Digestion Multi Query application was developed having multi query and multi enzyme options and that way can be customized for any digestion protocol. Availability and implementation: PDMQ is implemented in C# using the .NET framework and can be downloaded from http://www.agrar.mta.hu/_user/browser/File/bioinformatics/ProteinDigestion_v0_0_0_15.rar"}, {"title": "SWORD - a highly efficient protein database search", "url": "https://www.biorxiv.org/content/early/2015/02/02/014654", "tag": "Bioinformatics", "abstract": "Protein database search is one of the fundamental problems in bioinformatics. For decades, it has been explored and solved using different exact and heuristic approaches. However, exponential growth of data in size in recent years has brought significant challenges in improving already existing algorithms. BLASTP has been the most successful tool for protein database search, but is also becoming a bottleneck in many applications. Due to that, many different approaches have been developed to complement or replace BLASTP. In this paper, we present SWORD, an efficient protein database search implementation that runs 3-4 faster than BLASTP in the sensitive mode and up to 18 faster in the fast and less accurate mode and also provides guaranteed optimal alignments for candidate sequences. SWORD is designed to be used in nearly all database search environments,but is especially suitable for large databases. Its sensitivity exceeds that of BLASTP for majority of input datasets."}, {"title": "Gene regulatory network inference from perturbed time-series expression data via ordered dynamical expansion of non-steady state actors", "url": "https://www.biorxiv.org/content/early/2015/01/30/007906", "tag": "Bioinformatics", "abstract": "The reconstruction of gene regulatory networks from gene expression data has been the subject of intense research activity. A variety of models and methods have been developed to address different aspects of this important problem. However, these techniques are often difficult to scale, are narrowly focused on particular biological and experimental platforms, and require experimental data that are typically unavailable and difficult to ascertain. The more recent availability of higher-throughput sequencing platforms, combined with more precise modes of genetic perturbation, present an opportunity to formulate more robust and comprehensive approaches to gene network inference. Here, we propose a step-wise framework for identifying gene-gene regulatory interactions that expand from a known point of genetic or chemical perturbation using time series gene expression data. This novel approach sequentially identifies non-steady state genes post-perturbation and incorporates them into a growing series of low-complexity optimization problems. The governing ordinary differential equations of this model are rooted in the biophysics of stochastic molecular events that underlie gene regulation, delineating roles for both protein and RNA-mediated gene regulation. We show the successful application of our core algorithms for network inference using simulated and real datasets."}, {"title": "Sincell: Bioconductor package for the statistical assessment of cell-state hierarchies from single-cell RNA-seq data", "url": "https://www.biorxiv.org/content/early/2015/01/27/014472", "tag": "Bioinformatics", "abstract": "Summary: Cell differentiation processes are achieved through a continuum of hierarchical intermediate cell-states that might be captured by single-cell RNA seq. Existing computational approaches for the assessment of cell-state hierarchies from single-cell data might be formalized under a general framework composed of i) a metric to assess cell-to-cell similarities (combined or not with a dimensionality reduction step), and ii) a graph-building algorithm (optionally making use of a cells-clustering step). Sincell R package implements a methodological toolbox allowing flexible workflows under such framework. Furthermore, Sincell contributes new algo-rithms to provide cell-state hierarchies with statistical support while accounting for stochastic factors in single-cell RNA seq. Graphical representations and functional association tests are provided to interpret hierarchies. Sincell functionalities are illustrated in a real case study where its ability to discriminate noisy from stable cell-state hierarchies is demonstrated. Availability and implementation: Sincell is an open-source R/Bioconductor package available at http://bioconductor.org/packages/3.1/bioc/html/sincell.html. A detailed vignette describing functions and workflows is provided with the package."}, {"title": "Alignment by numbers: sequence assembly using compressed numerical representations", "url": "https://www.biorxiv.org/content/early/2015/01/27/011940", "tag": "Bioinformatics", "abstract": "Motivation: DNA sequencing instruments are enabling genomic analyses of unprecedented scope and scale, widening the gap between our abilities to generate and interpret sequence data. Established methods for computational sequence analysis generally use nucleotide-level resolution of sequences, and while such approaches can be very accurate, increasingly ambitious and data-intensive analyses are rendering them impractical for applications such as genome and metagenome assembly. Comparable analytical challenges are encountered in other data-intensive fields involving sequential data, such as signal processing, in which dimensionality reduction methods are routinely used to reduce the computational burden of analyses. We therefore seek to address the question of whether it is possible to improve the efficiency of sequence alignment by applying dimensionality reduction methods to numerically represented nucleotide sequences. Results: To explore the applicability of signal transformation and dimensionality reduction methods to sequence assembly, we implemented a short read aligner and evaluated its performance against simulated high diversity viral sequences alongside four existing aligners. Using our sequence transformation and feature selection approach, alignment time was reduced by up to 14-fold compared to uncompressed sequences and without reducing alignment accuracy. Despite using highly compressed sequence transformations, our implementation yielded alignments of similar overall accuracy to existing aligners, outperforming all other tools tested at high levels of sequence variation. Our approach was also applied to the de novo assembly of a simulated diverse viral population. Our results demonstrate that full sequence resolution is not a prerequisite of accurate sequence alignment and that analytical performance can be retained and even enhanced through appropriate dimensionality reduction of sequences."}, {"title": "Linked annotations: a middle ground for manual curation of biomedical databases and text corpora", "url": "https://www.biorxiv.org/content/early/2015/01/23/014274", "tag": "Bioinformatics", "abstract": "Annotators of text corpora and biomedical databases carry out the same labor-intensive task to manually extract structured data from unstructured text. Tasks are needlessly repeated because text corpora are widely scattered. We envision that a linked annotation resource unifying many corpora could be a game changer. Such an open forum will help focus on novel annotations and on optimally benefiting from the energy of many experts. As proof-of-concept, we annotated protein subcellular localization in 100 abstracts cited by UniProtKB. The detailed comparison between our new corpus and the original UniProtKB annotations revealed sustained novel annotations for 42% of the entries (proteins). In a unified linked annotation resource these could immediately extend the utility of text corpora beyond the text-mining community. Our example motivates the central idea that linked annotations from text corpora can complement database annotations."}, {"title": "Segmentation of Noisy Signals Generated By a Nanopore", "url": "https://www.biorxiv.org/content/early/2015/01/23/014258", "tag": "Bioinformatics", "abstract": "Nanopore-based single-molecule sequencing techniques exploit ionic current steps produced as biomolecules pass through a pore to reconstruct properties of the sequence. A key task in analyzing complex nanopore data is discovering the boundaries between these steps, which has traditionally been done in research labs by hand. We present an automated method of analyzing nanopore data, by detecting regions of ionic current corresponding to the translocation of a biomolecule, and then segmenting the region. The segmenter uses a divide-and-conquer method to recursively discover boundary points, with an implementation that works several times faster than real time and that can handle low-pass filtered signals."}, {"title": "JAFFA: High sensitivity transcriptome-focused fusion gene detection.", "url": "https://www.biorxiv.org/content/early/2015/01/23/013698", "tag": "Bioinformatics", "abstract": "Genomic instability is a hallmark of cancer and, as such, structural alterations and fusion genes are common events in the cancer landscape. RNA sequencing (RNA-Seq) is a powerful method for profiling cancers, but current methods for identifying fusion genes are optimized for short reads. JAFFA (https://code.google.com/p/jaffa-project/) is a sensitive fusion detection method that clearly out-performs other methods with reads of 100bp or greater. JAFFA compares a cancer transcriptome to the reference transcriptome, rather than the genome, where the cancer transcriptome is inferred using long reads directly or by de novo assembling short reads."}, {"title": "Alternative splicing QTLs in European and African populations using Altrans, a novel method for splice junction quantification", "url": "https://www.biorxiv.org/content/early/2015/01/22/014126", "tag": "Bioinformatics", "abstract": "With the advent of RNA-sequencing technology we now have the power to detect different types of alternative splicing and how DNA variation affects splicing. However, given the short read lengths used in most population based RNA-sequencing experiments, quantifying transcripts accurately remains a challenge. Here we present a novel method, Altrans, for discovery of alternative splicing quantitative trait loci (asQTLs). To assess the performance of Altrans we compared it to Cufflinks, a well-established transcript quantification method. Simulations show that in the presence of transcripts absent from the annotation, Altrans performs better in quantifications than Cufflinks. We have applied Altrans and Cufflinks to the Geuvadis dataset, which comprises samples from European and African populations, and discovered (FDR = 1%) 1806 and 243 asQTLs with Altrans, and 1596 and 288 asQTLs with Cufflinks for Europeans and Africans, respectively. Although Cufflinks results replicated better across the two populations, this likely due to the increased sensitivity of Altrans in detecting harder to detect associations. We show that, by discovering a set of asQTLs in a smaller subset of European samples and replicating these in the remaining larger subset of Europeans, both methods achieve similar replication levels (94% and 98% replication in Altrans and Cufflinks, respectively). We find that method specific asQTLs are largely due to different types of alternative splicing events detected by each method. We overlapped the asQTLs with biochemically active regions of the genome and observed significant enrichments for many functional marks and variants in splicing regions, highlighting the biological relevance of the asQTLs identified. All together, we present a novel approach for discovering asQTLs that is a more direct assessment of splicing compared to other methods and is complementary to other transcript quantification methods."}, {"title": "Reproducibility Of Parameter Learning With Missing Observations in Naive Wnt Bayesian Network Trained on Normal/Adenomas Samples and Doxycycline Treated LS174T Cell Lines", "url": "https://www.biorxiv.org/content/early/2015/01/22/014076", "tag": "Bioinformatics", "abstract": "Recent efforts in predicting Wnt signaling activation via inference methods have helped in developing diagnostic models for therapeutic drug targeting. In this manuscript the reproducibility of parameter learning with missing observations in a Bayesian Network and its effect on prediction results for Wnt signaling activation is tested, while training the networks on doxycycline treated LS174T cell lines as well as normal and adenomas samples. This is done in order to check the effectiveness of using Bayesian Network as a tool for modeling Wnt pathway when certain observations are missing. Experimental analysis suggest that prediction results are reproducible with negligible deviations. Anomalies in estimated parameters are accounted for due to the Bayesian Network model. Also, an interesting case regarding usage of hypothesis testing came up while proving the statistical significance of different design setups of the BN model which was trained on the data. It was found that hypothesis testing may not be the correct way to check the significance between design setups for the aforementioned case, especially when the structure of the model is same. Finally, in comparison to the biologically inspired models, the naive bayesian model may give accurate results but this accuracy comes at the cost of loss of crucial biological knowledge which might help reveal hidden relations among intra/extracellular factors affecting the Wnt pathway."}, {"title": "MultiMeta: an R package for meta-analysing multi-phenotype genome-wide association studies", "url": "https://www.biorxiv.org/content/early/2015/01/16/013920", "tag": "Bioinformatics", "abstract": "Summary: As new methods for multivariate analysis of Genome Wide Association Studies (GWAS) become available, it is important to be able to combine results from different cohorts in a meta-analysis. The R package MultiMeta provides an implementation of the inverse-variance based method for meta-analysis, generalized to an n-dimensional setting. Availability: The R package MultiMeta can be downloaded from CRAN Contact: dragana.vuckovic@burlo.trieste.it"}, {"title": "Phylesystem: a git-based data store for community curated phylogenetic estimates", "url": "https://www.biorxiv.org/content/early/2015/01/16/013862", "tag": "Bioinformatics", "abstract": "Phylogenetic estimates from published studies can be archived using general platforms like Dryad or TreeBASE. Such services fulfill a crucial role in ensuring transparency and reproducibility in phylogenetic research. However, digital tree data files often require some editing (e.g. rerooting) to improve the accuracy and reusability of the phylogenetic statements. Furthermore, establishing the mapping between tip labels used in a tree and taxa in a single common taxonomy dramatically improves the ability of other researchers to reuse phylogenetic estimates. Because the process of curating a published phylogenetic estimate is not error-free, retaining a full record of the provenance of edits to a tree is crucial for openness, allowing editors to receive credit for their work, and making errors introduced during curation easier to correct. Here we report the development of software infrastructure to support the open curation of phylogenetic data by the community of biologists. The backend of the system provides an interface for the standard database operations of creating, reading, updating, and deleting records by making commits to a git repository. The record of the history of edits to a tree is preserved by git's version control features. Hosting this data store on GitHub provides open access to the data store using tools familiar to many developers. We have deployed a server running the \"phylesystem-api\", which wraps the interactions with git and GitHub. The Open Tree of Life project has also developed and deployed a JavaScript application that uses the phylesystem-api and other web services to enable input and curation of published phylogenetic statements."}, {"title": "SW#db: GPU-accelerated exact sequence similarity database search", "url": "https://www.biorxiv.org/content/early/2015/01/14/013805", "tag": "Bioinformatics", "abstract": "The deluge of next-generation sequencing (NGS) data and expanding database poses higher requirements for protein similarity search. State-of-the-art tools such as BLAST are not fast enough to cope with these requirements. Because of that it is necessary to create new algorithms that will be faster while keeping similar sensitivity levels. The majority of protein similarity search methods are based on a seed-and-extend approach which uses standard dynamic programming algorithms in the extend phase. In this paper we present a SW#db tool and library for exact similarity search. Although its running times, as standalone tool, are comparable to running times of BLAST it is primarily designed for the extend phase where there are reduced number of candidates in the database. It uses both GPU and CPU parallelization and when we measured multiple queries on Swiss-prot and Uniref90 databases SW#db was 4 time faster than SSEARCH, 6-10 times faster than CUDASW++ and more than 20 times faster than SSW."}, {"title": "SCIE: Information Extraction for Spinal Cord Injury Preclinical Experiments - A Webservice and Open Source Toolkit", "url": "https://www.biorxiv.org/content/early/2015/01/12/013458", "tag": "Bioinformatics", "abstract": "Translational neuroscience in the field of spinal cord injuries (SCI) faces a strong disproportion between immense preclinical research efforts and a lack of therapeutic approaches successful in human patients: Currently, preclinical research on SCI yields more than 3,000 new publications per year (8,000 when including the whole central nervous system, growing at an exponential rate), whereas none of the resulting therapeutic concepts has led to functional recovery of neural tissue in humans. Improving clinical researchers' information access therefore carries the potential to support more effective selection of promising therapy candidates from preclinical studies. Thus, automated in- formation extraction from scientific publications contributes to enabling meta studies and therapy grading by aggregating relevant information from the entire body of previous work on SCI. We present SCIE, an automated information extraction pipeline capable of detecting relevant information in SCI publications based on ontological entity and probabilistic relation detection. The input are plain text or PDF documents. As output, the user choses between an online visualization or a machine-readable format. Compared to human gold standard annotations, our system achieves an average extraction performance of 76 % precision and 52 % recall (F1-measure 0.59). An instance of the webservice is available at http://scie.sc.cit-ec.uni-bielefeld.de/. SCIE is free software licensed under the AGPL and can be downloaded for local installation at http: //opensource.cit-ec.de/projects/scie/."}, {"title": "Merging OpenLifeData with SADI services using Galaxy and Docker", "url": "https://www.biorxiv.org/content/early/2015/01/10/013615", "tag": "Bioinformatics", "abstract": "Semantic Web technologies have been widely applied in Life Sciences, for example by data providers like OpenLifeData and Web Services frameworks like SADI. The recent OpenLifeData2SADI project offers access to the OpenLifeData data store through SADI services. This paper shows how to merge data from OpenLifeData with other extant SADI services in the Galaxy bioinformatics analysis platform, making semantic data amenable to complex analyses, as a worked example demonstrates. The whole setting is reproducible through a Docker image that includes a pre-configured Galaxy server with data and workflows that constitute the worked example"}, {"title": "When Less is More: \"Slicing\" Sequencing Data Improves Read Decoding Accuracy and De Novo Assembly Quality", "url": "https://www.biorxiv.org/content/early/2015/01/03/013425", "tag": "Bioinformatics", "abstract": "Since the invention of DNA sequencing in the seventies, computational biologists have had to deal with the problem de novo genome assembly with limited (or insufficient) depth of sequencing. In this work, for the first time we investigate the opposite problem, that is, the challenge of dealing with excessive depth of sequencing. Specifically, we explore the effect of ultra-deep sequencing data in two domains: (i) the problem of decoding reads to BAC clones (in the context of the combinatorial pooling design proposed by our group), and (ii) the problem of de novo assembly of BAC clones. Using real ultra-deep sequencing data, we show that when the depth of sequencing increases over a certain threshold, sequencing errors make these two problems harder and harder (instead of easier, as one would expect with error-free data), and as a consequence the quality of the solution degrades with more and more data. For the first problem, we propose an effective solution based on \"divide and conquer\": we \"slice\" a large dataset into smaller samples of optimal size, decode each slice independently, then merge the results. Experimental results on over 15,000 barley BACs and over 4,000 cowpea BACs demonstrate a significant improvement in the quality of the decoding and the final assembly. For the second problem, we show for the first time that modern de novo assemblers cannot take advantage of ultra-deep sequencing data."}, {"title": "Accounting for experimental noise reveals that mRNA levels, amplified by post-transcriptional processes, largely determine steady-state protein levels in yeast", "url": "https://www.biorxiv.org/content/early/2014/12/26/009472", "tag": "Bioinformatics", "abstract": "Cells respond to their environment by modulating protein levels through mRNA transcription and post-transcriptional control. Modest observed correlations between global steady-state mRNA and protein measurements have been interpreted as evidence that mRNA levels determine roughly 40% of the variation in protein levels, indicating dominant post-transcriptional effects. However, the techniques underlying these conclusions, such as correlation and regression, yield biased results when data are noisy, missing systematically, and collinear---properties of mRNA and protein measurements---which motivated us to revisit this subject. Noise-robust analyses of 24 studies of budding yeast reveal that mRNA levels explain more than 85% of the variation in steady-state protein levels. Protein levels are not proportional to mRNA levels, but rise much more rapidly. Regulation of translation suffices to explain this nonlinear effect, revealing post-transcriptional amplification of, rather than competition with, transcriptional signals. These results substantially revise widely credited models of protein-level regulation, and introduce multiple noise-aware approaches essential for proper analysis of many biological phenomena."}, {"title": "RAX2: genome-wide detection of condition-associated transcription variation", "url": "https://www.biorxiv.org/content/early/2014/12/24/013201", "tag": "Bioinformatics", "abstract": "Almost all of mammalian genes have mRNA variants due to alternative promoters, alternative splice sites, and alternative cleavage and polyadenylation sites. In most cases, change in transcript due to choosing alternative cleavage and polyadenylation sites does not lead to change in protein sequence, while selection of alternative promoters and alternative splice sites would alter protein sequence. Nevertheless, all these alternations would give rise to different RNA isoforms. Selection of alternative RNA isoforms has been found to be associated with change in condition. For example, many studies have revealed that alternative cleavage and polyadenylation (poly(A)) are correlated to proliferation, differentiation, and cellular transformation. Thus, unlike gene expression in microarray, change in usage of splice sites or poly(A) sites associated with conditions does not involve differential expression and hence cannot be detected by differential analysis methods but can be done by association methods. Traditional association methods such as Pearson chi-square test and Fisher Exact test are single test methods and do not work on the RNA count data derived from replicate libraries. For this reason, we here developed a large-scale association method, called ranking analysis of chi-squares (RAX2). Simulations demonstrated that RAX2 worked well for finding association of changes in usage of poly(A) sites with condition change. We applied our RAX2 to our primary T-cell transcriptomic data of over 9899 tags scattered in 3812 genes and found that 1610 (16.3%) tags were associated in transcription with immune stimulation at FDR < 0.05. Analysis of two and three tags within genes revealed that under immune stimulation, short RNA isoforms were significantly preferably used. Like cell proliferation and division, short RNA isoforms are highly prioritized to be used for cell growth."}, {"title": "ENVIRONMENTS and EOL: identification of Environment Ontology terms in text and the annotation of the Encyclopedia of Life", "url": "https://www.biorxiv.org/content/early/2014/12/23/011403", "tag": "Bioinformatics", "abstract": "Summary: The association of organisms to their environments is a key issue in exploring biodiversity patterns. This knowledge has traditionally been scattered, but textual descriptions of taxa and their habitats are now being consolidated in centralized resources. However, structured annotations are needed to facilitate large-scale analyses. Therefore, we developed ENVIRONMENTS, a fast dictionary-based tagger capable of identifying Environment Ontology (ENVO) terms in text. We evaluate the accuracy of the tagger on a new manually curated corpus of 600 Encyclopedia Of Life (EOL) species pages. We use the tagger to associate taxa with environments by tagging EOL text content monthly, and integrate the results into the EOL to disseminate them to a broad audience of users. Availability and implementation: The software and the corpus are available under the open-source BSD and the CC-BY-NC-SA 3.0 licenses, respectively, at http://environments.hcmr.gr"}, {"title": "FORGE : A tool to discover cell specific enrichments of GWAS associated SNPs in regulatory regions.", "url": "https://www.biorxiv.org/content/early/2014/12/20/013045", "tag": "Bioinformatics", "abstract": "Genome wide association studies provide an unbiased discovery mechanism for numerous human diseases. However, a frustration in the analysis of GWAS is that the majority of variants discovered do not directly alter protein-coding genes. We have developed a simple analysis approach that detects the tissue-specific regulatory component of a set of GWAS SNPs by identifying enrichment of overlap with DNase I hotspots from diverse tissue samples. Functional element Overlap analysis of the Results of GWAS Experiments (FORGE) is available as a web tool and as standalone software and provides tabular and graphical summaries of the enrichments. Conducting FORGE analysis on SNP sets for 260 phenotypes available from the GWAS catalogue reveals numerous overlap enrichments with tissue\u2013specific components reflecting the known aetiology of the phenotypes as well as revealing other unforeseen tissue involvements that may lead to mechanistic insights for disease."}, {"title": "HISAT: Hierarchical Indexing for Spliced Alignment of Transcripts", "url": "https://www.biorxiv.org/content/early/2014/12/15/012591", "tag": "Bioinformatics", "abstract": "HISAT is a new, highly efficient system for alignment of sequences from RNA sequencing experiments that achieves dramatically faster performance than previous methods. HISAT uses a new indexing scheme, hierarchical indexing, which is based on the Burrows-Wheeler transform and the Ferragina-Manzini (FM) index. Hierarchical indexing employs two types of indexes for alignment: (1) a whole-genome FM index to anchor each alignment, and (2) numerous local FM indexes for very rapid extensions of these alignments. HISAT?s hierarchical index for the human genome contains 48,000 local FM indexes, each representing a genomic region of ~64,000 bp. The algorithm includes several customized alignment strategies specifically designed for mapping RNA-seq reads across multiple exons. In tests on a variety of real and simulated data sets, we show that HISAT is the fastest system currently available, approximately 50 times faster than TopHat2 and 12 times faster than GSNAP, with equal or better accuracy than any other method. Despite its very large number of indexes, HISAT requires only 4.3 Gigabytes of memory to align reads to the human genome. HISAT supports genomes of any size, including those larger than 4 billion bases. HISAT is available as free, open-source software from http://www.ccb.jhu.edu/software/hisat."}, {"title": "Polyester: simulating RNA-seq datasets with differential transcript expression", "url": "https://www.biorxiv.org/content/early/2014/12/12/006015", "tag": "Bioinformatics", "abstract": "Motivation: Statistical methods development for differential expression analysis of RNA sequencing (RNA-seq) requires software tools to assess accuracy and error rate control. Since true differential expression status is often unknown in experimental datasets, artificially-constructed datasets must be utilized, either by generating costly spike-in experiments or by simulating RNA-seq data. Results: Polyester is an R package designed to simulate RNA-seq data, beginning with an experimental design and ending with collections of RNA-seq reads. Its main advantage is the ability to simulate reads indicating isoform-level differential expression across biological replicates for a variety of experimental designs. Data generated by Polyester is a reasonable approximation to real RNA-seq data and standard differential expression workflows can recover differential expression set in the simulation by the user. Availability and Implementation: Polyester is freely available from Bioconductor (http://bioconductor.org/)."}, {"title": "Heterogeneous Network Edge Prediction: A Data Integration Approach to Prioritize Disease-Associated Genes", "url": "https://www.biorxiv.org/content/early/2014/12/11/011569", "tag": "Bioinformatics", "abstract": "The first decade of Genome Wide Association Studies (GWAS) has uncovered a wealth of disease-associated variants. Two important derivations will be the translation of this information into a multiscale understanding of pathogenic variants, and leveraging existing data to increase the power of existing and future studies through prioritization. We explore edge prediction on heterogeneous networks\u2014graphs with multiple node and edge types\u2014for accomplishing both tasks. First we constructed a network with 18 node types\u2014genes, diseases, tissues, pathophysiologies, and 14 MSigDB (molecular signatures database)collections\u2014and 19 edge types from high-throughput publicly-available resources. From this network composed of 40,343 nodes and 1,608,168 edges, we extracted features that describe the topology between specific genes and diseases. Next, we trained a model from GWAS associations and predicted the probability of association between each protein-coding gene and each of 29 well-studied complex diseases. The model, which achieved 132-fold enrichment in precision at 10% recall, outperformed any individual domain, highlighting the benefit of integrative approaches. We identified pleiotropy, transcriptional signatures of perturbations, pathways, and protein interactions as fundamental mechanisms explaining pathogenesis. Our method successfully predicted the results (with AUROC = 0.79) from a withheld multiple sclerosis (MS) GWAS despite starting with only 13 previously associated genes. Finally, we combined our network predictions with statistical evidence of association to propose four novel MS genes, three of which (JAK2, REL, RUNX3) validated on the masked GWAS. Furthermore, our predictions provide biological support highlighting REL as the causal gene within its gene-rich locus. Users can browse all predictions online (http://het.io). Heterogeneous network edge prediction effectively prioritized genetic associations and provides a powerful new approach for data integration across multiple domains."}, {"title": "DensiTree 2: Seeing Trees Through the Forest", "url": "https://www.biorxiv.org/content/early/2014/12/08/012401", "tag": "Bioinformatics", "abstract": "Motivation: Phylogenetic analysis like Bayesian MCMC or bootstrapping result in a collection of trees. Trees are discrete objects and it is generally difficult to get a mental grip on a distributions over trees. Visualisation tools like DensiTree can give good intuition on tree distributions. It works by drawing all trees in the set transparently thus highlighting areas where the tree in the set agrees. In this way, both uncertainty in clade heights and uncertainty in topology can be visualised. In our experience, a vanilla DensiTree can turn out to be misleading in that it shows too much uncertainty due to wrongly ordering taxa or due to unlucky placement of internal nodes. Results: DensiTree is extended to allow visualisation of meta-data associated with branches such as population size and evolutionary rates. Furthermore, geographic locations of taxa can be shown on a map, making it easy to visually check there is some geographic pattern in a phylogeny. Taxa orderings have a large impact on the layout of the tree set, and advances have been made in finding better orderings resulting in significantly more informative visualisations. We also explored various methods for positioning internal nodes, which can improve the quality of the image. Together, these advances make it easier to comprehend distributions over trees. Availability: DensiTree is freely available from http://compevol. auckland.ac.nz/software/."}, {"title": "QuASAR: Quantitative Allele Specific Analysis of Reads", "url": "https://www.biorxiv.org/content/early/2014/12/05/007492", "tag": "Bioinformatics", "abstract": "Expression quantitative trait loci (eQTL) studies have discovered thousands of genetic variants that regulate gene expression, enabling a better understanding of the functional role of non-coding sequences. However, eQTL studies are costly, requiring large sample sizes and genome-wide genotyping of each sample. In contrast, analysis of allele specific expression (ASE) is becoming a popular approach to detect the effect of genetic variation on gene expression, even within a single individual. This is typically achieved by counting the number of RNA-seq reads matching each allele at heterozygous sites and testing the null hypothesis of a 1:1 allelic ratio. In principle, when genotype information is not readily available it could be inferred from the RNA-seq reads directly. However, there are currently no existing methods that jointly infer genotypes and conduct ASE inference, while considering uncertainty in the genotype calls. We present QuASAR, Quantitative Allele Specific Analysis of Reads, a novel statistical learning method for jointly detecting heterozygous genotypes and inferring ASE. The proposed ASE inference step takes into consideration the uncertainty in the genotype calls while including parameters that model base-call errors in sequencing and allelic over-dispersion. We validated our method with experimental data for which high quality genotypes are available. Results for an additional dataset with multiple replicates at different sequencing depths demonstrate that QuASAR is a powerful tool for ASE analysis when genotypes are not available."}, {"title": "SpeedSeq: Ultra-fast personal genome analysis and interpretation", "url": "https://www.biorxiv.org/content/early/2014/12/05/012179", "tag": "Bioinformatics", "abstract": "Comprehensive interpretation of human genome sequencing data is a challenging bioinformatic problem that typically requires weeks of analysis, with extensive hands-on expert involvement. This informatics bottleneck inflates genome sequencing costs, poses a computational burden for large-scale projects, and impedes the adoption of time-critical clinical applications such as personalized cancer profiling and newborn disease diagnosis, where the actionable timeframe can measure in hours or days. We developed SpeedSeq, an open-source genome analysis platform that vastly reduces computing time. SpeedSeq accomplishes read alignment, duplicate removal, variant detection and functional annotation of a 50X human genome in <24 hours, even using one low-cost server. SpeedSeq offers competitive or superior performance to current methods for detecting germline and somatic single nucleotide variants (SNVs), indels, and structural variants (SVs) and includes novel functionality for SV genotyping, SV annotation, fusion gene detection, and rapid identification of actionable mutations. SpeedSeq will help bring timely genome analysis into the clinical realm."}, {"title": "Searching and Indexing Genomic Databases via Kernelization", "url": "https://www.biorxiv.org/content/early/2014/12/04/012161", "tag": "Bioinformatics", "abstract": "The rapid advance of DNA sequencing technologies has yielded databases of thousands of genomes. To search and index these databases effectively, it is important that we take advantage of the similarity between those genomes. Several authors have recently suggested searching or indexing only one reference genome and the parts of the other genomes where they differ. In this paper we survey the twenty-year history of this idea and discuss its relation to kernelization in parameterized complexity."}, {"title": "Reveel: large-scale population genotyping using low-coverage sequencing data", "url": "https://www.biorxiv.org/content/early/2014/11/28/011882", "tag": "Bioinformatics", "abstract": "Population low-coverage whole-genome sequencing is rapidly emerging as a prominent approach for discovering genomic variation and genotyping a cohort. This approach combines substantially lower cost than full-coverage sequencing with whole-genome discovery of low-allele-frequency variants, to an extent that is not possible with array genotyping or exome sequencing. However, a challenging computational problem arises when attempting to discover variants and genotype the entire cohort. Variant discovery and genotyping are relatively straightforward on a single individual that has been sequenced at high coverage, because the inference decomposes into the independent genotyping of each genomic position for which a sufficient number of confidently mapped reads are available. However, in cases where low-coverage population data are given, the joint inference requires leveraging the complex linkage disequilibrium patterns in the cohort to compensate for sparse and missing data in each individual. The potentially massive computation time for such inference, as well as the missing data that confound low-frequency allele discovery, need to be overcome for this approach to become practical. Here, we present Reveel, a novel method for single nucleotide variant calling and genotyping of large cohorts that have been sequenced at low coverage. Reveel introduces a novel technique for leveraging linkage disequilibrium that deviates from previous Markov-based models. We evaluate Reveel???s performance through extensive simulations as well as real data from the 1000 Genomes Project, and show that it achieves higher accuracy in low-frequency allele discovery and substantially lower computation cost than previous state-of-the-art methods."}, {"title": "FlexAID: Revisiting docking on non native-complex structures", "url": "https://www.biorxiv.org/content/early/2014/11/26/011791", "tag": "Bioinformatics", "abstract": "Small-molecule protein docking is an essential tool in drug design and to understand molecular recognition. In the present work we introduce FlexAID, a small-molecule docking algorithm that accounts for target side-chain flexibility and utilizes a soft scoring function, i.e. one that is not highly dependent on specific geometric criteria, based on surface complementarity. The pairwise energy parameters were derived from a large dataset of true positive poses and negative decoys from the PDBbind dataset through an iterative process using Monte Carlo simulations. The prediction of binding poses is tested using the independent Astex dataset while performance in virtual screening is evaluated using a subset of the DUD dataset. We compare FlexAID to AutoDock Vina, FlexX, and rDock in an extensive number of scenarios to understand the strengths and limitations of the different programs as well as to reported results for Glide, GOLD and DOCK6 where applicable. The most relevant among these scenarios is that of docking on flexible non native-complex structures where as is the case in reality, the target conformation in the bound form is not known a priori. We demonstrate that FlexAID, unlike other programs, is robust against increasing structural variability. FlexAID obtains equivalent sampling success as GOLD and performs better than AutoDock Vina or FlexX in all scenarios against non native- complex structures. FlexAID is better than rDock when there is at least one critical side-chain movement required upon ligand binding. In virtual screening, FlexAID rescored results are comparable to those of AutoDock Vina and rDock. The higher accuracy in flexible targets where critical movements are required, intuitive PyMOL-integrated graphical user interface and free source code as well as pre-compiled executables for Windows, Linux and Mac OS make FlexAID a welcome addition to the arsenal of existing small-molecule protein docking methods."}, {"title": "CNVkit: Copy number detection and visualization for targeted sequencing using off-target reads", "url": "https://www.biorxiv.org/content/early/2014/11/25/010876", "tag": "Bioinformatics", "abstract": "Germline copy number variants (CNVs) and somatic copy number alterations (SCNAs) are of significant importance in syndromic conditions and cancer. Massive parallel sequencing is increasingly used to infer copy number information from variations in the read depth in sequencing data. However, this approach has limitations in the case of targeted re-sequencing, which leaves gaps in coverage between the regions chosen for enrichment and introduces biases related to the efficiency of target capture and library preparation. We present a method for copy number detection, implemented in the software package CNVkit, that uses both the targeted reads and the nonspecifically captured off-target reads to infer copy number evenly across the genome. This combination achieves both exon-level resolution in targeted regions and sufficient resolution in the larger intronic and intergenic regions to identify copy number changes. In particular, we successfully inferred copy number at equivalent to 100-kilobase resolution genome-wide from a platform targeting as few as 293 genes. After normalizing read counts to a pooled reference, we evaluated and corrected for three sources of bias that explain most of the extraneous variability in the sequencing read depth: GC content, target footprint size and spacing, and repetitive sequences. We compared the performance of CNVkit to copy number changes identified by array comparative genomic hybridization. We packaged the components of CNVkit so that it is straightforward to use and provides visualizations, detailed reporting of significant features, and export options for compatibility with other software. Availability: http://github.com/etal/cnvkit"}, {"title": "Mixture models reveal multiple positional bias types in RNA-Seq data and lead to accurate transcript concentration estimates", "url": "https://www.biorxiv.org/content/early/2014/11/24/011767", "tag": "Bioinformatics", "abstract": "Quantification of RNA transcripts with RNA-Seq is inaccurate due to positional fragment bias, which is not represented appropriately by current statistical models of RNA-Seq data. This article introduces the Mix2 (rd. \"mixquare\") model, which uses a mixture of probability distributions to model the transcript specific positional fragment bias. The parameters of the Mix2 model can be efficiently trained with the Expectation Maximization (EM) algorithm resulting in simultaneous estimates of the transcript abundances and transcript specific positional biases. Experiments are conducted on synthetic data and the Universal Human Reference (UHR) and Brain (HBR) sample from the Microarray quality control (MAQC) data set. Comparing the correlation between qPCR and FPKM values to state-of-the-art methods Cufflinks and PennSeq we obtain an increase in R2 value from 0.44 to 0.6 and from 0.34 to 0.54. In the detection of differential expression between UHR and HBR the true positive rate increases from 0.44 to 0.71 at a false positive rate of 0.1. Finally, the Mix2 model is used to investigate biases present in the MAQC data. This reveals 5 dominant biases which deviate from the common assumption of a uniform fragment distribution. The Mix2 software is available at http://www.lexogen.com/fileadmin/uploads/bioinfo/mix2model.tgz."}, {"title": "Cell-Line Annotation on Europe PubMed Central", "url": "https://www.biorxiv.org/content/early/2014/11/24/011700", "tag": "Bioinformatics", "abstract": "A cell line is a cell culture developed from a single cell and therefore consisting of cells with a uniform genetic make-up. A cell line has an important role as a research resource such as organisms, antibodies, constructs, knockdown reagents, etc. Unique identification of cell lines in the biomedical literature is important for the reproducibility of science. As data citation, resource citation is also important for resource re-use. In this paper, we mention the challenges of identifying cell lines and describe a system for cell line annotation with preliminary results."}, {"title": "CLASS: Accurate and Efficient Splice Variant Annotation from RNA-seq Reads", "url": "https://www.biorxiv.org/content/early/2014/11/20/011718", "tag": "Bioinformatics", "abstract": "Next generation sequencing of cellular RNA is making it possible to characterize genes and alternative splicing in unprecedented detail. However, designing bioinformatics tools to capture splicing variation accurately has proven difficult. Current programs find major isoforms of a gene but miss finer splicing variations, or are sensitive but highly imprecise. We present CLASS, a novel open source tool for accurate genome-guided transcriptome assembly from RNA-seq reads. CLASS employs a splice graph to represent a gene and its splice variants, combined with a linear program to determine an accurate set of exons and efficient splice graph-based transcript selection algorithms. When compared against reference programs, CLASS had the best overall accuracy and could detect up to twice as many splicing events with precision similar to the best reference program. Notably, it was the only tool that produced consistently reliable transcript models for a wide range of applications and sequencing strategies, including very large data sets and ribosomal RNA-depleted samples. Lightweight and multi-threaded, CLASS required <3GB RAM and less than one day to analyze a 350 million read set, and is an excellent choice for transcriptomics studies, from clinical RNA sequencing, to alternative splicing analyses, and to the annotation of new genomes."}, {"title": "A novel computational approach for genome-wide prediction of small RNAs in bacteria", "url": "https://www.biorxiv.org/content/early/2014/11/19/011668", "tag": "Bioinformatics", "abstract": "Small regulatory RNAs (sRNAs) are the most abundant post-transcriptional regulators in bacteria. They serve ubiquitous roles that control nearly every aspects of bacterial physiology. Identification of important features from sRNAs sequences will guide the computational prediction of new sRNA sequences for a better understanding of the pervasive sRNA-mediated regulation in bacteria. In this study, we have performed systematic analyses of many sequence and structural features that are possibly related to sRNA properties and identified a subset of significant features that effectively discriminate sRNAs sequences from random sequences. we then used a neural network model that integrated these subfeatures on unlabeled testing datasets, and it had achieved a 92.2% recall and 89.8% specificity. Finally, we applied this prediction model for genome-wide identification of sRNAs-encoded genes using a sliding-window approach. We recovered multiple known sRNAs and hundreds of predicted new sRNAs. These candidate novel sRNAs deserve extensive study to better understand the sRNA-mediated regulatory network in bacteria."}, {"title": "FusionCatcher - a tool for finding somatic fusion genes in paired-end RNA-sequencing data", "url": "https://www.biorxiv.org/content/early/2014/11/19/011650", "tag": "Bioinformatics", "abstract": "FusionCatcher is a software tool for finding somatic fusion genes in paired-end RNA-sequencing data from human or other vertebrates. FusionCatcher achieves competitive detection rates and real-time PCR validation rates in RNA-sequencing data from tumor cells. FusionCatcher is available at http://code.google.com/p/fusioncatcher"}, {"title": "Triticeae resources in Ensembl Plants", "url": "https://www.biorxiv.org/content/early/2014/11/18/011585", "tag": "Bioinformatics", "abstract": "Recent developments in DNA sequencing have enabled the large and complex genomes of many crop species to be determined for the first time, even those previously intractable due to their polyploid nature. Indeed, over the course of the last two years, the genome sequences of several commercially important cereals, notably barley and bread wheat, have become available, as well as those of related wild species. While still incomplete, comparison to other, more completely assembled species suggests that coverage of genic regions is likely to be high. Ensembl Plants (http://plants.ensembl.org) is an integrative resource organising, analysing and visualising genome-scale information for important crop and model plants. Available data includes reference genome sequence, variant loci, gene models and functional annotation. For variant loci, individual and population genotypes, linkage information and, where available, phenotypic information, are shown. Comparative analyses are performed on DNA and protein sequence alignments. The resulting genome alignments and gene trees, representing the implied evolutionary history the gene family, are made available for visualisation and analysis. Driven by the use case of bread wheat, specific extensions to the analysis pipelines and web interface have recently been developed to support polyploid genomes. Data in Ensembl Plants is accessible through a genome browser incorporating various specialist interfaces for different data types, and through a variety of additional methods for programmatic access and data mining. These interfaces are consistent with those offered through the Ensembl interface for the genomes of non-plant species, including those of plant pathogens, pests and pollinators, facilitating the study of the plant in its environment."}, {"title": "Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2", "url": "https://www.biorxiv.org/content/early/2014/11/17/002832", "tag": "Bioinformatics", "abstract": "In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq data, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data. DESeq2 uses shrinkage estimation for dispersions and fold changes to improve stability and interpretability of the estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression and facilitates downstream tasks such as gene ranking and visualization. DESeq2 is available as an R/Bioconductor package."}, {"title": "A robust statistical framework for reconstructing genomes from metagenomic data", "url": "https://www.biorxiv.org/content/early/2014/11/15/011460", "tag": "Bioinformatics", "abstract": "We present software that reconstructs genomes from shotgun metagenomic sequences using a reference-independent approach. This method permits the identification of OTUs in large complex communities where many species are unknown. Binning reduces the complexity of a metagenomic dataset enabling many downstream analyses previously unavailable. In this study we developed MetaBAT, a robust statistical framework that integrates probabilistic distances of genome abundance with sequence composition for automatic binning. Applying MetaBAT to a human gut microbiome dataset identified 173 highly specific genomes bins including many representing previously unidentified species."}, {"title": "Circumstantial Evidence? Comparison of Statistical Learning Methods using Functional Annotations for Prioritizing Risk Variants", "url": "https://www.biorxiv.org/content/early/2014/11/14/011445", "tag": "Bioinformatics", "abstract": "Although technology has triumphed in facilitating routine genome re-sequencing, new challenges have been created for the data analyst. Genome scale surveys of human disease variation generate volumes of data that far exceed capabilities for laboratory characterization, and importantly also create a substantial burden of type I error. By incorporating a variety of functional annotations as predictors, such as regulatory and protein coding elements, statistical learning has been widely investigated as a mechanism for the prioritization of genetic variants that are more likely to be associated with complex disease. These methods offer a hope of identification of sufficiently large numbers of truly associated variants, to make cost-effective the large-scale functional characterization necessary to progress genome scale experiments. We compared the results from three published prioritization procedures which use different statistical learning algorithms and different predictors with regard to the quantity, type and coding of the functional annotations. In this paper we also explore different combinations of algorithm and annotation set. We train the models in 60% of the data and reserve the remainder for testing the accuracy. As an application, we tested which methodology performed the best for prioritizing sub-genome-wide-significant variants using data from the first and second rounds of a large schizophrenia meta-analysis by the Psychiatric Genomics Consortium. Results suggest that all methods have considerable (and similar) predictive accuracies (AUCs 0.64-0.71). However, predictive accuracy results obtained from the test set do not always reflect results obtained from the application to the schizophrenia meta-analysis. In conclusion, a variety of algorithms and annotations seem to have a similar potential to effectively enrich true risk variants in genome scale datasets, however none offer more than incremental improvement in prediction. We discuss how methods might be evolved towards the step change in the risk variant prediction required to address the impending bottleneck of the new generation of genome re-sequencing studies."}, {"title": "Mapping a viral phylogeny onto outbreak trees to improve host transmission inference", "url": "https://www.biorxiv.org/content/early/2014/11/11/010389", "tag": "Bioinformatics", "abstract": "Developing methods to reconstruct transmission histories for viral outbreaks could provide critical information to support locating sources of disease transmission. Phylogenetic methods used to measure the degree of relatedness among sequenced viral samples have proven useful in identifying potential outbreak sources. The complex nature of infectious disease, however, makes it difficult to assign a rigorously defined quantitative confidence value assessing the likelihood of a true direct transmission event using genetic data alone. A new method is presented to calculate a confidence value assessing the likelihood of a transmission event using both phylogenetic inference and limited knowledge of incubation and infectious duration times. The method is applied to simulations of a foot and mouth disease (FMD) outbreak to demonstrate how the combination of both phylogenetic and epidemiology data can be used to strengthen the assessment of the likelihood of direct transmission over methods using just phylogenetic data or infection timing data alone. The method is applied to a previous FMD outbreak to identify areas where over confidence in previously inferred direct transmission may exist. Combining knowledge from viral evolution and epidemiology within a single integrated transmission inference framework is an important approach to assess the potential likelihood of transmission events and makes clear how specific features of a virus' spread through the course of an outbreak will directly determine the potential for confidence in inferred host transmission links."}, {"title": "Tools and Methods from the Anopheles 16 Genome Project", "url": "https://www.biorxiv.org/content/early/2014/11/07/011205", "tag": "Bioinformatics", "abstract": "The dramatic reduction in sequencing costs has resulted in many initiatives to sequence certain organisms and populations. These initiatives aim to not only sequence and assemble genomes but also to perform a more broader analysis of the population structure. As part of the Anopheline Genome Consortium, which has a vested interest in studying anpopheline mosquitoes, we developed novel methods and tools to further the communities goals. We provide a brief description of these methods and tools as well as assess the contributions that each offers to the broader study of comparative genomics."}, {"title": "Annotating RNA motifs in sequences and alignments", "url": "https://www.biorxiv.org/content/early/2014/11/06/011197", "tag": "Bioinformatics", "abstract": "RNA performs a diverse array of important functions across all cellular life. These functions include important roles in translation, building translational machinery and maturing messenger RNA. More recent discoveries include the miRNAs and bacterial sRNAs that regulate gene expression, the thermosensors, riboswitches and other cis-regulatory elements that help prokaryotes sense their environment and eukaryotic piRNAs that suppress transposition. However, there can be a long period between the initial discovery of a RNA and determining its function. We present a bioinformatic approach to characterise RNA motifs, which are the central building blocks of RNA structure. These motifs can, in some instances, provide researchers with functional hypotheses for uncharacterised RNAs. Moreover, we introduce a new profile-based database of RNA motifs - RMfam - and illustrate its application for investigating the evolution and functional characterisation of RNA. All the data and scripts associated with this work is available from: https://github.com/ppgardne/RMfam"}, {"title": "Identifying highly-penetrant disease causal mutations using next generation sequencing: Guide to whole process", "url": "https://www.biorxiv.org/content/early/2014/11/06/011130", "tag": "Bioinformatics", "abstract": "Recent technological advances have created challenges for geneticists and a need to adapt to a wide range of new bioinformatics tools and an expanding wealth of publicly available data (e.g. mutation databases, software). This wide range of methods and a diversity of file formats used in sequence analysis is a significant issue, with a considerable amount of time spent before anyone can even attempt to analyse the genetic basis of human disorders. Another point to consider is although many possess \"just enough\" knowledge to analyse their data, they do not make full use of the tools and databases that are available and also do not know how their data was created. The primary aim of this review is to document some of the key approaches and provide an analysis schema to make the analysis process more efficient and reliable in the context of discovering highly penetrant causal mutations/genes. This review will also compare the methods used to identify highly penetrant variants when data is obtained from consanguineous individuals as opposed to non-consanguineous; and when Mendelian disorders are analysed as opposed to common-complex disorders."}, {"title": "Enhanced Transcriptome Maps from Multiple Mouse Tissues Reveal Evolutionary Constraint in Gene Expression for Thousands of Genes", "url": "https://www.biorxiv.org/content/early/2014/10/30/010884", "tag": "Bioinformatics", "abstract": "We characterized by RNA-seq the transcriptional profiles of a large and heterogeneous collection of mouse tissues, augmenting the mouse transcriptome with thousands of novel transcript candidates. Comparison with transcriptome profiles obtained in human cell lines reveals substantial conservation of transcriptional programs, and uncovers a distinct class of genes with levels of expression across cell types and species, that have been constrained early in vertebrate evolution. This core set of genes capture a substantial and constant fraction of the transcriptional output of mammalian cells, and participates in basic functional and structural housekeeping processes common to all cell types. Perturbation of these constrained genes is associated with significant phenotypes including embryonic lethality and cancer. Evolutionary constraint in gene expression levels is not reflected in the conservation of the genomic sequences, but it is associated with strong and conserved epigenetic marking, as well as to a characteristic post-transcriptional regulatory program in which sub-cellular localization and alternative splicing play comparatively large roles."}, {"title": "Genome-wide comparative analysis reveals human- mouse regulatory landscape and evolution", "url": "https://www.biorxiv.org/content/early/2014/10/30/010926", "tag": "Bioinformatics", "abstract": "Background: Because species-specific gene expression is driven by species-specific regulation, understanding the relationship between sequence and function of the regulatory regions in different species will help elucidate how differences among species arise. Despite active experimental and computational research, the relationships among sequence, conservation, and function are still poorly understood. Results: We compared transcription factor occupied segments (TFos) for 116 human and 35 mouse TFs in 546 human and 125 mouse cell types and tissues from the Human and the Mouse ENCODE projects. We based the map between human and mouse TFos on a one-to-one nucleotide cross-species mapper, bnMapper, that utilizes whole genome alignments (WGA). Our analysis shows that TFos are under evolutionary constraint, but a substantial portion (25.1% of mouse and 25.85% of human on average) of the TFos does not have a homologous sequence on the other species; this portion varies among cell types and TFs. Furthermore, 47.67% and 57.01% of the homologous TFos sequence shows binding activity on the other species for human and mouse respectively. However, 79.87% and 69.22% is repurposed such that it binds the same TF in different cells or different TFs in the same cells. Remarkably, within the set of TFos not showing conservation of occupancy, the corresponding genome regions in the other species are preferred locations of novel TFos. These events suggest that a substantial amount of functional regulatory sequences is exapted from other biochemically active genomic material. Despite substantial repurposing of TFos, we did not find substantial changes in their predicted target genes, suggesting that CRMs buffer evolutionary events allowing little or no change in the TF \u2013 target gene associations. Thus, the small portion of TFos with strictly conserved occupancy underestimates the degree of conservation of regulatory interactions. Conclusion: We mapped regulatory sequences from an extensive number of TFs and cell types between human and mouse. A comparative analysis of this correspondence unveiled the extent of the shared regulatory sequence across TFs and cell types under study. Importantly, a large part of the shared regulatory sequence repurposed on the other species. This sequence, fueled by turnover events, provides a strong case for exaptation in regulatory elements."}, {"title": "FORMAL: A model to identify organisms present in metagenomes using Monte Carlo Simulation", "url": "https://www.biorxiv.org/content/early/2014/10/28/010801", "tag": "Bioinformatics", "abstract": "One of the major goals in metagenomics is to identify organisms present in the microbial community from a huge set of unknown DNA sequences. This profiling has valuable applications in multiple important areas of medical research such as disease diagnostics. Nevertheless, it is not a simple task, and many approaches that have been developed are slow and depend on the read length of the DNA sequences. Here we introduce an innovative and agile approach which k-mer and Monte Carlo simulation to profile and report abundant organisms present in metagenomic samples and their relative abundance without sequence length dependencies. The program was tested with a simulated metagenomes, and the results show that our approach predicts the organisms in microbial communities and their relative abundance."}, {"title": "Using 2k + 2 bubble searches to find SNPs in k-mer graphs", "url": "https://www.biorxiv.org/content/early/2014/10/27/004507", "tag": "Bioinformatics", "abstract": "Single Nucleotide Polymorphism (SNP) discovery is an important preliminary for understanding genetic variation. With current sequencing methods we can sample genomes comprehensively. SNPs are found by aligning sequence reads against longer assembled references. De Bruijn graphs are efficient data structures that can deal with the vast amount of data from modern technologies. Recent work has shown that the topology of these graphs captures enough information to allow the detection and characterisation of genetic variants, offering an alternative to alignment-based methods. Such methods rely on depth-first walks of the graph to identify closing bifurcations. These methods are conservative or generate many false-positive results, particularly when traversing highly inter-connected (complex) regions of the graph or in regions of very high coverage. We devised an algorithm that calls SNPs in converted De Bruijn graphs by enumerating 2k + 2 cycles. We evaluated the accuracy of predicted SNPs by comparison with SNP lists from alignment based methods. We tested accuracy of the SNP calling using sequence data from sixteen ecotypes of Arabidopsis thaliana and found that accuracy was high. We found that SNP calling was even across the genome and genomic feature types. Using sequence based attributes of the graph to train a decision tree allowed us to increase accuracy of SNP calls further. Together these results indicate that our algorithm is capable of finding SNPs accurately in complex sub-graphs and potentially comprehensively from whole genome graphs. The source code for a C++ implementation of our algorithm is available under the GNU Public Licence v3 at: https://github.com/redayounsi/2kplus2"}, {"title": "A theoretical justification for single molecule peptide sequencing", "url": "https://www.biorxiv.org/content/early/2014/10/22/010587", "tag": "Bioinformatics", "abstract": "The proteomes of cells, tissues, and organisms reflect active cellular processes and change continuously in response to intracellular and extracellular cues. Deep, quantitative profiling of the proteome, especially if combined with mRNA and metabolite measurements, should provide an unprecedented view of cell state, better revealing functions and interactions of cell components. Molecular diagnostics and biomarker should benefit particularly from the accurate quantification of proteomes, since complex diseases like cancer change protein abundances and modifications. Currently, shotgun mass spectrometry is the primary technology for high-throughput protein identification and quantification; while powerful, it lacks high sensitivity and coverage. We draw parallels with next-generation DNA sequencing and propose a strategy, termed fluorosequencing, for sequencing peptides in a complex protein sample at the level of single molecules. In the proposed approach, millions of individual fluorescently labeled peptides are visualized in parallel, monitoring changing patterns of fluorescence intensity as N-terminal amino acids are sequentially removed, and using the resulting fluorescence signatures (fluorosequences) to uniquely identify individual peptides. We introduce a theoretical foundation for fluorosequencing, and by using Monte Carlo computer simulations, we explore its feasibility, anticipate the most likely experimental errors, quantify their potential impact, and discuss the broad potential utility offered by a high-throughput peptide sequencing technology."}, {"title": "Rapid Core-Genome Alignment and Visualization for Thousands of Intraspecific Microbial Genomes", "url": "https://www.biorxiv.org/content/early/2014/10/20/007351", "tag": "Bioinformatics", "abstract": "Though many microbial species or clades now have hundreds of sequenced genomes, existing whole-genome alignment methods do not efficiently handle comparisons on this scale. Here we present the Harvest suite of core-genome alignment and visualization tools for quickly analyzing thousands of intraspecific microbial strains. Harvest includes Parsnp, a fast core-genome multi-aligner, and Gingr, a dynamic visual platform. Combined they provide interactive core-genome alignments, variant calls, recombination detection, and phylogenetic trees. Using simulated and real data we demonstrate that our approach exhibits unrivaled speed while maintaining the accuracy of existing methods. The Harvest suite is open-source and freely available from: http://github.com/marbl/harvest."}, {"title": "GalDrive: Pipeline for comparative identification of driver mutations using the Galaxy framework", "url": "https://www.biorxiv.org/content/early/2014/10/19/010538", "tag": "Bioinformatics", "abstract": "Identification of driver mutations can lead to a better understanding of the molecular mechanisms associated with cancer. This can be a first step towards developing diagnostic and prognostic markers. Various driver mutation prediction tools rely on different algorithm for prediction and hence there is little consensus in the predictions. The input and output formats vary across the tools. It has been suggested that an ensemble approach that takes into account various prediction scores might perform better. There is a need for a tool that can run multiple such tools on a dataset in a more accessible and modular manner, whose output can then be combined to select consensus drivers. We developed wrappers for various driver mutation predictions tools using Galaxy based framework. In order to perform predictions using multiple tools on the same dataset, we also developed Galaxy based workflows to convert VCF format to tool specific formats. The tools are publicly available at: https://github.com/saketkc/galaxy_tools The workflows are available at: https://github.com/saketkc/galaxy_tools/tree/master/workflows"}, {"title": "Ancestry Composition: A Novel, Efficient Pipeline for Ancestry Deconvolution", "url": "https://www.biorxiv.org/content/early/2014/10/18/010512", "tag": "Bioinformatics", "abstract": "Ancestry deconvolution, the task of identifying the ancestral origin of chromosomal segments in admixed individuals, has important implications, from mapping disease genes to identifying candidate loci under natural selection. To date, however, most existing methods for ancestry deconvolution are typically limited to two or three ancestral populations, and cannot resolve contributions from populations related at a sub-continental scale. We describe Ancestry Composition, a modular three-stage pipeline that efficiently and accurately identifies the ancestral origin of chromosomal segments in admixed individuals. It assumes the genotype data have been phased. In the first stage, a support vector machine classifier assigns tentative ancestry labels to short local phased genomic regions. In the second stage, an autoregressive pair hidden Markov model simultaneously corrects phasing errors and produces reconciled local ancestry estimates and confidence scores based on the tentative ancestry labels. In the third stage, confidence estimates are recalibrated using isotonic regression. We compiled a reference panel of almost 10,000 individuals of homogeneous ancestry, derived from a combination of several publicly available datasets and over 8,000 individuals reporting four grandparents with the same country-of-origin from the member database of the personal genetics company, 23andMe, Inc., and excluding outliers identified through principal components analysis (PCA). In cross-validation experiments, Ancestry Composition achieves high precision and recall for labeling chromosomal segments across over 25 different populations worldwide."}, {"title": "Making sense of RNA-Seq data: from low-level processing to functional analysis", "url": "https://www.biorxiv.org/content/early/2014/10/17/010488", "tag": "Bioinformatics", "abstract": "Numerous methods of RNA-Seq data analysis have been developed, and there are more under active development. In this paper, our focus is on evaluating the impact of each processing stage; from pre-processing of sequencing reads to alignment/counting to count normalization to differential expression testing to downstream functional analysis, on the inferred functional pattern of biological response. We assess the impact of 6,912 combinations of technical and biological factors on the resulting signature of transcriptomic functional response. Given the absence of the ground truth, we use two complementary evaluation criteria: a) consistency of the functional patterns identified in two similar comparisons, namely effects of a naturally-toxic medium and a medium with artificially reconstituted toxicity, and b) consistency of results in RNA-Seq and microarray versions of the same study. Our results show that despite high variability at the low-level processing stage (read pre-processing, alignment and counting) and the differential expression calling stage, their impact on the inferred pattern of biological response was surprisingly low; they were instead overshadowed by the choice of the functional enrichment method. The latter have an impact comparable in magnitude to the impact of biological factors per se."}, {"title": "Publishing DisGeNET as Nanopublications", "url": "https://www.biorxiv.org/content/early/2014/10/16/010397", "tag": "Bioinformatics", "abstract": "The increasing and unprecedented publication rate in the biomedical field is a major bottleneck for discovery in Life Sciences. The scientific community cannot process assertions from biomedical publications and integrate them into the current knowledge at the same rate. The automatic extraction of assertions about entities and their relationships by text-mining the scientific literature is an extended approach to structure up-to-date knowledge. For knowledge integration, the publication of assertions in the Semantic Web is gaining adoption, but it opens new challenges regarding the tracking of the provenance, and how to ensure versioned data linking. Nanopublications are a new way of publishing structured data that consists of an assertion along with its provenance. Trusty URIs is a novel approach to make resources in the Web immutable, and to ensure the unambiguity of the data linking in the (semantic) Web. We present the publication of DisGeNET nanopublications as a new Linked Dataset implemented in combination of the Trusty URIs approach. DisGeNET is a database of human gene-disease associations from expert-curated databases and text-mining the scientific literature. With a series of illustrative queries we demonstrate its utility."}, {"title": "An open framework for biodiversity databases", "url": "https://www.biorxiv.org/content/early/2014/10/16/010405", "tag": "Bioinformatics", "abstract": "OpenBioMaps is a recently developed, open and free web application with distributed database background, operated by several universities and national parks. This system provides free web map services and open database access by using standard OGC protocols. One of its' main features is that users can create new and unique database projects. The databases involved are maintained by the data providers themselves. Standard tools supplied to the users include repeatable and referable data queries, exporting, evaluations and tracking of data changes. The system also provides a programmable data service for promoting data processing."}, {"title": "Inferring tree causal models of cancer progression with probability raising", "url": "https://www.biorxiv.org/content/early/2014/10/10/000919", "tag": "Bioinformatics", "abstract": "Existing techniques to reconstruct tree models of progression for accumulative processes, such as cancer, seek to estimate causation by combining correlation and a frequentist notion of temporal priority. In this paper, we define a novel theoretical framework called CAPRESE (CAncer PRogression Extraction with Single Edges) to reconstruct such models based on the notion of probabilistic causation defined by Suppes. We consider a general reconstruction setting complicated by the presence of noise in the data due to biological variation, as well as experimental or measurement errors. To improve tolerance to noise we define and use a shrinkage-like estimator. We prove the correctness of our algorithm by showing asymptotic convergence to the correct tree under mild constraints on the level of noise. Moreover, on synthetic data, we show that our approach outperforms the state-of-the-art, that it is efficient even with a relatively small number of samples and that its performance quickly converges to its asymptote as the number of samples increases. For real cancer datasets obtained with different technologies, we highlight biologically significant differences in the progressions inferred with respect to other competing techniques and we also show how to validate conjectured biological relations with progression models."}, {"title": "Inference of interactions between chromatin modifiers and histone modifications: from ChIP-Seq data to chromatin-signaling", "url": "https://www.biorxiv.org/content/early/2014/10/08/010132", "tag": "Bioinformatics", "abstract": "Chromatin modifiers and histone modifications are components of a chromatin-signaling network involved in transcription and its regulation. The interactions between chromatin modifiers and histone modifications are often unknown, are based on the analysis of few genes, or are studied in vitro. Here, we apply computational methods to recover interactions between chromatin modifiers and histone modifications from genome-wide ChIP-Seq data. These interactions provide a high-confidence backbone of the chromatin-signaling network. Many recovered interactions have literature support; others provide hypotheses about yet unknown interactions. We experimentally verified two of these predicted interactions, leading to a link between H4K20me1 and members of the Polycomb Repressive Complexes 1 and 2. Our results suggest that our computationally derived interactions are likely to lead to novel biological insights required to establish the connectivity of the chromatin-signaling network involved in transcription and its regulation."}, {"title": "Use of OWL within the Gene Ontology", "url": "https://www.biorxiv.org/content/early/2014/10/07/010090", "tag": "Bioinformatics", "abstract": "The Gene Ontology (GO) is a ubiquitous tool in biological data analysis, and is one of the most well-known ontologies, in or outside the life sciences. Commonly conceived of as a simple terminology structured as a directed acyclic graph, the GO is actually well-axiomatized in OWL and is highly dependent on the OWL tool stack. Here we outline some of the lesser known features of the GO, describe the GO development process, and our prognosis for future development in terms of the OWL representation."}, {"title": "Crowdsourcing RNA structural alignments with an online computer game", "url": "https://www.biorxiv.org/content/early/2014/10/03/009902", "tag": "Bioinformatics", "abstract": "The annotation and classification of ncRNAs is essential to decipher molecular mechanisms of gene regulation in normal and disease states. A database such as Rfam maintains alignments, consensus secondary structures, and corresponding annotations for RNA families. Its primary purpose is the automated, accurate annotation of non-coding RNAs in genomic sequences. However, the alignment of RNAs is computationally challenging, and the data stored in this database are often subject to improvements. Here, we design and evaluate Ribo, a human-computing game that aims to improve the accuracy of RNA alignments already stored in Rfam. We demonstrate the potential of our techniques and discuss the feasibility of large scale collaborative annotation and classification of RNA families."}, {"title": "MyGene.info: gene annotation query as a service", "url": "https://www.biorxiv.org/content/early/2014/09/17/009332", "tag": "Bioinformatics", "abstract": "Biomedical knowledge is often represented as annotations of biological entities such as genes, genetic variants, diseases, and drugs. For gene annotations, they are fragmented across data repositories like NCBI Entrez, Ensembl, UniProt, and hundreds (or more) of other specialized databases. While the volume and breadth of annotations is valuable, their fragmentation across many data silos is often frustrating and inefficient. Bioinformaticians everywhere must continuously and repetitively engage in data wrangling in an effort to comprehensively integrate knowledge from all these resources, and these uncoordinated efforts represent an enormous duplication of work. We previously released MyGene.info (http://mygene.info) to enable bioinformatics developers to gain programmatic access to gene annotation data through our high-performance web services. This article focuses on the updates to MyGene.info since our last paper (2013 database issue). With the completely re-factored system, MyGene.info now expands the support from the original nine species to over 14K species, covering >17M genes with >50 gene-specific annotation types. Two simple web service endpoints provides high-performance query access to all these aggregated gene annotations. The infrastructure underlying MyGene.info is highly scalable, which offers both high-performance and high-concurrency, and makes MyGene.info particularly suitable for the use cases of real-time applications and analysis pipelines."}, {"title": "Reducing INDEL calling errors in whole-genome and exome sequencing data", "url": "https://www.biorxiv.org/content/early/2014/09/17/006148", "tag": "Bioinformatics", "abstract": "Background INDELs, especially those disrupting protein-coding regions of the genome, have been strongly associated with human diseases. However, there are still many errors with INDEL variant calling, driven by library preparation, sequencing biases, and algorithm artifacts. Methods We characterized whole genome sequencing (WGS), whole exome sequencing (WES), and PCR-free sequencing data from the same samples to investigate the sources of INDEL errors. We also developed a classification scheme based on the coverage and composition to rank high and low quality INDEL calls. We performed a large-scale validation experiment on 600 loci, and find high-quality INDELs to have a substantially lower error rate than low quality INDELs (7% vs. 51%). Results Simulation and experimental data show that assembly based callers are significantly more sensitive and robust for detecting large INDELs (>5bp) than alignment based callers, consistent with published data. The concordance of INDEL detection between WGS and WES is low (52%), and WGS data uniquely identifies 10.8-fold more high-quality INDELs. The validation rate for WGS-specific INDELs is also much higher than that for WES-specific INDELs (85% vs. 54%), and WES misses many large INDELs. In addition, the concordance for INDEL detection between standard WGS and PCR-free sequencing is 71%, and standard WGS data uniquely identifies 6.3-fold more low-quality INDELs. Furthermore, accurate detection with Scalpel of heterozygous INDELs requires 1.2-fold higher coverage than that for homozygous INDELs. Lastly, homopolymer A/T INDELs are a major source of low-quality INDEL calls, and they are highly enriched in the WES data. Conclusions Overall, we show that accuracy of INDEL detection with WGS is much greater than WES even in the targeted region. We calculated that 60X WGS depth of coverage from the HiSeq platform is needed to recover 95% of INDELs detected by Scalpel. While this is higher than current sequencing practice, the deeper coverage may save total project costs because of the greater accuracy and sensitivity. Finally, we investigate sources of INDEL errors (e.g. capture deficiency, PCR amplification, homopolymers) with various data that will serve as a guideline to effectively reduce INDEL errors in genome sequencing."}, {"title": "T-lex2: genotyping, frequency estimation and re-annotation of transposable elements using single or pooled next-generation sequencing data", "url": "https://www.biorxiv.org/content/early/2014/09/16/002964", "tag": "Bioinformatics", "abstract": "Transposable elements (TEs) constitute the most active, diverse and ancient component in a broad range of genomes. Complete understanding of genome function and evolution cannot be achieved without a thorough understanding of TE impact and biology. However, in-depth analysis of TEs still represents a challenge due to the repetitive nature of these genomic entities. In this work, we present a broadly applicable and flexible tool: T-lex2. T-lex2 is the only available software that allows routine, automatic, and accurate genotyping of individual TE insertions and estimation of their population frequencies both using individual strain and pooled next-generation sequencing (NGS) data. Furthermore, T-lex2 also assesses the quality of the calls allowing the identification of miss-annotated TEs and providing the necessary information to re-annotate them. The flexible and customizable design of T-lex2 allows running it in any genome and for any type of TE insertion. Here, we tested the fidelity of T-lex2 using the fly and human genomes. Overall, T-lex2 represents a significant improvement in our ability to analyze the contribution of TEs to genome function and evolution as well as learning about the biology of TEs. T-lex2 is freely available online at http://sourceforge.net/projects/tlex/."}, {"title": "Sequence co-evolution gives 3D contacts and structures of protein complexes", "url": "https://www.biorxiv.org/content/early/2014/09/15/004762", "tag": "Bioinformatics", "abstract": "Protein-protein interactions are fundamental to many biological processes. Experimental screens have identified tens of thousands of interactions and structural biology has provided detailed functional insight for select 3D protein complexes. An alternative rich source of information about protein interactions is the evolutionary sequence record. Building on earlier work, we show that analysis of correlated evolutionary sequence changes across proteins identifies residues that are close in space with sufficient accuracy to determine the three-dimensional structure of the protein complexes. We evaluate prediction performance in blinded tests on 76 complexes of known 3D structure, predict protein-protein contacts in 32 complexes of unknown structure, and demonstrate how evolutionary couplings can be used to distinguish between interacting and non-interacting protein pairs in a large complex. With the current growth of sequence databases, we expect that the method can be generalized to genome-wide elucidation of protein-protein interaction networks and used for interaction predictions at residue resolution."}, {"title": "Interpretable per Case Weighted Ensemble Method for Cancer Associations", "url": "https://www.biorxiv.org/content/early/2014/09/15/008185", "tag": "Bioinformatics", "abstract": "Motivation: Molecular measurements from cancer patients such as gene expression and DNA methylation are usually very noisy. Furthermore, cancer types can be very heterogeneous. Therefore, one of the main assumptions for machine learning, that the underlying unknown distribution is the same for all samples, might not be completely fullfilled. We introduce a method, that can estimate this bias on a per-feature level and incorporate calculated feature confidences into a weighted combination of classifiers with disjoint feature sets. Results: The new method achieves state-of-the-art performance on many different cancer data sets with measured DNA methylation or gene expression. Moreover, we show how to visualize the learned classifiers to find interesting associations with the target label. Applied to a leukemia data set we find several ribosomal proteins associated with leukemia's risk group that might be interesting targets for follow-up studies and support the hypothesis that the ribosomes are a new frontier in gene regulation. Availability: The method is available under GPLv3+ License at https: //github.com/adrinjalali/Network-Classifier."}, {"title": "Average genome size estimation enables accurate quantification of gene family abundance and sheds light on the functional ecology of the human microbiome", "url": "https://www.biorxiv.org/content/early/2014/09/11/009001", "tag": "Bioinformatics", "abstract": "Average genome size (AGS) is an important, yet often overlooked property of microbial communities. We developed MicrobeCensus to rapidly and accurately estimate AGS from short-read metagenomics data and applied our tool to over 1,300 human microbiome samples. We found that AGS differs significantly within and between body sites and tracks with major functional and taxonomic differences. For example, in the gut, AGS ranges from 2.5 to 5.8 megabases and is positively correlated with the abundance of Bacteroides and polysaccharide metabolism. Furthermore, we found that AGS variation can bias comparative analyses, and that normalization improves detection of differentially abundant genes."}, {"title": "MDTraj: a modern, open library for the analysis of molecular dynamics trajectories", "url": "https://www.biorxiv.org/content/early/2014/09/09/008896", "tag": "Bioinformatics", "abstract": "Summary: MDTraj is a modern, lightweight and efficient software package for analyzing molecular dynamics simulations. MDTraj reads trajectory data from a wide variety of commonly used formats. It provides a large number of trajectory analysis capabilities including RMSD, DSSP secondary structure assignment and the extraction of common order parameters. The package has a strong focus on interoperability with the wider scientific Python ecosystem, bridging the gap between molecular dynamics data and the rapidly-growing collection of industry-standard statistical analysis and visualization tools in Python. Availability: Package downloads, detailed examples and full documentation are available at http://mdtraj.org. The source code is distributed under the GNU Lesser General Public License at https://github.com/simtk/mdtraj."}, {"title": "DiffVar: A new method for detecting differential variability with application to methylation in cancer and aging", "url": "https://www.biorxiv.org/content/early/2014/09/06/008847", "tag": "Bioinformatics", "abstract": "Methylation of DNA is known to be essential to development and dramatically altered in cancers. The Illumina HumanMethylation450 BeadChip has been used extensively as a cost-effective way to profile nearly half a million CpG sites across the human genome. Here we present DiffVar, a novel method to test for differential variability between sample groups. DiffVar employs an empirical Bayes model framework that can take into account any experimental design and is robust to outliers. We applied DiffVar to several datasets from The Cancer Genome Atlas, as well as an aging dataset. DiffVar is available in the missMethyl Bioconductor R package."}, {"title": "Flexible analysis of transcriptome assemblies with Ballgown", "url": "https://www.biorxiv.org/content/early/2014/09/05/003665", "tag": "Bioinformatics", "abstract": "We have built a statistical package called Ballgown for estimating differential expression of genes, transcripts, or exons from RNA sequencing experiments. Ballgown is designed to work with the popular Cufflinks transcript assembly software and uses well-motivated statistical methods to provide estimates of changes in expression. It permits statistical analysis at the transcript level for a wide variety of experimental designs, allows adjustment for confounders, and handles studies with continuous covariates. Ballgown provides improved statistical significance estimates as compared to the Cuffdiff differential expression tool included with Cufflinks. We demonstrate the flexibility of the Ballgown package by re-analyzing 667 samples from the GEUVADIS study to identify transcript-level eQTLs and identify non-linear artifacts in transcript data. Our package is freely available from: https://github.com/alyssafrazee/ballgown"}, {"title": "OncoRep: An n-of-1 reporting tool to support genome-guided treatment for breast cancer patients using RNA-sequencing", "url": "https://www.biorxiv.org/content/early/2014/09/03/008748", "tag": "Bioinformatics", "abstract": "Breast cancer comprises multiple tumor entities associated with different biological features and clinical behaviors, making individualized medicine a powerful tool to bring the right drug to the right patient. Next generation sequencing of RNA (RNA-Seq) is a suitable method to detect targets for individualized treatment. Challenges that arise are i) preprocessing and analyzing RNA-Seq data in the n-of-1 setting, ii) extracting clinically relevant and actionable targets from complex data, iii) integrating drug databases, and iv) reporting results to clinicians in a timely and understandable manner. To address these challenges, we present OncoRep, an RNA-Seq based n-of-1 reporting tool for breast cancer patients. It reports molecular classification, altered genes and pathways, gene fusions, clinically actionable mutations and drug recommendations. It visualizes the data in an approachable html-based interactive report and a PDF clinical report, providing the clinician and tumor board with a tool to guide the treatment decision making process. OncoRep is free and open-source, thereby offering a platform for future development and innovation by the community."}, {"title": "diCal-IBD: demography-aware inference of identity-by-descent tracts in unrelated individuals", "url": "https://www.biorxiv.org/content/early/2014/09/03/005082", "tag": "Bioinformatics", "abstract": "Summary: We present a tool, diCal-IBD, for detecting identity-by-descent (IBD) tracts between pairs of genomic sequences. Our method builds on a recent demographic inference method based on the coalescent with recombination, and is able to incorporate demographic information as a prior. Simulation study shows that diCal-IBD has significantly higher recall and precision than that of existing SNP-based IBD detection methods, while retaining reasonable accuracy for IBD tracts as small as 0.1 cM. Availability: http://sourceforge.net/projects/dical-ibd Contact: yss@eecs.berkeley.edu"}, {"title": "BoCluSt: bootstrap clustering stability algorithm for community detection in networks", "url": "https://www.biorxiv.org/content/early/2014/09/02/008656", "tag": "Bioinformatics", "abstract": "The identification of modules or communities of related variables is a key step in the analysis and modelling of biological systems. Many module identification procedures are available, but few of these can determine the module partitions best fitting a given dataset in the absence of previous information, in an unsupervised way, and when the links between variables have different weights. Here I propose such a procedure, which uses the stability under bootstrap resampling of different alternative module structures as a criterion to identify the structure best fitting to a set of variables. In its present implementation, the procedure uses linear correlations as link weights. Computer simulations show that the procedure is useful for problems involving moderate numbers of variables, such as those commonly found in gene regulation cascades or metabolic pathways, and also that it can detect hierarchical network structures, in which modules are composed of smaller sub modules. The procedure becomes less practical as the number of variables increases, due to increases in processing time.The proposed procedure may be a valuable and robust network analysis tool. Because it is based on comparing the amount of evidence for different module partitions structures, this procedure may detect the existence of hierarchical network structures."}, {"title": "ProbAlign: a re-alignment method for long sequencing reads", "url": "https://www.biorxiv.org/content/early/2014/09/02/008698", "tag": "Bioinformatics", "abstract": "The incorrect alignments are a severe problem in variant calling, and remain as a challenge computational issue in Bioinformatics field. Although there have been some methods utilizing the re-alignment approach to tackle the misalignments, a standalone re-alignment tool for long sequencing reads is lacking. Hence, we present a standalone tool to correct the misalignments, called ProbAlign. It can be integrated into the pipelines of not only variant calling but also other genomic applications. We demonstrate the use of re-alignment in two diverse and important genomics fields: variant calling and viral quasispecies reconstruction. First, variant calling results in the Pacific Biosciences SMRT re-sequencing data of NA12878 show that false positives can be reduced by 43.5%, and true positives can be increased by 24.8% averagely, after re-alignment. Second, results in reconstructing a 5-virus-mix show that the viral population can be completely unraveled, and also the estimation of quasispecies frequencies has been improved, after re-alignment. ProbAlign is freely available in the PyroTools toolkit (https://github.com/homopolymer/PyroTools)."}, {"title": "MINI REVIEW: Statistical methods for detecting differentially methylated loci and regions", "url": "https://www.biorxiv.org/content/early/2014/08/29/007120", "tag": "Bioinformatics", "abstract": "DNA methylation, and specifically the reversible addition of methyl groups at CpG dinucleotides genome-wide, represents an important layer that is associated with the regulation of gene expression. In particular, aberrations in the methylation status have been noted across a diverse set of pathological states, including cancer. With the rapid development and uptake of large scale sequencing of short DNA fragments, there has been an explosion of data analytic methods for processing and discovering changes in DNA methylation across diverse data types. In this mini-review, we aim to condense many of the salient challenges, such as experimental design, statistical methods for differential methylation detection and critical considerations such as cell type composition and the potential confounding that can arise from batch effects, into a compact and accessible format. Our main interests, from a statistical perspective, include the practical use of empirical Bayes or hierarchical models, which have been shown to be immensely powerful and flexible in genomics and the procedures by which control of false discoveries are made. Of course, there are many critical platform-specific data preprocessing aspects that we do not discuss here. In addition, we do not make formal performance comparisons of the methods, but rather describe the commonly used statistical models and many of the pertinent issues; we make some recommendations for further study."}, {"title": "ReNette: a web-infrastructure for reproducible network analysis", "url": "https://www.biorxiv.org/content/early/2014/08/27/008433", "tag": "Bioinformatics", "abstract": "Summary: Here we introduce a novel web-infrastructure for differential network analysis. The aim of the web-site is to provide a comprehensive collection of tools for network inference, network comparison and network reproducibility analysis. Four main processes are available through the web service: the network inference process which include 11 reconstruction algorithms, the network distance process with 3 available metrics, the network stability process which includes all the network reconstruction methods and network distances and the netwok statistic process which computes the most common measure for network characterization. We introduce here a novel infrastructure which allows the user- interface logic to be separated from computing services and the asynchronous task management. Task submission is implemented mimicking the high performance computing queue submission system which allows to run multiple jobs without affecting the front-end server. Availability and Implementation: The web-site is available at https://renette.fbk.eu, the implementation is based on the django framework and Apache, with all major browsers supported. Furthermore, the whole project is Open Source under GPLv2 and the code is available on GitHub at https://github.com/MPBA/ renette for local installation."}, {"title": "DISEASES: Text mining and data integration of disease\u2013gene associations", "url": "https://www.biorxiv.org/content/early/2014/08/25/008425", "tag": "Bioinformatics", "abstract": "Text mining is a flexible technology that can be applied to numerous different tasks in biology and medicine. We present a system for extracting disease\u2013gene associations from biomedical abstracts. The system consists of a highly efficient dictionary-based tagger for named entity recognition of human genes and diseases, which we combine with a scoring scheme that takes into account co-occurrences both within and between sentences. We show that this approach is able to extract half of all manually curated associations with a false positive rate of only 0.16%. Nonetheless, text mining should not stand alone, but be combined with other types of evidence. For this reason, we have developed the DISEASES resource, which integrates the results from text mining with manually curated disease\u2013gene associations, cancer mutation data, and genome-wide association studies from existing databases. The DISEASES resource is accessible through a user-friendly web interface at http://diseases.jensenlab.org/, where the text-mining software and all associations are also freely available for download."}, {"title": "edgeRun: an R package for sensitive, functionally relevant differential expression discovery using an unconditional exact test", "url": "https://www.biorxiv.org/content/early/2014/08/25/008409", "tag": "Bioinformatics", "abstract": "Summary: Next-generation sequencing platforms for measuring digital expression such as RNA-Seq are displacing traditional microarray-based methods in biological experiments. The detection of differentially expressed genes between groups of biological conditions has led to the development of numerous bioinformatics tools, but so far few, exploit the expanded dynamic range afforded by the new technologies. We present edgeRun, an R package that implements an unconditional exact test that is a more powerful version of the exact test in edgeR. This increase in power is especially pronounced for experiments with as few as 2 replicates per condition, for genes with low total expression and with large biological coefficient of variation. In comparison with a panel of other tools, edgeRun consistently captures functionally similar differentially expressed genes. Availability: The package is freely available under the MIT license from CRAN (http://cran.r-project.org/web/packages/edgeRun) Contact: edimont@mail.harvard.edu"}, {"title": "Proportionality: a valid alternative to correlation for relative data", "url": "https://www.biorxiv.org/content/early/2014/08/25/008417", "tag": "Bioinformatics", "abstract": "In the life sciences, many measurement methods yield only the relative abundances of different components in a sample. With such relative---or compositional---data, differential expression needs careful interpretation, and correlation---a statistical workhorse for analyzing pairwise relationships---is an inappropriate measure of association. Using yeast gene expression data we show how correlation can be misleading and present proportionality as a valid alternative for relative data. We show how the strength of proportionality between two variables can be meaningfully and interpretably described by a new statistic \u03c6 which can be used instead of correlation as the basis of familiar analyses and visualization methods, including co-expression networks and clustered heatmaps. While the main aim of this study is to present proportionality as a means to analyse relative data, it also raises intriguing questions about the molecular mechanisms underlying the proportional regulation of a range of yeast genes."}, {"title": "Inference of Cancer Progression Models with Biological Noise", "url": "https://www.biorxiv.org/content/early/2014/08/25/008326", "tag": "Bioinformatics", "abstract": "Many applications in translational medicine require the understanding of how diseases progress through the accumulation of persistent events. Specialized Bayesian networks called monotonic progression networks offer a statistical framework for modeling this sort of phenomenon. Current machine learning tools to reconstruct Bayesian networks from data are powerful but not suited to progression models. We combine the technological advances in machine learning with a rigorous philosophical theory of causation to produce Polaris, a scalable algorithm for learning progression networks that accounts for causal or biological noise as well as logical relations among genetic events, making the resulting models easy to interpret qualitatively. We tested Polaris on synthetically generated data and showed that it outperforms a widely used machine learning algorithm and approaches the performance of the competing special-purpose, albeit clairvoyant algorithm that is given a priori information about the model parameters. We also prove that under certain rather mild conditions, Polaris is guaranteed to converge for sufficiently large sample sizes. Finally, we applied Polaris to point mutation and copy number variation data in Prostate cancer from The Cancer Genome Atlas (TCGA) and found that there are likely three distinct progressions, one major androgen driven progression, one major non-androgen driven progression, and one novel minor androgen driven progression."}, {"title": "Omics Pipe: A Computational Framework for Reproducible Multi-Omics Data Analysis", "url": "https://www.biorxiv.org/content/early/2014/08/23/008383", "tag": "Bioinformatics", "abstract": "Omics Pipe (https://bitbucket.org/sulab/omics_pipe) is a computational platform that automates multi-omics data analysis pipelines on high performance compute clusters and in the cloud. It supports best practice published pipelines for RNA-seq, miRNA-seq, Exome-seq, Whole Genome sequencing, ChIP-seq analyses and automatic processing of data from The Cancer Genome Atlas. Omics Pipe provides researchers with a tool for reproducible, open source and extensible next generation sequencing analysis."}, {"title": "HTSeq - A Python framework to work with high-throughput sequencing data", "url": "https://www.biorxiv.org/content/early/2014/08/19/002824", "tag": "Bioinformatics", "abstract": "Motivation: A large choice of tools exists for many standard tasks in the analysis of high-throughput sequencing (HTS) data. However, once a project deviates from standard work flows, custom scripts are needed. Results: We present HTSeq, a Python library to facilitate the rapid development of such scripts. HTSeq offers parsers for many common data formats in HTS projects, as well as classes to represent data such as genomic coordinates, sequences, sequencing reads, alignments, gene model information, variant calls, and provides data structures that allow for querying via genomic coordinates. We also present htseq-count, a tool developed with HTSeq that preprocesses RNA-Seq data for differential expression analysis by counting the overlap of reads with genes. Availability: HTSeq is released as open-source software under the GNU General Public Licence and available from http://www-huber.embl.de/HTSeq or from the Python Package Index, https://pypi.python.org/pypi/HTSeq"}, {"title": "RNA-Rocket: An RNA-Seq Analysis Resource for Infectious Disease Research", "url": "https://www.biorxiv.org/content/early/2014/08/14/007963.1", "tag": "Bioinformatics", "abstract": "Motivation: RNA-Seq is a method for profiling transcription using high-throughput sequencing and is an important component of many research projects that wish to study transcript isoforms, condition specific expression, and transcriptional structure. The methods, tools, and technologies employed to perform RNA-Seq analysis continue to change, creating a bioinformatics challenge for researchers who wish to exploit these data. Resources that bring together genomic data, analysis tools, educational material, and computational infrastructure can minimize the overhead required of life science researchers. Results: RNA-Rocket is a free service that provides access to RNA-Seq and ChIP-Seq analysis tools for studying infectious diseases. The site makes available thousands of pre-indexed genomes, their annotations, and the ability to stream results to the bioinformatics resources VectorBase, EuPathDB, and PATRIC. The site also provides a combination of experimental data and metadata, examples of pre-computed analysis, step-by-step guides, and a user interface designed to enable both novice and experienced users of RNA-Seq data. Availability: RNA-Rocket can be found at rnaseq.pathogenportal.org Source code for this project can be found at github.com/cidvbi/PathogenPortal"}, {"title": "The importance of study design for detecting differentially abundant features in high-throughput experiments", "url": "https://www.biorxiv.org/content/early/2014/08/14/007948", "tag": "Bioinformatics", "abstract": "The use of high-throughput experiments, such as RNA-seq, to simultaneously identify differentially abundant entities across conditions has become widespread, but the systematic planning of such studies is currently hampered by the lack of general-purpose tools to do so. Here we demonstrate that there is substantial variability in performance across statistical tests, normalization techniques and study conditions, potentially leading to significant wastage of resources and/or missing information in the absence of careful study design. We present a broadly applicable experimental design tool called EDDA, and the first for single-cell RNA-seq, Nanostring and Metagenomic studies, that can be used to i) rationally choose from a panel of statistical tests, ii) measure expected performance for a study and iii) plan experiments to minimize mis-utilization of valuable resources. Using case studies from recent single-cell RNA-seq, Nanostring and Metagenomics studies, we highlight its general utility and, in particular, show a) the ability to correctly model single-cell RNA-seq data and do comparisons with 1/5th the amount of sequencing currently used and b) that the selection of suitable statistical tests strongly impacts the ability to detect biomarkers in Metagenomic studies. Furthermore, we demonstrate that a novel mode-based normalization employed in EDDA uniformly improves in robustness over existing approaches (10-20%) and increases precision to detect differential abundance by up to 140%."}, {"title": "Assembling Large Genomes with Single-Molecule Sequencing and Locality Sensitive Hashing", "url": "https://www.biorxiv.org/content/early/2014/08/14/008003", "tag": "Bioinformatics", "abstract": "We report reference-grade de novo assemblies of four model organisms and the human genome from single-molecule, real-time (SMRT) sequencing. Long-read SMRT sequencing is routinely used to finish microbial genomes, but the available assembly methods have not scaled well to larger genomes. Here we introduce the MinHash Alignment Process (MHAP) for efficient overlapping of noisy, long reads using probabilistic, locality-sensitive hashing. Together with Celera Assembler, MHAP was used to reconstruct the genomes of Escherichia coli, Saccharomyces cerevisiae, Arabidopsis thaliana, Drosophila melanogaster, and human from high-coverage SMRT sequencing. The resulting assemblies include fully resolved chromosome arms and close persistent gaps in these important reference genomes, including heterochromatic and telomeric transition sequences. For D. melanogaster, MHAP achieved a 600-fold speedup relative to prior methods and a cloud computing cost of a few hundred dollars. These results demonstrate that single-molecule sequencing alone can produce near-complete eukaryotic genomes at modest cost."}, {"title": "Benchmarking undedicated cloud computing providers for analysis of genomic datasets.", "url": "https://www.biorxiv.org/content/early/2014/08/07/007724", "tag": "Bioinformatics", "abstract": "A major bottleneck in biological discovery is now emerging at the computational level. Cloud computing offers a dynamic means whereby small and medium-sized laboratories can rapidly adjust their computational capacity. We benchmarked two established cloud computing services, Amazon Web Services Elastic MapReduce (EMR) on Amazon EC2 instances and Google Compute Engine (GCE), using publicly available genomic datasets (E.coli CC102 strain and a Han Chinese male genome) and a standard bioinformatic pipeline on a Hadoop-based platform. Wall-clock time for complete assembly differed by 52.9% (95%CI: 27.5-78.2) for E.coli and 53.5% (95%CI: 34.4-72.6) for human genome, with GCE being more efficient than EMR. The cost of running this experiment on EMR and GCE differed significantly, with the costs on EMR being 257.3% (95%CI: 211.5-303.1) and 173.9% (95%CI: 134.6-213.1) more expensive for E.coli and human assemblies respectively. Thus, GCE was found to outperform EMR both in terms of cost and wall-clock time. Our findings confirm that cloud computing is an efficient and potentially cost-effective alternative for analysis of large genomic datasets. In addition to releasing our cost-effectiveness comparison, we present available ready-to-use scripts for establishing Hadoop instances with Ganglia monitoring on EC2 or GCE."}, {"title": "Lighter: fast and memory-efficient error correction without counting", "url": "https://www.biorxiv.org/content/early/2014/08/07/005579", "tag": "Bioinformatics", "abstract": "Lighter is a fast, memory-efficient tool for correcting sequencing errors. Lighter avoids counting k-mers. Instead, it uses a pair of Bloom filters, one holding a sample of the input k-mers and the other holding k-mers likely to be correct. As long as the sampling fraction is adjusted in inverse proportion to the depth of sequencing, Bloom filter size can be held constant while maintaining near-constant accuracy. Lighter is parallelized, uses no secondary storage, and is both faster and more memory-efficient than competing approaches while achieving comparable accuracy."}, {"title": "NxTrim: optimized trimming of Illumina mate pair reads", "url": "https://www.biorxiv.org/content/early/2014/08/06/007666", "tag": "Bioinformatics", "abstract": "Motivation: Mate pair protocols add to the utility of paired-end sequencing by boosting the genomic distance spanned by each pair of reads, potentially allowing larger repeats to be bridged and resolved. The Illumina Nextera Mate Pair (NMP) protocol employs a circularisation-based strategy that leaves behind 38bp adapter sequences which must be computationally removed from the data. While \"adapter trimming\" is a well-studied area of bioinformatics, existing tools do not fully exploit the particular properties of NMP data and discard more data than is necessary. Results: We present NxTrim, a tool that strives to discard as little sequence as possible from NMP reads. The sequence either side of the adapter site is triaged into \"virtual libraries\" of mate pairs, paired-end reads and single-ended reads. When combined, these data boost coverage and can substantially improve the de novo assembly of bacterial genomes."}, {"title": "A comparison of control samples for ChIP-seq of histone modifications", "url": "https://www.biorxiv.org/content/early/2014/08/01/007609", "tag": "Bioinformatics", "abstract": "The advent of high-throughput sequencing has allowed genome wide profiling of histone modifications by Chromatin ImmunoPrecipitation (ChIP) followed by sequencing (ChIP-seq). In this assay the histone mark of interest is enriched through a chromatin pull-down assay using an antibody for the mark. Due to imperfect antibodies and other factors, many of the sequenced fragments do not originate from the histone mark of interest, and are referred to as background reads. Background reads are not uniformly distributed and therefore control samples are usually used to estimate the background distribution at any given genomic position. The Encyclopedia of DNA Elements (ENCODE) Consortium guidelines suggest sequencing a whole cell extract (WCE, or \"input\") sample, or a mock ChIP reaction such as an IgG control, as a background sample. However, for a histone modification ChIP-seq investigation it is also possible to use a Histone H3 (H3) pull-down to map the underlying distribution of histones. In this paper we generated data from a hematopoietic stem and progenitor cell population isolated from mouse foetal liver to compare WCE and H3 ChIP-seq as control samples. The quality of the control samples is estimated by a comparison to pull-downs of histone modifications and to expression data. We find minor differences between WCE and H3 ChIP-seq, such as coverage in mitochondria and behaviour close to transcription start sites. Where the two controls differ, the H3 pull-down is generally more similar to the ChIP-seq of histone modifications. However, the differences between H3 and WCE have a negligible impact on the quality of a standard analysis."}, {"title": "UniqTag: Content-derived unique and stable identifiers for gene annotation", "url": "https://www.biorxiv.org/content/early/2014/08/01/007583", "tag": "Bioinformatics", "abstract": "When working on an ongoing genome sequencing and assembly project, it is rather inconvenient when gene identifiers change from one build of the assembly to the next. The gene labelling system described here, UniqTag, addresses this common challenge. UniqTag assigns a unique identifier to each gene that is a representative <em>k</em>-mer, a string of length <em>k</em>, selected from the sequence of that gene. Unlike serial numbers, these identifiers are stable between different assemblies and annotations of the same data without requiring that previous annotations be lifted over by sequence alignment. We assign UniqTag identifiers to nine builds of the Ensembl human genome spanning seven years to demonstrate this stability."}, {"title": "Faster sequence alignment through GPU-accelerated restriction of the seed-and-extend search space", "url": "https://www.biorxiv.org/content/early/2014/08/01/007641", "tag": "Bioinformatics", "abstract": "Motivation: In computing pairwise alignments of biological sequences, software implementations employ a variety of heuristics that decrease the computational effort involved in computing potential alignments. A key element in achieving high processing throughput is to identify and prioritize potential alignments where high-scoring mappings can be expected. These tasks involve list-processing operations that can be efficiently performed on GPU hardware. Results: We implemented a read aligner called A21 that exploits GPU-based parallel sort and reduction techniques to restrict the number of locations where potential alignments may be found. When compared with other high-throughput aligners, this approach finds more high-scoring mappings without sacrificing speed or accuracy. A21 running on a single GPU is about 10 times faster than comparable CPU-based tools; it is also faster and more sensitive in comparison with other recent GPU-based aligners."}, {"title": "poRe: an R package for the visualization and analysis of nanopore sequencing data", "url": "https://www.biorxiv.org/content/early/2014/07/29/007567", "tag": "Bioinformatics", "abstract": "Motivation: The Oxford Nanopore MinION device represents a unique sequencing technology. As a mobile sequencing device powered by the USB port of a laptop, the MinION has huge potential applications. To enable these applications, the bioinformatics community will need to design and build a suite of tools specifically for MinION data. Results: Here we present poRe, a package for the statistical software R that enables users to manipulate, organize, summarise and visualize MinION nanopore sequencing data. As a packge for R, poRe has been tested on both Windows and Linux. Crucially, the Windows version allows users to analyse MinION data on the Windows laptop attached to the device Availability: Pre-built R packages for Windows and Linux are available under a BSD license at http://sourceforge.net/projects/rpore/ Contact: mick.watson@roslin.ed.ac.uk"}, {"title": "Classification of RNA-Seq Data via Bagging Support Vector Machines", "url": "https://www.biorxiv.org/content/early/2014/07/28/007526", "tag": "Bioinformatics", "abstract": "Background RNA sequencing (RNA-Seq) is a powerful technique for transcriptome profiling of the organisms that uses the capabilities of next-generation sequencing (NGS) technologies. Recent advances in NGS let to measure the expression levels of tens to thousands of transcripts simultaneously. Using such information, developing expression-based classification algorithms is an emerging powerful method for diagnosis, disease classification and monitoring at molecular level, as well as providing potential markers of disease. Here, we present the bagging support vector machines (bagSVM), a machine learning approach and bagged ensembles of support vector machines (SVM), for classification of RNA-Seq data. The bagSVM basically uses bootstrap technique and trains each single SVM separately; next it combines the results of each SVM model using majority-voting technique. Results We demonstrate the performance of the bagSVM on simulated and real datasets. Simulated datasets are generated from negative binomial distribution under different scenarios and real datasets are obtained from publicly available resources. A deseq normalization and variance stabilizing transformation (vst) were applied to all datasets. We compared the results with several classifiers including Poisson linear discriminant analysis (PLDA), single SVM, classification and regression trees (CART), and random forests (RF). In slightly overdispersed data, all methods, except CART algorithm, performed well. Performance of PLDA seemed to be best and RF as second best for very slightly and substantially overdispersed datasets. While data become more spread, bagSVM turned out to be the best classifier. In overall results, bagSVM and PLDA had the highest accuracies. Conclusions According to our results, bagSVM algorithm after vst transformation can be a good choice of classifier for RNA-Seq datasets mostly for overdispersed ones. Thus, we recommend researchers to use bagSVM algorithm for the purpose of classification of RNA-Seq data. PLDA algorithm should be a method of choice for slight and moderately overdispersed datasets. An R/BIOCONDUCTOR package MLSeq with a vignette is freely available at http://www.bioconductor.org/packages/2.14/bioc/html/MLSeq.html Keywords: Bagging, machine learning, RNA-Seq classification, support vector machines, transcriptomics"}, {"title": "Data mining of Gene expression profiles of Saccharomyces cerevisiae in response to mild heat stress response", "url": "https://www.biorxiv.org/content/early/2014/07/25/007468", "tag": "Bioinformatics", "abstract": "We chose yeast as a model organism to explore how eukaryotic cells respond to heat stress. This study provides details on the way yeast responds to temperature changes and is therefore an empirical reference for basic cell research and industrial fermentation of yeast. We use the Qlucore Omics Explorer (QOE) bioinformatics software to analyze the gene expression profiles of the heat stress from Gene Expression Omnibus (GEO). Genes and their expression are listed in heat maps, and the gene function is analyzed against the biological processes and pathways. We can find that the expression of genes changed over time after heat stress. Gene expression changed rapidly from 0 min to 60 min after heat shock, and gene expression stabilized between 60 min to 360 min. The yeast cells begin to adjust themselves to the high temperatures in terms of the level of gene expression at about 60 min. In all of the involved pathways and biological processes, those related to ribosome and nucleic acid metabolism declined in about 15?30 min and those related to starch and sucrose increased in the same time frame. Temperature can be a simple way to control the biological processes and pathways of cell."}, {"title": "Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research", "url": "https://www.biorxiv.org/content/early/2014/07/24/007443", "tag": "Bioinformatics", "abstract": "Background Current biomedical research needs to leverage and exploit the large amount of information reported in publications. Automated text mining approaches, in particular those aimed at finding relationships between entities, are key for identification of actionable knowledge from free text repositories. We present the BeFree system aimed at identifying relationships between biomedical entities with a special focus on genes and their associated diseases. Results By exploiting morpho-syntactic information of the text BeFree is able to identify gene-disease, drug-disease and drug-target associations with state-of-the-art performance. The application of BeFree to real-case scenarios shows its effectiveness in extracting information relevant for translational research. We show the value of the gene-disease associations extracted by BeFree through a number of analyses and integration with other data sources. BeFree succeeds in identifying genes associated to a major cause of morbidity worldwide, depression, which are not present in other public resources. Moreover, large-scale extraction and analysis of gene-disease associations, and integration with current biomedical knowledge, provided interesting insights on the kind of information that can be found in the literature, and raised challenges regarding data prioritization and curation. We found that only a small proportion of the gene-disease associations discovered by using BeFree is collected in expert-curated databases. Thus, there is a pressing need to find alternative strategies to manual curation to review, prioritize and curate text-mining data and incorporate it into domain-specific databases. We present our strategy for data prioritization and discuss its implications for supporting biomedical research and applications. Conclusions BeFree is a novel text mining system that performs competitively for the identification of gene-disease, drug-disease and drug-target associations. Our analyses show that mining only a small fraction of MEDLINE results in a large dataset of gene-disease associations, and only a small proportion of this dataset is actually recorded in curated resources, raising several issues on data prioritization and curation. We propose that joint analysis of text mined data with data curated by experts appears as a suitable approach to both assess data quality and highlight novel and interesting information."}, {"title": "Butter: High-precision genomic alignment of small RNA-seq data", "url": "https://www.biorxiv.org/content/early/2014/07/23/007427", "tag": "Bioinformatics", "abstract": "Eukaryotes produce large numbers of small non-coding RNAs that act as specificity determinants for various gene-regulatory complexes. These include microRNAs (miRNAs), endogenous short interfering RNAs (siRNAs), and Piwi-associated RNAs (piRNAs). These RNAs can be discovered, annotated, and quantified using small RNA-seq, a variant RNA-seq method based on highly parallel sequencing. Alignment to a reference genome is a critical step in analysis of small RNA-seq data. Because of their small size (20-30 nts depending on the organism and sub-type) and tendency to originate from multi-gene families or repetitive regions, reads that align equally well to more than one genomic location are very common. Typical methods to deal with multi-mapped small RNA-seq reads sacrifice either precision or sensitivity. The tool 'butter' balances precision and sensitivity by placing multi-mapped reads using an iterative approach, where the decision between possible locations is dictated by the local densities of more confidently aligned reads. Butter displays superior performance relative to other small RNA-seq aligners. Treatment of multi-mapped small RNA-seq reads has substantial impacts on downstream analyses, including quantification of MIRNA paralogs, and discovery of endogenous siRNA loci. Butter is freely available under a GNU general public license."}, {"title": "Poretools: a toolkit for analyzing nanopore sequence data", "url": "https://www.biorxiv.org/content/early/2014/07/23/007401", "tag": "Bioinformatics", "abstract": "Motivation: Nanopore sequencing may be the next disruptive technology in genomics. Nanopore sequencing has many attractive properties including the ability to detect single DNA molecules without prior amplification, the lack of reliance on expensive optical components, and the ability to sequence very long fragments. The MinION from Oxford Nanopore Technologies (ONT) is the first nanopore sequencer to be commercialised and made available to early-access users. The MinION(TM) is a USB-connected, portable nanopore sequencer which permits real-time analysis of streaming event data. A cloud-based service is available to translate events into nucleotide base calls. However, software support to deal with such data is limited, and the community lacks a standardized toolkit for the analysis of nanopore datasets. Results: We introduce poretools, a flexible toolkit for manipulating and exploring datasets generated by nanopore sequencing devices from MinION for the purposes of quality control and downstream analysis. Poretools operates directly on the native FAST5 (a variant of the HDF5 standard) file format produced by ONT and provides a wealth of format conversion utilities and data exploration and visualization tools. Availability and implementation: Poretools is open source software and is written in Python as both a suite of command line utilities and a Python application programming interface. Source code and user documentation are freely available in Github at https://github.com/arq5x/poretools Contact: n.j.loman@bham.ac.uk, aaronquinlan@gmail.com Supplementary information: An IPython notebook demonstrating the use and functionality of poretools in greater detail is available from the Github repository."}, {"title": "Aligning sequence from molecular inversion probes", "url": "https://www.biorxiv.org/content/early/2014/07/19/007260", "tag": "Bioinformatics", "abstract": "Summary: Molecular inversion probes (MIPs) allow efficient enrichment of genomic regions of interest for the purpose of targeted sequencing. To date, there is a paucity of simple-to-use software to align sequences derived from this method. Here, we describe a single program that performs mapping, arm removal, and deduplication before outputting alignments in SAM format. Availability: bwa-mips is available at https://github.com/brentp/bwa-mips under the MIT license. Contact: bpederse@gmail.com"}, {"title": "Assessing allele specific expression across multiple tissues from RNA-seq read data", "url": "https://www.biorxiv.org/content/early/2014/07/17/007211", "tag": "Bioinformatics", "abstract": "Motivation: RNA sequencing enables allele specific expression (ASE) studies that complement standard genotype expression studies for common variants and, importantly, also allow measuring the regulatory impact of rare variants. The Genotype-Tissue Expression project (GTEx) is collecting RNA-seq data on multiple tissues of a same set of individuals and novel methods are required for the analysis of these data. Results: We present a statistical method to compare different patterns of ASE across tissues and to classify genetic variants according to their impact on the tissue-wide expression profile. We focus on strong ASE effects that we are expecting to see for protein-truncating variants, but our method can also be adjusted for other types of ASE effects. We illustrate the method with a real data example on a tissue-wide expression profile of a variant causal for lipoid proteinosis, and with a simulation study to assess our method more generally. Availability: MAMBA software: http://birch.well.ox.ac.uk/~rivas/mamba/ R source code and data examples: http://www.iki.fi/mpirinen/ Contact: matti.pirinen@helsinki.fi rivas@well.ox.ac.uk"}, {"title": "RNA-seq gene profiling - a systematic empirical comparison", "url": "https://www.biorxiv.org/content/early/2014/07/15/005207", "tag": "Bioinformatics", "abstract": "Accurately quantifying gene expression levels is a key goal of experiments using RNA-sequencing to assay the transcriptome. This typically requires aligning the short reads generated to the genome or transcriptome before quantifying expression of pre-defined sets of genes. Differences in the alignment/quantification tools can have a major effect upon the expression levels found with important consequences for biological interpretation. Here we address two main issues: do different analysis pipelines affect the gene expression levels inferred from RNA-seq data? And, how close are the expression levels inferred to the ``true'' expression levels? We evaluate fifty gene profiling pipelines in experimental and simulated data sets with different characteristics (e.g, read length and sequencing depth). In the absence of knowledge of the 'ground truth' in real RNAseq data sets, we used simulated data to assess the differences between the true expression and those reconstructed by the analysis pipelines. Even though this approach does not take into account all known biases present in RNAseq data, it still allows to assess the accuracy of the gene expression values inferred by different analysis pipelines. The results show that i) overall there is a high correlation between the expression levels inferred by the best pipelines and the true quantification values; ii) the error in the estimated gene expression values can vary considerably across genes; and iii) a small set of genes have expression estimates with consistently high error (across data sets and methods). Finally, although the mapping software is important, the quantification method makes a greater difference to the results."}, {"title": "Benchmark Analysis of Algorithms for Determining and Quantifying Full-length mRNA Splice Forms from RNA-Seq Data", "url": "https://www.biorxiv.org/content/early/2014/07/14/007088", "tag": "Bioinformatics", "abstract": "The advantages of RNA sequencing (RNA-Seq) suggest it will replace microarrays for highly parallel gene expression analysis. For example, in contrast to arrays, RNA-Seq is expected to be able to provide accurate identification and quantification of full-length transcripts. A number of methods have been developed for this purpose, but short error prone reads makes it a difficult problem in practice. It is essential to determine which algorithms perform best, and where and why they fail. However, there is a dearth of independent and unbiased benchmarking studies of these algorithms. Here we take an approach using both simulated and experimental benchmark data to evaluate their accuracy. We conclude that most methods are inaccurate even using idealized data, and that no is method sufficiently accurate once complicating factors such as polymorphisms, intron signal, sequencing error, and multiple splice forms are present. These results point to the pressing need for further algorithm development."}, {"title": "Modeling bi-modality improves characterization of cell cycle on gene expression in single cells", "url": "https://www.biorxiv.org/content/early/2014/07/10/002295", "tag": "Bioinformatics", "abstract": "Advances in high-throughput, single cell gene expression are allowing interrogation of cell heterogeneity. However, there is concern that the cell cycle phase of a cell might bias characterizations of gene expression at the single-cell level. We assess the effect of cell cycle phase on gene expression in single cells by measuring 333 genes in 930 cells across three phases and three cell lines. We determine each cell's phase non-invasively without chemical arrest and use it as a covariate in tests of differential expression. We observe bi-modal gene expression, a previously-described phenomenon, wherein the expression of otherwise abundant genes is either strongly positive, or undetectable within individual cells. This bi-modality is likely both biologically and technically driven. Irrespective of its source, we show that it should be modeled to draw accurate inferences from single cell expression experiments. To this end, we propose a semi-continuous modeling framework based on the generalized linear model, and use it to characterize genes with consistent cell cycle effects across three cell lines. Our new computational framework improves the detection of previously characterized cell-cycle genes compared to approaches that do not account for the bi-modality of single-cell data. We use our semi-continuous modelling framework to estimate single cell gene co-expression networks. These networks suggest that in addition to having phase-dependent shifts in expression (when averaged over many cells), some, but not all, canonical cell cycle genes tend to be co-expressed in groups in single cells. We estimate the amount of single cell expression variability attributable to the cell cycle. We find that the cell cycle explains only 5%-17% of expression variability, suggesting that the cell cycle will not tend to be a large nuisance factor in analysis of the single cell transcriptome."}, {"title": "Recurrent alternative splicing isoform switches in tumor samples provide novel signatures of cancer", "url": "https://www.biorxiv.org/content/early/2014/07/07/006908", "tag": "Bioinformatics", "abstract": "Cancer genomics has been instrumental to determine the genetic alterations that are predictive of various tumor conditions. However, the majority of these alterations occur at low frequencies, motivating the need to expand the catalogue of cancer signatures. Alternative pre-mRNA splicing alterations, which bear major importance for the understanding of cancer, have not been exhaustively studied yet in the context of recent cancer genome projects. In this article we analyze RNA sequencing data for more than 4000 samples from The Cancer Genome Atlas (TCGA) project, including paired normal samples, to detect recurrent alternative splicing isoform switches in 9 different cancer types. We first investigate whether alternative splicing isoform changes are predictive of tumors by applying a rank-based algorithm based on the reversal of the relative expression of transcript isoforms. We find that consistent alternative splicing isoform changes can separate with high accuracy tumor and normal samples, as well as some cancer subtypes. We then searched for those changes that occur in the most abundant isoform, i.e isoform switches, and are therefore more likely to have a functional impact. In total we detected 244 isoform switches, which are associated to functional pathways that are frequently altered in cancer and also separate tumor and normal samples accurately. We further assessed whether these isoform changes are associated to somatic mutations. Surprisingly, only a few cases appear to have association, including the putative tumor suppressor FBLN2 and the tumor driver MYH11, which show association of an isoform switch to mutations and indels on the alternatively spliced exon. However, the number of observed mutations is in general not sufficient to explain the frequency of the found isoform switches, suggesting that recurrent isoform switching in cancer is mostly independent of somatic mutations. In summary, we present an effective approach to detect novel alternative splicing signatures that are predictive of tumors. Moreover, the same methodology has led to uncover recurrent isoform switches in tumors, which may provide novel prognostic and therapeutic targets. Software and data are available at: https://bitbucket.org/regulatorygenomicsupf/iso-ktsp and http://dx.doi.org/10.6084/m9.figshare.1061917"}, {"title": "Reconstructing subclonal composition and evolution from whole genome sequencing of tumors", "url": "https://www.biorxiv.org/content/early/2014/06/27/006692", "tag": "Bioinformatics", "abstract": "Tumors often contain multiple, genetically distinct subpopulations of cancerous cells. These so-called subclonal populations are defined by distinct somatic mutations that include point mutations such as single nucleotide variants and small indels ? collectively called simple somatic mutations (SSMs) ? as well as larger structural changes that result in copy number variations (CNVs). In some cases, the genotype and prevalence of these subpopulations can be reconstructed based on high-throughput, short-read sequencing of DNA in one or more tumor samples. To date, no automated SSM-based subclonal reconstructions have been attempted on WGS data; and CNV-based reconstructions are limited to tumors with two or fewer cancerous subclonal populations and with a small number of CNVs. We describe a new automated method, PhyloWGS, that can be applied to WGS data from one or more tumor samples to perform subclonal reconstruction based on both CNVs and SSMs. PhyloWGS successfully recovers the composition of mixtures of a highly rearranged TGCA cell line when a CNV-based method fails. On WGS data with average read depth of 40 from five time-series chronic lymphocytic leukemia samples, PhyloWGS recovers the same tumor phylogeny previously reconstructed using deep targeted resequencing. To further explore the limits of WGS-based subclonal reconstruction, we ran PhyloWGS on simulated data: PhyloWGS can reliably reconstruct as many as three cancerous subpopulations based on 30-50x coverage WGS data from a single tumor sample with 10?s to 1000?s of SSMs per subpopulation. At least five cancerous subpopulations can be reconstructed if provided with read depths of 200 or more. PhyloWGS is the first automated method that can be applied to WGS tumor data that accurately reconstructs the frequency, genotype and phylogeny of the subclonal populations based on both SSMs and CNVs. It also provides a principled, automated approach to combining overlapping SSM and CNV data. By demonstrating the utility of PhyloWGS on medium depth WGS data, including from examples with highly rearranged chromosomes, we have greatly expanded the range of tumors for which subclonal reconstruction is possible."}, {"title": "Formalization of Genome Interval Relations", "url": "https://www.biorxiv.org/content/early/2014/06/27/006650", "tag": "Bioinformatics", "abstract": "In order to take full advantage of next generation genomics data, I need informatics methods to be based on agreed upon formally specified standards that can be implemented easily in a uniform fashion without ambiguity. These standards should be encoded as logical formulae, so that provably correct and efficient decision procedures can be used for query answering and validation. In this paper I present the core of such a standard for sequence data: a collection of definitions of relations that hold between genomic intervals, and an alegbra for performing operations upon these intervals. I show how these relations can be used to extend formalize concepts in the Sequence Ontology (SO)."}, {"title": "svaseq: removing batch effects and other unwanted noise from sequencing data", "url": "https://www.biorxiv.org/content/early/2014/06/26/006585", "tag": "Bioinformatics", "abstract": "It is now well known that unwanted noise and unmodeled artifacts such as batch effects can dramatically reduce the accuracy of statistical inference in genomic experiments. We introduced surrogate variable analysis for estimating these artifacts by (1) identifying the part of the genomic data only affected by artifacts and (2) estimating the artifacts with principal components or singular vectors of the subset of the data matrix. The resulting estimates of artifacts can be used in subsequent analyses as adjustment factors. Here I describe an update to the sva approach that can be applied to analyze count data or FPKMs from sequencing experiments. I also describe the addition of supervised sva (ssva) for using control probes to identify the part of the genomic data only affected by artifacts. These updates are available through the surrogate variable analysis (sva) Bioconductor package."}, {"title": "Accurate prediction of transmembrane \u03b2-barrel proteins from sequences", "url": "https://www.biorxiv.org/content/early/2014/06/25/006577", "tag": "Bioinformatics", "abstract": "Transmembrane \u03b2-barrels are known to play major roles in substrate transport and protein biogenesis in gram-negative bacteria, chloroplasts and mitochondria. However, the exact number of transmembrane \u03b2-barrel families is unknown and experimental structure determination is challenging. In theory, if one knows the number of strands in the \u03b2-barrel, then the 3D structure of the barrel could be trivial, but current topology predictions do not predict accurate structures and are unable to give information beyond the \u03b2-strands in the barrel. Recent work has shown successful prediction of globular and alpha-helical membrane proteins from sequence alignments, by using high ranked evolutionary couplings between residues as distance constraints to fold extended polypeptides. However, these methods, have not addressed the calculation of precise \u03b2-sheet hydrogen bonding that defines transmembrane \u03b2-barrels, and would be required to fold these proteins successfully. Hence we developed a method (EVFold_BB) that can successfully model transmembrane \u03b2-barrels by combining evolutionary couplings together with topology predictions. EVFold_BB is validated by the accurate all-atom 3D modeling of 18 proteins, representing all known membrane \u03b2-barrel families that have sufficient sequences available. To demonstrate the potential of our approach we predict the unknown 3D structure of the LptD protein, the plausibility of its accuracy is supported by the blindly predicted benchmarks, and is consistent with experimental observations. Our approach can naturally be extended to all unknown \u03b2-barrel proteins with sufficient sequence information."}, {"title": "Compression of short-read sequences using path encoding", "url": "https://www.biorxiv.org/content/early/2014/06/24/006551", "tag": "Bioinformatics", "abstract": "Storing, transmitting, and archiving the amount of data produced by next generation sequencing is becoming a significant computational burden. For example, large-scale RNA-seq meta-analyses may now routinely process tens of terabytes of sequence. We present here an approach to biological sequence compression that reduces the difficulty associated with managing the data produced by large-scale transcriptome sequencing. Our approach offers a new direction by sitting between pure reference-based compression and reference-free compression and combines much of the benefit of reference-based approaches with the flexibility of de novo encoding. Our method, called path encoding, draws a connection between storing paths in de Bruijn graphs --- a common task in genome assembly --- and context-dependent arithmetic coding. Supporting this method is a system, called a bit tree, to compactly store sets of kmers that is of independent interest. Using these techniques, we are able to encode RNA-seq reads using 3% -- 11% of the space of the sequence in raw FASTA files, which is on average more than 34% smaller than recent competing approaches. We also show that even if the reference is very poorly matched to the reads that are being encoded, good compression can still be achieved."}, {"title": "Finite-state discrete-time Markov chain models of gene regulatory networks", "url": "https://www.biorxiv.org/content/early/2014/06/23/006361", "tag": "Bioinformatics", "abstract": "In this study Markov chain models of gene regulatory networks (GRN) are developed. These models gives the ability to apply the well known theory and tools of Markov chains to GRN analysis. We introduce a new kind of the finite graph of the interactions called the combinatorial net that formally represent a GRN and the transition graphs constructed from interaction graphs. System dynamics are defined as a random walk on the transition graph that is some Markovian chain. A novel concurrent updating scheme (evolution rule) is developed to determine transitions in a transition graph. Our scheme is based on the firing of a random set of non-steady state vertices of a combinatorial net. We demonstrate that this novel scheme gives an advance in the modeling of the asynchronicity. Also we proof the theorem that the combinatorial nets with this updating scheme can asynchronously compute a maximal independent sets of graphs. As proof of concept, we present here a number of simple combinatorial models: a discrete model of auto-repression, a bi-stable switch, the Elowitz repressilator, a self-activation and show that this models exhibit well known properties."}, {"title": "Inferring restrictions in the temporal order of mutations during tumor progression: effects of passengers, evolutionary models, and sampling", "url": "https://www.biorxiv.org/content/early/2014/06/22/005587", "tag": "Bioinformatics", "abstract": "Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation of mutations are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Using simulated data sets, I conducted a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. In contrast to previous work, I embedded restrictions within evolutionary models of tumor progression that included passengers (mutations not responsible for the development of cancer, known to be very common). This allowed me to asses the effects of having to filter out passengers, of sampling schemes, and of deviations from order restrictions. Poor choices of method, filtering, and sampling lead to large errors in all performance metrics. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Overall, the best method for identifying order restrictions were Oncogenetic Trees, a fast and easy to use method that, although unable to recover dependencies of mutations on more than one mutation, showed good performance in most scenarios, superior to Conjunctive Bayesian Networks and Progression Networks. Single cell sampling provided no advantage, but sampling in the final stages of the disease vs.\\ sampling at different stages had severe effects. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. This paper provides practical recommendations for using these methods with experimental data. Moreover, it shows that it is both possible and necessary to embed assumptions about order restrictions and the nature of driver status within evolutionary models of cancer progression to evaluate the performance of inferential approaches."}, {"title": "Beyond library size: a field guide to NGS normalization", "url": "https://www.biorxiv.org/content/early/2014/06/19/006403", "tag": "Bioinformatics", "abstract": "Background: Next generation sequencing (NGS) is a widely used technology in both basic research and clinical settings and it will continue to have a major impact on biomedical sciences. However, the use of incorrect normalization methods can lead to systematic biases and spurious results, making the selection of an appropriate normalization strategy a crucial and often overlooked part of NGS analysis. Results: We present a basic introduction to the currently available normalization methods for differential expression and ChIP-seq applications, along with best use recommendations for different experimental techniques and datasets. We demonstrate that the choice of normalization technique can have a significant impact on the number of genes called as differentially expressed in an RNA-seq experiment or peaks called in a ChIP-seq experiment. Conclusions: The choice of the most adequate normalization method depends on both the distribution of signal in the dataset and the intended downstream applications. Depending on the design and purpose of the study, appropriate bias correction should also be considered."}, {"title": "Error correction and assembly complexity of single molecule sequencing reads.", "url": "https://www.biorxiv.org/content/early/2014/06/18/006395", "tag": "Bioinformatics", "abstract": "Third generation single molecule sequencing technology is poised to revolutionize genomics by enabling the sequencing of long, individual molecules of DNA and RNA. These technologies now routinely produce reads exceeding 5,000 basepairs, and can achieve reads as long as 50,000 basepairs. Here we evaluate the limits of single molecule sequencing by assessing the impact of long read sequencing in the assembly of the human genome and 25 other important genomes across the tree of life. From this, we develop a new data-driven model using support vector regression that can accurately predict assembly performance. We also present a novel hybrid error correction algorithm for long PacBio sequencing reads that uses pre-assembled Illumina sequences for the error correction. We apply it several prokaryotic and eukaryotic genomes, and show it can achieve near-perfect assemblies of small genomes (< 100Mbp) and substantially improved assemblies of larger ones. All source code and the assembly model are available open-source."}, {"title": "Accurate detection of de novo and transmitted INDELs within exome-capture data using micro-assembly", "url": "https://www.biorxiv.org/content/early/2014/06/18/001370", "tag": "Bioinformatics", "abstract": "We present a new open-source algorithm, Scalpel, for sensitive and specific discovery of INDELs in exome-capture data. By combining the power of mapping and assembly, Scalpel carefully searches the de Bruijn graph for sequence paths that span each exon. A detailed repeat analysis coupled with a self-tuning k-mer strategy allows Scalpel to outperform other state-of-the-art approaches for INDEL discovery. We extensively compared Scalpel with a battery of >10000 simulated and >1000 experimentally validated INDELs against two recent algorithms: GATK HaplotypeCaller and SOAPindel. We report anomalies for these tools to detect INDELs in regions containing near-perfect repeats. We also present a large-scale application of Scalpel for detecting de novo and transmitted INDELs in 593 families from the Simons Simplex Collection. Scalpel demonstrates enhanced power to detect long (\u226520bp) transmitted events, and strengthens previous reports of enrichment for de novo likely gene-disrupting INDELs in autistic children with many new candidate genes."}, {"title": "Evaluation of de novo transcriptome assemblies from RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2014/06/13/006338", "tag": "Bioinformatics", "abstract": "RNA-Seq assembly facilitates the study of transcriptomes for species without sequenced genomes, but it is challenging to select the most accurate assembly in this context. To address this challenge, we developed a model-based score, RSEM-EVAL, for evaluating assemblies when the ground truth is unknown. Our experiments show that RSEM-EVAL correctly reflects assembly accuracy, as measured by REF-EVAL, a refined set of ground-truth-based scores that we also developed. With the guidance of RSEM-EVAL, we assembled the transcriptome of the regenerating axolotl limb; this assembly compares favorably to a previous assembly."}, {"title": "Powerful tests for multi-marker association analysis using ensemble learning", "url": "https://www.biorxiv.org/content/early/2014/06/13/005405", "tag": "Bioinformatics", "abstract": "Multi-marker approaches are currently gaining a lot of interest in genome wide association studies and can enhance power to detect new associations under certain conditions. Gene- and pathway-based association tests are increasingly being viewed as useful complements to the more widely used single marker association analysis which have successfully uncovered numerous disease variants. A major drawback of single-marker based methods is that they do not consider pairwise and higher-order interactions between genetic variants. Here, we describe novel tests for multi-marker association analyses that are based on phenotype predictions obtained from machine learning algorithms. Instead of utilizing only a linear or logistic regression model, we propose the use of ensembles of diverse machine learning algorithms for constructing such association tests. As the true mathematical relationship between a phenotype and any group of genetic and clinical variables is unknown in advance and may be complex, such a strategy gives us a general and flexible framework to approximate this relationship across different sets of SNPs. We show how phenotype prediction obtained from ensemble learning algorithms can be used for constructing tests for the joint association of multiple variants. We first apply our method to simulated datasets to demonstrate its power and correctness. Then, we apply our method to previously studied asthma-related genes in two independent asthma cohorts to conduct association tests."}, {"title": "Identification, annotation and visualisation of extreme changes in splicing from RNA-seq experiments with SwitchSeq", "url": "https://www.biorxiv.org/content/early/2014/06/06/005967", "tag": "Bioinformatics", "abstract": "In the past years, RNA sequencing has become the method of choice for the study of transcriptome composition. When working with this type of data, several tools exist to quantify differences in splicing across conditions and to address the significance of those changes. However, the number of genes predicted to undergo differential splicing is often high, and further interpretation of the results becomes a challenging task. Here we present SwitchSeq, a novel set of tools designed to help the users in the interpretation of differential splicing events that affect protein coding genes. More specifically, we provide a framework to identify switch events, i.e., cases where, for a given gene, the identity of the most abundant transcript changes across conditions. The identified events are then annotated by incorporating information from several public databases and third-party tools, and are further visualised in an intuitive manner with the independent R package tviz. All the results are displayed in a self-contained HTML document, and are also stored in txt and json format to facilitate the integration with any further downstream analysis tools. Such analysis approach can be used complementarily to Gene Ontology and pathway enrichment analysis, and can also serve as an aid in the validation of predicted changes in mRNA and protein abundance. The latest version of SwitchSeq, including installation instructions and use cases, can be found at https://github.com/mgonzalezporta/SwitchSeq. Additionally, the plot capabilities are provided as an independent R package at https://github.com/mgonzalezporta/tviz."}, {"title": "iRAP - an integrated RNA-seq Analysis Pipeline", "url": "https://www.biorxiv.org/content/early/2014/06/06/005991", "tag": "Bioinformatics", "abstract": "RNA-sequencing (RNA-Seq) has become the technology of choice for whole-transcriptome profiling. However, processing the millions of sequence reads generated requires considerable bioinformatics skills and computational resources. At each step of the processing pipeline many tools are available, each with specific advantages and disadvantages. While using a specific combination of tools might be desirable, integrating the different tools can be time consuming, often due to specificities in the formats of input/output files required by the different programs. Here we present iRAP, an integrated RNA-seq analysis pipeline that allows the user to select and apply their preferred combination of existing tools for mapping reads, quantifying expression, testing for differential expression. iRAP also includes multiple tools for gene set enrichment analysis and generates web browsable reports of the results obtained in the different stages of the pipeline. Depending upon the application, iRAP can be used to quantify expression at the gene, exon or transcript level. iRAP is aimed at a broad group of users with basic bioinformatics training and requires little experience with the command line. Despite this, it also provides more advanced users with the ability to customise the options used by their chosen tools."}, {"title": "Significantly distinct branches of hierarchical trees: A framework for statistical analysis and applications to biological data", "url": "https://www.biorxiv.org/content/early/2014/06/05/002188", "tag": "Bioinformatics", "abstract": "Background One of the most common goals of hierarchical clustering is finding those branches of a tree that form quantifiably distinct data subtypes. Achieving this goal in a statistically meaningful way requires (a) a measure of distinctness of a branch and (b) a test to determine the significance of the observed measure, applicable to all branches and across multiple scales of dissimilarity. Results We formulate a method termed Tree Branches Evaluated Statistically for Tightness (TBEST) for identifying significantly distinct tree branches in hierarchical clusters. For each branch of the tree a measure of distinctness, or tightness, is defined as a rational function of heights, both of the branch and of its parent. A statistical procedure is then developed to determine the significance of the observed values of tightness. We test TBEST as a tool for tree-based data partitioning by applying it to five benchmark datasets, one of them synthetic and the other four each from a different area of biology. For each dataset there is a well-defined partition of the data into classes. In all test cases TBEST performs on par with or better than the existing techniques. Conclusions Based on our benchmark analysis, TBEST is a tool of choice for detection of significantly distinct branches in hierarchical trees grown from biological data. An R language implementation of the method is available from the Comprehensive R Archive Network: cran.r-project.org/web/packages/TBEST/index.html."}, {"title": "Simultaneous estimation of transcript abundances and transcript specific fragment distributions of RNA-Seq data with the Mix2 model", "url": "https://www.biorxiv.org/content/early/2014/06/04/005918", "tag": "Bioinformatics", "abstract": "Quantification of RNA transcripts with RNA-Seq is inaccurate due to positional fragmentation bias, which is not represented appropriately by current statistical models of RNA-Seq data. Another, less investigated, source of error is the inaccuracy of transcript start and end annotations. This article introduces the Mix2 (rd. \u0094mixquare\u0094) model, which uses a mixture of probability distributions to model the transcript specific positional fragment bias. The parameters of the Mix2 model can be efficiently trained with the EM algorithm and are tied between similar transcripts. Transcript specific shift and scale parameters allow the Mix2 model to automatically correct inaccurate transcript start and end annotations. Experiments are conducted on synthetic data covering 7 genes of different complexity, 4 types of fragment bias and correct as well as incorrect transcript start and end annotations. Abundance estimates obtained by Cufflinks 2.2.0, PennSeq and the Mix2 model show superior performance of the Mix2 model in the vast majority of test conditions."}, {"title": "Exploring community structure in biological networks with random graphs", "url": "https://www.biorxiv.org/content/early/2014/06/02/001545", "tag": "Bioinformatics", "abstract": "A modular pattern, also called community structure, is ubiquitous in biological networks. There has been an increased interest in unraveling the community structure of biological systems as it may provide important insights into a system's functional components and the impact of local structures on dynamics at a global scale. Choosing an appropriate community detection algorithm to identify the community structure in an empirical network can be difficult, however, as the many algorithms available are based on a variety of cost functions and are difficult to validate. Even when community structure is identified in an empirical system, disentangling the effect of community structure from other network properties such as clustering coefficient and assortativity can be a challenge. Here, we develop a generative model to produce undirected, simple, connected graphs with a specified degrees and pattern of communities, while maintaining a graph structure that is as random as possible. Additionally, we demonstrate two important applications of our model: (a) to generate networks that can be used to benchmark existing and new algorithms for detecting communities in biological networks; and (b) to generate null models to serve as random controls when investigating the impact of complex network features beyond the byproduct of degree and modularity in empirical biological networks. Our model allows for the systematic study of the presence of community structure and its impact on network function and dynamics. This process is a crucial step in unraveling the functional consequences of the structural properties of biological systems and uncovering the mechanisms that drive these systems."}, {"title": "A comparative study of techniques for differential expression analysis on RNA-Seq data", "url": "https://www.biorxiv.org/content/early/2014/05/28/005611", "tag": "Bioinformatics", "abstract": "Recent advances in next-generation sequencing technology allow high-throughput cDNA sequencing (RNA-Seq) to be widely applied in transcriptomic studies, in particular for detecting differentially expressed genes between groups. Many software packages have been developed for the identification of differentially expressed genes (DEGs) between treatment groups based on RNA-Seq data. However, there is a lack of consensus on how to approach an optimal study design and choice of suitable software for the analysis. In this comparative study we evaluate the performance of three of the most frequently used software tools: Cufflinks-Cuffdiff2, DESeq and edgeR. A number of important parameters of RNA-Seq technology were taken into consideration, including the number of replicates, sequencing depth, and balanced vs. unbalanced sequencing depth within and between groups. We benchmarked results relative to sets of DEGs identified through either quantitative RT-PCR or microarray. We observed that edgeR performs slightly better than DESeq and Cuffdiff2 in terms of the ability to uncover true positives. Overall, DESeq or taking the intersection of DEGs from two or more tools is recommended if the number of false positives is a major concern in the study. In other circumstances, edgeR is slightly preferable for differential expression analysis at the expense of potentially introducing more false positives."}, {"title": "Inference of phenotype-defining functional modules of protein families for microbial plant biomass degraders", "url": "https://www.biorxiv.org/content/early/2014/05/21/005355", "tag": "Bioinformatics", "abstract": "Background: Efficient industrial processes for converting plant lignocellulosic materials into biofuels are a key challenge in global efforts to use alternative energy sources to fossil fuels. Novel cellulolytic enzymes have been discovered from microbial genomes and metagenomes of microbial communities. However, the identification of relevant genes without known homologs, and elucidation of the lignocellulolytic pathways and protein complexes for different microorganisms remain a challenge. Results: We describe a new computational method for the targeted discovery of functional modules of plant biomass-degrading protein families based on their co-occurrence patterns across genomes and metagenome datasets, and the strength of association of these modules with the genomes of known degraders. From more than 6.4 million family annotations for 2884 microbial genomes and 332 taxonomic bins from 18 metagenomes, we identified five functional modules that are distinctive for plant biomass degraders, which we call plant biomass degradation modules (PDMs). These modules incorporated protein families involved in the degradation of cellulose, hemicelluloses and pectins, structural components of the cellulosome and additional families with potential functions in plant biomass degradation. The PDMs could be linked to 81 gene clusters in genomes of known lignocellulose degraders, including previously described clusters of lignocellulolytic genes. On average, 70% of the families of each PDM mapped to gene clusters in known degraders, which served as an additional confirmation of their functional relationships. The presence of a PDM in a genome or taxonomic metagenome bin allowed us to predict an organism's ability for plant biomass degradation accurately. For 15 draft genomes of a cow rumen metagenome, we validated by cross-linking with confirmed cellulolytic enzymes that the PDMs identified plant biomass degraders within a complex microbial community. Conclusions: Functional modules of protein families that realize different aspects of plant cell wall degradation can be inferred from co-occurrence patterns across (meta-)genomes with a probabilistic topic model. The PDMs represent a new resource of protein families and candidate genes implicated in microbial plant biomass degradation. They can be used to predict the ability to degrade plant biomass for a genome or taxonomic bin. The method would also be suitable for characterizing other microbial phenotypes."}, {"title": "READemption - A tool for the computational analysis of deep-sequencing-based transcriptome data", "url": "https://www.biorxiv.org/content/early/2014/05/19/003723", "tag": "Bioinformatics", "abstract": "Summary: RNA-Seq has become a potent and widely used method to qualitatively and quantitatively study transcriptomes. In order to draw biological conclusions based on RNA-Seq data, several steps some of which are computationally intensive, have to betaken. Our READemption pipeline takes care of these individual tasks and integrates them into an easy-to-use tool with a command line interface. To leverage the full power of modern computers, most subcommands of READemption offer parallel data processing. While READemption was mainly developed for the analysis of bacterial primary transcriptomes, we have successfully applied it to analyze RNA-Seq reads from other sample types, including whole transcriptomes, RNA immunoprecipitated with proteins, not only from bacteria, but also from eukaryotes and archaea. Availability and Implementation: READemption is implemented in Python and is published under the ISC open source license. The tool and documentation is hosted at http://pythonhosted.org/READemption (DOI:10.6084/m9.figshare.977849)."}, {"title": "SraTailor: GUI software for visualizing high-throughput sequence read archives", "url": "https://www.biorxiv.org/content/early/2014/05/16/005231", "tag": "Bioinformatics", "abstract": "Raw high-throughput sequence data are deposited in public databases as SRAs (Sequence Read Archives) and are publically available to every researcher. However, in order to graphically visualize the sequence data of interest, the corresponding SRAs must be downloaded and converted into BigWig format through complicated command-line processing. This task requires users to possess skill with script languages and sequence data processing, a requirement that prevents a wide range of biologists from exploiting SRAs. To address these challenges, we developed SraTailor, a GUI (Graphical User Interface) software package that automatically converts an SRA into a BigWig-formatted file. Simplicity of use is one of the most notable features of SraTailor: entering an accession number of an SRA and clicking the mouse are the only steps required in order to obtain BigWig-formatted files and to graphically visualize the extents of reads at given loci. SraTailor is also able to make peak calls and files of other formats, and the software also accepts various command-line-like options. Therefore, this software makes SRAs fully exploitable by a wide range of biologists. SraTailor is freely available at http://www.dev.med.kyushu-u.ac.jp/sra_tailor/."}, {"title": "A new insight for the screening of potential \u03b2-lactamase inhibitors", "url": "https://www.biorxiv.org/content/early/2014/05/15/005181", "tag": "Bioinformatics", "abstract": "The \u03b2-lactamase produces by Aeromonas hydrophila which enables to hydrolyze and inactivate \u03b2-lactam ring of antibiotics. The homology modeling was used to generate the 3-D model of \u03b2-lactamase by using known template 3-D structure. The stereochemical quality and torsion angle of 3-D model were validated. Total eleven effective drugs have been selected and targeted the active amino acid residues in \u03b2-lactamase. The drugs were derivative of \u03b2-lactam ring antibiotics and screening was made by docking. Out of 11 drugs, 3 drugs (Ampicillin, Astreonam and Sultamicillin) were found to be more potent on the basis of robust binding energy between protein-drug interactions. Additionally, homology of \u03b2-lactamase of A. hydrophila resembled with other pathogenic bacteria that used for phylogeny analysis. These findings suggest a new insight for better understanding and useful for designing of novel potent drugs."}, {"title": "qqman: an R package for visualizing GWAS results using Q-Q and manhattan plots", "url": "https://www.biorxiv.org/content/early/2014/05/14/005165", "tag": "Bioinformatics", "abstract": "Summary: Genome-wide association studies (GWAS) have identified thousands of human trait-associated single nucleotide polymorphisms. Here, I describe a freely available R package for visualizing GWAS results using Q-Q and manhattan plots. The qqman package enables the flexible creation of manhattan plots, both genome-wide and for single chromosomes, with optional highlighting of SNPs of interest. Availability: qqman is released under the GNU General Public License, and is freely available on the Comprehensive R Archive Network (http://cran.r-project.org/package=qqman). The source code is available on GitHub (https://github.com/stephenturner/qqman)."}, {"title": "Application of Global Transcriptome Data in Gene Ontology Classification and Construction of a Gene Ontology Interaction Network", "url": "https://www.biorxiv.org/content/early/2014/05/14/004911", "tag": "Bioinformatics", "abstract": "Gene Ontology (GO) classification of statistically significant over/under expressed genes is a commonly used to interpret transcriptomics data in functional genomic analysis. In this approach, all significant genes contribute equally to the final GO classification regardless of their actual expression levels. However, the original level of gene expression can significantly affect protein production and consequently GO term enrichment, and genes with low expression levels can participate in the final GO enrichment through cumulative effects. In addition, GO terms have regulatory relationships that allow the construction of a regulatory network that incorporates gene expression levels to better study biological mechanisms. In this report, we have used gene expression levels in bacteria to determine GO term enrichments. This approach provided the opportunity to enrich GO terms across the entire transcriptome (instead of a subset of differentially expressed genes). In the second part, we show a dynamically developed enriched interaction network between Biological Process GO terms for any gene samples. This type of network presents regulatory relationships between GO terms and their genes. We then demonstrate the efficiency of these methods using public data from two important bacterial pathogens as models. We also explain how these methods help us understand potential pathogenesis mechanisms employed by these bacteria."}, {"title": "Automatic Classification of Human Epithelial Type 2 Cell Indirect Immunofluorescence Images using Cell Pyramid Matching", "url": "https://www.biorxiv.org/content/early/2014/05/05/004739", "tag": "Bioinformatics", "abstract": "This paper describes a novel system for automatic classification of images obtained from Anti-Nuclear Antibody (ANA) pathology tests on Human Epithelial type 2 (HEp-2) cells using the Indirect Immunofluorescence (IIF) protocol. The IIF protocol on HEp-2 cells has been the hallmark method to identify the presence of ANAs, due to its high sensitivity and the large range of antigens that can be detected. However, it suffers from numerous shortcomings, such as being subjective as well as time and labour intensive. Computer Aided Diagnostic (CAD) systems have been developed to address these problems, which automatically classify a HEp-2 cell image into one of its known patterns (eg. speckled, homogeneous). Most of the existing CAD systems use handpicked features to represent a HEp-2 cell image, which may only work in limited scenarios. We propose a novel automatic cell image classification method termed Cell Pyramid Matching (CPM), which is comprised of regional histograms of visual words coupled with the Multiple Kernel Learning framework. We present a study of several variations of generating histograms and show the efficacy of the system on two publicly available datasets: the ICPR HEp-2 cell classification contest dataset and the SNPHEp-2 dataset."}, {"title": "Accounting for sources of bias and uncertainty in copy number-based statistical deconvolution of heterogeneous tumour samples", "url": "https://www.biorxiv.org/content/early/2014/04/30/004655", "tag": "Bioinformatics", "abstract": "Deconvolving heterogeneous tumour samples to identify constituent cell populations with differing copy number profiles using whole genome sequencing data is a challenging problem. Copy number calling algorithms have differential detection rates for different sizes and classes of copy number alterations. This paper describes how uncertainty in classification and differential detection rates can introduce biases in measures of clonal diversity. A simulation strategy is introduced that allows differential detection rates to be adjusted for and this process is shown to minimise bias."}, {"title": "Graph-based data integration predicts long-range regulatory interactions across the human genome", "url": "https://www.biorxiv.org/content/early/2014/04/29/004622", "tag": "Bioinformatics", "abstract": "Transcriptional regulation of gene expression is one of the main processes that affect cell diversification from a single set of genes. Regulatory proteins often interact with DNA regions located distally from the transcription start sites (TSS) of the genes. We developed a computational method that combines open chromatin and gene expression information for a large number of cell types to identify these distal regulatory elements. Our method builds correlation graphs for publicly available DNase-seq and exon array datasets with matching samples and uses graph-based methods to filter findings supported by multiple datasets and remove indirect interactions. The resulting set of interactions was validated with both anecdotal information of known long-range interactions and unbiased experimental data deduced from Hi-C and CAGE experiments. Our results provide a novel set of high-confidence candidate open chromatin regions involved in gene regulation, often located several Mb away from the TSS of their target gene."}, {"title": "A Comparison of Peak Callers Used for DNase-seq Data", "url": "https://www.biorxiv.org/content/early/2014/04/14/003608", "tag": "Bioinformatics", "abstract": "Genome-wide profiling of open chromatin regions using DNase I and high-throughput sequencing (DNase- seq) is an increasingly popular approach for finding and studying regulatory elements. A variety of algorithms have been developed to identify regions of open chromatin from raw sequence-tag data, which has motivated us to assess and compare their performance. In this study, four published, publicly available peak calling algorithms used for DNase-seq data analysis (F-seq, Hotspot, MACS and ZINBA) are assessed at a range of signal thresholds on two published DNase-seq datasets for three cell types. The results were benchmarked against an independent dataset of regulatory regions derived from ENCODE in vivo transcription factor binding data for each particular cell type. The level of overlap between peak regions reported by each algorithm and this ENCODE-derived reference set was used to assess sensitivity and specificity of the algorithms. Our study suggests that F-seq has a slightly higher sensitivity than the next best algorithms. Hotspot and the ChIP-seq oriented method, MACS, both perform competitively when used with their default parameters. However the generic peak finder ZINBA appears to be less sensitive than the other three. We also assess accuracy of each algorithm over a range of signal thresholds. In particular, we show that the accuracy of F-Seq can be considerably improved by using a threshold setting that is different from the default value."}, {"title": "A signature of power law network dynamics", "url": "https://www.biorxiv.org/content/early/2014/04/09/004028", "tag": "Bioinformatics", "abstract": "Can one hear the \u0091sound\u0092 of a growing network? We address the problem of recognizing the topology of evolving biological or social networks. Starting from percolation theory, we analytically prove a linear inverse relationship between two simple graph parameters\u0097the logarithm of the average cluster size and logarithm of the ratio of the edges of the graph to the theoretically maximum number of edges for that graph\u0097that holds for all growing power law graphs. The result establishes a novel property of evolving power-law networks in the asymptotic limit of network size. Numerical simulations as well as fitting to real-world citation co-authorship networks demonstrate that the result holds for networks of finite sizes, and provides a convenient measure of the extent to which an evolving family of networks belongs to the same power-law class."}, {"title": "KmerStream: Streaming algorithms for k-mer abundance estimation", "url": "https://www.biorxiv.org/content/early/2014/04/07/003962.1", "tag": "Bioinformatics", "abstract": "Motivation: Several applications in bioinformatics, such as genome assemblers and error corrections methods, rely on counting and keeping track of k-mers (substrings of length k). Histograms of k-mer frequencies can give valuable insight into the underlying distribution and indicate the error rate and genome size sampled in the sequencing experiment. Results: We present KmerStream, a streaming algorithm for computing statistics for high throughput sequencing data based on the frequency of k-mers. The algorithm runs in time linear in the size of the input and the space requirement are logarithmic in the size of the input. This very low space requirement allows us to deal with much larger datasets than previously presented algorithms. We derive a simple model that allows us to estimate the error rate of the sequencing experiment, as well as the genome size, using only the aggregate statistics reported by KmerStream and validate the accuracy on sequences from a PhiX control. As an application we show how KmerStream can be used to compute the error rate of a DNA sequencing experiment. We run KmerStream on a set of 2656 whole genome sequenced individuals and compare the error rate to quality values reported by the sequencing equipment. We discover that while the quality values alone are largely reliable as a predictor of error rate, there is considerable variability in the error rates between sequencing runs, even when accounting for reported quality values. Availability: The tool KmerStream is written in C++ and is released under a GPL license. It is freely available at https://github.com/pmelsted/KmerStream"}, {"title": "SplitMEM: Graphical pan-genome analysis with suffix skips", "url": "https://www.biorxiv.org/content/early/2014/04/06/003954", "tag": "Bioinformatics", "abstract": "Motivation: With the rise of improved sequencing technologies, genomics is expanding from a single reference per species paradigm into a more comprehensive pan-genome approach with multiple individuals represented and analyzed together. One of the most sophisticated data structures for representing an entire population of genomes is a compressed de Bruijn graph. The graph structure can robustly represent simple SNPs to complex structural variations far beyond what can be done from linear sequences alone. As such there is a strong need to develop algorithms that can efficiently construct and analyze these graphs. Results: In this paper we explore the deep topological relationships between the suffix tree and the compressed de Bruijn graph. We introduce a novel O(n log n) time and space algorithm called splitMEM, that directly constructs the compressed de Bruijn graph for a pan-genome of total length n. To achieve this time complexity, we augment the suffix tree with suffix skips, a new construct that allows us to traverse several suffix links in constant time, and use them to efficiently decompose maximal exact matches (MEMs) into the graph nodes. We demonstrate the utility of splitMEM by analyzing the pan- genomes of 9 strains of Bacillus anthracis and 9 strains of Escherichia coli to reveal the properties of their core genomes. Availability: The source code and documentation are available open- source at http://splitmem.sourceforge.net"}, {"title": "HGTector: An automated method facilitating genome-wide discovery of putative horizontal gene transfers", "url": "https://www.biorxiv.org/content/early/2014/04/02/003731", "tag": "Bioinformatics", "abstract": "A new computational method of rapid, exhaustive and genome-wide detection of HGT was developed, featuring the systematic analysis of BLAST hit distribution patterns in the context of a priori defined hierarchical evolutionary categories. Genes that fall beyond a series of statistically determined thresholds are identified as not adhering to the typical vertical his-tory of the organisms in question, but instead having a putative horizontal origin. Tests on simulated genomic data suggest that this approach effectively targets atypically distributed genes that are highly likely to be HGT-derived, and exhibits robust performance compared to conventional BLAST-based approaches. This method was further tested on real genomic datasets, including Rickettsia genomes, and was compared to previous studies. Results show consistency with currently employed cat-egories of HGT prediction methods. In-depth analysis of both simulated and real genomic data suggests that the method is no-tably insensitive to stochastic events such as gene loss, rate variation and database error, which are common challenges to the current methodology. An automated pipeline was created to implement this approach and was made publicly available at: https://github.com/DittmarLab/HGTector. The program is versatile, easily deployed, has low requirements for computation-al resources, and is an effective tool for initial or standalone large-scale discovery of candidate HGT-derived genes."}, {"title": "The Scramble Conversion Tool", "url": "https://www.biorxiv.org/content/early/2014/03/28/003640", "tag": "Bioinformatics", "abstract": "Motivation: The reference CRAM file format implementation is in Java. We present \"Scramble\": a new C implementation of SAM, BAM and CRAM file I/O. Results: The C API for CRAM is 1.5-1.7x slower than BAM at decoding, but 1.8-2.6x faster at encoding. We see file size savings of 40-50%. Availability: Source code is available from http://sourceforge.net/projects/staden/files/io lib/ Contact: jkb@sanger.ac.uk"}, {"title": "Divide and Conquer approach for Genome Classification based on subclass characterization", "url": "https://www.biorxiv.org/content/early/2014/03/20/003475", "tag": "Bioinformatics", "abstract": "Classification of large grass genome sequences has major challenges in functional genomes. The presence of motifs in grass genome chains can make the prediction of the functional behavior of grass genome possible. The correlation between grass genome properties and their motifs is not always obvious, since more than one motif may exist within a genome chain. Due to the complexity of this association most pattern classification algorithms are either vain or time consuming. Attempted to a reduction of high dimensional data that utilizes DAC technique is presented. Data are disjoining into equal multiple sets while preserving the original data distribution in each set. Then, multiple modules are created by using the data sets as independent training sets and classified into respective modules. Finally, the modules are combined to produce the final classification rules, containing all the previously extracted information. The methodology is tested using various grass genome data sets. Results indicate that the time efficiency of our algorithm is improved compared to other known data mining algorithms."}, {"title": "Analysis of stop-gain and frameshift variants in human innate immunity genes", "url": "https://www.biorxiv.org/content/early/2014/03/17/003376", "tag": "Bioinformatics", "abstract": "Loss-of-function variants in innate immunity genes are associated with Mendelian disorders in the form of primary immunodeficiencies. Recent resequencing projects report that stop-gains and frameshifts are collectively prevalent in humans and could be responsible for some of the inter-individual variability in innate immune response. Current computational approaches evaluating loss-of-function in genes carrying these variants rely on gene-level characteristics such as evolutionary conservation and functional redundancy across the genome. However, innate immunity genes represent a particular case because they are more likely to be under positive selection and duplicated. To create a ranking of severity that would be applicable to the innate immunity genes we first evaluated 17764 stop-gain and 13915 frameshift variants from the NHLBI Exome Sequencing Project and 1000 Genomes Project. Sequence-based features such as loss of functional domains, isoform-specific truncation and non-sense mediated decay were found to correlate with variant allele frequency and validated with gene expression data. We integrated these features in a Bayesian classification scheme and benchmarked its use in predicting pathogenic variants against OMIM disease stop-gains and frameshifts. The classification scheme was applied in the assessment of 335 stop-gains and 236 frameshifts affecting 227 interferon-stimulated genes. The sequence-based score ranks variants in innate immunity genes according to their potential to cause disease, and complements existing gene-based pathogenicity scores."}, {"title": "A reassessment of consensus clustering for class discovery", "url": "https://www.biorxiv.org/content/early/2014/03/11/002642", "tag": "Bioinformatics", "abstract": "Consensus clustering (CC) is an unsupervised class discovery method widely used to study sample heterogeneity in high-dimensional datasets. It calculates \"consensus rate\" between any two samples as how frequently they are grouped together in repeated clustering runs under a certain degree of random perturbation. The pairwise consensus rates form a between-sample similarity matrix, which has been used (1) as a visual proof that clusters exist, (2) for comparing stability among clusters, and (3) for estimating the optimal number (K) of clusters. However, the sensitivity and specificity of CC have not been systemically studied. To assess its performance, we investigated the most common implementations of CC; and compared CC with other popular methods that also focus on cluster stability and estimation of K. We evaluated these methods using simulated datasets with either known structure or known absence of structure. Our results showed that (1) CC was able to divide randomly generated unimodal data into pre-specified numbers of clusters, and was able to show apparent stability of these chance partitions of known cluster-less data; (2) for data with known structure, the proportion of ambiguously clustered (PAC) pairs infers the known number of clusters more reliably than several commonly used K estimating methods; and (3) validation of the optimal K by choosing the most discriminant genes from the discovery cohort and applying them in an independent cohort often exaggerates the confidence in K due to inherent gene-gene correlations among the selected genes. While these results do not yet prove that any of the published studies using CC has generated false positive findings, they show that datasets with subtle or no structure are fully capable of producing strong evidence of consensus clustering. We therefore recommend caution is using CC in class discovery and validation."}, {"title": "Alignathon: A competitive assessment of whole genome alignment methods.", "url": "https://www.biorxiv.org/content/early/2014/03/10/003285", "tag": "Bioinformatics", "abstract": "Background: Multiple sequence alignments (MSAs) are a prerequisite for a wide variety of evolutionary analyses. Published assessments and benchmark datasets for protein and, to a lesser extent, global nucleotide MSAs are available, but less effort has been made to establish benchmarks in the more general problem of whole genome alignment (WGA). Results: Using the same model as the successful Assemblathon competitions, we organized a competitive evaluation in which teams submitted their alignments, and assessments were performed collectively after all the submissions were received. Three datasets were used: two of simulated primate and mammalian phylogenies, and one of 20 real fly genomes. In total 35 submissions were assessed, submitted by ten teams using 12 different alignment pipelines. Conclusions: We found agreement between independent simulation-based and statistical assessments, indicating that there are substantial accuracy differences between contemporary alignment tools. We saw considerable difference in the alignment quality of differently annotated regions, and found few tools aligned the duplications analysed. We found many tools worked well at shorter evolutionary distances, but fewer performed competitively at longer distances. We provide all datasets, submissions and assessment programs for further study, and provide, as a resource for future benchmarking, a convenient repository of code and data for reproducing the simulation assessments."}, {"title": "Spectacle: Faster and more accurate chromatin state annotation using spectral learning", "url": "https://www.biorxiv.org/content/early/2014/03/09/002725", "tag": "Bioinformatics", "abstract": "Recently, a wealth of epigenomic data has been generated by biochemical assays and next-generation sequencing (NGS) technologies. In particular, histone modification data generated by the ENCODE project and other large-scale projects show specific patterns associated with regulatory elements in the human genome.It is important to build a unified statistical model to decipher the patterns of multiple histone modifications in a cell type to annotate chromatin states such as transcription start sites, enhancers and transcribed regions rather than to map histone modifications individually to regulatory elements. Several genome-wide statistical models have been developed based on hidden Markov models (HMMs). These methods typically use the Expectation-Maximization (EM) algorithm to estimate the parameters of the model.Here we used spectral learning, a state-of-the-art parameter estimation algorithm in machine learning.We found that spectral learning plus a few (up to five) iterations of local optimization of the likelihood outperforms the standard EM algorithm.We also evaluated our software implementation called Spectacle on independent biological datasets and found that Spectacle annotated experimentally defined functional elements such as enhancers significantly better than a previous state-of-the-art method. Spectacle can be downloaded from https://github.com/jiminsong/Spectacle ."}, {"title": "Detecting translational regulation by change point analysis of ribosome profiling datasets", "url": "https://www.biorxiv.org/content/early/2014/03/05/003210", "tag": "Bioinformatics", "abstract": "Ribo-Seq maps the location of translating ribosomes on mature mRNA transcripts. While ribosome density is constant along the length of the mRNA coding region, it can be altered by translational regulatory events. In this study, we developed a method to detect translational regulation of individual mRNAs from their ribosome profiles, utilizing changes in ribosome density. We used mathematical modelling to show that changes in ribosome density should occur along the mRNA at the point of regulation. We analyzed a Ribo-Seq dataset obtained for mouse embryonic stem cells and showed that normalization by corresponding RNA-Seq can be used to improve the Ribo-Seq quality by removing bias introduced by deep-sequencing and alignment artefacts. After normalization, we applied a change point algorithm to detect changes in ribosome density present in individual mRNA ribosome profiles. Additional sequence and gene isoform information obtained from the UCSC Genome Browser allowed us to further categorize the detected changes into different mechanisms of regulation. In particular, we detected several mRNAs with known post-transcriptional regulation, e.g. premature termination for selenoprotein mRNAs and translational control of Atf4, but also several more mRNAs with hitherto unknown translational regulation. Additionally, our approach proved useful for identification of new gene isoforms."}, {"title": "Genomic Repeat Element Analyzer for Mammals (GREAM)", "url": "https://www.biorxiv.org/content/early/2014/02/28/003111", "tag": "Bioinformatics", "abstract": "Background: Understanding the mechanism behind the transcriptional regulation of genes is still a challenge. Recent findings indicate that the genomic repeat elements (such as LINES, SINES and LTRs) could play an important role in the transcription control. Hence, it is important to further explore the role of genomic repeat elements in the gene expression regulation, and perhaps in other molecular processes. Although many computational tools exists for repeat element analysis, almost all of them simply identify and/or classifying the genomic repeat elements within query sequence(s); none of them facilitate identification of repeat elements that are likely to have a functional significance, particularly in the context of transcriptional regulation. Result: We developed the 'Genomic Repeat Element Analyzer for Mammals' (GREAM) to allow gene-centric analysis of genomic repeat elements in 17 mammalian species, and validated it by comparing with some of the existing experimental data. The output provides a categorized list of the specific type of transposons, retro-transposons and other genome-wide repeat elements that are statistically over-represented across specific neighborhood regions of query genes. The position and frequency of these elements, within the specified regions, are displayed as well. The tool also offers queries for position-specific distribution of repeat elements within chromosomes. In addition, GREAM facilitates the analysis of repeat element distribution across the neighborhood of orthologous genes. Conclusion: GREAM allows researchers to short-list the potentially important repeat elements, from the genomic neighborhood of genes, for further experimental analysis. GREAM is free and available for all at http://resource.ibab.ac.in/GREAM/"}, {"title": "Efficient synergistic single-cell genome assembly", "url": "https://www.biorxiv.org/content/early/2014/02/24/002972", "tag": "Bioinformatics", "abstract": "As the vast majority of all microbes are unculturable, single-cell sequencing has become a significant method to gain insight into microbial physiology. Single-cell sequencing methods, currently powered by multiple displacement genome amplification (MDA), have passed important milestones such as finishing and closing the genome of a prokaryote. However, the quality and reliability of genome assemblies from single cells are still unsatisfactory due to uneven coverage depth and the absence of scattered chunks of the genome in the final collection of reads caused by MDA bias. In this work, our new algorithm Hybrid De novo Assembler (HyDA) demonstrates the power of co-assembly of multiple single-cell genomic data sets through significant improvement of the assembly quality in terms of predicted functional elements and length statistics. Co-assemblies contain significantly more base pairs and protein coding genes, cover more subsystems, and consist of longer contigs compared to individual assemblies by the same algorithm as well as state-of-the-art single-cell assemblers SPAdes and IDBA-UD. Hybrid De novo Assembler (HyDA) is also able to avoid chimeric assemblies by detecting and separating shared and exclusive pieces of sequence for input data sets. By replacing one deep single-cell sequencing experiment with a few single-cell sequencing experiments of lower depth, the co-assembly method can hedge against the risk of failure and loss of the sample, without significantly increasing sequencing cost. Application of the single-cell co-assembler HyDA to the study of three uncultured members of an alkane-degrading methanogenic community validated the usefulness of the co-assembly concept."}, {"title": "Functional normalization of 450k methylation array data improves replication in large cancer studies", "url": "https://www.biorxiv.org/content/early/2014/02/23/002956", "tag": "Bioinformatics", "abstract": "We propose an extension to quantile normalization which removes unwanted technical variation using control probes. We adapt our algorithm, functional normalization, to the Illumina 450k methylation array and address the open problem of normalizing methylation data with global epigenetic changes, such as human cancers. Using datasets from The Cancer Genome Atlas and a large case-control study, we show that our algorithm outperforms all existing normalization methods with respect to replication of results between experiments, and yields robust results even in the presence of batch effects. Functional normalization can be applied to any microarray platform, provided suitable control probes are available."}, {"title": "A GWAS platform built on iPlant cyber-infrastructure", "url": "https://www.biorxiv.org/content/early/2014/02/20/002881", "tag": "Bioinformatics", "abstract": "We demonstrated a flexible Genome-Wide Association Study (GWAS) platform built upon the iPlant Collaborative Cyber-infrastructure. The platform supports big data management, sharing, and large scale study of both genotype and phenotype data on clusters. End users can add their own analysis tools, and create customized analysis workflows through the graphical user interfaces in both iPlant Discovery Environment and BioExtract server."}, {"title": "Sashimi plots: Quantitative visualization of alternative isoform expression from RNA-seq data", "url": "https://www.biorxiv.org/content/early/2014/02/11/002576", "tag": "Bioinformatics", "abstract": "Analysis of RNA sequencing (RNA-Seq) data revealed that the vast majority of human genes express multiple mRNA isoforms, produced by alternative pre-mRNA splicing and other mechanisms, and that most alternative isoforms vary in expression between human tissues. As RNA-Seq datasets grow in size, it remains challenging to visualize isoform expression across multiple samples. We present Sashimi plots, a quantitative multi-sample visualization of RNA-Seq reads aligned to gene annotations, which enables quantitative comparison of isoform usage across samples or experimental conditions. Given an input annotation and spliced alignments of reads from a sample, a region of interest is visualized in a Sashimi plot as follows: (i) alignments in exons are represented as read densities (optionally normalized by length of genomic region and coverage), and (ii) splice junction reads are drawn as arcs connecting a pair of exons, where arc width is drawn proportional to the number of reads aligning to the junction."}, {"title": "Automated ensemble assembly and validation of microbial genomes", "url": "https://www.biorxiv.org/content/early/2014/02/07/002469", "tag": "Bioinformatics", "abstract": "Background: The continued democratization of DNA sequencing has sparked a new wave of development of genome assembly and assembly validation methods. As individual research labs, rather than centralized centers, begin to sequence the majority of new genomes, it is important to establish best practices for genome assembly. However, recent evaluations such as GAGE and the Assemblathon have concluded that there is no single best approach to genome assembly. Instead, it is preferable to generate multiple assemblies and validate them to determine which is most useful for the desired analysis; this is a labor-intensive process that is often impossible or unfeasible. Results: To encourage best practices supported by the community, we present iMetAMOS, an automated ensemble assembly pipeline; iMetAMOS encapsulates the process of running, validating, and selecting a single assembly from multiple assemblies. iMetAMOS packages several leading open-source tools into a single binary that automates parameter selection and execution of multiple assemblers, scores the resulting assemblies based on multiple validation metrics, and annotates the assemblies for genes and contaminants. We demonstrate the utility of the ensemble process on 225 previously unassembled Mycobacterium tuberculosis genomes as well as a Rhodobacter sphaeroides benchmark dataset. On these real data, iMetAMOS reliably produces validated assemblies and identifies potential contamination without user intervention. In addition, intelligent parameter selection produces assemblies of R. sphaeroides that exceed the quality of those from the GAGE-B evaluation, affecting the relative ranking of some assemblers. Conclusions: Ensemble assembly with iMetAMOS provides users with multiple, validated assemblies for each genome. Although computationally limited to small or mid-sized genomes, this approach is the most effective and reproducible means for generating high-quality assemblies and enables users to select an assembly best tailored to their specific needs."}, {"title": "A phase diagram for gene selection and disease classification", "url": "https://www.biorxiv.org/content/early/2014/02/05/002360", "tag": "Bioinformatics", "abstract": "Identifying a small subset of discriminate genes is important for predicting clinical outcomes and facilitating disease diagnosis. Based on the model population analysis framework, we present a method, called PHADIA, which is able to output a phase diagram displaying the predictive ability of each variable, which provides an intuitive way for selecting informative variables. Using two publicly available microarray datasets, it\u0092s demonstrated that our method can selects a few informative genes and achieves significantly better or comparable classification accuracy compared to the reported results in the literature. The source codes are freely available at: www.libpls.net."}, {"title": "SNP-guided identification of monoallelic DNA-methylation events from enrichment-based sequencing data", "url": "https://www.biorxiv.org/content/early/2014/02/04/002352", "tag": "Bioinformatics", "abstract": "Monoallelic gene expression is typically initiated early in the development of an organism. Dysregulation of monoallelic gene expression has already been linked to several non-Mendelian inherited genetic disorders. In humans, DNA-methylation is deemed to be an important regulator of monoallelic gene expression, but only few examples are known. One important reason is that current, cost-affordable truly genome-wide methods to assess DNA-methylation are based on sequencing post enrichment. Here, we present a new methodology that combines methylomic data from MethylCap-seq with associated SNP profiles to identify monoallelically methylated loci. Using the Hardy-Weinberg theorem for each SNP locus, it could be established whether the observed frequency of samples featured by biallelic methylation was lower than randomly expected. Applied on 334 MethylCap-seq samples of very diverse origin, this resulted in the identification of 80 genomic regions featured by monoallelic DNA-methylation. Of these 80 loci, 49 are located in genic regions of which 25 have already been linked to imprinting. Further analysis revealed statistically significant enrichment of these loci in promoter regions, further establishing the relevance and usefulness of the method. Additional validation of the found loci was done using 14 whole-genome bisulfite sequencing data sets. Importantly, the developed approach can be easily applied to other enrichment-based sequencing technologies, such as the ChIP-seq-based identification of monoallelic histone modifications."}, {"title": "Improving Protein Docking with Constraint Programming and Coevolution Data", "url": "https://www.biorxiv.org/content/early/2014/02/03/002329", "tag": "Bioinformatics", "abstract": "Background: Constraint programming (CP) is usually seen as a rigid approach, focusing on crisp, precise, distinctions between what is allowed as a solution and what is not. At first sight, this makes it seem inadequate for bioinformatics applications that rely mostly on statistical parameters and optimisation. The prediction of protein interactions, or protein docking, is one such application. And this apparent problem with CP is particularly evident when constraints are provided by noisy data, as it is the case when using the statistical analysis of Multiple Sequence Alignments (MSA) to extract coevolution information. The goal of this paper is to show that this first impression is misleading and that CP is a useful technique for improving protein docking even with data as vague and noisy as the coevolution indicators that can be inferred from MSA. Results: Here we focus on the study of two protein complexes. In one case we used a simplified estimator of interaction propensity to infer a set of five candidate residues for the interface and used that set to constrain the docking models. Even with this simplified approach and considering only the interface of one of the partners, there is a visible focusing of the models around the correct configuration. Considering a set of 400 models with the best geometric contacts, this constraint increases the number of models close to the target (RMSD \u00a15\u00c5) from 2 to 5 and decreases the RMSD of all retained models from 26\u00c5 to 17.5\u00c5. For the other example we used a more standard estimate of coevolving residues, from the Co-Evolution Analysis using Protein Sequences (CAPS) software. Using a group of three residues identified from the sequence alignment as potentially co-evolving to constrain the search, the number of complexes similar to the target among the 50 highest scoring docking models increased from 3 in the unconstrained docking to 30 in the constrained docking. Conclusions: Although only a proof-of-concept application, our results show that, with suitably designed constraints, CP allows us to integrate coevolution data, which can be inferred from databases of protein sequences, even though the data is noisy and often \u0093fuzzy\u0094, with no well-defined discontinuities. This also shows, more generally, that CP in bioinformatics needs not be limited to the more crisp cases of finite domains and explicit rules but can also be applied to a broader range of problems that depend on statistical measurements and continuous data."}, {"title": "FALDO: A semantic standard for describing the location of nucleotide and protein feature annotation.", "url": "https://www.biorxiv.org/content/early/2014/02/02/002121", "tag": "Bioinformatics", "abstract": "Nucleotide and protein sequence feature annotations are essential to understand biology on the genomic, transcriptomic, and proteomic level. Using Semantic Web technologies to query biological annotations, there was no standard that described this potentially complex location information as subject-predicate-object triples. We have developed an ontology, the Feature Annotation Location Description Ontology (FALDO), to describe the positions of annotated features on linear and circular sequences. FALDO can be used to describe nucleotide features in sequence records, protein annotations, and glycan binding sites, among other features in coordinate systems of the aforementioned \u0093omics\u0094 areas. Using the same data format to represent sequence positions that are independent of file formats allows us to integrate sequence data from multiple sources and data types. The genome browser JBrowse is used to demonstrate accessing multiple SPARQL endpoints to display genomic feature annotations, as well as protein annotations from UniProt mapped to genomic locations. Our ontology allows users to uniformly describe \u0096 and potentially merge \u0096 sequence annotations from multiple sources. Data sources using FALDO can prospectively be retrieved using federalised SPARQL queries against public SPARQL endpoints and/or local private triple stores."}, {"title": "Genome-wide DNA methylome analysis reveals novel epigenetically dysregulated non-coding RNAs in human breast cancer", "url": "https://www.biorxiv.org/content/early/2014/01/28/002204", "tag": "Bioinformatics", "abstract": "The development of human breast cancer is driven by changes in the genetic and epigenetic landscape of the cell. Despite growing appreciation of the importance of epigenetics in breast cancers, our knowledge of epigenetic alterations of non-coding RNAs (ncRNAs) in breast cancers remains limited. Here, we explored the epigenetic patterns of ncRNAs in breast cancers via a sequencing-based comparative methylome analysis, mainly focusing on two most popular ncRNA biotypes, long non-coding RNAs (lncRNAs) and miRNAs. Besides global hypomethylation and extensive CpG islands (CGIs) hypermethylation, we observed widely aberrant methylation in the promoters of ncRNAs, which was higher than that of protein-coding genes. Specifically, intergenic ncRNAs were observed to contribute a large slice of the aberrantly methylated ncRNA promoters. Moreover, we summarized five patterns of ncRNA promoter aberrant methylation in the context of genomic CGIs, where aberrant methylation occurred not only on the CGIs, but also flanking regions and CGI sparse promoters. Integration with transcriptional datasets, we found that the ncRNA promoter methylation events were associated with transcriptional changes. Furthermore, a panel of ncRNAs were identified as biomarkers that were able to discriminate between disease phenotypes (AUCs>0.90). Finally, the potential functions for aberrantly methylated ncRNAs were predicted based on similar patterns, adjacency and/or target genes, highlighting that ncRNAs and coding genes coordinately mediated pathways dysregulation in the development and progression of breast cancers. This study presents the aberrant methylation patterns of ncRNAs, which will be a highly valuable resource for investigations at understanding epigenetic regulation of breast cancers."}, {"title": "A Powerful Approach for Identification of Differentially Transcribed mRNA Isoforms", "url": "https://www.biorxiv.org/content/early/2014/01/26/002097", "tag": "Bioinformatics", "abstract": "Next generation sequencing is being increasingly used for transcriptome-wide analysis of differential gene expression. The primary goal in profiling expression is to identify genes or RNA isoforms differentially expressed between specific conditions. Yet, the next generation sequence-based count data are essentially different from the microarray data that are continuous type, therefore, the statistical methods developed well over the last decades cannot be applicable. For this reason, a variety of new statistical methods based on count data of transcript reads has been correspondingly developed. But currently the transcriptomic count data coming only from a few replicate libraries have high technical noise and small sample size bias, performances of these new methods are not desirable. We here developed a new statistical method specifically applicable to small sample count data called mBeta t-test for identifying differentially expressed gene or isoforms on the basis of the Beta t-test. The results obtained from simulated and real data showed that the mBeta t-test method significantly outperformed the existing statistical methods in all given scenarios. Findings of our method were validated by qRT-PCR experiments. The mBeta t-test method significantly reduced true false discoveries in differentially expressed genes or isoforms so that it had high work efficiencies in all given scenarios. In addition, the mBeta t-test method showed high stability in performance of statistical analysis and in estimation of FDR. These strongly suggests that our mBeta t-test method would offer us a creditable and reliable result of statistical analysis in practice."}, {"title": "Joint variant and de novo mutation identification on pedigrees from high-throughput sequencing data", "url": "https://www.biorxiv.org/content/early/2014/01/24/001958", "tag": "Bioinformatics", "abstract": "The analysis of whole-genome or exome sequencing data from trios and pedigrees has being successfully applied to the identification of disease-causing mutations. However, most methods used to identify and genotype genetic variants from next-generation sequencing data ignore the relationships between samples, resulting in significant Mendelian errors, false positives and negatives. Here we present a Bayesian network framework that jointly analyses data from all members of a pedigree simultaneously using Mendelian segregation priors, yet providing the ability to detect de novo mutations in offspring, and is scalable to large pedigrees. We evaluated our method by simulations and analysis of WGS data from a 17 individual, 3-generation CEPH pedigree sequenced to 50X average depth. Compared to singleton calling, our family caller produced more high quality variants and eliminated spurious calls as judged by common quality metrics such as Ti/Tv, Het/Hom ratios, and dbSNP/SNP array data concordance. We developed a ground truth dataset to further evaluate our calls by identifying recombination cross-overs in the pedigree and testing variants for consistency with the inferred phasing, and we show that our method significantly outperforms singleton and population variant calling in pedigrees. We identify all previously validated de novo mutations in NA12878, concurrent with a 7X precision improvement. Our results show that our method is scalable to large genomics and human disease studies and allows cost optimization by rational sequencing capacity distribution."}, {"title": "Estimate of Within Population Incremental Selection Through Branch Imbalance in Lineage Trees", "url": "https://www.biorxiv.org/content/early/2014/01/23/002014", "tag": "Bioinformatics", "abstract": "Incremental selection within a population, defined as a limited fitness change following a mutation, is an important aspect of many evolutionary processes and can significantly affect a large number of mutations through the genome. Strongly advantageous or deleterious mutations are detected through the fixation of mutations in the population, using the synonymous to non-synonymous mutations ratio in sequences. There are currently to precise methods to estimate incremental selection occurring over limited periods. We here provide for the first time such a detailed method and show its precision and its applicability to the genomic analysis of selection. A special case of evolution is rapid, short term micro-evolution, where organism are under constant adaptation, occurring for example in viruses infecting a new host, B cells mutating during a germinal center reactions or mitochondria evolving within a given host. The proposed method is a novel mixed lineage tree/sequence based method to detect within population selection as defined by the effect of mutations on the average number of offspring. Specifically, we pro-pose to measure the log of the ratio between the number of leaves in lineage trees branches following synonymous and non-synonymous mutations. This method does not suffer from the need of a baseline model and is practically not affected by sampling biases. In order to show the wide applicability of this method, we apply it to multiple cases of micro-evolution, and show that it can detect genes and inter-genic regions using the selection rate and detect selection pressures in viral proteins and in the immune response to pathogens."}, {"title": "An Improved Search Algorithm to Find G-Quadruplexes in Genome Sequences", "url": "https://www.biorxiv.org/content/early/2014/01/23/001990", "tag": "Bioinformatics", "abstract": "A growing body of data suggests that the secondary structures adopted by G-rich polynucleotides may be more diverse than previously thought and that the definition of G-quadruplex-forming sequences should be broadened. We studied solution structures of a series of naturally occurring and model single-stranded DNA fragments defying the G3+NL1G3+NL2G3+NL3G3+ formula, which is used in most of the current GQ-search algorithms. The results confirm the GQ-forming potential of such sequences and suggest the existence of new types of GQs. We developed an improved (broadened) GQ-search algorithm (http://niifhm.ru/nauchnye-issledovanija/otdel-molekuljarnoj-biologii-i-genetiki/laboratorija-iskusstvennogo-antitelogeneza/497-2/) that accounts for the recently reported new types of GQs."}, {"title": "On the optimal trimming of high-throughput mRNAseq data", "url": "https://www.biorxiv.org/content/early/2014/01/14/000422", "tag": "Bioinformatics", "abstract": "The widespread and rapid adoption of high-throughput sequencing technologies has changed the face of modern studies of evolutionary genetics. Indeed, newer sequencing technologies, like Illumina sequencing, have afforded researchers the opportunity to gain a deep understanding of genome level processes that underlie evolutionary change. In particular, researchers interested in functional biology and adaptation have used these technologies to sequence mRNA transcriptomes of specific tissues, which in turn are often compared to other tissues, or other individuals with different phenotypes. While these techniques are extremely powerful, careful attention to data quality is required. In particular, because high-throughput sequencing is more error-prone than traditional Sanger sequencing, quality trimming of sequence reads should be an important step in all data processing pipelines. While several software packages for quality trimming exist, no general guidelines for the specifics of trimming have been developed. Here, using empirically derived sequence data, I provide general recommendations regarding the optimal strength of trimming, specifically in mRNA-Seq studies. Although very aggressive quality trimming is common, this study suggests that a more gentle trimming, specifically of those nucleotides whose Phred score < 2 or < 5, is optimal for most studies across a wide variety of metrics."}, {"title": "SIANN: Strain Identification by Alignment to Near Neighbors", "url": "https://www.biorxiv.org/content/early/2014/01/10/001727", "tag": "Bioinformatics", "abstract": "Next-generation sequencing is increasingly being used to study samples composed of mixtures of organisms, such as in clinical applications where the presence of a pathogen at very low abundance may be highly important. We present an analytical method (SIANN: Strain Identification by Alignment to Near Neighbors) specifically designed to rapidly detect a set of target organisms in mixed samples that achieves a high degree of species- and strain-specificity by aligning short sequence reads to the genomes of near neighbor organisms, as well as that of the target. Empirical benchmarking alongside the current state-of-the-art methods shows an extremely high Positive Predictive Value, even at very low abundances of the target organism in a mixed sample. SIANN is available as an Illumina BaseSpace app, as well as through Signature Science, LLC. SIANN results are presented in a streamlined report designed to be comprehensible to the non-specialist user, providing a powerful tool for rapid species detection in a mixed sample. By focusing on a set of (customizable) target organisms and their near neighbors, SIANN can operate quickly and with low computational requirements while delivering highly accurate results."}, {"title": "Algorithms in Stringomics (I): Pattern-Matching against \"Stringomes\"", "url": "https://www.biorxiv.org/content/early/2014/01/02/001669", "tag": "Bioinformatics", "abstract": "This paper reports an initial design of new data-structures that generalizes the idea of pattern- matching in stringology, from its traditional usage in an (unstructured) set of strings to the arena of a well-structured family of strings. In particular, the object of interest is a family of strings composed of blocks/classes of highly similar \u0093stringlets,\u0094 and thus mimic a population of genomes made by concatenating haplotype-blocks, further constrained by haplotype-phasing. Such a family of strings, which we dub \u0093stringomes,\u0094 is formalized in terms of a multi-partite directed acyclic graph with a source and a sink. The most interesting property of stringomes is probably the fact that they can be represented efficiently with compression up to their k-th order empirical entropy, while ensuring that the compression does not hinder the pattern-matching counting and reporting queries \u0096 either internal to a block or spanning two (or a few constant) adjacent blocks. The solutions proposed here have immediate applications to next-generation sequencing technologies, base-calling, expression profiling, variant-calling, population studies, onco-genomics, cyber security trace analysis and text retrieval."}, {"title": "A coarse-grained elastic network atom contact model and its use in the simulation of protein dynamics and the prediction of the effect of mutations", "url": "https://www.biorxiv.org/content/early/2013/12/20/001495", "tag": "Bioinformatics", "abstract": "Normal mode analysis (NMA) methods are widely used to study dynamic aspects of protein structures. Two critical components of NMA methods are coarse-graining in the level of simplification used to represent protein structures and the choice of potential energy functional form. There is a trade-off between speed and accuracy in different choices. In one extreme one finds accurate but slow molecular-dynamics based methods with all-atom representations and detailed atom potentials. On the other extreme, fast elastic network model (ENM) methods with C\u03b1\uf02donly representations and simplified potentials that based on geometry alone, thus oblivious to protein sequence. Here we present ENCoM, an Elastic Network Contact Model that employs a potential energy function that includes a pairwise atom-type non-bonded interaction term and thus makes it possible to consider the effect of the specific nature of amino-acids on dynamics within the context of NMA. ENCoM is as fast as existing ENM methods and outperforms such methods in the generation of conformational ensembles. Here we introduce a new application for NMA methods with the use of ENCoM in the prediction of the effect of mutations on protein stability. While existing methods are based on machine learning or enthalpic considerations, the use of ENCoM, based on vibrational normal modes, is based on entropic considerations. This represents a novel area of application for NMA methods and a novel approach for the prediction of the effect of mutations. We compare ENCoM to a large number of methods in terms of accuracy and self-consistency. We show that the accuracy of ENCoM is comparable to that of the best existing methods. We show that existing methods are biased towards the prediction of destabilizing mutations and that ENCoM is less biased at predicting stabilizing mutations."}, {"title": "libRoadRunner: A High Performance SBML Compliant Simulator", "url": "https://www.biorxiv.org/content/early/2013/12/12/001230", "tag": "Bioinformatics", "abstract": "We describe libRoadRunner, a cross-platform, open-source, high performance C++ library for running and analyzing SBML-compliant models. libRoadRunner was created primarily to achieve high performance, ease of use, portability and an extensible architecture. libRoadRunner includes a comprehensive API, Plugin support, Python scripting and additional functionality such as stoichiometric and metabolic control analysis. To maximize collaboration, we made libRoadRunner open source and released it under the Apache License, Version 2.0. To facilitate reuse, we have developed comprehensive Python bindings using SWIG (swig.org) and a C API. LibRoadRunner uses a number of statically linked third party libraries including: LLVM, libSBML, CVODE, NLEQ2, LAPACK and Poco. LibRoadRunner is supported on Windows, Mac OS X and Linux. Online documentation, build instructions and git source repository are available at http://www.libroadrunner.org"}, {"title": "A Bayesian Method to Incorporate Hundreds of Functional Characteristics with Association Evidence to Improve Variant Prioritization", "url": "https://www.biorxiv.org/content/early/2013/12/04/000984", "tag": "Bioinformatics", "abstract": "The increasing quantity and quality of functional genomic information motivate the assessment and integration of these data with association data, including data originating from genome-wide association studies (GWAS). We used previously described GWAS signals (\u0093hits\u0094) to train a regularized logistic model in order to predict SNP causality on the basis of a large multivariate functional dataset. We show how this model can be used to derive Bayes factors for integrating functional and association data into a combined Bayesian analysis. Functional characteristics were obtained from the Encyclopedia of DNA Elements (ENCODE), from published expression quantitative trait loci (eQTL) and from other sources of genome-wide characteristics. We trained the model using all GWAS signals combined, and also using phenotype-specific signals for autoimmune, brain-related, cancer, and cardiovascular disorders. The non-phenotype specific and the autoimmune GWAS signals gave the most reliable results. We found SNPs with higher predicted values showed an enrichment of more significant p-values compared to all GWAS SNPs in three large GWAS studies of complex traits. We investigated the ability of our Bayesian method to improve the identification of true causal signals in psoriasis GWAS data and found that combining functional data with association data improves the ability to prioritise novel hits. We used the predictions from the penalized logistic regression model to calculate Bayes factors relating to functional characteristics and supply these online alongside resources to integrate these data with association data."}, {"title": "PyRAD: assembly of de novo RADseq loci for phylogenetic analyses", "url": "https://www.biorxiv.org/content/early/2013/12/03/001081", "tag": "Bioinformatics", "abstract": "Restriction-site associated genomic markers are a powerful tool for investigating evolutionary questions at the population level, but are limited in their utility at deeper phylogenetic scales where fewer orthologous loci are typically recovered across disparate taxa. While this limitation stems in part from mutations to restriction recognition sites that disrupt data generation, an alternative source of data loss comes from the failure to identify homology during bioinformatic analyses. Clustering methods that allow for lower similarity thresholds and the inclusion of indel variation will perform better at assembling RADseq loci at the phylogenetic scale. PyRAD is a pipeline to assemble de novo RADseq loci with the aim of optimizing coverage across phylogenetic data sets. It utilizes a wrapper around an alignment-clustering algorithm which allows for indel variation within and between samples, as well as for incomplete overlap among reads (e.g., paired-end). Here I compare PyRAD with the program Stacks in their performance analyzing a simulated RADseq data set that includes indel variation. Indels disrupt clustering of homologous loci in Stacks but not in PyRAD, such that the latter recovers more shared loci across disparate taxa. I show through re-analysis of an empirical RADseq data set that indels are a common feature of such data, even at shallow phylogenetic scales. PyRAD utilizes parallel processing as well as an optional hierarchical clustering method which allow it to rapidly assemble phylogenetic data sets with hundreds of sampled individuals."}, {"title": "A null model for Pearson coexpression networks", "url": "https://www.biorxiv.org/content/early/2013/12/03/001065", "tag": "Bioinformatics", "abstract": "Gene coexpression networks inferred by correlation from high throughput profiling such as microarray data represent a simple but effective technique for discovering and interpreting linear gene relationships. In the last years several approach have been proposed to tackle the problem of deciding when the resulting correlation values are statistically significant. This is mostly crucial when the number of samples is small, yielding a non negligible chance that even high correlation values are due to random effects. Here we introduce a novel hard thresholding solution based on the assumption that a coexpression network inferred by randomly generated data is expected to be empty. The theoretical derivation of the new bound by geometrical methods is shown together with applications in onco- and neurogenomics."}, {"title": "Exploring DNA structures in real-time polymerase kinetics using Pacific Biosciences sequencer data", "url": "https://www.biorxiv.org/content/early/2013/12/02/001024", "tag": "Bioinformatics", "abstract": "Pausing of DNA polymerase can indicate the presence of a DNA structure that differs from the canonical double-helix. Here we detail a method to investigate how polymerase pausing in the Pacific Biosciences sequencer reads can be related to DNA structure. The Pacific Biosciences sequencer uses optics to view a polymerase and its interaction with a single DNA molecule in real-time, offering a unique way to detect potential alternative DNA structures. We have developed a new way to examine polymerase kinetics and relate it to the DNA sequence by using a wavelet transform of read information from the sequencer. We use this method to examine how polymerase kinetics are related to nucleotide base composition. We then examine tandem repeat sequences known for their ability to form different DNA structures: (CGG)n and (CG)n repeats which can, respectively, form G-quadruplex DNA and Z-DNA. We find pausing around the (CGG)n repeat that may indicate the presence of G-quadruplexes in some of the sequencer reads. The (CG)n repeat does not appear to cause polymerase pausing, but its kinetics signature nevertheless suggests the possibility that alternative nucleotide conformations may sometimes be present. We discuss the implications of using our method to discover DNA sequences capable of forming alternative structures. The analyses presented here can be reproduced on any Pacific Biosciences kinetics data for any DNA pattern of interest using an R package that we have made publicly available."}, {"title": "Comment on \u201cTopHat2: accurate alignment of transcriptomes in the presence of insertions, deletions and gene fusions\u201d by Kim et al.", "url": "https://www.biorxiv.org/content/early/2013/11/22/000851", "tag": "Bioinformatics", "abstract": "In the recent paper by Kim et al. (Genome biology, 2013. 14(4): p. R36) the accuracy of TopHat2 was compared to other RNA-seq aligners. In this comment we re-examine most important analyses from this paper and identify several deficiencies that significantly diminished performance of some of the aligners, including incorrect choice of mapping parameters, unfair comparison metrics, and unrealistic simulated data. Using STAR (Dobin et al., Bioinformatics, 2013. 29(1): p. 15-21) as an exemplar, we demonstrate that correcting these deficiencies makes its accuracy equal or better than that of TopHat2. Furthermore, this exercise highlighted some serious issues with the TopHat2 algorithms, such as poor recall of alignments with a moderate (>3) number of mismatches, low sensitivity and high false discovery rate for splice junction detection, loss of precision for the realignment algorithm, and large number of false chimeric alignments."}, {"title": "Unexpected links reflect the noise in networks", "url": "https://www.biorxiv.org/content/early/2013/11/15/000497", "tag": "Bioinformatics", "abstract": "Gene regulatory networks are commonly used for modeling biological processes and revealing underlying molecular mechanisms. The reconstruction of gene regulatory networks from observational data is a challenging task, especially, considering the large number of involved players (e.g. genes) and much fewer biological replicates available for analysis. Herein, we proposed a new statistical method of estimating the number of erroneous edges that strongly enhances the commonly used inference approaches. This method is based on special relationship between correlation and causality, and allows to identify and to remove approximately half of erroneous edges. Using the mathematical model of Bayesian networks and positive correlation inequalities we established a mathematical foundation for our method. Analyzing real biological datasets, we found a strong correlation between the results of our method and the commonly used false discovery rate (FDR) technique. Furthermore, the simulation analysis demonstrates that in large networks, our new method provides a more precise estimation of the proportion of erroneous links than FDR."}, {"title": "Gappy TotalReCaller for RNASeq Base-Calling and Mapping", "url": "https://www.biorxiv.org/content/early/2013/11/15/000489", "tag": "Bioinformatics", "abstract": "Understanding complex mammalian biology depends crucially on our ability to define a precise map of all the transcripts encoded in a genome, and to measure their relative abundances. A promising assay depends on RNASeq approaches, which builds on next generation sequencing pipelines capable of interrogating cDNAs extracted from a cell. The underlying pipeline starts with base-calling, collect the sequence reads and interpret the raw-read in terms of transcripts that are grouped with respect to different splice-variant isoforms of a messenger RNA. We address a very basic problem involved in all of these pipeline, namely accurate Bayesian base-calling, which could combine the analog intensity data with suitable underlying priors on base-composition in the transcripts. In the context of sequencing genomic DNA, a powerful approach for base-calling has been developed in the TotalReCaller pipeline. For these purposes, it uses a suitable reference whole-genome sequence in a compressed self-indexed format to derive its priors. However, TotalReCaller faces many new challenges in the transcriptomic domain, especially since we still lack a fully annotated library of all possible transcripts, and hence a sufficiently good prior. There are many possible solutions, similar to the ones developed for TotalReCaller, in applications addressing de novo sequencing and assembly, where partial contigs or string-graphs could be used to boot-strap the Bayesian priors on base-composition. A similar approach would be applicable here too, partial assembly of transcripts can be used to characterize the splicing junctions or organize them in incom- patibility graphs and then used as priors for TotalReCaller. The key algorithmic techniques for this purpose have been addressed in a forthcoming paper on Stringomics. Here, we address a related but fundamental problem, by assuming that we only have a reference genome, with certain intervals marked as candidate regions for ORF (Open Reading Frames), but not necessarily complete annotations regarding the 5\u0092 or 3\u0092 termini of a gene or its exon-intron structure. The algorithms we describe find the most accurate base-calls of a cDNA with the best possible segmentation, all mapped to the genome appropriately."}, {"title": "Functional Annotation Signatures of Disease Susceptibility Loci Improve SNP Association Analysis", "url": "https://www.biorxiv.org/content/early/2013/11/07/000158", "tag": "Bioinformatics", "abstract": "We describe the development and application of a Bayesian statistical model for the prior probability of phenotype-genotype association that incorporates data from past association studies and publicly available functional annotation data regarding the susceptibility variants under study. The model takes the form of a binary regression of association status on a set of annotation variables whose coefficients were estimated through an analysis of associated SNPs housed in the GWAS Catalog (GC). The set of functional predictors we examined includes measures that have been demonstrated to correlate with the association status of SNPs in the GC and some whose utility in this regard is speculative: summaries of the UCSC Human Genome Browser ENCODE super-track data, dbSNP function class, sequence conservation summaries, proximity to genomic variants included in the Database of Genomic Variants (DGV) and known regulatory elements included in the Open Regulatory Annotation database (ORegAnno), PolyPhen-2 probabilities and RegulomeDB categories. Because we expected that only a fraction of the annotation variables would contribute to predicting association, we employed a penalized likelihood method to reduce the impact of non-informative predictors and evaluated the model's ability to predict GC SNPs not used to construct the model. We show that the functional data alone are predictive of a SNP's presence in the GC. Further, using data from a genome-wide study of ovarian cancer, we demonstrate that their use as prior data when testing for association is practical at the genome-wide scale and improves power to detect associations."}]