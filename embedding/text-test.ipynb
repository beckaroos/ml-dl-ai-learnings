{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import text_utils as tu\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_examples(path):\n",
    "    raw_reports = np.load(path)\n",
    "    dirty_reports = [report['body'] for report in raw_reports]\n",
    "    clean_reports, _ = tu.clean_report(dirty_reports, clean=1) # first pass removes \\n's and weird characters\n",
    "    tokenised_reports, report_vocab = tu.clean_report(clean_reports, clean=2) # second pass tokenises and builds vocab\n",
    "    vocab, embeddings = tu.load_glove('/home/rohanmirchandani/glove/glove.6B.50d.w2vformat.txt', report_vocab, 50)\n",
    "    vocab['<SOS>'] = embeddings.shape[0]\n",
    "    embeddings = np.vstack((embeddings, np.zeros((1, 50))))\n",
    "    vocab['<EOS>'] = embeddings.shape[0]\n",
    "    embeddings = np.vstack((embeddings, np.ones((1, 50))))\n",
    "    vocab['<UNK>'] = embeddings.shape[0]\n",
    "    embeddings = np.vstack((embeddings, -np.ones((1, 50))))\n",
    "    for i, tokens in enumerate(tokenised_reports): # should multithread this at some point\n",
    "        tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "        length = len(tokens)\n",
    "        if length > 300 or length < 10:\n",
    "            continue\n",
    "        vecs = np.array([[vocab[token] if token in vocab.keys() else vocab['<UNK>'] for token in tokens]]).transpose()\n",
    "        print(vecs.shape)\n",
    "        padding_size = 300 - vecs.shape[0]\n",
    "        padding = np.zeros((padding_size, 1))\n",
    "        vecs = np.vstack((vecs, padding))\n",
    "        data = data = {'tokens': tokens, \"vectors\": vecs}\n",
    "        name = \"example_{}\".format(i)\n",
    "        np.save(os.path.join('/home/rohanmirchandani/maxwell-pt-test/examples/', name), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report_examples(path='/home/rohanmirchandani/maxwell-pt-test/points.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EncoderGRU, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRU(self.input_dim, self.hidden_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        result = Variable(torch.zeros(1, bs, self.hidden_dim))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim, hidden_dim):\n",
    "        super(DecoderGRU, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRU(self.hidden_dim, self.hidden_dim)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        result = Variable(torch.zeros(1, bs, self.hidden_dim))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = tu.create_dataloader('/home/rohanmirchandani/maxwell-pt-test/examples/', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "torch.Size([1, 35, 50])\n"
     ]
    }
   ],
   "source": [
    "tokens, vectors = next(iterator)\n",
    "print(len(tokens))\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = EncoderGRU(50, 50)\n",
    "D = DecoderGRU(50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_hidden = E.init_hidden(bs=vectors.shape[1])\n",
    "d_hidden = D.init_hidden(bs=vectors.shape[1])\n",
    "inputs = Variable(vectors.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, z_hidden = E(inputs, e_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, final_hidden = D(z_hidden, d_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.8827 -3.9737 -3.8523  ...  -4.0064 -3.8536 -3.9493\n",
       "-3.8884 -3.9655 -3.8116  ...  -3.9746 -3.8851 -3.9170\n",
       "-3.8900 -3.9792 -3.8736  ...  -3.9952 -3.8475 -3.9095\n",
       "          ...             â‹±             ...          \n",
       "-3.9041 -3.9987 -3.8122  ...  -4.0027 -3.8208 -3.9384\n",
       "-3.8993 -3.9575 -3.8215  ...  -3.9901 -3.8706 -3.9202\n",
       "-3.9041 -3.9987 -3.8122  ...  -4.0027 -3.8208 -3.9384\n",
       "[torch.FloatTensor of size 35x50]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "E = nn.DataParallel(EncoderGRU(50, 50).cuda())\n",
    "D = nn.DataParallel(DecoderGRU(50, 50).cuda())\n",
    "\n",
    "optm = optim.Adam(list(E.parameters()) + list(D.parameters()))\n",
    "\n",
    "dataloader = tu.create_dataloader('/home/rohanmirchandani/maxwell-pt-test/examples/', batch_size=1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for tokens, vectors in dataloader:\n",
    "        \n",
    "        e_hidden = Variable(torch.zeros(1, vectors.shape[1], embedding_dim)).cuda()\n",
    "        d_hidden = Variable(torch.zeros(1, vectors.shape[1], embedding_dim)).cuda()\n",
    "        \n",
    "        optm.zero_grad()\n",
    "        \n",
    "        inputs = Variable(vectors.float()).cuda()\n",
    "        z_output, e_hidden = E(inputs, e_hidden)\n",
    "        outputs, d_hidden = D(e_hidden, d_hidden)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optm.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
